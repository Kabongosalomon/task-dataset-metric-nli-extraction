<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Inverse Reinforcement Learning with Probabilistic Context Variables</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
							<email>lantaoyu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
							<email>tianheyu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
							<email>cbfinn@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Inverse Reinforcement Learning with Probabilistic Context Variables</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Providing a suitable reward function to reinforcement learning can be difficult in many real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, existing methods typically assume homogeneous demonstrations for a single behavior or task, while in practice, it might be easier to collect datasets of heterogeneous but related behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from demonstrations of distinct but related tasks in an unsupervised way. Critically, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While reinforcement learning (RL) has been successfully applied to a range of decision-making and control tasks in the real world, it relies on a key assumption: having access to a well-defined reward function that measures progress towards the completion of the task. Although it can be straightforward to provide a high-level description of success conditions for a task, existing RL algorithms usually require a more informative signal to expedite exploration and learn complex behaviors in a reasonable time. While reward functions can be hand-specified, reward engineering can require significant human effort. Moreover, for many real-world tasks, it can be challenging to manually design reward functions that actually benefit RL training, and reward mis-specification can hamper autonomous learning <ref type="bibr" target="#b1">[2]</ref>.</p><p>Learning from demonstrations <ref type="bibr" target="#b30">[31]</ref> sidesteps the reward specification problem by instead learning directly from expert demonstrations, which can be obtained through teleoperation <ref type="bibr" target="#b38">[39]</ref> or from humans experts <ref type="bibr" target="#b37">[38]</ref>. Demonstrations can often be easier to provide than rewards, as humans can complete many real-world tasks quite efficiently. Two major methodologies of learning from demonstrations include imitation learning and inverse reinforcement learning. Imitation learning is simple and often exhibits good performance <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16]</ref>. However, it lacks the ability to transfer learned policies to new settings where the task specification remains the same but the underlying environment dynamics change. As the reward function is often considered as the most succinct, robust and transferable representation of a task <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>, the problem of inferring reward functions from expert demonstrations, i.e. inverse RL (IRL) <ref type="bibr" target="#b22">[23]</ref>, is important to consider.</p><p>While appealing, IRL still typically relies on large amounts of high-quality expert data, and it can be prohibitively expensive to collect demonstrations that cover all kinds of variations in the wild (e.g. opening all kinds of doors or navigating to all possible target positions). As a result, these methods are data-inefficient, particularly when learning rewards for individual tasks in isolation, starting from scratch. On the other hand, meta-learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b3">4]</ref>, also known as learning to learn, seeks to exploit the structural similarity among a distribution of tasks and optimizes for rapid adaptation to unknown settings with a limited amount of data. As the reward function is able to succinctly capture the structure of a reinforcement learning task, e.g. the goal to achieve, it is promising to develop methods that can quickly infer the structure of a new task, i.e. its reward, and train a policy to adapt to it. Xu et al. <ref type="bibr" target="#b35">[36]</ref> and Gleave and Habryka <ref type="bibr" target="#b11">[12]</ref> have proposed approaches that combine IRL and gradient-based meta-learning <ref type="bibr" target="#b8">[9]</ref>, which provide promising results on deriving generalizable reward functions. However, they have been limited to tabular MDPs <ref type="bibr" target="#b35">[36]</ref> or settings with provided task distributions <ref type="bibr" target="#b11">[12]</ref>, which are challenging to gather in real-world applications.</p><p>The primary contribution of this paper is a new framework, termed Probabilistic Embeddings for Meta-Inverse Reinforcement Learning (PEMIRL), which enables meta-learning of rewards from unstructured multi-task demonstrations. In particular, PEMIRL combines and integrates ideas from context-based meta-learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, deep latent variable generative models <ref type="bibr" target="#b16">[17]</ref>, and maximum entropy inverse RL <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref>, into a unified graphical model (see <ref type="figure" target="#fig_5">Figure 4</ref> in Appendix D) that bridges the gap between few-shot reward inference and learning from unstructured, heterogeneous demonstrations. PEMIRL can learn robust reward functions that generalize to new tasks with a single demonstration on complex domains with continuous state-action spaces, while meta-training on a set of unstructured demonstrations without specified task groupings or labeling for each demonstration. Our experiment results on various continuous control tasks including Point-Maze, Ant, Sweeper, and Sawyer Pusher demonstrate the effectiveness and scalability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Markov Decision Process (MDP). A discrete-time finite-horizon MDP is defined by a tuple (T, S, A, P, r, ?), where T is the time horizon; S is the state space; A is the action space; P : S ? A ? S ? [0, 1] describes the (stochastic) transition process between states; r : S ? A ? R is a bounded reward function; ? ? P(S) specifies the initial state distribution, where P(S) denotes the set of probability distributions over the state space S. We use ? to denote a trajectory, i.e. a sequence of state action pairs for one episode. We also use ? ? (s t ) and ? ? (s t , a t ) to denote the state and state-action marginal distribution encountered when executing a policy ?(a t |s t ).</p><p>Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL). The maximum entropy reinforcement learning (MaxEnt RL) objective is defined as:</p><formula xml:id="formula_0">max ? T t=1 E (st,at)??? [r(s t , a t ) + ?H(?(?|s t ))]<label>(1)</label></formula><p>which augments the reward function with a causal entropy regularization term H(?) = E ? [? log ?(a|s)]. Here ? is an optional parameter to control the relative importance of reward and entropy. For notational simplicity, without loss of generality, in the following we will assume ? = 1. Given some expert policy ? E that is obtained by above MaxEnt RL procedure , the MaxEnt IRL framework <ref type="bibr" target="#b41">[42]</ref> aims to find a reward function that rationalizes the expert behaviors, which can be interpreted as solving the following maximum likelihood estimation (MLE) problem:</p><formula xml:id="formula_1">p ? (? ) ? ?(s 1 ) T t=1 P (s t+1 |s t , a t ) exp T t=1 r ? (s t , a t ) = p ? (? )<label>(2)</label></formula><p>arg min</p><formula xml:id="formula_2">? D KL (p ? E (? ) ||p ? (? )) = arg max ? E p? E (? ) [log p ? (? )] = E ? ?? E T t=1 r ? (s t , a t ) ? log Z ?</formula><p>Here, ? is the parameter of the reward function and Z ? is the partition function, i.e. p ? (? )d? , an integral over all possible trajectories consistent with the environment dynamics. Z ? is intractable to compute when state-action spaces are large or continuous, or environment dynamics are unknown.</p><p>Finn et al. <ref type="bibr" target="#b6">[7]</ref> and Fu et al. <ref type="bibr" target="#b10">[11]</ref> proposed the adversarial IRL (AIRL) framework as an efficient sampling-based approximation to MaxEnt IRL, which resembles Generative Adversarial Networks <ref type="bibr" target="#b12">[13]</ref>. Specially, in AIRL, there is a discriminator D ? (a binary classifier) parametrized by ? and an adaptive sampler ? ? (a policy) parametrized by ?. The discriminator takes a particular form: D ? (s, a) = exp(f ? (s, a))/(exp(f ? (s, a)) + ? ? (a|s)) , where f ? (s, a) is the learned reward function and ? ? (a|s) is pre-computed as an input to the discriminator. The discriminator is trained to distinguish between the trajectories sampled from the expert and the adaptive sampler; while the adaptive sampler ? ? (a|s) is trained to maximize E ?? ? [log D ? (s, a) ? log(1 ? D ? (s, a))], which is equivalent to maximizing the following entropy regularized policy objective (with f ? (s, a) serving as the reward function):</p><formula xml:id="formula_3">E ?? T t=1 log(D ? (s t , a t )) ? log(1 ? D ? (s t , a t )) = E ?? T t=1 f ? (s t , a t ) ? log ? ? (a t |s t )<label>(3)</label></formula><p>Under certain conditions, it can be shown that the learned reward function will recover the groundtruth reward up to a constant (Theorem C.1 in <ref type="bibr" target="#b10">[11]</ref>).</p><p>3 Probabilistic Embeddings for Meta-Inverse Reinforcement Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Before defining our meta-inverse reinforcement learning problem (Meta-IRL), we first define the concept of optimal context-conditional policy.</p><p>We start by generalizing the notion of MDP with a probabilistic context variable denoted as m ? M, where M is the (discrete or continuous) value space of m. For example, in a navigation task, the context variables could represent different goal positions in the environment. Now, each component of the MDP has an additional dependency on the context variable m. For example, by slightly overloading the notation, the reward function is now defined as r : S ? A ? M ? R. For simplicity, the state space, action space, initial state distribution and transition dynamics are often assumed to be independent of m <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, which we will follow in this work. Intuitively, different m's correspond to different tasks with shared structures.</p><p>Given above definitions, the context-conditional trajectory distribution induced by a contextconditional policy ? : S ? M ? P(A) can be written as:</p><formula xml:id="formula_4">p ? (? = {s 1:T , a 1:T }|m) = ?(s 1 ) T t=1 ?(a t |s t , m)P (s t+1 |s t , a t )<label>(4)</label></formula><p>Let p(m) denote the prior distribution of the latent context variable (which is a part of the problem definition). With the conditional distribution defined above, the optimal entropy-regularized contextconditional policy is defined as:</p><formula xml:id="formula_5">? * = arg max ? E m?p(m), (s 1:T ,a 1:T )?p?(?|m) T t=1 r(s t , a t , m) ? log ?(a t |s t , m)<label>(5)</label></formula><p>Now, let us introduce the problem of Meta-IRL from heterogeneous multi-task demonstration data. Suppose there is some ground-truth reward function r(s, a, m) and a corresponding expert policy ? E (a t |s t , m) obtained by solving the optimization problem defined in Equation <ref type="bibr" target="#b4">(5)</ref>. Given a set of demonstrations i.i.d. sampled from the induced marginal distribution p ? E (? ) = M p(m)p ? E (? |m)dm, the goal is to meta-learn an inference model q(m|? ) and a reward function f (s, a, m), such that given some new demonstration ? E generated by sampling m ? p(m), ? E ? p ? E (? |m ), withm being inferred asm ? q(m|? E ), the learned reward function f (s, a,m) and the ground-truth reward r(s, a, m ) will induce the same set of optimal policies <ref type="bibr" target="#b23">[24]</ref>.</p><p>Critically, we assume no knowledge of the prior task distribution p(m), the latent context variable m associated with each demonstration, nor the transition dynamics P (s t+1 |s t , a t ) during meta-training. Note that the entire supervision comes from the provided unstructured demonstrations, which means we also do not assume further interactions with the experts as in Ross et al. <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Meta-IRL with Mutual Information Regularization over Context Variables</head><p>Under the framework of MaxEnt IRL, we first parametrize the context variable inference model q ? (m|? ) and the reward function f ? (s, a, m) (where the input m is inferred by q ? ), The induced ?-parametrized trajectory distribution is given by:</p><formula xml:id="formula_6">p ? (? = {s 1:T , a 1:T }|m) = 1 Z(?) ?(s 1 ) T t=1 P (s t+1 |s t , a t ) exp T t=1 f ? (s t , a t , m)<label>(6)</label></formula><p>where Z(?) is the partition function, i.e., an integral over all possible trajectories. Without further constraints over m, directly applying AIRL to learning the reward function (by augmenting each component of AIRL with an additional context variable m inferred by q ? ) could simply ignore m, which is similar to the case of InfoGAIL <ref type="bibr" target="#b20">[21]</ref>. Therefore, some connection between the reward function and the latent context variable m need to be established. With MaxEnt IRL, a parametrized reward function will induce a trajectory distribution. From the perspective of information theory, the mutual information between the context variable m and the trajectories sampled from the reward induced distribution will provide an ideal measure for such a connection.</p><p>Formally, the mutual information between two random variables m and ? under joint distribution p ? (m, ? ) = p(m)p ? (? |m) is given by:</p><formula xml:id="formula_7">I p ? (m; ? ) = E m?p(m),? ?p ? (? |m) [log p ? (m|? ) ? log p(m)]<label>(7)</label></formula><p>where p ? (? |m) is the conditional distribution (Equation <ref type="formula" target="#formula_6">(6)</ref>), and p ? (m|? ) is the corresponding posterior distribution.</p><p>As we do not have access to the prior distribution p(m) and posterior distribution p ? (m|? ), directly optimizing the mutual information in Equation <ref type="formula" target="#formula_7">(7)</ref> is intractable. Fortunately, we can leverage q ? (m|? ) as a variational approximation to p ? (m|? ) to reason about the uncertainty over tasks, as well as conduct approximate sampling from p(m) (we will elaborate this later in Section 3.3). Formally, let p ? E (? ) denote the expert trajectory distribution, we have the following desiderata:</p><formula xml:id="formula_8">Desideratum 1. Matching conditional distributions: E p(m) [D KL (p ? E (? |m)||p ? (? |m))] = 0 Desideratum 2. Matching posterior distributions: E p ? (? ) [D KL (p ? (m|? )||q ? (m|? ))] = 0</formula><p>The first desideratum will encourage the ?-induced conditional trajectory distribution to match the empirical distribution implicitly defined by the expert demonstrations, which is equivalent to the MLE objective in the MaxEnt IRL framework. Note that they also share the same marginal distribution over the context variable p(m), which implies that matching the conditionals in Desideratum 1 will also encourage the joint distributions, conditional distributions p ? E (m|? ) and p ? (m|? ), and marginal distributions over ? to be matched. The second desideratum will encourage the variational posterior q ? (m|? ) to be a good approximation to p ? (m|? ) such that q ? (m|? ) can correctly infer the latent context variable given a new expert demonstration sampled from a new task.</p><p>With the mutual information (Equation <ref type="formula" target="#formula_7">(7)</ref>) being the objective, and Desideratum 1 and 2 being the constraints, the meta-inverse reinforcement learning with probabilistic context variables problem can be interpreted as a constrained optimization problem, whose Lagrangian dual function is given by:</p><formula xml:id="formula_9">min ?,? ?I p ? (m; ? ) + ? ? E p(m) [D KL (p ? E (? |m)||p ? (? |m))] + ? ? E p ? (? ) [D KL (p ? (m|? )||q ? (m|? ))]<label>(8)</label></formula><p>With the Lagrangian multipliers taking specific values (? = 1, ? = 1) <ref type="bibr" target="#b39">[40]</ref>, the above Lagrangian dual function can be rewritten as:</p><formula xml:id="formula_10">min ?,? E p(m) [D KL (p ? E (? |m)||p ? (? |m))] + E p ? (m,? ) log p(m) p ? (m|? ) + log p ? (m|? ) q ? (m|? ) ? max ?,? ?E p(m) [D KL (p ? E (? |m)||p ? (? |m))] + E m?p(m),? ?p ? (? |m) [log q ? (m|? )]<label>(9)</label></formula><p>= max</p><formula xml:id="formula_11">?,? ?E p(m) [D KL (p ? E (? |m)||p ? (? |m))] + L info (?, ?)<label>(10)</label></formula><p>Here the negative entropy <ref type="formula" target="#formula_10">(9)</ref>) as it can be treated as a constant in the optimization procedure of parameters ? and ?.</p><formula xml:id="formula_12">term ?H p (m) = E p ? (m,? ) [log p(m)] = E p(m) [log p(m)] is omitted (in Eq.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Achieving Tractability with Sampling-Based Gradient Estimation</head><p>Note that Equation <ref type="formula" target="#formula_0">(10)</ref> cannot be evaluated directly, as the first term requires estimating the KL divergence between the empirical expert distribution and the energy-based trajectory distribution p ? (? |m) (induced by the ?-parametrized reward function), and the second term requires sampling from it. For the purpose of optimizing the first term in Equation <ref type="formula" target="#formula_0">(10)</ref>, as introduced in Section 2, we can employ the adversarial reward learning framework <ref type="bibr" target="#b10">[11]</ref> to construct an efficient sampling-based approximation to the maximum likelihood objective. Note that different from the original AIRL framework, now the adaptive sampler ? ? (a|s, m) is additionally conditioned on the context variable m. Furthermore, we here introduce the following lemma, which will be helpful for deriving the optimization of the second term in Equation <ref type="formula" target="#formula_0">(10)</ref>. Lemma 1. In context variable augmented Adversarial IRL (with the adaptive sampler being ? ? (a|s, m) and the discriminator being D ? (s, a, m) = exp(f ? (s,a,m)) exp(f ? (s,a,m))+??(a|s,m) ) , under deterministic dynamics, when training the adaptive sampler ? ? with reward signal (log D ? ? log(1 ? D ? )) to optimality, the trajectory distribution induced by ? * ? corresponds to the maximum entropy trajectory distribution with f ? (s, a, m) serving as the reward function:</p><formula xml:id="formula_13">p ? * ? (? |m) = 1 Z ? ?(s 1 ) T t=1 P (s t+1 |s t , a t ) exp T t=1 f ? (s t , a t , m) = p ? (? |m) Proof. See Appendix A.</formula><p>Now we are ready to introduce how to approximately optimize the second term of the objective in Equation <ref type="formula" target="#formula_0">(10)</ref> w.r.t. ? and ?. First, we observe that the gradient of L info (?, ?) w.r.t. ? is given by:</p><formula xml:id="formula_14">? ?? L info (?, ?) = E m?p(m),? ?p ? (? |m) 1 q(m|?, ?) ?q(m|?, ?) ??<label>(11)</label></formula><p>Thus to construct an estimate of the gradient in Equation <ref type="formula" target="#formula_0">(11)</ref>, we need to obtain samples from the ?-induced trajectory distribution p ? (? |m). With Lemma 1, we know that when the adaptive sampler ? ? in AIRL is trained to optimality, we can use ? * ? to construct samples, as the trajectory distribution p ? * ? (? |m) matches the desired distribution p ? (? |m). Also note that the expectation in Equation <ref type="formula" target="#formula_0">(11)</ref> is also taken over the prior task distribution p(m). In cases where we have access to the ground-truth prior distribution, we can directly sample m from it and use p ? * ? (? |m) to construct a gradient estimation. For the most general case, where we do not have access to p(m) but instead have expert demonstrations sampled from p ? E (? ), we use the following generative process:</p><formula xml:id="formula_15">? ? p ? E (? ) , m ? q ? (m|? )<label>(12)</label></formula><p>to synthesize latent context variables, which approximates the prior task distribution when ? and ? are trained to optimality.</p><p>To optimize L info (?, ?) w.r.t. ?, which is an important step of updating the reward function parameters such that it encodes the information of the latent context variable, different from the optimization of Equation <ref type="formula" target="#formula_0">(11)</ref>, we cannot directly replace p ? (? |m) with p ?? (? |m). The reason is that we can only use the approximation of p ? to do inference (i.e. computing the value of an expectation). When we want to optimize an expectation (L info (?, ?)) w.r.t. ? and the expectation is taken over p ? itself, we cannot instead replace p ? with ? ? to do the sampling for estimating the expectation. In the following, we discuss how to estimate the gradient of L info (?, ?) w.r.t. ? with empirical samples from ? ? . Lemma 2. The gradient of L info (?, ?) w.r.t. ? can be estimated with:</p><formula xml:id="formula_16">E m?p(m),? ?p ? * ? (? |m) log q ? (m|? ) T t=1 ? ?? f ? (s t , a t , m) ? E ? ?p ? * ? (? |m) T t=1 ? ?? f ? (s t , a t ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>m)</head><p>When ? is trained to optimality, the estimation is unbiased.</p><p>Proof. See Appendix B.</p><p>With Lemma 2, as before, we can use the generative process in Equation <ref type="formula" target="#formula_0">(12)</ref> to sample m and use the conditional trajectory distribution p ? * ? (? |m) to sample trajectories for estimating ? ?? L info (?, ?). The overall training objective of PEMIRL is:</p><formula xml:id="formula_17">min ? max ?,? E ? E ?p? E (? ),m?q ? (m|? E ),(s,a)??? ? (s,a|m) log(1 ? D ? (s, a, m))+ E ? E ?p? E (? ),m?q ? (m|? E ) log(D ? (s, a, m)) + L info (?, ?)<label>(13)</label></formula><p>where D ? (s, a, m) = exp(f ? (s, a, m))/(exp(f ? (s, a, m)) + ? ? (a|s, m))</p><p>We summarize the meta-training procedure in Algorithm 1 and the meta-test procedure in Appendix C. Update ? to increase Linfo(?, ?) with gradients in Equation <ref type="formula" target="#formula_0">(11)</ref>, with samples from D.</p><p>Update ? to increase Linfo(?, ?) with gradients in Equation <ref type="formula" target="#formula_0">(15)</ref>, with samples from D.</p><p>Update ? to decrease the binary classification loss: </p><formula xml:id="formula_18">E (s,a,m)?D [? ? log D ? (s, a, m)] + E ? E ?D E ,m?q ? (m|? E ) [? ? log(1 ? D ? (s,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Inverse reinforcement learning (IRL), first introduced by Ng and Russell <ref type="bibr" target="#b22">[23]</ref>, is the problem of learning reward functions directly from expert demonstrations. Prior work tackling IRL include margin-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref> and maximum entropy (MaxEnt) methods <ref type="bibr" target="#b41">[42]</ref>. Margin-based methods suffer from being an underdefined problem, while MaxEnt requires the algorithm to solve the forward RL problem in the inner loop, making it challenging to use in non-tabular settings. Recent works have scaled MaxEnt IRL to large function approximators, such as neural networks, by only partially solving the forward problem in the inner loop, developing an adversarial framework for IRL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25]</ref>. Other imitation learning approaches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> are also based on the adversarial framework, but they do not recover a reward function. We build upon the ideas in these single-task IRL works. Instead of considering the problem of learning reward functions for a single task, we aim at the problem of inferring a reward that is disentangled from the environment dynamics and can quickly adapt to new tasks from a single demonstration by leveraging prior data.</p><p>We base our work on the problem of meta-learning. Prior work has proposed memory-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref> and methods that learn an optimizer and/or a parameter initialization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref>. We adopt a memory-based meta-learning method similar to <ref type="bibr" target="#b25">[26]</ref>, which uses a deep latent variable generative model <ref type="bibr" target="#b16">[17]</ref> to infer different tasks from demonstrations. While prior multi-task and meta-RL methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> have investigated the effectiveness of applying latent variable generative models to learning task embeddings, we focus on the IRL problem instead. Meta-IRL <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref> incorporates meta-learning and IRL, showing fast adaptation of the reward functions to unseen tasks. Unlike these approaches, our method is not restrictred to discrete tabular settings and does not require access to grouped demonstrations sampled from a task distribution. Meanwhile, one-shot imitation learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37]</ref> demonstrates impressive results on learning new tasks using a single demonstration; yet, they also require paired demonstrations from each task and hence need prior knowledge on the task distribution. More importantly, one-shot imitation learning approaches only recover a policy, and cannot use additional trials to continue to improve, which is possible when a reward function is inferred instead. Several prior approaches for multi-task imitation learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref> propose to use unstructured demonstrations without knowing the task distribution, but they neither study quick generalization to new tasks nor provide a reward function. Our work is thus driven by the goal of extending meta-IRL to addressing challenging high-dimensional control tasks with the help of an unstructured demonstration dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we seek to investigate the following two questions: (1) Can PEMIRL learn a policy with competitive few-shot generalization abilities compared to one-shot imitation learning methods using only unstructured demonstrations? (2) Can PEMIRL efficiently infer robust reward functions of new continuous control tasks where one-shot imitation learning fails to generalize, enabling an agent to continue to improve with more trials?</p><p>We evaluate our method on four simulated domains using the Mujoco physics engine <ref type="bibr" target="#b34">[35]</ref>. To our knowledge, there's no prior work on designing meta-IRL or one-shot imitation learning methods for complex domains with high-dimensional continuous state-action spaces with unstructured demonstrations. Hence, we also designed the following variants of existing state-of-the-art (one-shot) imitation learning and IRL methods so that they can be used as fair comparisons to our method:</p><p>? AIRL: The original AIRL algorithm without incorporating latent context variables, trained across all demonstrations.</p><p>? Meta-Imitation Learning with Latent Context Variables (Meta-IL): As in <ref type="bibr" target="#b25">[26]</ref>, we use the inference model q ? (m|? ) to infer the context of a new task from a single demonstrated trajectory, denoted asm, and then train the conditional imitaiton policy ? ? (a|s,m) using the same demonstration. This approach also resembles <ref type="bibr" target="#b5">[6]</ref>.</p><p>? Meta-InfoGAIL: Similar to the method above, except that an additional discriminator D(s, a) is introduced to distinguish between expert and sample trajectories, and trained along with the conditional policy using InfoGAIL <ref type="bibr" target="#b20">[21]</ref> objective.</p><p>We use trust region policy optimization (TRPO) <ref type="bibr" target="#b32">[33]</ref> as our policy optimization algorithm across all methods. We collect demonstrations by training experts with TRPO using ground truth reward. However, the ground truth reward is not available to imitation learning and IRL algorithms. We provide full hyperparameters, architecture information, data efficiency, and experimental setup details in Appendix F. We also include ablation studies on sensitivity of the latent dimensions, importance of the mutual information objective and the performance on stochastic environments in Appendix E. Full video results are on the anonymous supplementary website 2 and our code is open-sourced on GitHub 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Policy Performance on Test Tasks</head><p>We first answer our first question by showing that our method is able to learn a policy that can adapt to test tasks from a single demonstration, on four continuous control domains: Point Maze Navigation:</p><p>In this domain, a pointmass needs to navigate around a barrier to reach the goal. Different tasks correspond to different goal positions and the reward function measures the distance between the pointmass and the goal position; Ant: Similar to <ref type="bibr" target="#b8">[9]</ref>, this locomotion task requires fast adaptation to walking directions of the ant where the ant needs to learn to move backward or forward depending on the demonstration; Sweeper: A robot arm needs to sweep an object to a particular goal position. Fast adaptation of this domain corresponds to different goal locations in the plane; Sawyer Pusher: A simulated Sawyer robot is required to push a mug to a variety of goal positions and generalize to unseen goals. We illustrate the set-up for these experimental domains in <ref type="figure" target="#fig_0">Figure 1</ref>.    We summarize the results in <ref type="table" target="#tab_1">Table 1</ref>. PEMIRL achieves comparable imitation performance compared to Meta-IL and Meta-InfoGAIL, while AIRL is incapable of handling multi-task scenarios without incorporating the latent context variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reward Adaptation to Challenging Situations</head><p>After demonstrating that the policy learned by our method is able to achieve competitive "one-shot" generalization ability, we now answer the second question by showing PEMIRL learns a robust reward that can adapt to new and more challenging settings where the imitation learning methods and the original AIRL fail. Specifically, after providing the demonstration of an unseen task to the agent, we change the underlying environment dynamics but keep the same task goal. In order to succeed in the task with new dynamics, the agent must correctly infer the underlying goal of the task instead of simply mimicking the demonstration. We show the effectiveness of our reward generalization by training a new policy with TRPO using the learned reward functions on the new task.  Point-Maze Navigation with a Shifted Barrier. Following the setup of Fu et al. <ref type="bibr" target="#b10">[11]</ref>, at meta-test time, after showing a demonstration moving towards a new target position, we change the position of the barrier from left to right. As the agent must adapt by reaching the target with a different path from what was demonstrated during meta-training, it cannot succeed without correctly inferring the true goal (the target position in the maze) and learning from trial-and-error. As a result, all direct policy generalization approaches fail as all the policies are still directing the pointmass to the right side of the maze. As shown in <ref type="figure" target="#fig_3">Figure 2</ref>, PEMIRL learns disentangled reward functions that successfully infer the underlying goal of the new task without much reward shaping. Such reward functions enable the RL agent to bypass the right barrier and reach the true goal position. The RL agent trained with the reward learned by AIRL also fail to bypass the barrier and navigate to the target position, as without incorporating the latent context variables and treating the demonstration as multi-modal, AIRL learns an "average" reward and policy among different tasks. We also use the output of the discriminator of Meta-InfoGAIL as reward signals and evaluate its adaptation performance. The agent trained by this reward fails to complete the task since Meta-InfoGAIL does not explicitly optimize for reward learning and the discriminator output converges to uninformative uniform distribution at convergence.</p><p>Disabled Ant Walking. As in Fu et al. <ref type="bibr" target="#b10">[11]</ref>, we disable and shorten two front legs of the ant such that it cannot walk without changing its gait to a large extent. Similar to Point-Maze-Shift, all imitaiton policies fail to maneuver the disabled ant to the right direction. As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, reward functions learned by PEMIRL encourage the RL policy to orient the ant towards the demonstrated direction and move along that direction using two healthy legs, which is only possible when the inferred reward corresponds to the true underlying goal and is disentangled with the dynamics. In contrast, the learned reward of original AIRL as well as the discriminator output of Meta-InfoGAIL cannot infer the underlying goal of the task and provide precise supervision signal, which leads to the unsatisfactory performance of the induced RL policies. Quantitative results are presented in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a new meta-inverse reinforcement learning algorithm, PEMIRL, which is able to efficiently infer robust reward functions that are disentangled from the dynamics and highly correlated with the ground-truth rewards under meta-learning settings. To our knowledge, PEMIRL is the first model-free Meta-IRL algorithm that can achieve this and scale to complex domains with continuous state-action spaces. PEMIRL generalizes to new tasks by performing inference over a latent context variable with a single demonstration, on which the recovered policy and reward function are conditioned. Extensive experimental results demonstrate the scalability and effectiveness of our method against strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Lemma 1</head><p>From Section 2.3 and 2.4 in <ref type="bibr" target="#b18">[19]</ref>, we know that the policy whose induced trajectory distribution is Equation (6) takes the following energy-based form: <ref type="figure">(s t , a , m)</ref>)da which corresponds to the optimal policy to the following entropy regularized reinforcement learning problem (for a certain value of m):</p><formula xml:id="formula_19">? ? (a t |s t , m) = exp(Q soft (s t , a t , m) ? V soft (s t , m)) Q soft (s t , a t , m) = f ? (s t , a t , m) + log E st+1?P (?|st,at,m) [exp(V soft (s t+1 , m))] V soft (s t , m) = log A exp(Q soft</formula><formula xml:id="formula_20">max ? E ? T t=1 f ? (s t , a t , m) ? log ?(a t |s t , m)<label>(14)</label></formula><p>From Section 2, we know that Equation <ref type="formula" target="#formula_0">(14)</ref> is exactly the training objective for the adaptive sampler ? ? in AIRL. Thus, the trajectory distribution of the optimal policy ? ? * matches p ? (? |m) defined in Equation <ref type="formula" target="#formula_6">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Lemma 2</head><p>First, the gradient of L info (?, ?) w.r.t. ? can be written as:</p><formula xml:id="formula_21">? ?? L info (?, ?) = E m?p(m),? ?p(? |m,?) log q(m|?, ?) ? ?? log p ? (? |m)<label>(15)</label></formula><p>As p ? (? |m) is an energy-based distribution (Equation <ref type="formula" target="#formula_6">(6)</ref>), we need to derive the gradient of log p(? |m, ?) w.r.t. ?:</p><formula xml:id="formula_22">? ?? log p(? |m, ?) = ? ?? log ?(s 1 ) T t=1 P (s t+1 |s t , a t ) + T t=1 f ? (s t , a t , m) ? log Z(?) (16) = T t=1 ? ?? f ? (s t , a t , m) ? ? ?? log Z(?) (17) = T t=1 ? ?? f ? (s t , a t , m) ? E ? ?p(? |m,?) T t=1 ? ?? f ? (s t , a t , m)<label>(18)</label></formula><p>Substituting Equation <ref type="formula" target="#formula_0">(18)</ref> into Equation (15), we get:</p><formula xml:id="formula_23">E m?p(m),? ?p ? (? |m) log q ? (m|? ) T t=1 ? ?? f ? (s t , a t , m) ? E ? ?p ? (? |m) T t=1 ? ?? f ? (s t , a t ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>m)</head><p>With Lemma 1, we know that when ? is trained to optimality, we can sample from p ? * ? (? |m) to construct an unbiased gradient estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Meta-Testing Procedure of PEMIRL</head><p>We summarize the meta-test stage of PEMIRL for adapting reward functions to new tasks in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Graphical Model of PEMIRL</head><p>Here we show the graphical model of the PEMIRL framework in <ref type="figure" target="#fig_5">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation Studies</head><p>In this section, we perform ablation studies on the sensitivity of the latent dimensions, importance of the mutual information loss (L info ) term, and stochasticity of the environment. We conduct each ablation study on the Point-Maze-Shift environment to evaluation the reward adaptation performance.</p><p>Sensitivity of the latent dimension. We first investigate the sensitivity of different latent dimensions by running PEMIRL with latent dimension picked from {1, 3, 5} on Point-Maze-Shift where the ground-truth latent dimension is 3. The results are summarized in <ref type="table" target="#tab_6">Table 3</ref>. We can observe that PEMIRL with various latent dimension specifications all outperform the best baseline (return -28.61) stably and is hence robust to dimension mis-specifications.</p><p>Importance of L info . As shown in <ref type="table" target="#tab_7">Table 4</ref>, the reward function learned by PEMIRL without the mutual information objective failed to induce a good policy in the reward adaptation setting, which demonstrates the importance of using L info .</p><p>Performance on stochastic environment. We create a stochastic version of Point-Maze-Shift (maze size: 60?100 cm) by changing its deterministic transition dynamics into a stochastic one. Specifically, p(s t+1 |s t , a t ) is now realized as a Gaussian with standard deviation being 1 cm. As shown in <ref type="table">Table 5</ref>, the average return of PEMIRL outperforms the best baseline Meta-IL by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Experimental Details F.1 Network Architectures</head><p>For all methods except AIRL, q ? (m|? ) and ? ? (a|s, m) are represented as 2-layer fully-connected neural networks with 128 and 64 hidden units respectively and ReLU as the activation function.</p><p>Following <ref type="bibr" target="#b10">[11]</ref>, to alleviate the reward ambiguity problem, we represent the reward function with two components (a context-dependent disentangled reward estimator r ? (s, m) and a context-dependent potential function h ? (s, m)):</p><formula xml:id="formula_24">f ?,? (s t , a t , s t+1 , m) = r ? (s t , m) + ?h ? (s t+1 , m) ? h ? (s t , m)</formula><p>Here r ? (s, m) and h ? (s, m) are realized as a 2-layer fully-connected neural networks with 32 hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Environment Details</head><p>Point-Maze. The ground-truth reward corresponds to negative distance toward the goal position as well as controlling the pointmass from moving too fast. We use 100 meta-training tasks and 30 meta-training tasks.    <ref type="table">Table 5</ref>: PEMIRL excels in stochastic env.</p><p>Ant. The ground-truth reward corresponds to moving as far as possible forward or backward without being flipped. We have 2 tasks in this domain.</p><p>Sweeper. The ground-truth reward is the negative distance from the sweeper to the object plus the negative distance from the object to the goal position. We train all methods on 100 meta-training tasks and test them on 30 meta-test tasks.</p><p>Sawyer Pushing. The ground-truth reward in this domain is similar to Sweeper, and we also use 100 meta-training tasks and 30 meta-test tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Training Details</head><p>Training the policy. During training TRPO, we use an entropy regularizer 1.0 for Point-Maze, and 0.1 for the other three domains. We find that adding an imitation objective in PEMIRL that maximizes the log-likelihood of the sampled expert trajectory conditioned on the latent context variable inferred by q ? with scaling factor 0.01 accelerates policy training.</p><p>Training the inference network and the reward model. We train q ? (m|? ), r ? (s, m) and h ? (s, m) using the Adam optimizer with default hyperparameters.</p><p>Scaling up the mutual information regularization. Note that in Equation 10, ? does not necessarily need to be equal to 1. Adjusting ? is equivalent to scaling L info (?, ?). We scale L info (?, ?) by 0.1 for all of our experiments.</p><p>Policy and inference network initialization. We initialize and q ? (m|? ) using Meta-IL discussed in Section 5 while randomly initializing the policy ? ? (a|s, m).</p><p>Stabilizing adversarial training. As in <ref type="bibr" target="#b10">[11]</ref>, we mix policy samples generated from previous 20 training iterations and use them as negatives when training the disriminator. We find that such a strategy prevents the discriminator from overfitting to samples from the current iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Data Efficiency</head><p>During meta-training, for the Point-Maze environment, it takes about 32M simulation steps to converge (similar to other methods such as Meta-InfoGAIL that takes 28M), which amounts to about 2 hours on one Nvidia Titan-Xp GPU; for the Ant environment, it takes about 13.8M simulation steps (Meta-InfoGAIL takes 12M) and about 40 hours on the same hardware (the state-action dimension is much larger than that of Point-Maze).</p><p>At meta-testing phase, the data efficiency of PEMIRL is comparable to RL training with the oracle ground-truth reward as shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-Maze-Shift</head><p>Disabled-Ant RL w/ oracle reward 4M env steps 15M env steps PEMIRL 5.4M env steps 18M env steps <ref type="table">Table 6</ref>: Comparison on data efficiency between RL trained with reward learned by PERMIL and RL trained with oracle reward. The methods have been shown to have similar data efficiency on Point-Maze-Shift and Disabled-Ant.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>PEMIRL Meta-Training Input: Expert trajectories DE = {? j E }; Initial parameters of f ? , ??, q ? . repeat Sample two batches of unlabeled demonstrations: ?E, ? E ? DE Infer a batch of latent context variables from the sampled demonstrations: m ? q ? (m|?E) Sample trajectories D from ??(? |m), with the latent context variable fixed during each rollout and included in D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a, m))]Update ? with TRPO to increase the following objective:E (s,a,m)?D [log D ? (s, a, m)] until ConvergenceOutput: Learned inference model q ? (m|? ), reward function f ? (s, a, m) and policy ??(a|s, m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Experimental domains (left to right): Point-Maze, Ant, Sweeper, and Sawyer Pusher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Visualizations of learned reward functions for point-maze navigation. The red star represents the target position and the white circle represents the initial position of the agent (both are different across different iterations). The black horizontal line represents the barrier that cannot be crossed. To show the generalization ability, the expert demonstration used to infer the target position are sampled from new target positions that have not been seen in the meta-training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>From top to bottom, we show the disabled ant running forward and backward respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Graphical model underlying PEMIRL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.21 ? 0.93 968.80 ? 27.11 ?50.86 ? 4.75 ?23.36 ? 2.54 Random ?51.39 ? 10.31 ?55.65 ? 18.39 ?259.71 ? 11.24 ?106.88 ? 18.06 AIRL [11] ?18.15 ? 3.17 127.61 ? 27.34 ?152.78 ? 7.39 ?51.56 ? 8.57 Meta-IL ?6.68 ? 1.51 218.53 ? 26.48 ?89.02 ? 7.06 ?28.13 ? 4.93</figDesc><table><row><cell></cell><cell>Point Maze</cell><cell>Ant</cell><cell>Sweeper</cell><cell>Sawyer Pusher</cell></row><row><cell cols="2">Expert ?5Meta-InfoGAIL ?7.66 ? 1.85</cell><cell>871.93 ? 31.28</cell><cell>?87.06 ? 6.57</cell><cell>?27.56 ? 4.86</cell></row><row><cell>PEMIRL (ours)</cell><cell>?7.37 ? 1.02</cell><cell>846.18 ? 25.81</cell><cell>?74.17 ? 5.31</cell><cell>?27.16 ? 3.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>One-shot policy generalization to test tasks on four experimental domains. Average return and standard deviations are reported over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on direct policy generalization and reward adaptation to challenging situations. Policy generalization examines if the policy learned by Meta-IL is able to generalize to new tasks with new dynamics, while reward adaptation tests if the learned RL can lead to efficient RL training in the same setting. The RL agent learned by PEMIRL rewards outperforms other methods in such challenging settings.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 2 PEMIRL Meta-Test for Reward Adaptation Input: A test context variable m ? p(m), a test expert demonstration ? E ? p ? E (? |m), and ground-truth reward r(s, a, m). Infer the latent context variable from the test demonstration:m ? q ? (m|? E ). Train a policy using TRPO w.r.t. adapted reward function f ? (s, a,m).</figDesc><table><row><cell>Evaluate the learned policy with r(s, a, m).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>latent dim. return 1 ?10.58 ? 1.27 3 ?14.13 ? 1.21 5 ?15.41 ? 1.40</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>PEMIRL is robust to latent dimensions. method return PEMIRL w/o MI ?39.24 ? 3.48 PEMIRL ?14.13 ? 1.21</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The MI term is important for training PEMIRL. method return Meta-IL ?30.58 ? 4.17 PEMIRL ?17.39 ? 0.84</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Video results can be found at: https://sites.google.com/view/pemirl 3 Our implementation of PEMIRL can be found at: https://github.com/ermongroup/MetaIRL</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by Toyota Research Institute, NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024). The authors would like to thank Chris Cundy for discussions over the paper draft.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Man?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in AI safety</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Cloutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN-91-Seattle International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02779</idno>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. Rl$?2$: Fast reinforcement learning via slow reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">One-shot imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradly</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03852</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guided cost learning: Deep inverse optimal control via policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One-shot visual imitation learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11248</idno>
		<title level="m">Learning robust rewards with adversarial inverse reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Habryka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08882</idno>
		<title level="m">Multi-task maximum entropy inverse reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal imitation learning from unstructured demonstrations using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sukhatme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1235" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning an embedding space for transferable robot skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Burn-in demonstrations for multi-modal imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kuefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS &apos;18</title>
		<meeting>the 17th International Conference on Autonomous Agents and MultiAgent Systems, AAMAS &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00909</idno>
		<title level="m">Reinforcement learning and control as probabilistic inference: Tutorial and review</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to optimize neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00441</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Infogail: Interpretable imitation learning from visual demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3812" to="3822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Meta networks. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning, ICML &apos;00</title>
		<meeting>the Seventeenth International Conference on Machine Learning, ICML &apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daishi</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Toyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00821</idno>
		<title level="m">Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient off-policy meta-reinforcement learning via probabilistic context variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurick</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08254</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum margin planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steind?r</forename><surname>Saemundsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07551</idno>
		<title level="m">Meta reinforcement learning with latent variable gaussian processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Computational approaches to motor learning by imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auke</forename><surname>Ijspeert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philosophical Transactions of the Royal Society of London B: Biological Sciences</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<title level="m">Trust region policy optimization. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Directed-info GAIL: learning hierarchical policies from unsegmented demonstrations using directed information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01266</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning a prior over intent via meta-inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellis</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12573</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">One-shot hierarchical imitation learning of compound visuomotor tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11043</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">One-shot imitation from observing humans via domain-adaptive meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (R:SS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep imitation learning for complex manipulation tasks from virtual reality teleoperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoe</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Jow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04615</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06514</idno>
		<title level="m">The information autoencoding family: A lagrangian perspective on latent variable generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anind K</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aaai</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
