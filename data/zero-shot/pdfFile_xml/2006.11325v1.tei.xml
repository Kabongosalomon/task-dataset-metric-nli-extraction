<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Prototypical Transfer Learning for Few-Shot Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Medina</surname></persName>
							<email>carlos.medinatemme@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences ?cole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnout</forename><surname>Devos</surname></persName>
							<email>arnout.devos@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences ?cole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grossglauser</surname></persName>
							<email>matthias.grossglauser@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Communication Sciences ?cole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Prototypical Transfer Learning for Few-Shot Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most approaches in few-shot learning rely on costly annotated data related to the goal task domain during (pre-)training. Recently, unsupervised meta-learning methods have exchanged the annotation requirement for a reduction in few-shot classification performance. Simultaneously, in settings with realistic domain shift, common transfer learning has been shown to outperform supervised meta-learning. Building on these insights and on advances in self-supervised learning, we propose a transfer learning approach which constructs a metric embedding that clusters unlabeled prototypical samples and their augmentations closely together. This pretrained embedding is a starting point for few-shot classification by summarizing class clusters and fine-tuning. We demonstrate that our self-supervised prototypical transfer learning approach ProtoTransfer outperforms state-of-the-art unsupervised meta-learning methods on few-shot tasks from the mini-ImageNet dataset. In fewshot experiments with domain shift, our approach even has comparable performance to supervised methods, but requires orders of magnitude fewer labels.</p><p>Nevertheless, most few-shot classification methods still require much annotated data for pre-training. Recently, several unsupervised meta-learning approaches, constructing episodes via pseudo-labeling <ref type="bibr" target="#b15">(Hsu et al., 2019;</ref><ref type="bibr" target="#b16">Ji et al., 2019)</ref> or image augmentations <ref type="bibr" target="#b17">(Khodadadeh et al., 2019;</ref><ref type="bibr" target="#b0">Antoniou and Storkey, 2019;</ref><ref type="bibr" target="#b26">Qin et al., 2020)</ref>, have addressed this problem. To our knowledge, unsupervised non-episodical techniques for transfer learning to few-shot tasks have not yet been explored.</p><p>Our approach ProtoTransfer performs self-supervised pre-training on an unlabeled training domain and can transfer to few-shot target domain tasks. During pre-training, we minimize a pairwise distance loss in order to learn an embedding that clusters noisy transformations of the same image * Equal contribution ? Most experiments by CM. Work performed as a semester project at EPFL-INDY supervised by AD, MG.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot classification <ref type="bibr" target="#b8">(Fei-Fei et al., 2006)</ref> is a learning task in which a classifier must adapt to distinguish novel classes not seen during training, given only a few examples (shots) of these classes. Meta-learning <ref type="bibr" target="#b9">(Finn et al., 2017;</ref><ref type="bibr" target="#b28">Ren et al., 2018</ref>) is a popular approach for few-shot classification by mimicking the test setting during training through so-called episodes of learning with few examples from the training classes. However, several works <ref type="bibr" target="#b11">Guo et al., 2019)</ref> show that common (non-episodical) transfer learning outperforms meta-learning methods on the realistic cross-domain setting, where training and novel classes come from different distributions. around the original image. Our pre-training loss can be seen as a self-supervised version of the prototypical loss in <ref type="bibr" target="#b31">Snell et al. (2017)</ref> in line with contrastive learning, which has driven recent advances in self-supervised representation learning <ref type="bibr" target="#b40">(Ye et al., 2019;</ref>. In the few-shot target task, in line with pre-training, we summarize class information in class prototypes for nearest neighbor inference similar to ProtoNet <ref type="bibr" target="#b31">(Snell et al., 2017)</ref> and we support fine-tuning to improve performance when multiple examples are available per class.</p><formula xml:id="formula_0">f ? (xi) f ? (xi,1) f ? (xi,2) f ? (xi,3) f ? (x2) f ? (x1) c1 c2 c3 f ? (q)</formula><p>We highlight our main contributions and results:</p><p>1. We show that our approach outperforms state-of-the-art unsupervised meta-learning methods by 4% to 8% on mini-ImageNet few-shot classification tasks and has competitive performance on Omniglot.</p><p>2. Compared to the fully supervised setting, our approach achieves competitive performance on mini-ImageNet and multiple datasets from the cross-domain transfer learning CDFSL benchmark, with the benefit of not requiring labels during training.</p><p>3. In an ablation study and cross-domain experiments we show that using a larger number of equivalent training classes than commonly possible with episodical meta-learning, and parametric fine-tuning are key to obtaining performance matching supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Self-Supervised Prototypical Transfer Learning Algorithm</head><p>Section 2.1 introduces the few-shot classification setting and relevant terminology. Further, we describe ProtoTransfer's pre-training stage, ProtoCLR, in Section 2.2 and its fine-tuning stage, ProtoTune, in Section 2.3. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>The goal of few-shot classification is to predict classes for a set of unlabeled points (the query set) given a small set of labeled examples (the support set) from the same classes. Few-shot classification approaches commonly consist of two subsequent learning phases, each using its own set of classes.</p><p>The first learning phase utilizes samples from N b base (training) classes contained within a training set </p><formula xml:id="formula_1">D b = {(x, y)} ? I ? Y b , where x ? I is a sample with label y in label set Y b .</formula><formula xml:id="formula_2">let (i, q) = ? log exp(?d[f (xi,q),f (xi)]) N k=1 exp(?d[f (xi,q),f (x k )]) 12: L = 1 N Q N i=1 Q q=1 (i, q) 13: ? ? ? ? ?? ? L 14: end while 15: return embedding function f ? (?)</formula><p>Concretely, an N n -way K-shot classification task consists of K labeled examples for each of the N n novel classes. In the few-shot learning literature a task is also commonly referred to as an episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Prototypical Pre-Training: ProtoCLR</head><p>Similar to the few-shot target tasks, we frame every ProtoCLR pre-training learning step as an N -way 1-shot classification task optimized by a contrastive loss function as described below. In this, we draw inspiration from recent progress in unsupervised meta-learning <ref type="bibr" target="#b17">(Khodadadeh et al., 2019)</ref> and self-supervised visual contrastive learning of representations <ref type="bibr" target="#b40">Ye et al., 2019)</ref>.</p><p>Algorithm 1 details ProtoCLR and it comprises the following parts:</p><p>? Batch generation (Algorithm 1 lines 4-10): Each mini-batch contains N random samples {x i } i=1...N from the training set. As our self-supervised setting does not assume any knowledge about the base class labels Y b , we treat each sample as it's own class. Thus, each sample x i serves as a 1-shot support sample and class prototype. For each prototype x i , Q different randomly augmented versionsx i,q are used as query samples.</p><p>? Contrastive prototypical loss optimization (Algorithm 1 lines 11-13): The pre-training loss encourages clustering of augmented query samples {x i,q } around their prototype x i in the embedding space through a distance metric d[?, ?]. The softmax cross-entropy loss over N classes is minimized with respect to the embedding parameters ? with mini-batch stochastic gradient descent (SGD).</p><p>Commonly, unsupervised pre-training approaches for few-shot classification <ref type="bibr" target="#b15">(Hsu et al., 2019;</ref><ref type="bibr" target="#b17">Khodadadeh et al., 2019;</ref><ref type="bibr" target="#b0">Antoniou and Storkey, 2019;</ref><ref type="bibr" target="#b26">Qin et al., 2020;</ref><ref type="bibr" target="#b16">Ji et al., 2019)</ref> rely on meta-learning. Thus, they are required to create small artificial N -way (K-shot) tasks identical to the downstream few-shot classification tasks. Our approach does not use meta-learning and can use any batch size N . Larger batch sizes have been shown to help self-supervised representation learning  and supervised pre-training for few-shot classification <ref type="bibr" target="#b31">(Snell et al., 2017)</ref>. We also find that larger batches yield a significant performance improvement for our approach (see Section 3.3). To generate the query examples, we use image augmentations similar to  and adjust them for every dataset. The exact transformations are listed in Appendix A.3. Following <ref type="bibr" target="#b31">Snell et al. (2017)</ref>, we use the Euclidean distance, but our method is generic and works with any metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Supervised Prototypical Fine-Tuning: ProtoTune</head><p>After pre-training the metric embedding f ? (?), we address the target task of few-shot classification. For this, we extend the prototypical nearest-neighbor classifier ProtoNet <ref type="bibr" target="#b31">(Snell et al., 2017)</ref> with prototypical fine-tuning of a final classification layer, which we refer to as ProtoTune. First, the class prototypes c n are computed as the mean of the class samples in the support set S of the few-shot task:</p><formula xml:id="formula_3">c n = 1 |S n | (xi,yi=n)?S f ? (x i ).</formula><p>ProtoNet uses non-parametric nearest-neighbor classification with respect to c n and can be interpreted as a linear classifier applied to a learned representation f ? (x). Following the derivation in <ref type="bibr" target="#b31">Snell et al. (2017)</ref>, we initialize a final linear layer with weights W n = 2c n and biases b n = ?||c n || 2 . Then, this final layer is fine-tuned with a softmax cross-entropy loss on samples from S, while keeping the embedding function parameters ? fixed. <ref type="bibr" target="#b34">Triantafillou et al. (2020)</ref> proposed a similar fine-tuning approach with prototypical initialization, but their approach always fine-tunes all model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We carry out several experiments to benchmark and analyze ProtoTransfer. In Section 3.1, we conduct in-domain classification experiments on the Omniglot <ref type="bibr" target="#b20">(Lake et al., 2011)</ref> and mini-ImageNet <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref> benchmarks to compare to state-of-the-art unsupervised few-shot learning approaches and methods with supervised pre-training. In Section 3.2, we test our method on a more challenging cross-domain few-shot learning benchmark <ref type="bibr" target="#b11">(Guo et al., 2019)</ref>. Section 3.3 contains an ablation study showing how the different components of ProtoTransfer contribute to its performance. In Section 3.4, we study how pre-training with varying class diversities affects performance. In Section 3.5, we give insight in generalization from training classes to novel classes from both unsupervised and supervised perspectives. Experimental details can be found in Appendix A and code is made available 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-Domain Few-shot Classification: Omniglot and mini-ImageNet</head><p>For our in-domain experiments, where the disjoint training class set and novel class set come from the same distribution, we used the popular few-shot datasets Omniglot <ref type="bibr" target="#b20">(Lake et al., 2011)</ref> and mini-ImageNet <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref>. For comparability we use the Conv-4 architecture proposed in <ref type="bibr" target="#b36">Vinyals et al. (2016)</ref>. Specifics on the datasets, architecture and optmization can be found in Appendices A.1 and A.2. We apply limited hyperparameter tuning, as suggested in <ref type="bibr" target="#b25">Oliver et al. (2018)</ref>, and use a batch size of N = 50 and number of query augmentations Q = 3 for all datasets.</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we report few-shot accuracies on the mini-ImageNet and Omniglot benchmarks. We compare to unsupervised clustering based methods CACTUs <ref type="bibr" target="#b15">(Hsu et al., 2019)</ref> and UFLST <ref type="bibr" target="#b16">(Ji et al., 2019)</ref> as well as the augmentation based methods UMTRA <ref type="bibr" target="#b17">(Khodadadeh et al., 2019)</ref>, AAL <ref type="bibr" target="#b0">(Antoniou and Storkey, 2019)</ref> and ULDA <ref type="bibr" target="#b26">(Qin et al., 2020)</ref>. More details on how these approaches compare to ours can be found in Section 4. Pre+Linear represents classical supervised transfer learning, where a deep neural network classifier is (pre)trained on the training classes and then only the last linear layer is fine-tuned on the novel classes. On mini-ImageNet, ProtoTransfer outperforms all other state-of-the-art unsupervised pre-training approaches by at least 4% up to 8% and mostly outperforms the supervised meta-learning method MAML <ref type="bibr" target="#b9">(Finn et al., 2017)</ref>, while requiring orders of magnitude fewer labels (N K vs 38400 + N K). On Omniglot, ProtoTransfer shows competitive performance with most unsupervised meta-learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-domain Few-Shot Classification: CDFSL benchmark</head><p>For our cross-domain experiments, where training and novel classes come from different distributions, we turn to the CDFSL benchmark <ref type="bibr" target="#b11">(Guo et al., 2019)</ref>. This benchmark specifically tests how well methods trained on mini-ImageNet can transfer to few-shot tasks with only limited similarity to mini-ImageNet. In order of decreasing similarity, the four datasets are plant disease images from CropDiseases (Mohanty et al., 2016), satellite images from EuroSAT <ref type="bibr" target="#b13">(Helber et al., 2019)</ref>, dermatological images from ISIC2018  and grayscale chest X-ray images from ChestX <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>. Following <ref type="bibr" target="#b11">Guo et al. (2019)</ref>, we use a ResNet-10 neural network architecture. As there is no validation data available for the target tasks in CDFSL, we keep the same ProtoTransfer hyperparameters N = 50, Q = 3 as used in the mini-ImageNet experiments. Experimental details are listed in Appendices A.1.2 and A.2.2. For comparison to unsupervised meta-learning, we include our results on UMTRA-ProtoNet and its fine-tuned version UMTRA-ProtoTune <ref type="bibr" target="#b17">(Khodadadeh et al., 2019)</ref>. Both use our augmentations instead of those from <ref type="bibr" target="#b17">(Khodadadeh et al., 2019)</ref>. For further comparison, we include ProtoNet <ref type="bibr" target="#b31">(Snell et al., 2017)</ref> for supervised few-shot learning and Pre+Mean-Centroid and Pre+Linear as the best-on-average performing transfer learning approaches from <ref type="bibr" target="#b11">Guo et al. (2019)</ref>. As the CDFSL benchmark presents a large domain shift with respect to mini-ImageNet, all model parameters are fine-tuned in ProtoTransfer during the few-shot fine-tuning phase with ProtoTune.</p><p>We report results on the CDFSL benchmark in <ref type="table" target="#tab_2">Table 2</ref>. ProtoTransfer consistently outperforms its meta-learned counterparts by at least 0.7% up to 19% and performs mostly on par with the supervised transfer learning approaches. Comparing the results of UMTRA-ProtoNet and UMTRA-ProtoTune, starting from 5 shots, parametric fine-tuning gives improvements ranging from 1% to 13%. Notably, on the dataset with the largest domain shift (ChestX), ProtoTransfer outperforms all other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study: Batch Size, Number of Queries, and Fine-Tuning</head><p>We conduct an ablation study of ProtoTransfer's components to see how they contribute to its performance. Starting from ProtoTransfer we successively remove components to arrive at the equivalent UMTRA-ProtoNet which shows similar performance to the original UMTRA approach <ref type="bibr" target="#b17">(Khodadadeh et al., 2019</ref>) on mini-ImageNet. As a reference, we provide results of a ProtoNet classifier on top of a fixed randomly initialized network. <ref type="table" target="#tab_3">Table 3</ref> shows that increasing the batch size from N = 5 for UMTRA-ProtoNet to 50 for ProtoCLR-ProtoNet, keeping everything else equal, is crucial to our approach and yields a 5% to 9% performance improvement. Importantly, UMTRA-ProtoNet uses our augmentations instead of those from <ref type="bibr" target="#b17">(Khodadadeh et al., 2019)</ref>. Thus, this improvement cannot be attributed to using different augmentations than UMTRA. Increasing the training query number to Q = 3 gives better gradient information and yields a relatively small but consistent performance improvement. Fine-tuning in the target domain does not always give a net improvement. Generally, when many shots are available, fine-tuning gives a significant boost in performance as exemplified by ProtoCLR-ProtoTune and UMTRA-MAML in the 50-shot case. Interestingly, our approach reaches competitive performance in the few-shot regime even before fine-tuning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Number of Training Classes and Samples</head><p>While ProtoTransfer already does not require any labels during pre-training, for some applications, e.g. rare medical conditions, even the collection of sufficiently similar data might be difficult. Thus, we test our approach when reducing the total number of available training images under the controlled setting of mini-ImageNet. Moreover, not all training datasets will have such a diverse set of classes to learn from as the different animals, vehicles and objects in mini-ImageNet. Therefore, we also test the effect of reducing the number of training classes and thereby the class diversity. To contrast the effects of reducing the number of classes or reducing the number of samples, we either remove whole classes from the mini-ImageNet training set or remove the corresponding amount of samples randomly from all classes. The number of samples are decreased in multiples of 600 as each mini-ImageNet class contains exactly 600 samples. We compare the mini-ImageNet few-shot classification accuracies of ProtoTransfer to the popular supervised transfer learning baseline Pre+Linear in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>As expected, when uniformily reducing the number of images from all classes <ref type="figure" target="#fig_2">(Figure 2a</ref>   <ref type="table" target="#tab_1">Table 11</ref> in the appendix.  <ref type="figure" target="#fig_2">Figures 2a and 2b</ref>, there is only a small difference between reducing images randomly from all classes or taking entire classes away. In contrast, the supervised baseline performance suffers substantially from having fewer classes.</p><p>To validate these in-domain observations in a cross-domain setting, following <ref type="bibr" target="#b6">Devos and Grossglauser (2019)</ref>, we compare few-shot classification performance when training on CUB <ref type="bibr" target="#b39">(Welinder et al., 2010;</ref><ref type="bibr" target="#b37">Wah et al., 2011)</ref> and testing on mini-ImageNet <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref>. CUB consists of 200 classes of birds, while only three of the 64 mini-ImageNet training classes are birds (see A.1.3, A.2.3 for details on CUB). Thus CUB possesses a lower class diversity than mini-ImageNet. <ref type="table" target="#tab_4">Table 4</ref> confirms our previous observation numerically and shows that ProtoTransfer has a superior transfer accuracy of 2% to 4% over the supervised approach when limited diversity is available in the training classes.</p><p>We conjecture that this difference is due to the fact that our self-supervised approach does not make a difference between samples coming from the same or different (latent) classes during training. Thus, we expect it to learn discriminative features despite a low training class diversity. In contrast, the supervised case forces multiple images with rich features into the same classes. We thus expect the generalization gap between tasks coming from training classes and testing classes to be smaller with self-supervision. We provide evidence to support this conjecture in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Task Generalization Gap</head><p>To compare the generalization of ProtoCLR with its supervised embedding learning counterpart ProtoNet <ref type="bibr" target="#b31">(Snell et al., 2017)</ref>, we visualize the learned embedding spaces with t-SNE (Maaten and Hinton, 2008) in <ref type="figure">Figure 3</ref>. We compare both methods on samples from 5 random classes from the training and testing sets of mini-ImageNet. In <ref type="figure">Figures 3a and 3b</ref> we observe that, for the same training classes, ProtoNet shows more structure. Comparing all subfigures in <ref type="figure">Figure 3</ref>  These visual observations are supported numerically in <ref type="table" target="#tab_5">Table 5</ref>. Self-supervised embedding approaches, such as UMTRA and our ProtoCLR approach, show a much smaller task generalization gap than supervised ProtoNet. ProtoCLR shows virtually no classification performance drop. However, supervised ProtoNet suffers a significant accuracy reduction of 6% to 12%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Unsupervised meta-learning: Both CACTUs <ref type="bibr" target="#b15">(Hsu et al., 2019)</ref> and UFLST <ref type="bibr" target="#b16">(Ji et al., 2019)</ref> alternate between clustering for support and query set generation and employing standard metalearning. In contrast, our method unifies self-supervised clustering and inference in a single model. <ref type="bibr" target="#b17">Khodadadeh et al. (2019)</ref> propose an unsupervised model-agnostic meta-learning approach (UMTRA), where artifical N -way 1-shot tasks are generated by randomly sampling N support examples from the training set and generating N corresponding queries by augmentation. <ref type="bibr" target="#b0">Antoniou and Storkey (2019)</ref> (AAL) generalize this approach to more support shots by randomly grouping augmented images into classes for classification tasks. ULDA <ref type="bibr" target="#b26">(Qin et al., 2020)</ref> induce a distribution shift between the support and query set by applying different types of augmentations to each. In contrast, ProtoTransfer uses a single un-augmented support sample, similar to <ref type="bibr" target="#b17">Khodadadeh et al. (2019)</ref>, but extends to several query samples for better gradient signals and steps away from artificial few-shot task sampling by using larger batch sizes, which is key to learning stronger embeddings.</p><p>Supervised meta-learning aided by self-supervision: Several works have proposed to use a selfsupervised loss either alongside supervised meta-learning episodes <ref type="bibr" target="#b10">(Gidaris et al., 2019;</ref> or to initialize a model prior to supervised meta-learning on the source domain <ref type="bibr" target="#b32">Su et al., 2019)</ref>. In contrast, we do not require any labels during training.</p><p>Fine-tuning for few-shot classification:  show that adaptation on the target task is key for good cross-domain few-shot classification performance. Similar to <ref type="bibr">ProtoTune, Triantafillou et al. (2020)</ref> also initialize a final layer with prototypes after supervised meta-learning, but always fine-tune all parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive loss learning:</head><p>Contrastive losses have fueled recent progress in learning strong embedding functions <ref type="bibr" target="#b40">(Ye et al., 2019;</ref><ref type="bibr" target="#b33">Tian et al., 2020;</ref>. Most similar to our approach is <ref type="bibr" target="#b40">Ye et al. (2019)</ref>. They propose a per-batch contrastive loss that minimizes the distance between an image and an augmented version of it. Different to us, they do not generalize to using multiple augmented query images per prototype and use 2 extra fully connected layers during training. Concurrently,  also use a prototype-based contrastive loss. They compute the prototypes as centroids after clustering augmented images via k-Means. They also separate learning and clustering procedures, which ProtoTransfer achieves in a single procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed ProtoTransfer for few-shot classification. ProtoTransfer performs transfer learning from an unlabeled source domain to a target domain with only a few labeled examples. Our experiments show that on mini-ImageNet it outperforms all prior unsupervised few-shot learning approaches by a large margin. On a more challenging cross-domain few-shot classification benchmark, ProtoTransfer shows similar performance to fully supervised approaches. Our ablation studies show that large batch sizes are crucial to learning good representations for downstream few-shot classification tasks and that parametric fine-tuning on target tasks can significantly boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Details</head><p>A.1 Datasets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 In-domain datasets</head><p>For our in-domain experiments we used the popular few-shot datasets Omniglot <ref type="bibr" target="#b20">(Lake et al., 2011)</ref> and mini-ImageNet <ref type="bibr" target="#b36">(Vinyals et al., 2016)</ref>.</p><p>Omniglot consists of 1623 handwritten characters from 50 alphabets and 20 examples per character. Identical to <ref type="bibr" target="#b36">Vinyals et al. (2016)</ref>, the grayscale images are resized to 28x28. Following <ref type="bibr" target="#b30">Santoro et al. (2016)</ref>, we use 1200 characters for training and 423 for testing.</p><p>Mini-ImageNet is a subset of the ILSVRC-12 dataset <ref type="bibr" target="#b29">(Russakovsky et al., 2015)</ref>, which contains 60,000 color images that we resized to 84x84. For comparability, we use the splits introduced by Ravi and Larochelle (2017) over 100 classes with 600 images each. 64 classes are used for pre-training and 20 for testing. We only use the 16 validation set classes for limited hyperparameter tuning of batch size N , number of queries Q and the augmentation strengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Cross-domain datasets</head><p>We evaluate all cross-domain experiments the CDFSL-benchmark <ref type="bibr" target="#b11">(Guo et al., 2019)</ref>. It comprises four datasets with decreasing similarity to mini-ImageNet. In order of similarity, they are plant disease images from CropDiseases (Mohanty et al., 2016), satellite images from EuroSAT <ref type="bibr" target="#b13">(Helber et al., 2019)</ref>, dermatological images from ISIC2018  and grayscale chest x-ray images from ChestX <ref type="bibr" target="#b38">(Wang et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Caltech-UCSD Birds-200-2011 (CUB) dataset</head><p>We use the Caltech-UCSD Birds-200-2011 (CUB) dataset <ref type="bibr" target="#b39">Welinder et al. (2010)</ref>; <ref type="bibr" target="#b37">Wah et al. (2011)</ref> in our ablation studies. It is composed of 11,788 images from 200 different bird species. We follow the splits proposed by <ref type="bibr" target="#b14">Hilliard et al. (2018)</ref> with 100 training, 50 validation and 50 test classes. We do not use the validation set classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Architecture and Optimization Parameters</head><p>In the following, we describe the experimental details for the individual experiments. We deliberately stay close to the parameters reported in prior work and do not perform an extensive hyperparameter search for our specific setup, as this can easily lead to performance overestimation compared to simpler approaches <ref type="bibr" target="#b25">(Oliver et al., 2018)</ref>). <ref type="table" target="#tab_6">Table 6</ref> summarizes the hyperparameters we used for ProtoTransfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 In-Domain Experiments</head><p>Our mini-ImageNet and Omniglot experiments use the Conv-4 architecture proposed in <ref type="bibr" target="#b36">Vinyals et al. (2016)</ref> for comparability. Its four convolutional blocks each apply a 64-filters 3x3 convolution, batch normalization, a ReLU nonlinearity and 2x2 max-pooling. The pre-training mostly mirrors <ref type="bibr" target="#b31">Snell et al. (2017) and</ref><ref type="bibr">uses Adam (Kingma and</ref><ref type="bibr" target="#b18">Ba, 2015)</ref> with an initial learning rate of 0.001, which is multiplied by a factor of 0.5 every 25000 iterations. We use a batch size of 50. We do not use the validation set to select the best training epoch. Instead training stops after 20.000 iterations without improvement in training accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Cross-Domain Experiments</head><p>Our experiments on the CDFSL-Challenge are based on the code provided by <ref type="bibr" target="#b11">Guo et al. (2019)</ref>. Following <ref type="bibr" target="#b11">Guo et al. (2019)</ref>, we use a ResNet10 architecture that is pre-trained on mini-Imagenet images of size 224x224 for 400 epochs with Adam (Kingma and Ba, 2015) and the default learning rate of 0.001 for best comparability with the results reported in <ref type="bibr" target="#b11">Guo et al. (2019)</ref>. The batch size for self-supervised pre-training is 50. We do not use a validation set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Caltech-UCSD Birds-200-2011 (CUB) Experiments</head><p>The CUB training is identical in terms of architecture (Conv-4) and optimization to the setup for our in-domain experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Prototypical Fine-Tuning</head><p>During the fine-tuning stage we add a fully connected classification layer after the embedding function and initialize as described in Section 2.3. We split the support examples into batches of 5 images each and perform 15 fine-tuning epochs with <ref type="bibr">Adam (Kingma and Ba, 2015)</ref> and an initial learning rate of 0.001. For target datasets mini-ImageNet and Omniglot only the last fully connected layer is optimized, while for the CDFSL benchmark experiments the embedding network is adapted as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Augmentations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 CDFSL transforms</head><p>For the CDFSL-benchmark <ref type="bibr" target="#b11">(Guo et al., 2019)</ref> experiments we employ the same augmentations as , as these have proven to work well for ImageNet <ref type="bibr" target="#b29">(Russakovsky et al., 2015)</ref> images of size 224x224. They are as follows:</p><p>1. Random crop and resize: scale ? [0.08, 1.0] , aspect ratio ? [3/4, 4/3], Bilinear filter with interpolation = 2 2. Random horizontal flip 3. Random (p = 0.8) color jitter: brightness = contrast = saturation = 0.8, hue=0.2 4. Random (p = 0.2) grayscale 5. Gaussian blur, random radius ? ? [0.1, 0.2]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 mini-ImageNet &amp; CUB transforms</head><p>For the mini-Imagenet and CUB experiments we used lighter versions of the  augmentations, namely no Gaussian blur, lower color jitter strengths and smaller rescaling and cropping ranges. They are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Classes for t-SNE Plots</head><p>The classes in the t-SNE plots are a random subset of classes from the mini-ImageNet base classes (classes 1-5) and the mini-ImageNet novel classes (classes 6-10). Their corresponding labels are the following:   : Accuracy (%) of methods on N -way K-shot classification tasks of the CDFSL benchmark <ref type="bibr" target="#b11">(Guo et al., 2019)</ref>. All models are trained on mini-ImageNet with ResNet-10. All results are reported with 95% confidence intervals over 600 randomly generated test episodes. Results style: best and second best.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Self-Supervised Prototypical Transfer Learning. (a): In the embedding, original images x i serve as class prototypes around which their Q augmented viewsx i,q should cluster. (b): Prototypes c n are the means of embedded support examples for each class n and initialize a final linear layer for fine-tuning. An embedded query point q is classified via a softmax over the fine-tuned linear layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>), the few-shot classification accuracy is reduced as well. The performance of ProtoTransfer and the supervised baseline closely match in this case. When reducing the number of training classes inFigure 2b, ProtoTransfer consistently and significantly outperforms the supervised baseline when the number of mini-ImageNet training classes drops below 16. For example in the 20-shot case with only two training classes, ProtoTransfer outperforms the supervised baseline by a large margin of 16.9% Varying number of training classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>5-way K-shot accuracies with 95% confidence intervals on mini-ImageNet as a function of training images and classes. Methods: ProtoTransfer ( ), transfer learning baseline Pre+Linear ( ). Note the logarithmic scale. Detailed results available in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, ProtoCLR shows more closely related embeddings in Figures 3a and 3c than ProtoNet in Figures 3b and 3d. t-SNE plots of trained embeddings on 5 classes from the training and testing sets of mini-ImageNet. Trained embeddings considered are self-supervised ProtoCLR and supervised 20-way 5-shot ProtoNet. For details on the depicted classes, please refer to Appendix A.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Self-Supervised Prototypical Pre-Training (ProtoCLR) 1: input: batch size N , augmentations size Q, embedding function f ? , set of random transformations T , step size ?, distance function d[?, ?] 2: Randomly initialize ? 3: while not done do</figDesc><table><row><cell>4:</cell><cell>Sample minibatch {x i } N i=1</cell></row><row><cell>5:</cell><cell>for all i ? {1, . . . , N } do</cell></row><row><cell>6:</cell><cell>for all q ? {1, . . . , Q} do</cell></row><row><cell>7:</cell><cell>draw a random transformation t ? T</cell></row><row><cell cols="2">8:x i,q = t(x i )</cell></row><row><cell>9:</cell><cell>end for</cell></row><row><cell>10:</cell><cell>end for</cell></row><row><cell>11:</cell><cell></cell></row></table><note>An important aspect of our specific unsupervised learning setting is that the first phase has no access to the per-sample label information, the distribution of classes, nor the size of the label set Y b , for pre-training. This first phase serves as a preparation for the actual few-shot learning in the target domain, i.e. the second learning phase. This second supervised learning phase contains N n novel (testing) classes as D n = {(x, y)} ? I ? Y n , where only few examples for each of the classes in Y n are available. Algorithm 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) of unsupervised pre-training methods on N -way K-shot classification tasks on Omniglot and mini-Imagenet on a Conv-4 architecture. For detailed results, seeTables 7 and 8in the Appendix. Results style: best and second best. ) 88.00 96.48 72.27 89.08 45.67 62.99 72.34 77.22</figDesc><table><row><cell>Method (N,K)</cell><cell>(5,1)</cell><cell cols="4">(5,5) (20,1) (20,5) (5,1)</cell><cell>(5,5) (5,20) (5,50)</cell></row><row><cell></cell><cell></cell><cell cols="2">Omniglot</cell><cell></cell><cell></cell><cell>mini-ImageNet</cell></row><row><cell>Training (scratch)</cell><cell cols="6">52.50 74.78 24.91 47.62 27.59 38.48 51.53 59.63</cell></row><row><cell>CACTUs-MAML</cell><cell cols="6">68.84 87.78 48.09 73.36 39.90 53.97 63.84 69.64</cell></row><row><cell>CACTUs-ProtoNet</cell><cell cols="6">68.12 83.58 47.75 66.27 39.18 53.36 61.54 63.55</cell></row><row><cell>UMTRA</cell><cell cols="6">83.80 95.43 74.25 92.12 39.93 50.73 61.11 67.15</cell></row><row><cell>AAL-ProtoNet</cell><cell cols="6">84.66 89.14 68.79 74.28 37.67 40.29</cell><cell>-</cell><cell>-</cell></row><row><cell>AAL-MAML++</cell><cell cols="6">88.40 97.96 70.21 88.32 34.57 49.18</cell><cell>-</cell><cell>-</cell></row><row><cell>UFLST</cell><cell cols="6">97.03 99.19 91.28 97.37 33.77 45.03 53.35 56.72</cell></row><row><cell>ULDA-ProtoNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">40.63 55.41 63.16 65.20</cell></row><row><cell>ULDA-MetaOptNet</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">40.71 54.49 63.58 67.65</cell></row><row><cell>ProtoTransfer (oursSupervised training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>MAML 94.46 98.83 84.60 96.29 46.81 62.13 71.03 75.54 ProtoNet 97.70 99.28 94.40 98.39 46.44 66.33 76.73 78.91 Pre+Linear 94.30 99.08 86.05 97.11 43.87 63.01 75.46 80.17</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy (%) of methods on N -way K-shot (N ,K) classification tasks of the CDFSL benchmark<ref type="bibr" target="#b11">(Guo et al., 2019)</ref>. Both our results on methods with unsupervised pre-training (UnSup) and results on methods with supervised pre-training from CDFSL are listed. All models are trained on mini-ImageNet with ResNet-10. For detailed results, see AppendixTable 9. Results style: best and second best.</figDesc><table><row><cell>Method</cell><cell cols="2">UnSup (5,5) (5,20) (5,50) (5,5) (5,20) (5,50)</cell></row><row><cell></cell><cell>ChestX</cell><cell>ISIC</cell></row><row><cell>ProtoNet</cell><cell cols="2">24.05 28.21 29.32 39.57 49.50 51.99</cell></row><row><cell>Pre+Mean-Centroid</cell><cell cols="2">26.31 30.41 34.68 47.16 56.40 61.57</cell></row><row><cell>Pre+Linear</cell><cell cols="2">25.97 31.32 35.49 48.11 59.31 66.48</cell></row><row><cell>UMTRA-ProtoNet</cell><cell cols="2">24.94 28.04 29.88 39.21 44.62 46.48</cell></row><row><cell>UMTRA-ProtoTune</cell><cell cols="2">25.00 30.41 35.63 38.47 51.60 60.12</cell></row><row><cell>ProtoTransfer (ours)</cell><cell cols="2">26.71 33.82 39.35 45.19 59.07 66.15</cell></row><row><cell></cell><cell>EuroSat</cell><cell>CropDiseases</cell></row><row><cell>ProtoNet</cell><cell cols="2">73.29 82.27 80.48 79.72 88.15 90.81</cell></row><row><cell>Pre+Mean-Centroid</cell><cell cols="2">82.21 87.62 88.24 87.61 93.87 94.77</cell></row><row><cell>Pre+Linear</cell><cell cols="2">79.08 87.64 91.34 89.25 95.51 97.68</cell></row><row><cell>UMTRA-ProtoNet</cell><cell cols="2">74.91 80.42 82.24 79.81 86.84 88.44</cell></row><row><cell>UMTRA-ProtoTune</cell><cell cols="2">68.11 81.56 85.05 82.67 92.04 95.46</cell></row><row><cell>ProtoTransfer (ours)</cell><cell cols="2">75.62 86.80 90.46 86.53 95.06 97.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell>batch size</cell><cell>Q</cell><cell>FT (5,1)</cell><cell>(5,5) (5,20) (5,50)</cell></row><row><cell>n.a.</cell><cell>ProtoNet</cell><cell>n.a.</cell><cell cols="3">n.a. no 27.05 34.12 39.68 41.40</cell></row><row><cell>UMTRA</cell><cell>MAML</cell><cell>N (= 5)</cell><cell>1</cell><cell cols="2">yes 39.93 50.73 61.11 67.15</cell></row><row><cell>UMTRA</cell><cell>ProtoNet</cell><cell>N (= 5)</cell><cell>1</cell><cell cols="2">no 39.17 53.78 62.41 64.40</cell></row><row><cell cols="2">ProtoCLR ProtoNet</cell><cell>50</cell><cell>1</cell><cell cols="2">no 44.53 62.88 70.86 73.93</cell></row><row><cell cols="2">ProtoCLR ProtoNet</cell><cell>50</cell><cell>3</cell><cell cols="2">no 44.89 63.35 72.27 74.31</cell></row><row><cell cols="2">ProtoCLR ProtoTune</cell><cell>50</cell><cell>3</cell><cell cols="2">yes 45.67 62.99 72.34 77.22</cell></row></table><note>Accuracy (%) of methods on N -way K-shot (N, K) classification tasks on mini-ImageNet with a Conv-4 architecture for different training image batch sizes, number of training queries (Q) and optional finetuning on target tasks (FT). UMTRA-MAML results are taken from Khodadadeh et al. (2019), where UMTRA uses AutoAugment (Cubuk et al., 2019) augmentations. For detailed results see Table 10 in the Appendix. Results style: best and second best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) on N -way K-shot (N, K) classification tasks on mini-ImageNet for methods trained on the CUB training set (5885 images) with a Conv-4 architecture. All results indicate 95% confidence intervals over 600 randomly generated test episodes. Results style: best and second best.</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell>(5,1)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,50)</cell></row><row><cell>ProtoCLR</cell><cell>ProtoNet</cell><cell cols="4">34.56 ? 0.61 52.76 ? 0.63 62.76 ? 0.59 66.01 ? 0.55</cell></row><row><cell>ProtoCLR</cell><cell cols="5">ProtoTune 35.37 ? 0.63 52.38 ? 0.66 63.82 ? 0.59 68.95 ? 0.57</cell></row><row><cell cols="2">Pre(training) Linear</cell><cell cols="4">33.10 ? 0.60 47.01 ? 0.65 59.94 ? 0.62 65.75 ? 0.63</cell></row><row><cell cols="3">(64.59% vs 47.68%). Comparing ProtoTransfer in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>.03 ? 0.79 56.43 ? 0.78 64.48 ? 0.71 66.28 ? 0.66 UMTRA ProtoNet Test 38.92 ? 0.69 53.37 ? 0.68 61.69 ? 0.66 65.12 ? 0.59 ProtoCLR ProtoNet Train 45.33 ? 0.63 63.47 ? 0.58 71.51 ? 0.51 73.99 ? 0.49 ProtoCLR ProtoNet Test 44.89 ? 0.58 63.35 ? 0.54 72.27 ? 0.45 74.31 ? 0.45</figDesc><table><row><cell cols="7">: Accuracy (%) of N -way K-shot (N,K) classification tasks from the training and testing split</cell></row><row><cell cols="7">of mini-ImageNet. Following Snell et al. (2017), ProtoNet is trained with 30-way 1-shot for 1-shot</cell></row><row><cell cols="7">tasks and 20-way K-shot otherwise. All results use a Conv-4 architecture. All results show 95%</cell></row><row><cell cols="5">confidence intervals over 600 randomly generated episodes.</cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell>Testing</cell><cell>Data</cell><cell>(5,1)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,50)</cell></row><row><cell>ProtoNet</cell><cell cols="6">ProtoNet Train 53.74 ? 0.95 79.09 ? 0.69 85.53 ? 0.53 86.62 ? 0.48</cell></row><row><cell>ProtoNet</cell><cell>ProtoNet</cell><cell>Val</cell><cell cols="4">46.62 ? 0.82 67.34 ? 0.69 76.44 ? 0.57 79.00 ? 0.53</cell></row><row><cell>ProtoNet</cell><cell cols="6">ProtoNet Test 46.44 ? 0.78 66.33 ? 0.68 76.73 ? 0.54 78.91 ? 0.57</cell></row><row><cell>UMTRA</cell><cell cols="3">ProtoNet Train 41</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>ProtoTransfer hyperparameter summary.</figDesc><table><row><cell></cell><cell>in-domain</cell><cell></cell><cell cols="2">cross-domain</cell></row><row><cell>Hyperparameter</cell><cell cols="3">Omniglot mini-ImageNet mini-ImageNet</cell><cell>CUB</cell></row><row><cell>Model architecture</cell><cell>Conv-4</cell><cell>Conv-4</cell><cell cols="2">ResNet-10 Conv-4</cell></row><row><cell>Image input size</cell><cell>28 ? 28</cell><cell>84 ? 84</cell><cell cols="2">224 ? 224 84 ? 84</cell></row><row><cell>Optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Learning rate decay factor</cell><cell>0.5</cell><cell>0.5</cell><cell>/</cell><cell>0.5</cell></row><row><cell>Learning rate decay period</cell><cell>25,000</cell><cell>25,000</cell><cell>/</cell><cell>25,000</cell></row><row><cell>Support examples</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Augmented queries (Q)</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>Training batch size (N )</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>50</cell></row><row><cell>Augmentation appendix</cell><cell>A.3.3</cell><cell>A.3.2</cell><cell>A.3.1</cell><cell>A.3.2</cell></row><row><cell>Fine-tuning optimizer</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Fine-tuning learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Fine-tuning batch size</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Fine-tuning epochs</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>15</cell></row><row><cell>Fine-tune last layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fine-tune backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Accuracy (%) of methods on N -way K-shot classification tasks on Omniglot and a Conv-4 architecture. All results are reported with 95% confidence intervals over 600 randomly generated test episodes. Results style: best and second best. ? 0.64 96.48 ? 0.26 72.27 ? 0.47 89.08 ? 0.23 ? 0.35 98.83 ? 0.12 84.60 ? 0.32 96.29 ? 0.13 ProtoNet 97.70 ? 0.29 99.28 ? 0.10 94.40 ? 0.23 98.39 ? 0.08 Pre+Linear 94.30 ? 0.43 99.08 ? 0.10 86.05 ? 0.34 97.11 ? 0.11</figDesc><table><row><cell cols="3">Method (N,K)</cell><cell>(5,1)</cell><cell>(5,5)</cell><cell>(20,1)</cell><cell>(20,5)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Omniglot</cell></row><row><cell cols="3">Training (scratch)</cell><cell cols="4">52.50 ? 0.84 74.78 ? 0.69 24.91 ? 0.33 47.62 ? 0.44</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CACTUs-MAML</cell><cell cols="4">68.84 ? 0.80 87.78 ? 0.50 48.09 ? 0.41 73.36 ? 0.34</cell></row><row><cell cols="3">CACTUs-ProtoNet 1</cell><cell cols="4">68.12 ? 0.84 83.58 ? 0.61 47.75 ? 0.43 66.27 ? 0.37</cell></row><row><cell cols="2">UMTRA</cell><cell>2</cell><cell>83.80 ? -</cell><cell>95.43 ? -</cell><cell>74.25 ? -</cell><cell>92.12 ? -</cell></row><row><cell cols="3">AAL-ProtoNet 3</cell><cell cols="4">84.66 ? 0.70 89.14 ? 0.27 68.79 ? 1.03 74.28 ? 0.46</cell></row><row><cell cols="3">AAL-MAML++ 3</cell><cell cols="4">88.40 ? 0.75 97.96 ? 0.32 70.21 ? 0.86 88.32 ? 1.22</cell></row><row><cell>UFLST</cell><cell>4</cell><cell></cell><cell>97.03 ? -</cell><cell>99.19 ? -</cell><cell>91.28 ? -</cell><cell>97.37 ? -</cell></row><row><cell cols="4">ProtoTransfer (ours) 88.00 Supervised training</cell><cell></cell><cell></cell></row><row><cell cols="2">1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MAML 94.46 1</cell><cell></cell><cell></cell></row><row><cell cols="3">Hsu et al. (2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Khodadadeh et al. (2019)</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Antoniou and Storkey (2019)</cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Ji et al. (2019)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Accuracy (%) of methods on N -way K-shot classification tasks mini-Imagenet and a Conv-4 architecture. All results are reported with 95% confidence intervals over 600 randomly generated test episodes. Results style: best and second best. ? 0.70 45.03 ? 0.73 53.35 ? 0.59 56.72 ? 0.67 ULDA-ProtoNet 5 40.63 ? 0.61 55.41 ? 0.57 63.16 ? 0.51 65.20 ? 0.50 ULDA-MetaOptNet 5 40.71 ? 0.62 54.49 ? 0.58 63.58 ? 0.51 67.65 ? 0.48 ProtoTransfer (ours) 45.67 ? 0.79 62.99 ? 0.75 72.34 ? 0.58 77.22 ? 0.52 ? 0.77 62.13 ? 0.72 71.03 ? 0.69 75.54 ? 0.62 ProtoNet 46.44 ? 0.78 66.33 ? 0.68 76.73 ? 0.54 78.91 ? 0.57 Pre+Linear 43.87 ? 0.69 63.01 ? 0.71 75.46 ? 0.58 80.17 ? 0.51</figDesc><table><row><cell cols="2">Method (N,K)</cell><cell>(5,1)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,50)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mini-ImageNet</cell><cell></cell></row><row><cell cols="2">Training (scratch)</cell><cell cols="4">27.59 ? 0.59 38.48 ? 0.66 51.53 ? 0.72 59.63 ? 0.74</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CACTUs-MAML</cell><cell cols="4">39.90 ? 0.74 53.97 ? 0.70 63.84 ? 0.70 69.64 ? 0.63</cell></row><row><cell cols="2">CACTUs-ProtoNet 1</cell><cell cols="4">39.18 ? 0.71 53.36 ? 0.70 61.54 ? 0.68 63.55 ? 0.64</cell></row><row><cell>UMTRA</cell><cell>2</cell><cell>39.93 ? -</cell><cell>50.73 ? -</cell><cell>61.11 ? -</cell><cell>67.15 ? -</cell></row><row><cell cols="2">AAL-ProtoNet 3</cell><cell cols="2">37.67 ? 0.39 40.29 ? 0.68</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">AAL-MAML++ 3</cell><cell cols="2">34.57 ? 0.74 49.18 ? 0.47</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">UFLST 4 33.77 Supervised training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MAML 46.81 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hsu et al. (2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Khodadadeh et al. (2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Antoniou and Storkey (2019)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ji et al. (2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>5 Qin et al. (2020)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>24.05 ? 1.01 28.21 ? 1.15 29.32 ? 1.12 39.57 ? 0.57 49.50 ? 0.55 51.99 ? 0.52 Pre+Mean-Centroid * 26.31 ? 0.42 30.41 ? 0.46 34.68 ? 0.46 47.16 ? 0.54 56.40 ? 0.53 61.57 ? 0.66 Pre+Linear * 25.97 ? 0.41 31.32 ? 0.45 35.49 ? 0.45 48.11 ? 0.64 59.31 ? 0.48 66.48 ? 0.56 UMTRA-ProtoNet 24.94 ? 0.43 28.04 ? 0.44 29.88 ? 0.43 39.21 ? 0.53 44.62 ? 0.49 46.48 ? 0.47 UMTRA-ProtoTune 25.00 ? 0.43 30.41 ? 0.44 35.63 ? 0.48 38.47 ? 0.55 51.60 ? 0.54 60.12 ? 0.50 ProtoTransfer (ours) 26.71 ? 0.46 33.82 ? 0.48 39.35 ? 0.50 45.19 ? 0.56 59.07 ? 0.55 66.15 ? 0.57 73.29 ? 0.71 82.27 ? 0.57 80.48 ? 0.57 79.72 ? 0.67 88.15 ? 0.51 90.81 ? 0.43 Pre+Mean-Centroid * 82.21 ? 0.49 87.62 ? 0.34 88.24 ? 0.29 87.61 ? 0.47 93.87 ? 0.68 94.77 ? 0.34 Pre+Linear * 79.08 ? 0.61 87.64 ? 0.47 91.34 ? 0.37 89.25 ? 0.51 95.51 ? 0.31 97.68 ? 0.21 UMTRA-ProtoNet 74.91 ? 0.72 80.42 ? 0.66 82.24 ? 0.61 79.81 ? 0.65 86.84 ? 0.50 88.44 ? 0.46 UMTRA-ProtoTune 68.11 ? 0.70 81.56 ? 0.54 85.05 ? 0.50 82.67 ? 0.60 92.04 ? 0.43 95.46 ? 0.31 ProtoTransfer (ours) 75.62 ? 0.67 86.80 ? 0.42 90.46 ? 0.37 86.53 ? 0.56 95.06 ? 0.32 97.01 ? 0.26</figDesc><table><row><cell>Method</cell><cell>UnSup</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,50)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,50)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ChestX</cell><cell></cell><cell></cell><cell>ISIC</cell><cell></cell></row><row><cell cols="4">ProtoNet EuroSat</cell><cell></cell><cell></cell><cell>CropDiseases</cell><cell></cell></row><row><cell>ProtoNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*** Results from Guo et al. (2019)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Accuracy (%) of methods on N -way K-shot classification tasks on Mini-ImageNet with a Conv-4 architecture for different batch sizes, number of training queries (P ) and optional finetuning on target tasks (FT). UMTRA-MAML uses different augmentations. All results are reported with 95% confidence intervals over 600 randomly generated test episodes.Results style: best and second best. ? 0.53 53.78 ? 0.53 62.41 ? 0.49 64.40 ? 0.46 ProtoCLR ProtoNet 50 1 no 44.53 ? 0.60 62.88 ? 0.54 70.86 ? 0.48 73.93 ? 0.44 ProtoCLR ProtoNet 50 3 no 44.89 ? 0.58 63.35 ? 0.54 72.27 ? 0.45 74.31 ? 0.45 ProtoCLR ProtoNet 50 5 no 45.00 ? 0.57 63.17 ? 0.55 71.70 ? 0.48 73.98 ? 0.44 ProtoCLR ProtoNet 50 10 no 44.98 ? 0.58 62.56 ? 0.53 70.78 ? 0.48 73.69 ? 0.44 ProtoCLR ProtoTune 50 3 yes 45.67 ? 0.76 62.99 ? 0.75 72.34 ? 0.58 77.22 ? 0.52</figDesc><table><row><cell>Training</cell><cell>Testing</cell><cell cols="3">batch size P FT</cell><cell>(5,1)</cell><cell>(5,5)</cell><cell>(5,20)</cell><cell>(5,50)</cell></row><row><cell cols="2">UMTRA * MAML</cell><cell>5</cell><cell cols="2">1 yes</cell><cell>39.93 ? -</cell><cell>50.73 ? -</cell><cell>61.11 ? -</cell><cell>67.15 ? -</cell></row><row><cell>UMTRA</cell><cell>ProtoNet</cell><cell>5</cell><cell>1</cell><cell cols="2">no 39.17</cell><cell></cell></row></table><note>* Khodadadeh et al. (2019)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Accuracy (%) of methods on N -way K-shot classification tasks on Mini-ImageNet with a Conv-4 architecture when reducing the number of pre-training classes or images. All results are reported with 95% confidence intervals over 600 randomly generated test episodes. Results style: best and second best.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our code and pre-trained models are available at https://www.github.com/indy-lab/ProtoTransfer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work received support from the European Union's Horizon 2020 research and innovation programme under the Marie Sk?odowska-Curie grant agreement No. 754354.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09884</idno>
		<title level="m">Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.06045</idno>
		<title level="m">Self-Supervised Learning For Few-Shot Image Classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Closer Look at Few-shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 : 7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadi</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
	</analytic>
	<monogr>
		<title level="m">A Challenge Hosted by the International Skin Imaging Collaboration (ISIC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnout</forename><surname>Devos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grossglauser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13613</idno>
		<title level="m">Subspace Networks for Few-Shot Classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved Regularization of Convolutional Neural Networks With Cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-Shot Learning of Object Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Boosting Few-Shot Visual Learning with Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8059" to="8068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07200</idno>
		<title level="m">Tajana Rosing, and Rogerio Feris. A New Benchmark for Evaluation of Cross-Domain Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Eurosat: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2217" to="2226" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few-shot learning with metric-agnostic conditional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Art?m</forename><surname>Yankov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><forename type="middle">D</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">O</forename><surname>Hodas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised Learning via Meta-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2019 : 7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised Few-shot Learning via Selfsupervised Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12178</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Meta-Learning for Few-Shot Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10132" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet Classification With Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot Learning of Simple Visual Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">33</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical Contrastive Learning of Unsupervised Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-Supervised Generalisation with Meta-Auxiliary Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1677" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using Deep Learning for Image-Based Plant Disease Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salath?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Realistic Evaluation of Deep Semi-Supervised Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised Few-shot Learning via Distribution Shift-based Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiexin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05805</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimization as a Model for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2017 : International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-Learning for Semi-Supervised Few-Shot Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Conference on Learning Representations ICLR</title>
		<meeting>6th International Conference on Learning Representations ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">One-shot learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06065</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03560</idno>
		<title level="m">When Does Self-Supervision Improve Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Common Pigmented Skin Lesions. Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180161</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Matching Networks for One Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Random Erasing Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020 : The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint/>
	</monogr>
	<note>2020. 1. n02687172 aircraft carrier 2. n04251144 snorkel 3. n02823428 beer bottle 4. n03676483 lipstick 5. n03400231 frying pan 6. n03272010 electric guitar 7. n07613480 trifle 8. n03775546 mixing bowl 9. n03127925 crate 10. n04146614 school bus Each of the t-SNE plots in Figure 3 shows 500 randomly selected embedded images from within those classes</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<title level="m">A.5 Results With Full Confidence Intervals &amp; References</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
