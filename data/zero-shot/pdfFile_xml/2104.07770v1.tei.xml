<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
							<email>haojin.yang@hpi.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename><surname>Cloud</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Zhao</surname></persName>
							<email>zhaoyucheng.joe@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bytedance</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hasso Plattner Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AsymmNet: Towards ultralight convolution neural networks using asymmetrical bottlenecks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks (CNN) have achieved astonishing results in a large variety of applications. However, using these models on mobile or embedded devices is difficult due to the limited memory and computation resources. Recently, the inverted residual block becomes the dominating solution for the architecture design of compact CNNs. In this work, we comprehensively investigated the existing design concepts, rethink the functional characteristics of two pointwise convolutions in the inverted residuals. We propose a novel design, called asymmetrical bottlenecks. Precisely, we adjust the first pointwise convolution dimension, enrich the information flow by feature reuse, and migrate saved computations to the second pointwise convolution. Doing so we can further improve the accuracy without increasing the computation overhead. The asymmetrical bottlenecks can be adopted as a dropin replacement for the existing CNN blocks. We can thus create AsymmNet by easily stack those blocks according to proper depth and width conditions. Extensive experiments demonstrate that our proposed block design is more beneficial than the original inverted residual bottlenecks for mobile networks, especially useful for those ultralight CNNs within the regime of &lt;220M MAdds. Code is available at https://github.com/Spark001/AsymmNet * Equal contribution. This work is done when Haojin Yang and Yuncheng Zhao are with Alibaba Cloud.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The recent success of deep Convolution Neural Networks (CNN) is like the jewel in the crown of modern AI waves <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37]</ref>. However, the current CNN models are heavily relying on high-performance computation hardware, such as GPU and TPU, which are normally deployed in a cloud computing environment. Thus, the client applications have to transmit user data to the cloud to gain deep CNN models' benefits. This constraint strongly limits such models' applicability on resource-constrained devices, e.g., mobile phones, IoT devices, and embedded devices. Moreover, sending user data to a remote server increases the risk of privacy leakage. Therefore, in recent years, various works aim to solve this problem by reducing memory footprints and accelerating inference. We roughly categorize those works into following directions: network pruning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, knowledge distillation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref>, low-bit quantization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref>, and compact network designs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref>. The latter has been recognized as the most popular approach that has a massive impact on industrial applications. The compact networks achieved promising accuracy with generally fewer parameters and less computation. Although significant progress has been made, there is still much room for improvement, especially in the ultralight regime with multiply adds (MAdds) &lt;220M.</p><p>In this paper, our efforts focus on improving ultralight CNNs by the handcrafted design of basic building blocks. We first made a thorough investigation on existing block designs of mobile CNNs, and we argue that the two pointwise (PW) convolutions contribute differently in the original inverted residual bottleneck block. <ref type="bibr" target="#b4">[5]</ref> first proposes decoupling spatial correlation and channel correlation using the combination of a depthwise (DW) convolution and a PW convolution. <ref type="bibr" target="#b37">[38]</ref> further emphasises that the second PW convolution has essential characteristics in the inverted residual bottlenecks since it is responsible for learning new features from different channels, which is especially crucial for expressiveness. The first PW convolution is responsible for channel expansion, on the other hand. Therefore, we propose to partially reduce or migrate the first PW computations to the second one. By following this idea, we introduce a novel Asymmetrical Bottleneck Block, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c). Furthermore, we can create Asymm-Net by easily stack a sequence of asymmetrical blocks according to proper depth and width conditions. We only consider the handcrafted design of the network architecture in this work. However, the proposed CNN block is orthogonal to the recent approaches based on Neural Architecture Search (NAS) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34]</ref>. Experimental results show that, AsymmNet is especially superior in the ultralight CNN regime, and we have achieved promising results in image classification and other four downstream vision tasks. Summarized our core contributions in this paper are:</p><p>? We thoroughly investigated the existing mobile CNN designs and further proposed a novel asymmetrical bottleneck block.</p><p>? We propose AsymmNet based on asymmetrical bottlenecks, which achieves promising performance under the ultralight CNN regime (&lt;220M MAdds) on Im-ageNet classification and multiple downstream vision tasks.</p><p>The rest of the paper is organized as follows: Section 2 briefly review the related work. Subsequently, we present the proposed asymmetrical bottleneck design and Asymm-Net in Section 3, followed by experimental results and discussions (Section 4). Finally, Section 5 concludes the paper and provides an outlook on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section provides a thorough overview of the recent efforts in the research domain of model compression and compact network design.</p><p>In the model compression area, knowledge distillation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33]</ref> aims to generate small "student" networks trained by using distilled supervision signals derived from a cumbersome "teacher" network. The student network is expected to be more compact and as accurate as of the teacher. Connection pruning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> and channel pruning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref> respectively remove low-rank connections between neurons or weakly weighted channels for model shrinking and acceleration. Low-bit quantization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b0">1]</ref> is another crucial complementary approach to improve network efficiency through reduced precision arithmetic or even bitwise (binary) operators. Among them, Bethge et al. <ref type="bibr" target="#b0">[1]</ref> introduced a novel block design that suggests applying DenseNet <ref type="bibr" target="#b21">[22]</ref> style concatenation for preserving a rich information flow and a subsequent improvement block to update the newly added features, which can reduce the computation overhead as well. Our approach is also partially inspired by this concept.</p><p>The compact network methods use full precision floating point numbers as weights but reduce the total number of parameters and operations through compact architecture design while minimizing accuracy loss. The commonly used techniques include replacing a large portion of 3?3 filters with smaller 1?1 filters <ref type="bibr" target="#b23">[24]</ref>; Using depthwise separable convolution to reduce operations <ref type="bibr" target="#b4">[5]</ref>; Utilizing channel shuffling and group convolutions in addition to depthwise convolution <ref type="bibr" target="#b45">[46]</ref>. Among those approaches, the MobileNet series (V1-V3) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> are so far the most successful lightweight CNN models based on depthwise separable convolution and intelligent architecture design. Specifically, MobileNetV3 combines handcrafted block design and architecture search techniques. GhostNet <ref type="bibr" target="#b13">[14]</ref> adopts the network architecture of MobileNetV3 but proposed to use a computationally cheaper block design replacing the inverted bottleneck block. Zhou et al. <ref type="bibr" target="#b48">[49]</ref> proposed a sandglass block to replace the commonly used inverted bottleneck block, whilst better accuracy can be achieved compared to MobileNetV2 without increasing parameters and computation.</p><p>NAS techniques aim to automatically search efficient network architectures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34]</ref>. However, the most efficient basic building block design still requires human expertise <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b13">14]</ref>. Furthermore, such methods need to repeat the network design process and retrain the network from scratch for each setting, which will result in excessive energy consumption and CO 2 emission. E.g., a Transformer language model <ref type="bibr" target="#b42">[43]</ref> with NAS will cause CO 2 emission as much as 5 cars' lifetime <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we first revisit the existing building block designs for lightweight CNN models. We then introduce the proposed asymmetrical bottleneck block and AsymmNet, discuss the design concept and main differences compared to the existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Depthwise separable convolution is proposed by Chollet in the Xception network <ref type="bibr" target="#b4">[5]</ref>, which is way more efficient than other CNN networks at that time. Subsequently, this design has been applied in many lightweight CNN architectures such as MobileNet series <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20]</ref> and ShuffleNet series <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. It assumes that separately learning spatial and channel correlations should be more efficient and easier for a CNN learner. Specifically, it replaces a standard convolutional operator by splitting convolution into two separate operators (layers). The first one is called depthwise convolution, which adopts single channel filters to learn spatial correlations among locations within each channel separately. The second operator is a 1 ? 1 convolution, also served as pointwise convolution, which is utilized for learning new features through computing linear combinations across all the input channels. According to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, depthwise separable convolution can reduce computation overhead by a factor of around kernel size 2 compared to a standard convolution operator with the same kernel size. We also apply depthwise separable convolution in the proposed approach due to its computational efficiency.</p><p>(a) Bottleneck block <ref type="bibr" target="#b16">[17]</ref> (b) Inverted residual block <ref type="bibr" target="#b37">[38]</ref> (c) Sandglass block <ref type="bibr" target="#b48">[49]</ref> (d) ShuffleBlock v2 <ref type="bibr" target="#b30">[31]</ref> (e) Ghost module <ref type="bibr" target="#b13">[14]</ref> (f) Asymmetrical bottleneck <ref type="figure">Figure 1</ref>. Different types of basic convolution blocks. "Pwise" denotes 1 ? 1 pointwise convolution, "Dwise" denotes 3 ? 3 depthwise convolution. The dotted rectangles and arrows represent feature maps and feature reuse, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inverted bottleneck is proposed by Sandler et al. for</head><p>MobileNetV2 <ref type="bibr" target="#b37">[38]</ref>. Unlike the original bottleneck design <ref type="bibr" target="#b39">[40]</ref> (see <ref type="figure">Figure 1</ref>(a)), the inverted bottleneck block adopts a low-dimensional (the number of input channels) input tensor and expands it to a higher dimensional tensor using a pointwise convolution. The expanded high-dimensional tensor will then be fed into a depthwise separable convolution, by which the corresponding pointwise convolution generates low-dimensional new features by linearly combining channels after a depthwise convolution. It can be seen that the first pointwise convolution expands the information flow, which increases the capacity, and the subsequent convolution operators are responsible for the expressiveness of the proper layer. This speculation is derived based on the analysis of the block's capacity and expressiveness in <ref type="bibr" target="#b37">[38]</ref>. <ref type="figure">Figure 1</ref>(b) shows the design idea of an inverted bottleneck block.</p><p>Cheap operations for more features <ref type="table">Table 1</ref>  overhead is mainly concentrated on the pointwise convolution part, as e.g., 95% of MobileNetV1, 84.4% of Mo-bileNetV2 and 88.5% of MobileNetV3. If we want to reduce the computational complexity further, the optimization of this part of the network is the first choice. Han et al.</p><p>proposed to use the Ghost module (see Section 3.2) to replace the pointwise convolution layers and partially remove the depthwise convolution layers (only preserve those for downsampling). The core idea of the Ghost module is to generate more features using computationally cheaper operators. Our proposed design is also partially inspired by this concept, and we assume that the two pointwise convolutions contribute differently in the structural point of view. We thus shift the amount of calculation to the more important one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Revisit existing design</head><p>In this section, we review several commonly used design principles: original bottleneck block <ref type="bibr" target="#b16">[17]</ref>, inverted residual block <ref type="bibr" target="#b37">[38]</ref>, shuffleblock v2 <ref type="bibr" target="#b30">[31]</ref>, Ghost module <ref type="bibr" target="#b13">[14]</ref>, and Sandglass block <ref type="bibr" target="#b48">[49]</ref>. <ref type="figure">Figure 1</ref> demonstrates the specific properties of each design.</p><p>Bottleneck <ref type="figure">(Figure 1(a)</ref>) is the fundamental building block of ResNet <ref type="bibr" target="#b16">[17]</ref>, where a pointwise convolution reduces the feature dimension with the factor t, apply a 3 ? 3 convolution to the narrowed features, and then utilize another pointwise convolution to restore the feature dimension to be equal to the input size. The key difference between Inverted Residual block (also called MMBlock, see <ref type="figure">Figure  1</ref>(b)) and original Bottleneck is that the latter applies standard convolution on narrowed features, while MMBlock uses the first pointwise convolution to expand the feature dimension, and applies depthwise convolution on expanded features. It is so because standard 3 ? 3 convolutions are highly computational intensive in Bottlenecks. However, depthwise convolutions in MMBlock can significantly reduce the computational complexity. Thus, increasing the feature dimension will be beneficial for improving the representative capacity of the block.</p><p>Shuffleblock v2 is the following work of Shuffleblock v1 <ref type="bibr" target="#b45">[46]</ref>, in which the group convolution is removed for practical efficiency. Furthermore, the input feature map in Shuffleblock v2 is split into two equal channels of narrowed feature maps <ref type="figure">(Figure 1(d)</ref>). One is transformed with a special Bottleneck block without internal dimension changes (the solid arrow path on the right); The other (the dashed arrow path on the left) keeps unchanged until concatenated and shuffled with the transformed feature map. This design reveals that partially reusing the input features doesn't impair the expressiveness of the convolution blocks, but can effectively reduce computational complexity.</p><p>Ghost module <ref type="figure">(Figure 1(e)</ref>) is proposed to reduce redundancy of feature maps generated by pointwise convolutions in an MMBlock. Specifically, the amount of output channels of pointwise convolutions is reduced to make more room for integrating cheaper intrinsic features. To keep the output dimension consistent, a series of linear transformation such as depthwise convolutions is used for generating intrinsic features, which will be concatenated with the output of the pointwise convolution to form the final feature vector. <ref type="bibr" target="#b48">[49]</ref> propposed Sandglass block (see <ref type="figure">Figure 1(c)</ref>), which suggests keeping the standard bottleneck structure. It prefers to perform identity mapping and spatial transformation at a higher dimension to alleviate information loss and gradient confusion. Therefore, the sandglass block flips the position of depthwise and pointwise convolutions, which aims to preserve dense information flow and suppress the computation cost. Based on the previous findings, we argue that improving the capacity by using cheaper intrinsic features or even directly feature reuse is beneficial. We thus rethink the functional characteristics of two pointwise convolutions in the inverted residuals and propose a novel asymmetrical bottleneck described in the next section.  As demonstrated in <ref type="table">Table 1</ref>, pointwise (PW) convolution is the most computationally intensive part in inverted residual bottlenecks (see <ref type="figure" target="#fig_1">Figure 2(a)</ref>). The first PW convo- lution is adopted to expand the feature tensor's dimension and the second one is significant for learning feature correlations from different channels after the depthwise convolution (DW). We can figure out that the first PW expands the information flow, which increases the capacity, and the second PW convolution is mainly responsible for the expressiveness. We argue that cheaper transformations or even feature reuse can enhance the information flow, but learning channel correlations should not be simplified in relative terms. Therefore, we infer that the second PW has more essential characteristics in the structure. To verify our speculation, we first designed a pruned version (referred to as pruned block subsequently) based on inverted residual bottlenecks, as shown by <ref type="figure" target="#fig_1">Figure 2(b)</ref>. The output of the first PW is expressed by Eq. 1:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Asymmetrical bottlenecks</head><formula xml:id="formula_0">Input Operator Output h ? w ? c 1 ? 1,conv2d,non-linear h ? w ? (t ? r)c h ? w ? (t ? r)c Concat h ? w ? (t + r)c h ? w ? (t + r)c DW(k) s=s,non-linear h s ? w s ? (t + r)c h s ? w s ? (t + r)c 1 ? 1,conv2d,linear h s ? w s ? c</formula><formula xml:id="formula_1">Y pw1 = Concat(X, Y t?1 (X))<label>(1)</label></formula><p>where X ? R h?w?c denotes the input tensor, while h, w, and c denote the height, width, and channel dimension, respectively. Y t?1 ? R h?w?(t?1)?c is the output of the first PW, where t is an expansion factor introduced in <ref type="bibr" target="#b37">[38]</ref>. In the pruned block, we reduce the output channels of the first PW by c. Thus, the pruned block can be formulated as:</p><formula xml:id="formula_2">Y p = X + P W (DW (Concat(X, Y t?1 (X))))<label>(2)</label></formula><p>where we omit ReLU and BatchNorm for simplicity in the formulation. Pruned block can save computation, but at the same time, it also brings a small amount of accuracy loss. Therefore, we consider migrating the saved computation of the first PW to the second PW to construct an asymmetrical structure, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c). Experimental results show that the performance can be improved with this asymmetrical structure while the computation amount is basically unchanged. Mathematically, the asymmetrical bottleneck block can be expressed by Eq. 3:</p><formula xml:id="formula_3">Y = X + P W (DW (Concat(2r ? X, Y t?r (X))))<label>(3)</label></formula><p>where Y t?r ? R h?w?(t?r)?c is the output of the first PW, t denotes the expansion factor and r ? [0, t) is a new parameter which controls the asymmetry rate. To achieve a good trade-off between accuracy and efficiency, we set r to 1 in all experiments. If r = 0 it degenerates into an inverted residual bottleneck.  <ref type="table">Table 3</ref>. Specification for AsymmNet-L using MobileNetV3-large base. Each row shows a conv2d layer or an asymmetrical bottleneck block. c denotes the output channel size, k denotes the kernel size, and s is the stride number of the convolution layer. "Input" and "Operator" indicate the shape of the input tensor and the operator type. p denotes the expanded channel size of the corresponding asymmetrical bottleneck blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Computational complexity</head><p>Similar to MMBlock, the theoretical computation complexity of AsymmBlock is C = C pw1 + C dw + C pw2 . For simplicity, we only calculate the blocks whose stride = 1. So the theoretical complexity ratio of AsymmBlock and MM-Block can be calculated as </p><formula xml:id="formula_4">= 1 + rk 2 2tc + k 2 t ? 1.</formula><p>(4) where t denotes the expansion factor, r represents the asymmetry rate, and k indicates the kernel size of DW convolution. We set r = 1 in our experiments and k 2 c. R c ? 1 means that the AsymmBlock can transfer the computation cost from the first PW to the second and keep total complexity roughly unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">AsymmNet</head><p>We further develop several efficient CNN architectures based on the proposed asymmetrical bottleneck design. To gain the best practical benefits, we follow the basic network architecture of MobileNetV3-large and MobileNetV3-small <ref type="bibr" target="#b19">[20]</ref>, as shown in <ref type="table">Table 3</ref> and  our choice is that the core hyper-parameters such as kernel size, expand size, and network depth of MobileNetV3 are determined through a NAS algorithm and exhaustive search process. Our efforts thus mainly focus on the manual improvement of the basic block design. For a fair comparison, we keep those automatically selected hyper-parameters unchanged. Therefore, the main building blocks of AsymmNet consists of a sequence of stacked asymmetrical bottleneck blocks, which gradually downsample the feature map resolution and increase the channel number to maintain the whole network's information capacity. We consider the presented architecture in this work as a basic design, while we believe that the automatic architecture and hyper-parameter search methods can further boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section presents detailed experimental results. We first evaluate the model performance on the ImageNet classification task under various complexity (MAdds) settings. We further validate the generalization ability and effectiveness of the proposed approach on four downstream tasks, including face recognition, action recognition, pose estimation, and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment setup</head><p>We utilize the deep learning framework MXNet <ref type="bibr" target="#b3">[4]</ref> and the off-the-shelf toolbox Gluon-CV <ref type="bibr" target="#b17">[18]</ref> to implement our models. We use the standard SGD optimizer for model training with both decay and momentum of 0.9 and the weight decay is 3e-5. We use the cosine learning rate scheduler with the initial learning rate of 2.6 for eight GPUs. The corresponding batch size was set to 256. Without special declaration, we train all the models for 360 epochs, in which five epochs are conducted for a warm-up phase. Detailed configurations can be found in our open-source codes 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image classification 4.2.1 Compare to MobileNetV3</head><p>This section extensively studies the advantages of the proposed AsymmNet and pruned model over MobileNetV3 (MBV3) on the ImageNet ILSVRC 2012 dataset <ref type="bibr" target="#b7">[8]</ref>. As shown in <ref type="table" target="#tab_5">Table 5</ref>, we compare their performance under multiple complexity settings by tuning the weight multiplier. We consider both V3-large and V3-small architecture as references and comprehensively evaluated the classification accuracy, computation complexity (MAdds), and inference latency. Doing so can help reveal the performance advantage of the full spectrum of model architecture configurations. We applied the DNN inference toolkit MNN <ref type="bibr" target="#b24">[25]</ref> for the latency evaluation since MNN is specifically optimized for mobile devices. All the inference tests have been done on an Android phone equipped with a Qualcomm snapdragon-855 CPU with 8G RAM in the single thread modus. We average the latency results of 1000 runs for each model. To conduct a fair comparison, we replace the corresponding convolution blocks and keep all the hyperparameters unchanged.</p><p>We can figure out that, AsymmNet outperforms Mo-bileNetV3 on classification accuracy in almost all the complexity settings, while the pruned model demonstrates better efficiency with slight accuracy drops. However, the accuracy loss becomes negligible when the MAdds getting smaller or even reversed, e.g., pruned model outperforms MobileNetV3 by 3.4% at the level of 0.35-Small. Specifically, when the model gets smaller and less complex, the accuracy advantage of AsymmNet becomes more apparent. This phenomenon effectively reveals the proposed asymmetrical bottleneck block's superiority in the spectrum of extremely lightweight models (a regime &lt;220M MAdds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Ablation study on asymmetry rate</head><p>We evaluate the asymmetry rate r on the ImageNet dataset to obtain the best choice in terms of accuracy and complexity. <ref type="table">Table 6</ref> shows the result, where r = 1 demonstrates the best trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face recognition</head><p>Face recognition is a crucial identity authentication technology used in many mobile or embedded applications such as mobile payment and device unlock. In this subsection, we employ AsymmNet-L and the proposed AsymmNet-s as network backbone for face recognition. Following Mobile-FaceNet <ref type="bibr" target="#b2">[3]</ref>, we use a global depthwise convolution layer  <ref type="table">Table 6</ref>. Ablation study on asymmetry rate r using ImageNet dataset. We apply both large and small AsymmNet for this evaluation. r = 1 shows the best performance.</p><p>rather than a global average pooling layer to output discriminative feature vectors. Both models are trained on MS-Celeb-1M <ref type="bibr" target="#b12">[13]</ref> dataset from scratch by ArcFace <ref type="bibr" target="#b8">[9]</ref> loss, for a fair comparison between them. The input image size is 112 ? 112. We report result on different dataset including LFW <ref type="bibr" target="#b22">[23]</ref>, CALFW <ref type="bibr" target="#b47">[48]</ref>, CPLFW <ref type="bibr" target="#b46">[47]</ref>, AgeDB-30 <ref type="bibr" target="#b31">[32]</ref>, and VGGFace2 <ref type="bibr" target="#b1">[2]</ref> as in <ref type="table">Table 7</ref>. As shown in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Action recognition</head><p>Action recognition has drawn a significant amount of attention from the academic community, owing to its applications in many areas like security and behavior analysis. We evaluate and compare AsymmNet and MobileNetV3  as feature extractors for action recognition following <ref type="bibr" target="#b43">[44]</ref> on the HMDB51 dataset <ref type="bibr" target="#b25">[26]</ref>. The input image size is 224 ? 224. As summarized in <ref type="table">Table 8</ref>, AsymmNet achieves superior results compared to MobileNetV3 at both large and small scales. Furthermore, it reaches an accuracy close to ResNet50 v1b <ref type="bibr" target="#b11">[12]</ref>, while its MAdds is about 19 times smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Pose estimation</head><p>There has been significant progress in pose estimation and increasing interest in pose tracking in recent years. Thus, we evaluated AsymmNet as the backbone on this task by using the challenging COCO human pose benchmark <ref type="bibr" target="#b27">[28]</ref>. Our approach is based on the SimplePose model <ref type="bibr" target="#b44">[45]</ref>, which estimates heat maps from deep and low-resolution feature maps. We replace the backbone with mobile CNN models and evaluate the accuracy for a fair comparison. The input image size is 256 ? 192. The test set results are given in <ref type="table" target="#tab_8">Table 9</ref>. Our pose estimation results with AsymmNet-s backbone surpass MBV3-s in all three metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Object detection</head><p>We apply AsymmNet as a drop-in replacement for the original backbone in YOLO-v3 detector <ref type="bibr" target="#b35">[36]</ref>. We compare our results to MobileNetV3 on the PASCAL VOC dataset <ref type="bibr" target="#b9">[10]</ref> on object detection. We change the base model of the adopted YOLO-v3 architecture and train our models on the  <ref type="table" target="#tab_9">Table 10</ref> illustrates the results compared to other models based on MobileNet backbones. AsymmNet-s outperforms MBV3-s on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In this paper, we introduced a novel design for ultralight CNN models. We investigated important design choices, redesigned two pointwise convolutions of the inverted residual block, and developed a novel asymmetrical bottleneck. We can see that AsymmNet has a consistent accuracy advantage over MobileNetV3 in the ultralight model regime through our experiments on a series of downstream tasks. It thus can be used as a practical complementary approach to existing state-of-the-art CNN models in the regime of &lt;220M MAdds.</p><p>However, we observed that AsymmBlock also has its limitations. For instance, with the increase of MAdds, it can not continue to show the advantage of accuracy, as the comparison result with MobileNetV3-1.25. Also, the AsymmNet-L model does not demonstrate benefits in the object detection task. One possible explanation is that the current AsymmNet architecture is based on MBV3, which is searched using MMBlock that not necessarily the most suitable architecture for AsymmNet. Thus, we will continue to optimize it in future work. As the next step, we will combine automatic search techniques with asymmetrical bottlenecks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>We would like to thank Chao Qian for his valuable technical support on inference speed evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Detailed illustration of the inverted residual block, pruned block, and asymmetrical bottleneck block. Brown fillings represent the feature maps generated by convolutions, while white fillings denote feature map reuse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>R c = hwc(tc ? rc) + k 2 hw(tc + rc) + hw(tc + rc)c hwc(tc) + k 2 hw(tc) + hw(tc)c = 2hwtc 2 + k 2 hwtc + hwrck 2 2hwtc 2 + k 2 hwtc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Asymmetrical bottleneck block with stride s, asymmetry rate r, and expansion factor t.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The main reason forNo.</figDesc><table><row><cell></cell><cell>Input</cell><cell>Operator</cell><cell>k</cell><cell>p</cell><cell>c</cell><cell>s</cell></row><row><cell>1</cell><cell>224 2 ? 3</cell><cell>conv2d</cell><cell>3</cell><cell>-</cell><cell>16</cell><cell>2</cell></row><row><cell>2</cell><cell>112 2 ? 16</cell><cell>asymm-bneck</cell><cell>3</cell><cell>16</cell><cell>16</cell><cell>2</cell></row><row><cell>3</cell><cell>56 2 ? 16</cell><cell>asymm-bneck</cell><cell>3</cell><cell>72</cell><cell>24</cell><cell>2</cell></row><row><cell>4 5</cell><cell>28 2 ? 24</cell><cell>asymm-bneck</cell><cell>3 5</cell><cell>88 96</cell><cell>24 40</cell><cell>1 2</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell>5</cell><cell>240</cell><cell>40</cell><cell>1</cell></row><row><cell>7</cell><cell>14 2 ? 40</cell><cell>asymm-bneck</cell><cell>5</cell><cell>240</cell><cell>40</cell><cell>1</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell>5</cell><cell>120</cell><cell>48</cell><cell>1</cell></row><row><cell>9 10</cell><cell>14 2 ? 48</cell><cell>asymm-bneck</cell><cell>5 5</cell><cell>144 288</cell><cell>48 96</cell><cell>1 2</cell></row><row><cell>11 12</cell><cell>7 2 ? 96</cell><cell>asymm-bneck</cell><cell>5</cell><cell>576</cell><cell>96</cell><cell>1</cell></row><row><cell>13</cell><cell>7 2 ? 96</cell><cell>conv2d</cell><cell>1</cell><cell>-</cell><cell>576</cell><cell>1</cell></row><row><cell>14</cell><cell>7 2 ? 576</cell><cell>avgpool</cell><cell>7</cell><cell>-</cell><cell>-</cell><cell>1</cell></row><row><cell>15</cell><cell>1 ? 576</cell><cell>conv2d</cell><cell>1</cell><cell>-</cell><cell>1024</cell><cell>1</cell></row><row><cell>16</cell><cell>1 ? 1024</cell><cell>conv2d</cell><cell>1</cell><cell>-</cell><cell>1000</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Specification for AsymmNet-S using MobileNetV3-small base.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison between AsymmNet, Pruned model, and MobileNetV3 across a large variety of scale levels. We consider both V3-Large and V3-small architecture as references and comprehensively evaluated the accuracy, computation complexity (MAdds), and inference efficiency. Top-1 accuracy is on the ImageNet dataset, and all latency are obtained by averaging the inference time of 1000 executions on a Qualcomm snapdragon-855 CPU with 8G RAM in the single thread modus. To conduct a fair comparison, we replace the corresponding convolution blocks and keep all the hyper-parameters unchanged.</figDesc><table><row><cell cols="4">Multiplier Model Scale</cell><cell cols="2">Networks</cell><cell>Top-1 Acc (%) MAdds (M) Params (M) Latency (ms)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AsymmNet</cell><cell>65.4</cell><cell>43</cell><cell>2.2</cell><cell>7.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Large</cell><cell cols="2">Pruned</cell><cell>63.3</cell><cell>36.9</cell><cell>2.1</cell><cell>5.2</cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell cols="3">MBV3 AsymmNet</cell><cell>64.2 55</cell><cell>40 15</cell><cell>2.2 1.7</cell><cell>6.3 3.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Small</cell><cell cols="2">Pruned</cell><cell>53.2</cell><cell>13.6</cell><cell>1.7</cell><cell>2.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MBV3</cell><cell>49.8</cell><cell>12</cell><cell>1.4</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AsymmNet</cell><cell>69.2</cell><cell>67.2</cell><cell>2.8</cell><cell>10.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Large</cell><cell cols="2">Pruned</cell><cell>68.3</cell><cell>59</cell><cell>2.6</cell><cell>7.1</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell cols="3">MBV3 AsymmNet</cell><cell>68.8 58.9</cell><cell>69 20.6</cell><cell>2.6 1.9</cell><cell>8.8 4.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Small</cell><cell cols="2">Pruned</cell><cell>57.3</cell><cell>18.6</cell><cell>1.9</cell><cell>3.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MBV3</cell><cell>58</cell><cell>21</cell><cell>1.6</cell><cell>3.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AsymmNet</cell><cell>73.5</cell><cell>142.1</cell><cell>4.2</cell><cell>19.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Large</cell><cell cols="2">Pruned</cell><cell>72.6</cell><cell>125.3</cell><cell>3.8</cell><cell>13.6</cell></row><row><cell></cell><cell>0.75</cell><cell></cell><cell cols="3">MBV3 AsymmNet</cell><cell>73.3 65.6</cell><cell>155 40.8</cell><cell>4 2.5</cell><cell>16.2 6.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Small</cell><cell cols="2">Pruned</cell><cell>64</cell><cell>36.9</cell><cell>2.3</cell><cell>6.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MBV3</cell><cell>65.4</cell><cell>44</cell><cell>2</cell><cell>6.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AsymmNet</cell><cell>75.4</cell><cell>216.9</cell><cell>5.99</cell><cell>27.1</cell></row><row><cell></cell><cell></cell><cell cols="2">Large</cell><cell cols="2">Pruned</cell><cell>74.9</cell><cell>193.6</cell><cell>5.3</cell><cell>19.5</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell cols="3">MBV3 AsymmNet</cell><cell>75.2 68.4</cell><cell>216.5 57.7</cell><cell>5.4 3.1</cell><cell>23.3 8.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Small</cell><cell cols="2">Pruned</cell><cell>67</cell><cell>52.5</cell><cell>2.6</cell><cell>7.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MBV3</cell><cell>67.5</cell><cell>57</cell><cell>2.5</cell><cell>8.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">AsymmNet</cell><cell>76.4</cell><cell>349.8</cell><cell>8.3</cell><cell>38.8</cell></row><row><cell></cell><cell></cell><cell cols="2">Large</cell><cell cols="2">Pruned</cell><cell>76.1</cell><cell>311</cell><cell>7.2</cell><cell>29.2</cell></row><row><cell></cell><cell>1.25</cell><cell></cell><cell cols="3">MBV3 AsymmNet</cell><cell>76.6 70.6</cell><cell>356 91.7</cell><cell>7.5 3.9</cell><cell>34.9 12.7</cell></row><row><cell></cell><cell></cell><cell cols="2">Small</cell><cell cols="2">Pruned</cell><cell>69.8</cell><cell>83.1</cell><cell>3.5</cell><cell>11.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MBV3</cell><cell>70.4</cell><cell>91</cell><cell>3.6</cell><cell>11.7</cell></row><row><cell>Scale</cell><cell>r</cell><cell>Top-1 Acc (%)</cell><cell cols="2">MAdds (M)</cell><cell cols="2">Params (M)</cell></row><row><cell></cell><cell>0</cell><cell>75.2</cell><cell>216.6</cell><cell></cell><cell>5.4</cell></row><row><cell>Large</cell><cell>1</cell><cell>75.4</cell><cell>216.9</cell><cell></cell><cell>5.9</cell></row><row><cell></cell><cell>2</cell><cell>74.8</cell><cell>217.3</cell><cell></cell><cell>6.6</cell></row><row><cell></cell><cell>0</cell><cell>67.4</cell><cell>56.9</cell><cell></cell><cell>2.9</cell></row><row><cell>Small</cell><cell>1</cell><cell>68.4</cell><cell>57.7</cell><cell></cell><cell>3.1</cell></row><row><cell></cell><cell>2</cell><cell>68.0</cell><cell>58.5</cell><cell></cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Pose estimation results on COCO human pose dataset using SimplePose method. All the AP results are in percentage.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Object detection results on the PASCAL VOC dataset using YOLO-v3 detector. combination of VOC2007 trainval and VOC2012 trainval, test on VOC2007 test set. The input image size is 416?416.</figDesc><table><row><cell>Backbone</cell><cell cols="3">mAP (%) MAdds(G) Params(M)</cell></row><row><cell>AsymmNet-s</cell><cell>69.80</cell><cell>7.99</cell><cell>9.33</cell></row><row><cell>MBV3-s</cell><cell>68.98</cell><cell>7.99</cell><cell>9.16</cell></row><row><cell>AsymmNet-L</cell><cell>76.18</cell><cell>8.58</cell><cell>11.97</cell></row><row><cell>MBV3-L</cell><cell>76.64</cell><cell>8.58</cell><cell>11.39</cell></row><row><cell>MobileNetV1[12]</cell><cell>75.8</cell><cell>9.92</cell><cell>11.83</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Spark001/AsymmNet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meliusnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05936</idno>
		<title level="m">Can binary neural networks achieve mobilenet-level accuracy</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Biometric Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="428" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moonshine: Distilling with cheap convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2888" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gluoncv and gluonnlp: Deep learning in computer vision and natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Lausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database forstudying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mnn: A universal and efficient inference engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<editor>MLSys</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03488</idno>
		<title level="m">Reactnet: Towards precise binary neural network with generalized activation functions</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="722" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stylianos</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasios</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05668</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02243</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyue</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking bottleneck structure for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
