<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARADE: Passage Representation Aggregation for Document Reranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
							<email>licanjia17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
							<email>ayates@mpi-inf.mpg.desean@ir.cs.georgetown.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">IR Lab</orgName>
								<orgName type="institution">Georgetown University</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
							<email>benhe@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
							<email>yfsun@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARADE: Passage Representation Aggregation for Document Reranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretrained transformer models, such as BERT and T5, have shown to be highly effective at ad-hoc passage and document ranking. Due to inherent sequence length limits of these models, they need to be run over a document's passages, rather than processing the entire document sequence at once. Although several approaches for aggregating passage-level signals have been proposed, there has yet to be an extensive comparison of these techniques. In this work, we explore strategies for aggregating relevance signals from a document's passages into a final ranking score. We find that passage representation aggregation techniques can significantly improve over techniques proposed in prior work, such as taking the maximum passage score. We call this new approach PARADE. In particular, PARADE can significantly improve results on collections with broad information needs where relevance signals can be spread throughout the document (such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation techniques may work better on collections with an information need that can often be pinpointed to a single passage (such as TREC DL and TREC Genomics). We also conduct efficiency analyses, and highlight several strategies for improving transformer-based aggregation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained language models (PLMs), such as BERT <ref type="bibr" target="#b18">[19]</ref>, ELEC-TRA <ref type="bibr" target="#b11">[12]</ref> and T5 <ref type="bibr" target="#b58">[59]</ref>, have achieved state-of-the-art results on standard ad-hoc retrieval benchmarks. The success of PLMs mainly relies on learning contextualized representations of input sequences using the transformer encoder architecture <ref type="bibr" target="#b67">[68]</ref>. The transformer uses a self-attention mechanism whose computational complexity is quadratic with respect to the input sequence's length. Therefore, PLMs generally limit the sequence's length (e.g., to 512 tokens) to reduce computational costs. Consequently, when applied to the ad-hoc ranking task, PLMs are commonly used to predict the relevance of passages or individual sentences <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b79">80]</ref>. The max or -max passage scores (e.g., top 3) are then aggregated to produce a document relevance score. Such approaches have achieved state-of-the-art results on a variety of ad-hoc retrieval benchmarks. * This work was conducted while the author was an intern at the Max Planck Institute for Informatics. Documents are often much longer than a single passage, however, and intuitively there are many types of relevance signals that can only be observed in a full document. For example, the Verbosity Hypothesis <ref type="bibr" target="#b59">[60]</ref> states that relevant excerpts can appear at different positions in a document. It is not necessarily possible to account for all such excerpts by considering only the top passages. Similarly, the ordering of passages itself may affect a document's relevance; a document with relevant information at the beginning is intuitively more useful than a document with the information at the end <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref>. Empirical studies support the importance of full-document signals. Wu et al. study how passage-level relevance labels correspond to document-level labels, finding that more relevant documents also contain a higher number of relevant passages <ref type="bibr" target="#b72">[73]</ref>. Additionally, experiments suggest that aggregating passage-level relevance scores to predict the document's relevance score outperforms the common practice of using the maximum passage score (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref>).</p><p>On the other hand, the amount of non-relevant information in a document can also be a signal, because relevant excerpts would make up a large fraction of an ideal document. IR axioms encode this idea in the first length normalization constraint (LNC1), which states that adding non-relevant information to a document should decrease its score <ref type="bibr" target="#b20">[21]</ref>. Considering a full document as input has the potential to incorporate signals like these. Furthermore, from the perspective of training a supervised ranking model, the common practice of applying document-level relevance labels to individual passages is undesirable, because it introduces unnecessary noise into the training process.</p><p>In this work, we provide an extensive study on neural techniques for aggregating passage-level signals into document scores. We study how PLMs like BERT and ELECTRA can be applied to the ad-hoc document ranking task while preserving many documentlevel signals. We move beyond simple passage score aggregation strategies (such as Birch <ref type="bibr" target="#b79">[80]</ref>) and study passage representation aggregation. We find that aggregation over passage representations using architectures like CNNs and transformers outperforms passage score aggregation. Since the utilization of the full-text increases memory requirements, we investigate using knowledge distillation to create smaller, more efficient passage representation aggregation models that remain effective. In summary, our contributions are:</p><p>? The formalization of passage score and representation aggregation strategies, showing how they can be trained end-to-end, arXiv:2008.09093v2 [cs.IR] 10 Jun 2021</p><p>? A thorough comparison of passage aggregation strategies on a variety of benchmark datasets, demonstrating the value of passage representation aggregation, ? An analysis of how to reduce the computational cost of transformer-based representation aggregation by decreasing the model size, ? An analysis of how the effectiveness of transformer-based representation aggregation is influenced by the number of passages considered, and ? An analysis into dataset characteristics that can influence which aggregation strategies are most effective on certain benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We review four lines of related research related to our study. Contextualized Language Models for IR. Several neural ranking models have been proposed, such as DSSM <ref type="bibr" target="#b33">[34]</ref>, DRMM <ref type="bibr" target="#b23">[24]</ref>, (Co-)PACRR <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, (Conv-)KNRM <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b73">74]</ref>, and TK <ref type="bibr" target="#b30">[31]</ref>. However, their contextual capacity is limited by relying on pre-trained unigram embeddings or using short n-gram windows. Benefiting from BERT's pre-trained contextual embeddings, BERT-based IR models have been shown to be superior to these prior neural IR models. We briefly summarize related approaches here and refer the reader to a survey on transformers for text ranking by Lin et al. <ref type="bibr" target="#b45">[46]</ref> for further details. These approaches use BERT as a relevance classifier in a cross-encoder configuration (i.e., BERT takes both a query and a document as input). Nogueira et al. first adopted BERT to passage reranking tasks <ref type="bibr" target="#b55">[56]</ref> using BERT's [CLS] vector. Birch <ref type="bibr" target="#b79">[80]</ref> and BERT-MaxP <ref type="bibr" target="#b16">[17]</ref> explore using sentence-level and passage-level relevance scores from BERT for document reranking, respectively. CEDR proposed a joint approach that combines BERT's outputs with existing neural IR models and handled passage aggregation via a representation aggregation technique (averaging) <ref type="bibr" target="#b52">[53]</ref>. In this work, we further explore techniques for passage aggregation and consider an improved CEDR variant as a baseline. We focus on the underexplored direction of representation aggregation by employing more sophisticated strategies, including using CNNs and transformers. Other researchers trade off PLM effectiveness for efficiency by utilizing the PLM to improve document indexing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b57">58]</ref>, precomputing intermediate Transformer representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>, using the PLM to build sparse representations <ref type="bibr" target="#b51">[52]</ref>, or reducing the number of Transformer layers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>Several works have recently investigated approaches for improving the Transformer's efficiency by reducing the computational complexity of its attention module, e.g., Sparse Transformer <ref type="bibr" target="#b10">[11]</ref> and Longformer <ref type="bibr" target="#b3">[4]</ref>. QDS-Transformer tailors Longformer to the ranking task with query-directed sparse attention <ref type="bibr" target="#b37">[38]</ref>. We note that representation-based passage aggregation is more effective than increasing the input text size using the aforementioned models, but representation aggregation could be used in conjunction with such models.</p><p>Passage-based Document Retrieval. Callan first experimented with paragraph-based and window-based methods of defining passages <ref type="bibr" target="#b6">[7]</ref>. Several works drive passage-based document retrieval in the language modeling context <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>, indexing context <ref type="bibr" target="#b46">[47]</ref>, and learning to rank context <ref type="bibr" target="#b62">[63]</ref>. In the realm of neural networks, HiNT demonstrated that aggregating representations of passage level relevance can perform well in the context of pre-BERT models <ref type="bibr" target="#b19">[20]</ref>. Others have investigated sophisticated evidence aggregation approaches <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref>. Wu et al. explicitly modeled the importance of passages based on position decay, passage length, length with position decay, exact match, etc <ref type="bibr" target="#b72">[73]</ref>. In a contemporaneous study, they proposed a model that considers passage-level representations of relevance in order to predict the passage-level cumulative gain of each passage <ref type="bibr" target="#b71">[72]</ref>. In this approach the final passage's cumulative gain can be used as the document-level cumulative gain. Our approaches share some similarities, but theirs differs in that they use passage-level labels to train their model and perform passage representation aggregation using a LSTM. Representation Aggregation Approaches for NLP. Representation learning has been shown to be powerful in many NLP tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">50]</ref>. For pre-trained language models, a text representation is learned by feeding the PLM with a formatted text like</p><formula xml:id="formula_0">[CLS] TextA [SEP] or [CLS] TextA [SEP] TextB [SEP].</formula><p>The vector representation of the prepended [CLS] token in the last layer is then regarded as either a text overall representation or a text relationship representation. Such representations can also be aggregated for tasks that requires reasoning from multiple scopes of evidence. Gear aggregates the claim-evidence representations by max aggregator, mean aggregator, or attention aggregator for fact checking <ref type="bibr" target="#b82">[83]</ref>. Transformer-XH uses extra hop attention that bears not only in-sequence but also inter-sequence information sharing <ref type="bibr" target="#b81">[82]</ref>. The learned representation is then adopted for either question answering or fact verification tasks. Several lines of work have explored hierarchical representations for document classification and summarization, including transformer-based approaches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b80">81]</ref>. In the context of ranking, SMITH <ref type="bibr" target="#b75">[76]</ref>, a long-to-long text matching model, learns a document representation with hierarchical sentence representation aggregation, which shares some similarities with our work. Rather than learning independent document (and query) representations, SMITH is a bi-encoder approach that learns separate representations for each. While such approaches have efficiency advantages, current bi-encoders do not match the effectiveness of cross-encoders, which are the focus of our work <ref type="bibr" target="#b45">[46]</ref>. Knowledge Distillation. Knowledge distillation is the process of transferring knowledge from a large model to a smaller student model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref>. Ideally, the student model performs well while consisting of fewer parameters. One line of research investigates the use of specific distilling objectives for intermediate layers in the BERT model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b63">64]</ref>, which is shown to be effective in the IR context <ref type="bibr" target="#b8">[9]</ref>. Turc et al. pre-train a family of compact BERT models and explore transferring task knowledge from large fine-tuned models <ref type="bibr" target="#b66">[67]</ref>. Tang et al. distill knowledge from the BERT model into Bi-LSTM <ref type="bibr" target="#b65">[66]</ref>. Tahami et al. propose a new cross-encoder architecture and transfer knowledge from this model to a bi-encoder model for fast retrieval <ref type="bibr" target="#b64">[65]</ref>. Hofst?tter et al. also proposes a cross-architecture knowledge distillation framework using a Margin Mean Squared Error loss in a pairwise training manner <ref type="bibr" target="#b27">[28]</ref>. We demonstrate the approach in <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref> can be applied to our proposed representation aggregation approach to improve efficiency without substantial reductions in effectiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we formalize approaches for aggregating passage representations into document ranking scores. We make the distinction between the passage score aggregation techniques explored in prior work with passage representation aggregation (PARADE) techniques, which have received less attention in the context of document ranking. Given a query and a document , a ranking method aims to generate a relevance score ( , ) that estimates to what degree document satisfies the query . As described in the following sections, we perform this relevance estimation by aggregating passage-level relevance representations into a document-level representation, which is then used to produce a relevance score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Creating Passage Relevance Representations</head><p>As introduced in Section 1, a long document cannot be considered directly by the BERT model 1 due to its fixed sequence length limitation. As in prior work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>, we split a document into passages that can be handled by BERT individually. To do so, a sliding window of 225 tokens is applied to the document with a stride of 200 tokens, formally expressed as = { 1 , . . . , } where is the number of passages. Afterward, these passages are taken as input to the BERT model for relevance estimation. Following prior work <ref type="bibr" target="#b55">[56]</ref>, we concatenate a query and passage pair with a [SEP] token in between and another [SEP] token at the end. The special [CLS] token is also prepended, in which the corresponding output in the last layer is parameterized as a relevance representation ? R , denoted as follows:</p><formula xml:id="formula_1">= BERT( , )<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Score vs Representation Aggregation</head><p>Previous approaches like BERT-MaxP <ref type="bibr" target="#b16">[17]</ref> and Birch <ref type="bibr" target="#b79">[80]</ref> use a feedforward network to predict a relevance score from each passage representation , which are then aggregated into a document relevance score with a score aggregation approach. <ref type="figure">Figure 1a</ref> illustrates common score aggregation approaches like max pooling ("MaxP"), sum pooling, average pooling, and k-max pooling. Unlike score aggregation approaches, our proposed representation aggregation approaches generate an overall document relevance representation by aggregating passage representations directly (see <ref type="figure">Figure 1b</ref>). We describe the representation aggregators in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregating Passage Representations</head><p>Given the passage relevance representations = { 1 , . . . , }, PARADE summarizes into a single dense representation ? R in one of several different ways, as illustrated in <ref type="figure">Figure 2</ref>.</p><p>PARADE-Max utilizes a robust max pooling operation on the passage relevance features 2 in . As widely applied in Convolution Neural Network, max pooling has been shown to be effective in obtaining position-invariant features <ref type="bibr" target="#b61">[62]</ref>. Herein, each element at index in is obtained by a element-wise max pooling operation on the passage relevance representations over the same index.</p><formula xml:id="formula_2">[ ] = max( 1 [ ], . . . , [ ])</formula><p>(2) PARADE-Attn assumes that each passage contributes differently to the relevance of a document to the query. A simple yet effective way to learn the importance of a passage is to apply a feed-forward network to predict passage weights:</p><formula xml:id="formula_3">1 , . . . , = softmax( 1 , . . . , ) (3) = ?? =1 (4)</formula><p>where softmax is the normalization function and ? R is a learnable weight.</p><p>For completeness of study, we also introduce a PARADE-Sum that simply sums the passage relevance representations. This can be regarded as manually assigning equal weights to all passages (i.e., = 1). We also introduce another PARADE-Avg that is combined with document length normalization(i.e., = 1/ ). PARADE-CNN, which operates in a hierarchical manner, stacks Convolutional Neural Network (CNN) layers with a window size of ? 2 and a stride of 2. In other words, the CNN filters operate on every pair of passage representations without overlap. Specifically, we stack 4 layers of CNN, which halve the number of representations in each layer, as shown in <ref type="figure">Figure 2b</ref>.</p><p>PARADE-Transformer enables passage relevance representations to interact by adopting the transformer encoder <ref type="bibr" target="#b67">[68]</ref> in a hierarchical way. Specifically, BERT's [CLS] token embedding and all are concatenated, resulting in an input = ( , 1 , . . . , ) that is consumed by transformer layers to exploit the ordering of and dependencies among passages. That is,</p><formula xml:id="formula_4">? = LayerNorm( + MultiHead( ) (5) +1 = LayerNorm(? + FFN(?))<label>(6)</label></formula><p>where LayerNorm is the layer-wise normalization as introduced in <ref type="bibr" target="#b2">[3]</ref>, MultiHead is the multi-head self-attention <ref type="bibr" target="#b67">[68]</ref>, and FFN is a two-layer feed-forward network with a ReLu activation in between. As shown in <ref type="figure">Figure 2c</ref>, the [CLS] vector of the last Transformer output layer, regarded as a pooled representation of the relevance between query and the whole document, is taken as .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generating the Relevance Score</head><p>For all PARADE variants except PARADE-CNN, after obtaining the final embedding, a single-layer feed-forward network (FFN) is adopted to generate a relevance score, as follows:</p><formula xml:id="formula_5">( , ) =<label>(7)</label></formula><p>where ? R is a learnable weight. For PARADE-CNN, a FFN with one hidden layer is applied to every CNN representation, and the final score is determined by the sum of those FFN output scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Aggregation Complexity</head><p>We note that the computational complexity of representation aggregation techniques are dominated by the passage processing itself. In  <ref type="table" target="#tab_0">Table 1</ref>. Note that the average document length is obtained only from the documents returned by BM25. Documents in GOV2 and Genomics are much longer than Robust04, making it more challenging to train an end-to-end ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare PARADE against the following traditional and neural baselines, including those that employ other passage aggregation techniques. <ref type="table">Table 2</ref>: Ranking effectiveness of PARADE on the Robust04 and GOV2 collection. Best performance is in bold. Significant difference between PARADE-Transformer and the corresponding method is marked with ? ( &lt; 0.05, two-tailed paired -test). We also report the current best-performing model on Robust04 (T5-3B from <ref type="bibr" target="#b56">[57]</ref>). BM25 is an unsupervised ranking model based on IDF-weighted counting <ref type="bibr" target="#b60">[61]</ref>. The documents retrieved by BM25 also serve as the candidate documents used with reranking methods.</p><formula xml:id="formula_6">Robust04 Title Robust04 Description GOV2 Title GOV2 Description MAP P@20 nDCG@20 MAP P@20 nDCG@20 MAP P@20 nDCG@20 MAP P@20 nDCG@20 BM25 0.2531 ? 0.3631 ? 0.4240 ? 0.2249 ? 0.3345 ? 0.4058 ? 0.3056 ? 0.5362 ? 0.4774 ? 0.2407 ? 0.4705 ? 0.4264 ? BM25+RM3 0.3033 ? 0.3974 ? 0.4514 ? 0.2875 ? 0.3659 ? 0.4307 ? 0.3350 ? 0.5634 ? 0.4851 ? 0.2702 ? 0.4993 ? 0.4219 ? Birch 0.3763 0.4749 ? 0.5454 ? 0.4009 ? 0.5120 ? 0.5931 ? 0.3406 ? 0.6154 ? 0.5520 ? 0.3270 0.6312 ? 0.5763 ? ELECTRA-MaxP 0.3183 ? 0.4337 ? 0.4959 ? 0.3464 ? 0.4731 ? 0.5540 ? 0.3193 ? 0.5802 ? 0.5265 ? 0.2857 ? 0.5872 ? 0.5319 ? T5-3B (from [57]) - - - 0.4062 - 0.6122 - - - - - - ELECTRA-KNRM 0.3673 ? 0.4755 ? 0.5470 ? 0.4066 0.5255 0.6113 0.3469 ? 0.6342 ? 0.5750 ? 0.3269 0.6466 0.5864 ? CEDR-KNRM (Max) 0.3701 ? 0.4769 ? 0.5475 ? 0.3975 ? 0.5219 0.6044 ? 0.3481 ? 0.6332 ? 0.5773 ? 0.3354 ? 0.6648 0.6086 PARADE-Avg 0.3352 ? 0.4464 ? 0.5124 ? 0.3640 ? 0.4896 ? 0.5642 ? 0.3174 ? 0.6225 ? 0.5741 ? 0.2924 ? 0.6228 ? 0.5710 ? PARADE-Sum 0.3526 ? 0.4711 ? 0.5385 ? 0.3789 ? 0.5100 ? 0.5878 ? 0.3268 ? 0.6218 ? 0.5747 ? 0.3075 ? 0.6436 ? 0.5879 ? PARADE-Max 0.3711 ? 0.4723 ? 0.5442 ? 0.3992 ? 0.5217 0.6022 0.3352 ? 0.6228 ? 0.5636 ? 0.3160 ? 0.6275 ? 0.5732 ? PARADE-Attn 0.3462 ? 0.4576 ? 0.5266 ? 0.3797 ? 0.5068 ? 0.5871 ? 0.3306 ? 0.6359 ? 0.5864 ? 0.3116 ? 0.</formula><p>BM25+RM3 is a query expansion model based on RM3 <ref type="bibr" target="#b42">[43]</ref>. We used Anserini's <ref type="bibr" target="#b76">[77]</ref> implementations of BM25 and BM25+RM3. Documents are indexed and retrieved with the default settings for keywords queries. For description queries, we set = 0.6 and changed the number of expansion terms to 20.</p><p>Birch aggregates sentence-level evidence provided by BERT to rank documents <ref type="bibr" target="#b79">[80]</ref>. Rather than using the original Birch model provided by the authors, we train an improved "Birch-Passage" variant. Unlike the original model, Birch-Passage uses passages rather than sentences as input, it is trained end-to-end, it is fine-tuned on the target corpus rather than being applied zero-shot, and it does not interpolate retrieval scores with the first-stage retrieval method. These changes bring our Birch variant into line with the other models and baselines (e.g., using passages inputs and no interpolating), and they additionally improved effectiveness over the original Birch model in our pilot experiments.</p><p>ELECTRA-MaxP adopts the maximum score of passages within a document as an overall relevance score <ref type="bibr" target="#b16">[17]</ref>. However, rather than fine-tuning BERT-base on a Bing search log, we improve performance by fine-tuning on the MSMARCO passage ranking dataset. We also use the more recent and efficient pre-trained ELECTRA model rather than BERT.</p><p>ELECTRA-KNRM is a kernel-pooling neural ranking model based on query-document similarity matrix <ref type="bibr" target="#b73">[74]</ref>. We set the kernel size as 11. Different from the original work, we use the embeddings from the pre-trained ELECTRA model for model initialization.</p><p>CEDR-KNRM (Max) combines the advantages from both KNRM and pre-trained model <ref type="bibr" target="#b52">[53]</ref>. It digests the kernel features learned from KNRM and the [CLS] representation as ranking feature. We again replace the BERT model with the more effective ELECTRA. We also use a more effective variant that performs max-pooling on the passages' [CLS] representations, rather than averaging.</p><p>T5-3B defines text ranking in a sequence-to-sequence generation context using the pre-trained T5 model <ref type="bibr" target="#b56">[57]</ref>. For document reranking task, it utilizes the same score max-pooling technique as in BERT-MaxP <ref type="bibr" target="#b16">[17]</ref>. Due to its large size and expensive training, we present the values reported by <ref type="bibr" target="#b56">[57]</ref> in their zero-shot setting, rather than training it ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>To prepare the ELECTRA model for the ranking task, we first finetune ELECTRA on the MSMARCO passage ranking dataset <ref type="bibr" target="#b54">[55]</ref>. The fine-tuned ELECTRA model is then used to initialize PA-RADE's PLM component. For PARADE-Transformer we use two randomly initialized transformer encoder layers with the same hyperparemeters (e.g., number of attention heads, hidden size, etc.) used by BERT-base. Training of PARADE and the baselines was performed on a single Google TPU v3-8 using a pairwise hinge loss. We use the Tensorflow implementation of PARADE available in the Capreolus toolkit <ref type="bibr" target="#b78">[79]</ref>, and a standalone imiplementation is also available 8 . We train on the top 1,000 documents returned by a first-stage retrieval method; documents that are labeled relevant in the ground-truth are taken as positive samples and all other documents serve as negative samples. We use BM25+RM3 for first-stage retrieval on Robust04 and BM25 on the other datasets with parameters tuned on the dev sets via grid search. We train for 36 "epochs" consisting of 4,096 pairs of training examples with a learning rate of 3e-6, warm-up over the first ten epochs, and a linear decay rate of 0.1 after the warm-up. Due to its larger memory requirements, we use a batch size of 16 with CEDR and a batch size of 24 with all other methods. Each instance comprises a query and all split passages in a document. We use a learning rate of 3e-6 with warm-up over the first 10 proportions of training steps.</p><p>Documents are split into a maximum of 16 passages. As we split the documents using a sliding window of 225 tokens with a stride of 200 tokens, a maximum number of 3,250 tokens in each document are retained. The maximum passage sequence length is set as 256. Documents with fewer than the maximum number of passages are padded and later masked out by passage level masks. For documents </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>Following prior work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b52">53]</ref>, we use 5-fold cross-validation. We set the reranking threshold to 1000 on the test fold as trade-off between latency and effectiveness. The reported results are based on the average of all test folds. Performance is measured in terms of the MAP, Precision, ERR and nDCG ranking metrics using trec_eval 9 with different cutoff. For NTCIR WWW-3, the results are reported using NTCIREVAL 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Main Results</head><p>The reranking effectiveness of PARADE on the two commonly-used Robust04 and GOV2 collections is shown in <ref type="table">Table 2</ref>. Considering the three approaches that do not introduce any new weights, PARADE-Max is usually more effective than PARADE-Avg and PARADE-Sum, though the results are mixed on GOV2. PARADE-Max is consistently better than PARADE-Attn on Robust04, but PARADE-Attn sometimes outperforms PARADE-Max on GOV2. The two variants that consume passage representations in a hierarchical manner, PARADE-CNN and PARADE-Transformer, consistently outperforms the four other variants. This confirms the effectiveness of our proposed passage representation aggregation approaches. Considering the baseline methods, PARADE-Transformer significantly outperforms the Birch and ELECTRA-MaxP score aggregation approaches for most metrics on both collections. PARADE-Transformer's ranking effectiveness is comparable with T5-3B on the Robust04 collection while using only 4% of the parameters, though it is worth noting that T5-3B is being used in a zero-shot setting. CEDR-KNRM and ELECTRA-KNRM, which both use  <ref type="table" target="#tab_2">Table 3</ref>. We first observe that this is a surprisingly challenging task for neural models. Unlike Robust04 and GOV2, where transformer-based models are clearly state-of-the-art, we observe that all of the methods we consider almost always underperform a simple BM25 baseline, and they perform well below the best-performing TREC submission. It is unclear whether this is due to the specialized domain, the smaller amount of training data, or some other factor. Nevertheless, we observe some interesting trends. First, we see that PARADE approaches can outperform score aggregation baselines. However, we note that statistical significance can be difficult to achieve on this dataset, given the small sample size (64 queries). Next, we notice that PARADE-Max performs the best among neural methods. This is in contrast with what we observed on Robust04 and GOV2, and suggests that hierarchically aggregating evidence from different passages is not required on the Genomics dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results on the TREC DL Track and NTCIR WWW-3 Track</head><p>We additionally study the effectiveness of PARADE on the TREC DL Track and NTCIR WWW-3 Track. We report results in this section and refer the readers to the TREC and NTCIR task papers for details on the specific hyperparameters used <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Results from the TREC Deep Learning Track are shown in Table 4. In TREC DL'19, we include comparisons with competitive runs from TREC: ucas_runid1 <ref type="bibr" target="#b9">[10]</ref> used BERT-MaxP <ref type="bibr" target="#b16">[17]</ref> as the reranking method, TUW19-d3-re <ref type="bibr" target="#b29">[30]</ref> is a Transformerbased non-BERT method, and idst_bert_r1 <ref type="bibr" target="#b74">[75]</ref> utilizes struct-BERT <ref type="bibr" target="#b70">[71]</ref>, which is intended to strengthen the modeling of sentence   <ref type="table">Table 2</ref>. We explore this further in Section 5.4. Results from the NTCIR WWW-3 Track are shown in <ref type="table" target="#tab_4">Table 5</ref>. KASYS-E-CO-NEW-1 is a Birch-based method <ref type="bibr" target="#b79">[80]</ref> that uses BERT-Large and Technion-E-CO-NEW-1 is a cluster-based method. As shown in <ref type="table" target="#tab_4">Table 5</ref>, PARADE-Transformer's effectiveness is comparable with KASYS-E-CO-NEW-1 across metrics. On this benchmark, PARADE-Transformer outperforms PARADE-Max by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS</head><p>In this section, we consider the following research questions:</p><p>? RQ1: How does PARADE perform compare with transformers that support long text? ? RQ2: How can BERT's efficiency be improved while maintaining its effectiveness? ? RQ3: How does the number of document passages preserved influence effectiveness? ? RQ4: When is the representation aggregation approach preferable to score aggregation?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Long-Text Transformers (RQ1)</head><p>Recently, a line of research focuses on reducing the redundant computation cost in the transformer block, allowing models to support longer sequences. Most approaches design novel sparse attention mechanism for efficiency, which makes it possible to input longer documents as a whole for ad-hoc ranking. We consider the results reported by Jiang et al. <ref type="bibr" target="#b37">[38]</ref> to compare some of these approaches with passage representation aggregation. The results are shown in <ref type="table" target="#tab_5">Table 6</ref>. In this comparison, long-text transformer approaches achieve similar effectiveness and underperform PARADE-Transformer by a large margin. However, it is worth noting that these approaches use the CLS representation as features for a downstream model rather than using it to predict a relevance score directly, which may contribute to the difference in effectiveness. A larger study using the various approaches in similar configurations is needed to draw conclusions. For example, it is possible that QDS-Transformer's effectiveness would increase when trained with maximum score aggregation; this approach could also be combined with PARADE to handle documents longer than Longformer's maximum input length of 2048 tokens. Our approach is less efficient than that taken by the Longformer family of models, so we consider the question of how to improve PARADE's efficiency in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Reranking Effectiveness vs. Efficiency (RQ2)</head><p>While BERT-based models are effective at producing high-quality ranked lists, they are computationally expensive. However, the reranking task is sensitive to efficiency concerns, because documents must be reranked in real time after the user issues a query. In this section we consider two strategies for improving PARADE's efficiency.</p><p>Using a Smaller BERT Variant. As smaller models require fewer computations, we study the reranking effectiveness of PARADE when using pre-trained BERT models of various sizes, providing guidance for deploying a retrieval system. To do so, we use the pre-trained BERT provided by Turc et al. <ref type="bibr" target="#b66">[67]</ref>. In this analysis we change several hyperparameters to reduce computational requirements: we rerank the top 100 documents from BM25, train with a cross-entropy loss using a single positive or negative document, reduce the passage length 150 tokens, and reduce the stride to 100 tokens. We additionally use BERT models in place of ELECTRA so that we can consider models with LM distillation (i.e., distillation using self-supervised PLM objectives), which Gao et al. <ref type="bibr" target="#b21">[22]</ref> found to be more effective than ranker distillation alone (i.e., distillation using a supervised ranking objective). From <ref type="table" target="#tab_6">Table 7</ref>, it can be seen that as the size of models is reduced, their effectiveness decline monotonously. The hidden layer size (#6 vs #7, #8 vs #9) plays a more critical role for performance than the number of layers (#3 vs #4, #5 vs #6). An example is the comparison between models #7 and #8. Model #8 performs better; it has fewer layers but contains more parameters. The number of parameters and inference time are also given in <ref type="table" target="#tab_6">Table 7</ref> to facilitate the study of trade-offs between model complexity and effectiveness.</p><p>Distilling Knowledge from a Large Model. To further explore the limits of smaller PARADE models, we apply knowledge distillation to leverage knowledge from a large teacher model. We use PARADE-Transformer trained with BERT-Base on the target collection as the  teacher model. Smaller student models then learn from the teacher at the output level. We use mean squared error as the distilling objective, which has been shown to work effectively <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>. The learning objective penalizes the student model based on both the ground-truth and the teacher model:</p><formula xml:id="formula_7">= ? + (1 ? ) ? || ? || 2<label>(8)</label></formula><p>where is the cross-entropy loss with regard to the logit of the student model and the ground truth, weights the importance of the learning objectives, and and are logits from the teacher model and student model, respectively.</p><p>As shown in <ref type="table" target="#tab_6">Table 7</ref>, the nDCG@20 of distilled models always increases. The PARADE model using 8 layers (#4) can achieve comparable results with the teacher model. Moreover, the PARADE model using 10 layers (#3) can outperform the teacher model with 11% fewer parameters. The PARADE model trained with BERT-Small achieves a nDCG@20 above 0.5, which outperforms BERT-MaxP using BERT-Base, while requiring only 1.14 ms to perform inference on one document. Thus, when reranking 100 documents, the inference time for each query is approximately 0.114 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Number of Passages Considered (RQ3)</head><p>One hyper-parameter in PARADE is the maximum number of passages being used, i.e., preserved data size, which is studied to answer RQ3 in this section. We consider title queries on the GOV2 dataset given that these documents are longer on average than in Robust04. We use the same hyperparameters as in Section 5.2. <ref type="figure" target="#fig_2">Figure 3</ref> depicts nDCG@20 of PARADE-Transformer with the number of passages varying from 8 to 64. Generally, larger preserved data size results in better performance for PARADE-Transformer, which suggests that a document can be better understood from document-level context with more preservation of its content. For PARADE-Max and PARADE-Attn, however, the performance degrades a little when using 64 passages. Both max pooling (Max) and simple attention mechanism (Attn) have limited capacity and are challenged when dealing with such longer documents. The PARADE-Transformer model is able to improve nDCG@20 as the number of passages increases, demonstrating its superiority in detecting relevance when documents become much longer.</p><p>However, considering more passages also increases the number of computations performed. One advantage of the PARADE models is that the number of parameters remains constant as the number of passages in a document varies. Thus, we consider the impact of varying the number of passages considered between training and inference. As shown in <ref type="table" target="#tab_7">Table 8</ref>, rows indicate the number of passages considered at training time while columns indicate the number used to perform inference. The diagonal indicates that preserving more of the passages in a document consistently improves nDCG.</p><p>Similarly, increasing the number of passages considered at inference time (columns) or at training time (rows) usually improves nDCG. In conclusion, the number of passages considered plays a crucial role in PARADE's effectiveness. When trading off efficiency for effectiveness, PARADE models' effectiveness can be improved by training on more passages than will be used at inference time. This generally yields a small nDCG increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">When is the representation aggregation</head><p>approach preferable to score aggregation? We test this hypothesis by using passage-level relevance judgments to compare the number of highly relevant passages per document in various collections. To do so, we use mappings between relevant passages and documents for those collections with passagelevel judgments available: TREC DL, TREC Genomics, and GOV2. We create a mapping between the MS MARCO document and passage collections by using the MS MARCO Question Answering (QnA) collection to map passages to document URLs. This mapping can then be used to map between passage and document judgments in DL'19 and DL'20. With DL'19, we additionally use the FIRA passage relevance judgments <ref type="bibr" target="#b32">[33]</ref> to map between documents and passages. The FIRA judgments were created by asking annotators to identify relevant passages in every DL'19 document with a relevance label of 2 or 3 (i.e., the two highest labels). Our mapping covers nearly the entire MS MARCO collection, but it is limited by the fact that DL's passage-level relevance judgments may not be complete. The FIRA mapping covers only highly-relevant DL'19 documents, but the passage annotations are complete and it was created by human annotators with quality control. In the case of TREC Genomics, we use the mapping provided by TREC. For GOV2, we use the sentence-level relevance judgments available in WebAP <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, which cover 82 queries.</p><p>We compare passage judgments across collections by using each collection's annotation guidelines to align their relevance labels with MS MARCO's definition of a relevant passage as one that is sufficient to answer the question query. With GOV2 we consider passages with a relevance label of 3 or 4 to be relevant. With DL documents we consider a label of 2 or 3 to be relevant and passages with a label of 3 to be relevant. With FIRA we consider label 3 to be relevant. With Genomics we consider labels 1 or 2 to be relevant.</p><p>We align the maximum passage lengths in GOV2 to FIRA's maximum length so that they can be directly compared. To do so, we convert GOV2's sentence judgments to passage judgments by collapsing sentences following a relevant sentence into a single passage with a maximum passage length of 130 tokens, as used by FIRA <ref type="bibr" target="#b10">11</ref> . We note that this process can only decrease the number of relevant passages per document observed in GOV2, which we expect to have the highest number. With the DL collections using the MS MARCO mapping, the passages are much smaller than these lengths, so collapsing passages could only decrease the number of relevant passages per document. We note that Genomics contains "natural" passages that can be longer; this should be considered when drawing conclusions. In all cases, the relevant passages comprise a small fraction of the document.</p><p>In each collection, we calculate the number of relevant passages per document using the collection's associated document and passage judgments. The results are shown in <ref type="table" target="#tab_9">Table 9</ref>. First, considering the GOV2 and MS MARCO collections that we expect to lie at opposite ends of the spectrum, we see that 38% of GOV2 documents contain a single relevant passage, whereas 98-99% of MS MARCO documents contain a single relevant passage. This confirms that MS MARCO documents contain only 1-2 highly relevant passages per document by nature of the collection's construction. The percentages are the lowest on GOV2 as expected. While we would prefer to put these percentages in the context of another collection like Robust04, the lack of passage-level judgments on such collections prevents us from doing so. Second, considering the Deep Learning collections, we see that DL'19 and DL'20 exhibit similar trends regardless of whether our mapping or the FIRA mapping is used. In these collections, the majority of documents contain a single relevant passage and the vast majority of documents contain one or two relevant passages. We call this a "maximum passage bias." The fact that the queries are shared with MS MARCO likely contributes to this observation, since we know the vast majority of MS MARCO question queries can be answered by a single passage. Third, considering Genomics 2006, we see that this collection is similar to the DL collections. The majority of documents contain only one relevant passage, and the vast majority contain one or two relevant passages. Thus, this analysis supports our hypothesis that the difference in PARADE-Transformer's effectiveness across collections is related to the number of relevant passages per document in these collections. PARADE-Max performs better when the number is low, which may reflect the reduced importance of aggregating relevance signals across passages on these collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed the PARADE end-to-end document reranking model and demonstrated its effectiveness on ad-hoc benchmark collections.</p><p>Our results indicate the importance of incorporating diverse relevance signals from the full text into ad-hoc ranking, rather than basing it on a single passage. We additionally investigated how   0.7391 0.7589 0.6132 0.3993 <ref type="table" target="#tab_0">Table 12</ref>: Ranking effectiveness of different retrieval systems in the TREC-COVID Round 4.</p><p>In response to the urgent demand for reliable and accurate retrieval of COVID-19 academic literature, TREC has been developing the TREC-COVID challenge to build a test collection during the pandemic <ref type="bibr" target="#b68">[69]</ref>. The challenge uses the CORD-19 data set <ref type="bibr" target="#b69">[70]</ref>, which is a dynamic collection enlarged over time. There are supposed to be 5 rounds for the researchers to iterate their systems. TREC develops a set of COVID-19 related topics, including queries (key-word based), questions, and narratives. A retrieval system is supposed to generate a ranking list corresponding to these queries.</p><p>We began submitting PARADE runs to TREC-COVID from Round 2. By using PARADE, we are able to utilize the full-text of the COVID-19 academic papers. We used the question topics since it works much better than other types of topics. In all rounds, we employ the PARADE-Transformer model. In Round 3, we additionally tested PARADE-Attn and a combination of PARADE-Transformer and PARADE-Attn using reciprocal rank fusion <ref type="bibr" target="#b12">[13]</ref>.</p><p>Results from TREC-COVID Rounds 2-4 are shown in <ref type="table" target="#tab_0">Table 10</ref>, <ref type="table" target="#tab_0">Table 11, and Table 12</ref>, respectively. <ref type="bibr" target="#b11">12</ref> In Round 2, PARADE achieves the highest nDCG, further supporting its effectiveness. <ref type="bibr" target="#b12">13</ref> In Round 3, our runs are not as competitive as the previous round. One possible reason is that the collection doubles from Round 2 to Round 3, which can introduce more inconsistencies between training and testing data as we trained PARADE on Round 2 data and tested on Round 3 data. In particular, our run mpiid5_run3 performed poorly. We found that it tends to retrieve more documents that are not likely to be included in the judgment pool. When considering the bpref metric that takes only the judged documents into account, its performance is comparable to that of the other variants. As measured by nDCG, PARADE's performance improved in Round 4 ( <ref type="table" target="#tab_0">Table 12</ref>), but is again outperformed by other approaches. It is worth noting that the PARADE runs were created by single models (excluding the fusion run from Round 3), whereas e.g. the UPrrf38rrf3-r4 run in Round 4 is an ensemble of more than 20 runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Conference' 17 ,</head><label>17</label><figDesc>July 2017, Washington, DC, USA 2021. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Comparison between score aggregation approaches and PARADE's representation aggregation mechanism. (a) Max, Avg, Max, and Attn Aggregators (b) CNN Aggregator (c) Transformer Aggregator Representation aggregators take passages' [CLS] representations as inputs and output a final document representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Reranking effectiveness of PARADE-Transformer when different number of passages are being used on Gov2 title dataset. nDCG@20 is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Collection statistics. (There are 43 test queries in DL'19 and 45 test queries in DL'20.)    We experiment with several ad-hoc ranking collections. Robust04 3 is a newswire collection used by the TREC 2004 Robust track. GOV2 4 is a web collection crawled from US government websites used in the TREC Terabyte 2004-06 tracks. For Robust04 and GOV2, we consider both keyword (title) queries and description queries in our experiments. The Genomics dataset<ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> consists of scientific articles from the Highwire Press 5 with natural-language queries about specific genes, and was used in the TREC Genomics 2006-07 track. The MSMARCO document ranking dataset 6 is a large-scale collection and is used in TREC 2019-20 Deep Learning Tracks<ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. To create document labels for the development and training sets, passage-level labels from the MSMARCO passage dataset are transferred to the corresponding source document that contained the passage. In other words, a document is considered relevant as long as it contains a relevant passage, and each query can be satisfied by a single passage. The ClueWeb12-B13 dataset 7 is a large-scale collection crawled from the web between February 10, 2012 and May 10, 2012. It is used for the NTCIR We Want Web 3 (WWW-3) Track[? ]. The statistics of these datasets are shown in</figDesc><table><row><cell>Collection</cell><cell cols="3"># Queries # Documents # tokens / doc</cell></row><row><cell>Robust04</cell><cell>249</cell><cell>0.5M</cell><cell>0.7K</cell></row><row><cell>GOV2</cell><cell>149</cell><cell>25M</cell><cell>3.8K</cell></row><row><cell>Genomics</cell><cell>64</cell><cell>162K</cell><cell>6.5K</cell></row><row><cell>MSMARCO</cell><cell>43/45</cell><cell>3.2M</cell><cell>1.3K</cell></row><row><cell>ClueWeb12-B13</cell><cell>80</cell><cell>52M</cell><cell>1.9K</cell></row><row><cell cols="4">the case of PARADE-Max, Attn, and Sum, the methods are inex-</cell></row><row><cell cols="4">pensive. For PARADE-CNN and PARADE-Transformer, there are</cell></row><row><cell cols="4">inherently fewer passages in a document than total tokens, and (in</cell></row><row><cell cols="4">practice) the aggregation network is shallower than the transformer</cell></row><row><cell cols="2">used for passage modeling.</cell><cell></cell><cell></cell></row><row><cell cols="2">4 EXPERIMENTS</cell><cell></cell><cell></cell></row><row><cell>4.1 Datasets</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ranking effectiveness on the Genomics collection. Significant difference between PARADE-Transformer and the corresponding method is marked with ? ( &lt; 0.05, two-tailed paired -test). The top neural results are listed in bold, and the top overall scores are underlined.</figDesc><table><row><cell></cell><cell>MAP</cell><cell>P@20</cell><cell>nDCG@20</cell></row><row><cell>BM25</cell><cell>0.3108</cell><cell>0.3867</cell><cell>0.4740</cell></row><row><cell>TREC Best</cell><cell>0.3770</cell><cell>0.4461</cell><cell>0.5810</cell></row><row><cell>Birch</cell><cell>0.2832</cell><cell>0.3711</cell><cell>0.4601</cell></row><row><cell>BioBERT-MaxP</cell><cell>0.2577</cell><cell>0.3469</cell><cell>0.4195  ?</cell></row><row><cell>BioBERT-KNRM</cell><cell>0.2724</cell><cell>0.3859</cell><cell>0.4605</cell></row><row><cell>CEDR-KNRM (Max)</cell><cell>0.2486</cell><cell cols="2">0.3516  ? 0.4290</cell></row><row><cell>PARADE-Avg</cell><cell cols="2">0.2514  ? 0.3602</cell><cell>0.4381</cell></row><row><cell>PARADE-Sum</cell><cell cols="2">0.2579  ? 0.3680</cell><cell>0.4483</cell></row><row><cell>PARADE-Max</cell><cell>0.2972</cell><cell cols="2">0.4062  ? 0.4902</cell></row><row><cell>PARADE-Attn</cell><cell cols="2">0.2536  ? 0.3703</cell><cell>0.4468</cell></row><row><cell>PARADE-CNN</cell><cell>0.2803</cell><cell>0.3820</cell><cell>0.4625</cell></row><row><cell cols="2">PARADE-Transformer 0.2855</cell><cell>0.3734</cell><cell>0.4652</cell></row></table><note>longer than required, the first and last passages are always kept while the remaining are uniformly sampled as in [17].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ranking effectiveness on TREC DL Track document ranking task. PARADE's best result is in bold. The top overall result of of each track is underlined.</figDesc><table><row><cell cols="3">Year Group Runid</cell><cell cols="2">MAP nDCG@10</cell></row><row><cell></cell><cell></cell><cell>BM25</cell><cell>0.237</cell><cell>0.517</cell></row><row><cell>2019</cell><cell>TREC</cell><cell>ucas_runid1 [10] TUW19-d3-re [30] idst_bert_r1 [75]</cell><cell>0.264 0.271 0.291</cell><cell>0.644 0.644 0.719</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">PARADE-Max PARADE-Transformer 0.274 0.287</cell><cell>0.679 0.650</cell></row><row><cell></cell><cell></cell><cell>BM25</cell><cell>0.379</cell><cell>0.527</cell></row><row><cell></cell><cell></cell><cell>bcai_bertb_docv</cell><cell>0.430</cell><cell>0.627</cell></row><row><cell></cell><cell>TREC</cell><cell>fr_doc_roberta</cell><cell>0.442</cell><cell>0.640</cell></row><row><cell>2020</cell><cell></cell><cell>ICIP_run1</cell><cell>0.433</cell><cell>0.662</cell></row><row><cell></cell><cell></cell><cell>d_d2q_duo</cell><cell>0.542</cell><cell>0.693</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">PARADE-Max PARADE-Transformer 0.403 0.420</cell><cell>0.613 0.601</cell></row></table><note>some form of representation aggregation, are significantly worse than PARADE-Transformer on title queries and have comparable effectiveness on description queries. Overall, PARADE-CNN and PARADE-Transformer are consistently among the most effective approaches, which suggests the importance of performing complex representation aggregation on these datasets. Results on the Genomics dataset are shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ranking effectiveness of PARADE on NTCIR WWW-3 task. PARADE's best result is in bold. The best result of the Track is underlined.</figDesc><table><row><cell>Model</cell><cell cols="3">nDCG@10 Q@10 ERR@10</cell></row><row><cell>BM25</cell><cell>0.5748.</cell><cell>0.5850</cell><cell>0.6757</cell></row><row><cell>Technion-E-CO-NEW-1</cell><cell>0.6581</cell><cell>0.6815</cell><cell>0.7791</cell></row><row><cell>KASYS-E-CO-NEW-1</cell><cell>0.6935</cell><cell>0.7123</cell><cell>0.7959</cell></row><row><cell>PARADE-Max</cell><cell>0.6337</cell><cell>0.6556</cell><cell>0.7395</cell></row><row><cell>PARADE-Transformer</cell><cell>0.6897</cell><cell>0.7016</cell><cell>0.8090</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison with transformers that support longer text sequences on the Robust04 collection. Baseline results are from<ref type="bibr" target="#b37">[38]</ref>. Since this run's pre-trained structBERT model is not publicly available, we are not able to embed it into PARADE and make a fair comparison. In TREC DL'20, the best TREC run d_d2q_duo is a T5-3B model. Moreover, PARADE-Max again outperforms PARADE-Transformer, which is in line to the Genomics results and in contrast to results on Robust04 and GOV2.</figDesc><table><row><cell>Model</cell><cell cols="2">nDCG@20 ERR@20</cell></row><row><cell>Sparse-Transformer</cell><cell>0.449</cell><cell>0.119</cell></row><row><cell>Longformer-QA</cell><cell>0.448</cell><cell>0.113</cell></row><row><cell>Transformer-XH</cell><cell>0.450</cell><cell>0.123</cell></row><row><cell>QDS-Transformer</cell><cell>0.457</cell><cell>0.126</cell></row><row><cell cols="2">PARADE-Transformer 0.565</cell><cell>0.149</cell></row><row><cell cols="3">relationships. All PARADE variants outperform ucas_runid1</cell></row><row><cell cols="3">and TUW19-d3-re in terms of nDCG@10, but cannot outperform</cell></row><row><cell>idst_bert_r1.</cell><cell></cell><cell></cell></row></table><note>contrast to the previous result in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>PARADE-Transformer's effectiveness using BERT models of varying sizes on Robust04 title queries. Significant improvements of distilled over non-distilled models are marked with ?. ( &lt; 0.01, two-tailed paired t-test.)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Robust04</cell><cell cols="4">Robust04 (Distilled) Parameter Inference Time</cell></row><row><cell cols="2">ID Model</cell><cell>L / H</cell><cell cols="2">P@20 nDCG@20 P@20</cell><cell cols="2">nDCG@20 Count</cell><cell>(ms / doc)</cell></row><row><cell>1</cell><cell>BERT-Large</cell><cell cols="2">24 / 1024 0.4508 0.5243</cell><cell>\</cell><cell>\</cell><cell>360M</cell><cell>15.93</cell></row><row><cell>2</cell><cell>BERT-Base</cell><cell>12 / 768</cell><cell>0.4486 0.5252</cell><cell>\</cell><cell>\</cell><cell>123M</cell><cell>4.93</cell></row><row><cell>3</cell><cell>\</cell><cell>10 / 768</cell><cell>0.4420 0.5168</cell><cell cols="2">0.4494  ? 0.5296  ?</cell><cell>109M</cell><cell>4.19</cell></row><row><cell>4</cell><cell>\</cell><cell>8 / 768</cell><cell>0.4428 0.5168</cell><cell cols="2">0.4490  ? 0.5231</cell><cell>95M</cell><cell>3.45</cell></row><row><cell>5</cell><cell cols="2">BERT-Medium 8 / 512</cell><cell>0.4303 0.5049</cell><cell cols="2">0.4388  ? 0.5110</cell><cell>48M</cell><cell>1.94</cell></row><row><cell>6</cell><cell>BERT-Small</cell><cell>4 / 512</cell><cell>0.4257 0.4983</cell><cell cols="2">0.4365  ? 0.5098  ?</cell><cell>35M</cell><cell>1.14</cell></row><row><cell>7</cell><cell>BERT-Mini</cell><cell>4 / 256</cell><cell>0.3922 0.4500</cell><cell cols="2">0.4046  ? 0.4666  ?</cell><cell>13M</cell><cell>0.53</cell></row><row><cell>8</cell><cell>\</cell><cell>2 / 512</cell><cell>0.4000 0.4673</cell><cell>0.4038</cell><cell>0.4729</cell><cell>28M</cell><cell>0.74</cell></row><row><cell>9</cell><cell>BERT-Tiny</cell><cell>2 / 128</cell><cell>0.3614 0.4216</cell><cell cols="2">0.3831  ? 0.4410  ?</cell><cell>5M</cell><cell>0.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Reranking effectiveness of PARADE-Transformer using various preserved data size on GOV2 title dataset. nDCG@20 is reported. The indexes of columns and rows are number of passages being used.</figDesc><table><row><cell>Train \ Eval</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell>8</cell><cell cols="4">0.5554 0.5648 0.5648 0.5680</cell></row><row><cell>16</cell><cell cols="4">0.5621 0.5685 0.5736 0.5733</cell></row><row><cell>32</cell><cell cols="4">0.5610 0.5735 0.5750 0.5802</cell></row><row><cell>64</cell><cell cols="4">0.5577 0.5665 0.5760 0.5815</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>document by nature of its construction. This query overlap suggests that the queries in both TREC DL collections can be sufficiently answered by a single highly relevant passage. However, unlike the shallow labels in MS MARCO, documents in the DL collections contains deep relevance labels from NIST assessors. It is unclear how often documents in DL also have only a few relevant passages per document.</figDesc><table><row><cell>)</cell></row><row><cell>While PARADE variants are effective across a range of datasets and</cell></row><row><cell>the PARADE-Transformer variant is generally the most effective,</cell></row><row><cell>this is not always the case. In particular, PARADE-Max outperforms</cell></row><row><cell>PARADE-Transformer on both years of TREC DL and on TREC</cell></row><row><cell>Genomics. We hypothesize that this difference in effectiveness is</cell></row><row><cell>a result of the focused nature of queries in both collections. Such</cell></row><row><cell>queries may result in a lower number of highly relevant passages per</cell></row><row><cell>document, which would reduce the advantage of using more complex</cell></row><row><cell>aggregation methods like PARADE-Transformer and PARADE-</cell></row><row><cell>CNN. This theory is supported by the fact that TREC DL shares</cell></row><row><cell>queries and other similarities with MS MARCO, which only has 1-2</cell></row><row><cell>relevant passages per</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Percentage of documents with a given number of relevant passages. performance, finding that knowledge distillation on PARADE boosts the performance of smaller PARADE models while substantially reducing their parameters. Finally, we analyzed dataset characteristics are to explore when representation aggregation strategies are more effective. Results on the TREC-COVID Challenge</figDesc><table><row><cell># Relevant</cell><cell>GOV2</cell><cell>DL19</cell><cell cols="4">DL19 DL20 MS MARCO Genomics</cell></row><row><cell>Passages</cell><cell></cell><cell cols="3">(FIRA) (Ours) (Ours)</cell><cell>train / dev</cell><cell>2006</cell></row><row><cell>1</cell><cell>38%</cell><cell>66%</cell><cell>66%</cell><cell>67%</cell><cell>99% / 98%</cell><cell>62%</cell></row><row><cell>1-2</cell><cell>60%</cell><cell>87%</cell><cell>86%</cell><cell>81%</cell><cell>100% / 100%</cell><cell>80%</cell></row><row><cell>3+</cell><cell>40%</cell><cell>13%</cell><cell>14%</cell><cell>19%</cell><cell>0% / 0%</cell><cell>20%</cell></row><row><cell cols="2">model size affects</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ranking effectiveness of different retrieval systems in the TREC-COVID Round 2.</figDesc><table><row><cell></cell><cell>runid</cell><cell cols="2">nDCG@10 P@5</cell><cell>bpref</cell><cell>MAP</cell></row><row><cell>1</cell><cell>covidex.r3.t5_lr</cell><cell>0.7740</cell><cell cols="2">0.8600 0.5543 0.3333</cell></row><row><cell>2</cell><cell>BioInfo-run1</cell><cell>0.7715</cell><cell cols="2">0.8650 0.5560 0.3188</cell></row><row><cell>3</cell><cell>UIowaS_Rd3Borda</cell><cell>0.7658</cell><cell cols="2">0.8900 0.5778 0.3207</cell></row><row><cell>4</cell><cell cols="2">udel_fang_lambdarank 0.7567</cell><cell cols="2">0.8900 0.5764 0.3238</cell></row><row><cell cols="2">11 sparse-dense-SBrr-2</cell><cell>0.7272</cell><cell cols="2">0.8000 0.5419 0.3134</cell></row><row><cell cols="2">13 mpiid5_run2</cell><cell>0.7235</cell><cell cols="2">0.8300 0.5947 0.3193</cell></row><row><cell cols="3">16 mpiid5_run1 (Fusion) 0.7060</cell><cell cols="2">0.7800 0.6084 0.3010</cell></row><row><cell cols="2">43 mpiid5_run3 (Attn)</cell><cell>0.3583</cell><cell cols="2">0.4250 0.5935 0.2317</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Ranking effectivenes of different retrieval systems in the TREC-COVID Round 3.</figDesc><table><row><cell>runid</cell><cell cols="2">nDCG@20 P@20 bpref</cell><cell>MAP</cell></row><row><cell>1 UPrrf38rrf3-r4</cell><cell>0.7843</cell><cell cols="2">0.8211 0.6801 0.4681</cell></row><row><cell>2 covidex.r4.duot5.lr</cell><cell>0.7745</cell><cell cols="2">0.7967 0.5825 0.3846</cell></row><row><cell>3 UPrrf38rrf3v2-r4</cell><cell>0.7706</cell><cell cols="2">0.7856 0.6514 0.4310</cell></row><row><cell>4 udel_fang_lambdarank</cell><cell>0.7534</cell><cell cols="2">0.7844 0.6161 0.3907</cell></row><row><cell cols="2">5 run2_Crf_A_SciB_MAP 0.7470</cell><cell cols="2">0.7700 0.6292 0.4079</cell></row><row><cell>6 run1_C_A_SciB</cell><cell>0.7420</cell><cell cols="2">0.7633 0.6256 0.3992</cell></row><row><cell>7 mpiid5_run1</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We refer to BERT since it is the most common PLM. In some of our later experiments, we consider the more recent and effective ELECTRA model<ref type="bibr" target="#b11">[12]</ref>; the same limitations apply to it and to most PLMs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that max pooling is performed on passage representations, not over passage relevance scores as in prior work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://trec.nist.gov/data/qa/T8_QAdata/disks4_5. html 4 http://ir.dcs.gla.ac.uk/test_collections/gov2summary.htm 5 https://www.highwirepress.com/ 6 https://microsoft.github.io/TREC-2019-Deep-Learning 7 http://lemurproject.org/clueweb12/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/canjiali/PARADE/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://trec.nist.gov/trec_eval 10 http://research.nii.ac.jp/ntcir/tools/ntcirevalen.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">Applying the same procedure to both FIRA and WebAP with longer maximum lengths did not substantially change the trend.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Further details and system descriptions can be found at https://ir.nist. gov/covidSubmit/archive.html<ref type="bibr" target="#b12">13</ref> To clarify, the run type of the PARADE runs is feedback, but they were cautiously marked as manual due to the fact that they rerank a first-stage retrieval approach based on udel_fang_run3. Many participants did not regard this as sufficient to change a run's type to manual, however, and the PARADE runs would be regarded as feedback runs following this consensus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by Google Cloud and the Tensor-Flow Research Cloud.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Neural Passage Model for Ad-hoc Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">10772</biblScope>
			<biblScope unit="page" from="537" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do Deep Nets Really Need to be Deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The Long-Document Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Utilizing Passage-Based Language Models for Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">4956</biblScope>
			<biblScope unit="page" from="162" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Passage-Level Evidence in Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM/Springer</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Catena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">Ioana</forename><surname>Muntean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tonellotto</surname></persName>
		</author>
		<title level="m">Enhanced News Retrieval: Passages Lead the Way! Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simplified Tiny-BERT: Knowledge Distillation for Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/2009.07531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UCAS at TREC-2019 Deep Learning Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingfei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating Long Sequences with Sparse Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buettcher</surname></persName>
		</author>
		<title level="m">Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods. In SIGIR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2020 deep learning track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2019 deep learning track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno>abs/1910.10687</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeper Text Understanding for IR with Contextual Neural Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="985" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling Diverse Relevance Patterns in Ad-hoc Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diagnostic Evaluation of Information Retrieval Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding BERT Rankers Under Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on the Theory of Information Retrieval</title>
		<meeting>the ACM International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICTIR 2020</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">EARL: Speedup Transformerbased Rankers with Pre-computed Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<idno>ArXiv abs/2004.13313</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Deep Relevance Matching Model for Ad-hoc Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Ruslen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoebe</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics Track Overview. In TREC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>TREC</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoebe</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><forename type="middle">Krishna</forename><surname>Rekapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics Track Overview. In TREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Althammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schr?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Sertkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno>abs/2010.02666</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local Self-Attention over Long Text for Efficient Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TU Wien @ TREC Deep Learning &apos;19 -Simple Contextualization for Re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretable &amp; Time-Budget-Constrained Contextualization for Re-Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th European Conference on Artificial Intelligence</title>
		<meeting>the 24th European Conference on Artificial Intelligence<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Interpretable &amp; Time-Budget-Constrained Contextualization for Re-Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno>abs/2002.01854</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Zlabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mete</forename><surname>Sertkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schr?der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3031" to="3038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PACRR: A Position-Aware Neural IR Model for Relevance Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long Document Ranking with Query-Directed Sparse Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyun-Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Jung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4594" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">TinyBERT: Distilling BERT for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1909.10351</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evaluating answer passages using summarization measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Keikha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="963" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Retrieving passages and finding answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Keikha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Australasian Document Computing Symposium</title>
		<meeting>the 2014 Australasian Document Computing Symposium</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relevance-Based Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.]. MPII at the TREC 2020 Deep Learning Track. ([n. d.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MPII at the NTCIR-15 WWW-3 Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-15</title>
		<meeting>NTCIR-15</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06467</idno>
		<title level="m">Pretrained transformers for text ranking: Bert and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Is searching full text more effective than searching abstracts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Passage retrieval based on language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W. Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical Transformers for Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Representation Learning for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient Document Re-Ranking for Transformers by Precomputing Term Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Expansion via Prediction of Importance with Contextualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CEDR: Contextualized Embeddings for Document Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1101" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Conformer-Kernel with Query Term Independence for Document Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<idno>abs/2007.10434</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Hamed Zamani, and Nick Craswell</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">CoCo@NIPS (CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Passage Re-ranking with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1901.04085</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Document Ranking with a Pretrained Sequence-to-Sequence Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Document Expansion by Query Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1904.08375</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM/Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Okapi at TREC-4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Payne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">C</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN (3) (Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6354</biblScope>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A passage-based approach to learning to rank documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eilon</forename><surname>Sheetrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Kurland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr. J</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="159" to="186" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Patient Knowledge Distillation for BERT Model Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Distilling Knowledge for Fast Retrieval-based Chat-bots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Vakili Tahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Ghajar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Shakery</surname></persName>
		</author>
		<idno>abs/2004.11045</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1903.12136</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1908.08962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention is All you Need. In NIPS</title>
		<imprint>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasmeer</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2005.04474</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">CORD-19: The Covid-19 Open Research Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dewey</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devvret</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Stilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wilhelm</surname></persName>
		</author>
		<editor>Boya Xie, Douglas Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>CoRR abs/2004.10706 (2020</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Leveraging Passage-level Cumulative Gain for Document Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM / IW3C2</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2421" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Investigating Passage-level Relevance and Its Role in Document-level Relevance Judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">End-to-End Neural Ad-hoc Ranking with Kernel Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR. ACM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">IDST at TREC 2019 Deep Learning Track: Deep Cascade Ranking with Generation-based Document Expansion and Pre-trained Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM. ACM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Anserini: Reproducible Ranking Baselines Using Lucene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Flexible IR pipelines with Capreolus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Martin</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3181" to="3188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Applying BERT to Document Retrieval with Birch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Zeynep Akkalyoncu Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">HIBERT: Document Level Pretraining of Hierarchical Bidirectional Transformers for Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<editor>ICLR. OpenReview.net</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="892" to="901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
