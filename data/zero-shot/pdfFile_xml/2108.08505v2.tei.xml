<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Weixia</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Meng</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Guangtao</forename><forename type="middle">Zhai</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Perceptual quality assessment of the videos acquired in the wilds is of vital importance for quality assurance of video services. The inaccessibility of reference videos with pristine quality and the complexity of authentic distortions pose great challenges for this kind of blind video quality assessment (BVQA) task. Although model-based transfer learning is an effective and efficient paradigm for the BVQA task, it remains to be a challenge to explore what and how to bridge the domain shifts for better video representation. In this work, we propose to transfer knowledge from image quality assessment (IQA) databases with authentic distortions and large-scale action recognition with rich motion patterns. We rely on both groups of data to learn the feature extractor and use a mixed list-wise ranking loss function to train the entire model on the target VQA databases. Extensive experiments on six benchmarking databases demonstrate that our method performs very competitively under both individual database and mixed databases training settings. We also verify the rationality of each component of the proposed method and explore a simple ensemble trick for further improvement.</p><p>Index Terms-Blind video quality assessment, transfer learning, list-wise ranking loss, in-the-wild videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Perceptual quality assessment of the videos acquired in the wilds is of vital importance for quality assurance of video services. The inaccessibility of reference videos with pristine quality and the complexity of authentic distortions pose great challenges for this kind of blind video quality assessment (BVQA) task. Although model-based transfer learning is an effective and efficient paradigm for the BVQA task, it remains to be a challenge to explore what and how to bridge the domain shifts for better video representation. In this work, we propose to transfer knowledge from image quality assessment (IQA) databases with authentic distortions and large-scale action recognition with rich motion patterns. We rely on both groups of data to learn the feature extractor and use a mixed list-wise ranking loss function to train the entire model on the target VQA databases. Extensive experiments on six benchmarking databases demonstrate that our method performs very competitively under both individual database and mixed databases training settings. We also verify the rationality of each component of the proposed method and explore a simple ensemble trick for further improvement.</p><p>Index Terms-Blind video quality assessment, transfer learning, list-wise ranking loss, in-the-wild videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE Global Internet Phenomena declares that video streaming has already made up more than 60% of the whole Internet traffic <ref type="bibr" target="#b0">[1]</ref>. And a report by a Cisco project shows that online videos will account for more than 82% of all consumer Internet traffic by 2022 <ref type="bibr" target="#b1">[2]</ref>. Confronted With various video providers, consumers always expect favorable qualityof-experience (QoE) <ref type="bibr" target="#b2">[3]</ref> when they are paying for these video services. Therefore, it is of high importance to develop reliable video quality assessment (VQA) models to ensure the quality of video services.</p><p>Because humans are the ultimate receivers of videos, the most promising VQA methodology is subjective quality testing. However, conducting such testing is labor-intensive and time-consuming, resulting in poor scalability to large-scale applications. As an alternative, objective VQA aims at automatically predicting the quality of videos. Objective VQA includes three categories: full-reference (FR) VQA, reducedreference (RR) VQA, and no-reference/blind (NR/B) VQA. FR(RR)-VQA methods (partially) rely on non-distorted videos for making quality predictions, thus are not applicable for applications where the pristine videos are inaccessible or even not existing <ref type="bibr" target="#b3">[4]</ref>. Consequently, increasing attention has been paid to BVQA over the past years.</p><p>Early BVQA methods were mainly developed for specific distortion types such as transmission and compression <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Although a plethora of general-purpose BVQA models was developed subsequently, they were still designed for handling synthetic distortions using hand-crafted features <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. These methods usually struggle for VQA in the wild <ref type="bibr" target="#b10">[11]</ref>, where the distortions are naturally introduced during the video acquisition. Such authentic distortions may originate from various factors, including amateurish photographing, low-end camera devices, poor shooting environments, inappropriate post-processing, etc.</p><p>Due to the remarkable representation learning capability, deep neural networks (DNNs) have presented their promises in various vision applications over the past years. However, direct applications of the powerful DNNs for VQA tasks usually suffer from two main challenges: 1), prohibitively high computational complexity and memory consumption for processing the whole videos (usually with high spatial resolutions); 2), insufficient corpus with human-annotated quality labels for training effective DNNs from scratch. Recent work may suggest leveraging a large number of videos with pseudolabels <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> to train 3D models from scratch. While methods of this kind handle videos with synthetic distortions (e.g., compression, transmission errors) well, they are found to present sub-optimal generalizability to in-the-wild videos due to the distributional shifts <ref type="bibr" target="#b13">[14]</ref>. To mitigate the above issues, previous methods follow a paradigm to employ pre-trained DNNs on large-scale image classification databases <ref type="bibr" target="#b14">[15]</ref> to extract frame-level features <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The philosophy behind this paradigm is straightforward because videos are composed of sequences of images. Despite being empirically effective and efficient for VQA in the wild, this paradigm inevitably confronts the problem of distributional shifts <ref type="bibr" target="#b17">[18]</ref> between the source domains (e.g., image classification) and the target domains (in-the-wild VQA), resulting in sub-optimal feature representation. In addition to the frame-level spatial features, motion information also plays an important role in human perception of videos <ref type="bibr" target="#b18">[19]</ref>. However, the frame-level feature extraction paradigm inherently hinders the exploitation of spatio-temporal information for estimating the quality of Copyright ? 20xx IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending an email to pubs-permissions@ieee.org. arXiv:2108.08505v2 [eess.IV] 5 Apr 2022 videos.</p><p>In this work, we aim for dealing with the aforementioned limitations through model-based transfer learning strategies. Specifically, instead of leveraging pre-trained DNNs on object recognition <ref type="bibr" target="#b14">[15]</ref> for feature extraction, we propose to use human-annotated IQA databases to learn quality-aware framelevel feature representation. In addition, we employ a pretrained 3D network <ref type="bibr" target="#b19">[20]</ref> to capture the motion information. Two groups of features are delicately aggregated, leading to a complementary and effective spatio-temporal video representation. Moreover, we employ a mixed list-wise ranking loss function to train the entire BVQA model, which introduces additional performance gain. We summarize our contributions as follows:</p><p>? We propose an effective and efficient method to learn a frame-level feature extractor for the VQA in the wild.</p><p>We conduct a quality-aware pre-training on multiple IQA databases for transferring perceptually meaningful knowledge. ? We transfer the knowledge from an action recognition domain to perceive the motion distortion of videos. We empirically validate that the motion information is complementary to spatial features. ? We introduce a mixed list-wise ranking loss function for training the entire model, through which we obtain further performance improvement. ? Through extensive experiments, we verify that the proposed BVQA metric achieves the state-of-the-art (SOTA) results on six in-the-wild VQA databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>An intuitive solution to the BVQA task is applying a BIQA metric on videos frame by frame, followed by a features/scores pooling stage. In addition, motion information has also shown its promises in the perceived quality of videos <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Therefore, incorporating both spatial and motion information has become a promising paradigm for BVQA. We briefly review related BVQA methods following this line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classical BVQA</head><p>A plethora of classical BVQA models relies on natural scene statistics (NSS), with an underlying assumption that the quality can be measured by the disturbance of NSS <ref type="bibr" target="#b21">[22]</ref>. NSSbased methods are derived from transform domains <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, spatial domains <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b24">[25]</ref>, or hybrid domains <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Based on the 2D discrete-time transform (DCT) features of video frame-difference statistics, Saad et al. <ref type="bibr" target="#b8">[9]</ref> further introduced motion information to enhance the representation capacity. Li et al. <ref type="bibr" target="#b27">[28]</ref> captured the spatial and temporal regularities simultaneously using the 3D-DCT coefficients. Mittal et al. <ref type="bibr" target="#b9">[10]</ref> designed a completely blind VQA metric by modeling the statistical naturalness of the videos and excavating the intersubband correlations. Dendi et al. <ref type="bibr" target="#b28">[29]</ref> raised an asymmetric generalized Gaussian distribution (AGGD) to model the spatio-temporal statistics using 3D mean subtract contrast normalized coefficients and bandpass filter coefficients. Another line of work is the codebook-based methodology. Motivated by CORNIA <ref type="bibr" target="#b29">[30]</ref>, Xu et al. <ref type="bibr" target="#b30">[31]</ref> proposed to learn the frame-level features via an unsupervised learning method and then used the support vector regression (SVR) to map feature representations to frame-level quality scores. The global video quality score is obtained using a temporal pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN-based BVQA</head><p>In recent years, DNNs are inclined to dominate the BVQA field. Li et al. <ref type="bibr" target="#b31">[32]</ref> extracted natural scene statistics using 3D shearlet transform and then made them more discriminative using a DNN, where a logistic regression function is used for training. Following an end-to-end learning framework MEON <ref type="bibr" target="#b32">[33]</ref>, Liu et al. <ref type="bibr" target="#b33">[34]</ref> devised a BVQA model that jointly optimizes the feature extractor, the codec classifier, and the quality predictor with a two-step training strategy. Zhang et al. <ref type="bibr" target="#b11">[12]</ref> pre-trained a DNN using the 3D-DCT coefficients with proxy labels. They then utilized a frequency histogram function to map the block-wise scores collected from the previous network to the perceptual quality. You et al. <ref type="bibr" target="#b34">[35]</ref> designed a BVQA model with a 3D convolutional neural network (CNN) as the feature extractor and a Long Short-Term Memory (LSTM) for the overall quality prediction. Li et al. proposed a VSFA <ref type="bibr" target="#b10">[11]</ref> model for quality assessment of in-thewild videos, where two crucial effects of HVS, i.e., contentdependency and temporal-memory effects, are incorporated to account for quality-aware features. Based on VSFA, they then proposed a mixed databases training strategy towards a universal BVQA model (MDTVSFA) <ref type="bibr" target="#b15">[16]</ref>. Ying et al. <ref type="bibr" target="#b35">[36]</ref> created a local-to-global region-based BVQA architecture using a DNN that computes both 2D and 3D video features. Wang et al. <ref type="bibr" target="#b36">[37]</ref> aggregated several complementary 2D and 3D DNNs to incorporate different features for the BVQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this section, we first describe the quality-aware pretraining strategy for learning the frame-level feature extractor. We then incorporate motion information to form a spatiotemporal representation, where special care is taken to fuse two groups of features in a reasonable way. Finally, we introduce a mixed list-wise ranking loss function to optimize the overall VQA model. The framework of our model is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>A. Quality-aware Pre-training 1) Transfer Learning: The lack of large video databases with human quality annotations is a common obstacle to applying DNNs for BVQA. As a consequence, the pretraining followed by fine-tuning is a widely-used transfer learning paradigm due to its favorable flexibility that poses no constraint on the label spaces of the source and target domains <ref type="bibr" target="#b37">[38]</ref>, for which we have a formulation <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_0">f * s = argmin fs?H 1 N s Ns i=1 s (f s (x s,i , q s,i )) + ? s R (D t , f s )) (1)</formula><p>where (x s,i , q s,i ) is the i-th tuple of the sample and label in the source domain, N s is the number of samples in the source domain, f s is a function that lies in a Hilbert space H, which we optimize with the loss function s using the data of source domain D s , and R(?) is a regularization term controlled by a weight ? s , whose objective is leveraging or finetuning the f s in the target domain D t . Although being popular in VQA <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, the effectiveness of this paradigm is limited by distributional shifts between the source (object recognition) and target domains (VQA).</p><p>2) Source Domain Selection: Considering that videos are composed of multiple stacked images (frames), we aim for transferring the knowledge from the IQA databases with authentic distortions, which we assume to be source domains that better match the target domains, i.e., VQA in the wild.</p><p>To verify the rationality of the selected source domains, we quantify the distances between the source and target domains. Specifically, we use the CORAL <ref type="bibr" target="#b39">[40]</ref> as a proxy metric to measure the feature distance between the source and target domains. For the source domain, we take one image classification database (i.e., ImageNet <ref type="bibr" target="#b14">[15]</ref>) and four IQA databases (i.e., BID <ref type="bibr" target="#b40">[41]</ref>, LIVE Challenge <ref type="bibr" target="#b41">[42]</ref>, KonIQ-10k <ref type="bibr" target="#b42">[43]</ref>, and SPAQ <ref type="bibr" target="#b43">[44]</ref>) for comparison. Here, we uniformly sample the same number of images from ImageNet <ref type="bibr" target="#b14">[15]</ref> across all semantic categories. As for the target domains, we acquire videos from five in-the-wild VQA databases, i.e., CVD2014 <ref type="bibr" target="#b44">[45]</ref>, KoNViD-1k <ref type="bibr" target="#b45">[46]</ref>, LIVE-Qualcomm <ref type="bibr" target="#b46">[47]</ref>, LIVE-VQC <ref type="bibr" target="#b47">[48]</ref>, and YouTube-UGC <ref type="bibr" target="#b48">[49]</ref>. We then use the pre-trained ResNet-50 <ref type="bibr" target="#b49">[50]</ref> on ImageNet to extract features of all samples in the source and target domains. Note that the target features of videos are obtained by the average pooling of their frame-level features. The pairwise CORAL distances are shown in <ref type="table" target="#tab_9">Table X</ref> in Section IV-F, from which we observe that IQA databases are statistically closer to the target VQA domain compared with ImageNet. Besides, we find that no single IQA database is consistently to be the closest domain with different VQA databases. Thus, it is highly desirable to specify an effective learning scheme to transfer knowledge from multiple diverse IQA databases, which can cover a broad range of appropriate content and authentic distortions.</p><p>3) Training Frame-level Feature Extractor: Inspired by the database combination method <ref type="bibr" target="#b50">[51]</ref>, we leverage multiple IQA databases for pre-training the function f s , which will serve as a frame-level feature extractor. We first formulate the loss function s to make full use of the training data. Given an image pair (x s , y s ) sampled from an IQA database, under the Thurstone's model <ref type="bibr" target="#b51">[52]</ref>, their perceptual quality s(x s ) and s(y s ) are assumed to follow Gaussian distributions with means (?(x s ), ?(y s )) and standard deviations (std) (?(x s ), ?(y s )), respectively. Assuming the variability of quality across images is uncorrelated, their quality difference s(x s ) ? s(y s ) also conforms to a Gaussian distribution with mean ?(x s ) ? ?(y s ) and std ? 2 (x s ) + ? 2 (y s ). Through a frame-level quality prediction network f s = {?, h ? , h ? } parameterized by a vector w, where ? and h ? / h ? denote the backbone network and the fully-connected (FC) layers, the estimated mean and std can be computed as ? w (?) = h ? (?(?)) and ? w (?) = h ? (?(?)), respectively. The estimated quality difference is also assumed to follow a Gaussian distribution with mean ? w (x s ) ? ? w (y s ) and std ? 2 w (x s ) + ? 2 w (y s ), for which we simultaneously supervise the learning of mean and std. The probabilities (Pr) that x s is of higher perceptual quality than y s according to the ground truths and the predicted scores are as follows:</p><formula xml:id="formula_1">p(x s , y s ) = Pr(s(x s ) ? s(y s )) = ? ?(x s ) ? ?(y s ) ? 2 (x s ) + ? 2 (y s )<label>(2)</label></formula><formula xml:id="formula_2">p w (x s , y s ) = Pr(s w (x s ) ? s w (y s )) = ? ? w (x s ) ? ? w (y s ) ? 2 w (x s ) + ? 2 w (y s )<label>(3)</label></formula><p>where ?(?) denotes the Gaussian cumulative distribution function. Note that when scaling ? w (?) ? ?? w (?) and ? w (?) ? ?? w (?), the probability p w (?, ?) inferred by Eq. (3) is unchanged. To avoid this scaling ambiguity and supply ? w (?) with a direct supervision, we enforce a regularizer of ? w (?) for std learning. For an image pair (x s , y s ), a binary label g is assigned as g(x s , y s ) = sign(?(x s ) ? ?(y s )). Empirically, the similarity of Gaussian distribution and the uncertainty of the regularizer can be measured by the fidelity loss <ref type="bibr" target="#b52">[53]</ref> and the hinge loss respectively as follows:</p><formula xml:id="formula_3">P {(x s , y s ), p; w} = 1 ? p(x s , y s ) ? p w (x s , y s ) ? (1 ? p(x s , y s )) ? (1 ? p w (x, y s )) (4) G {(x s , y s ), g; w} = max(0, ? ? sign(?(x s ) ? ?(y s )) ? (? w (x s ) ? ? w (y s )))<label>(5)</label></formula><p>where ? is a margin constant. In practice, we randomly sample a large number of image pairs from the aforementioned four IQA databases, resulting in a set</p><formula xml:id="formula_4">? = {(x s , y s ) i , p i , g i } Ns i=1</formula><p>for training. At the training stage, we utilize every batch B to optimize w using the overall loss:</p><formula xml:id="formula_5">s (B; w) = 1 |B| B?? P {(x s , y s ), p; w} + ? G {(x s , y s ), g; w} (6)</formula><p>where ? is a balance coefficient. In practice, we use a variant of stochastic gradient descent (SGD) algorithm with a L 2 weight decay as the regularizer R to optimize the network. Once the training is completed, we extract the frame-level features of videos using the backbone network ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Qualitative Demonstration:</head><p>To verify the rationality of the quality-aware pre-training more intuitively, we present some visual examples with representative types of realistic impairments between the source and target domains, which including "Blurry", "Grainy", "Underexposed", "Shaky", "Overexposed", and "Poor Color" as shown in <ref type="figure">Fig. 2</ref>. Each sample is labeled with a single dominant distortion for better visualization. From <ref type="figure">Fig. 2</ref>, we can observe similar distortion patterns between images sampled from public IQA databases and single frames from videos on VQA databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion Perception</head><p>In addition to spatial appearance, dynamic changes are deemed as the most distinctive characteristic of videos <ref type="bibr" target="#b53">[54]</ref>. A plethora of biological researches on the primate visual structure <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b56">[57]</ref> demonstrated that there are approximately 15-20% M-cells sensitive to fast temporal changes. Therefore, incorporating motion information is helpful to facilitate video quality estimation. Previous work captured motion information using various hand-crafted features such as silhouette <ref type="bibr" target="#b57">[58]</ref> and optical flow <ref type="bibr" target="#b58">[59]</ref>. These methods are either computationally expensive or with less representational power. We resort to a learning-based method for extracting motion features. Similar to the model-based transfer learning philosophy stated in Section III-A, we make use of a pre-trained 3D-DNN on the action recognition to extract motion features of videos. Specifically, we resort to the fast pathway of the pre-trained SlowFast (dubbed as SlowFast F ) network <ref type="bibr" target="#b19">[20]</ref> on Kinetics-400 <ref type="bibr" target="#b59">[60]</ref>, which contains rich motion-related contents. SlowFast F can produce motion features with high temporal resolution since it maintains temporal fidelity as much as possible by prohibiting temporal downsampling before the last pooling layer. Besides, SlowFast F is formed in a lightweight manner with low channel capacity, which makes it more computationally efficient. As a result, the extracted features are sensitive to fast motion, which is complementary to the spatial features.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, we exhibit continuous frames of videos sampled from Kinetics-400 and that from VQA databases, from which we observe a similar distortion pattern. Specifically, all the compared videos undergo significant motion blur distortion regardless of their contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Features Fusion</head><p>Let {T, H ?W, C} denote the temporal, spatial, and channel dimensions of a raw video clip z = {z t } T t=1 where z t is the t-th frame of the video. We use the proposed qualityaware pre-training scheme to train a ResNet-50 <ref type="bibr" target="#b49">[50]</ref> as the frame-level feature extractor ?. The network architecture is briefly summarized in "Spatial" column of <ref type="table" target="#tab_0">Table I</ref>. To aggregate the spatial information, we leverage the activation of the last convolution of the "Spatial" pipeline. To obtain a rich spatial feature representation, we use both the global average pooling (GAP) and the global standard deviation pooling (GSP) to aggregate spatial features of a single frame as v s,t = GAP(?(z t )) ? GSP(?(z t )), where ? denotes the concatenation operation and ?(z t ) is with the size of {T, H/32 ? W/32, 2048}. As a result, we can obtain 4,096dimensional feature vectors with a temporal length of T .</p><p>In the SlowFast F pipeline, we use the default parameters as described in <ref type="bibr" target="#b19">[20]</ref> where the temporal stride in the slow pathway is ? = 8, the speed and channel ratios in the fast pathway are ? = 4 and ? = 1/8, respectively. The network architecture is briefly summarized in the "Motion" column of <ref type="table" target="#tab_0">Table I</ref>. Given the video clip z, SlowFast F can produce an activation with a size of {T /2, H/32?W/32, 256}. Similarly, the features are spatially pooled using GAP and GSP, resulting in a sequence of 512-dimensional frame-level features {v m,t } T /2 t=1 . It remains to fuse the spatial and motion features in a reasonable manner. To match the temporal resolution of the motion pipeline, we temporally sample one out of every two frames of the spatial feature tensor, resulting in a 4,096dimensional tensor with a length of T /2. We then concatenate the spatial and motion features along the channel dimension as v t = v s,t ?v m,t . Finally, we have 4,608-dimensional framelevel feature vectors with a temporal length of T /2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Temporal Modeling and Quality Prediction</head><p>Similar to <ref type="bibr" target="#b15">[16]</ref>, we take the temporal-memory effect into consideration. Specifically, we use a gated recurrent unit <ref type="bibr" target="#b60">[61]</ref> (GRU) to model the temporal information. To enable efficient learning of the GRU, a dimension reduction is performed to the frame-level feature vectors v t using a FC layer:</p><formula xml:id="formula_6">v t = W v v t + b v .<label>(7)</label></formula><p>where W v and b v are learnable parameters of the dimension reduction FC layer. Given {v t |t = 1, 2, ..., T } as the input to GRU, the hidden state at the t-th time step h t depends on both the previous state h t?1 and the current input featurev t as:</p><formula xml:id="formula_7">h t = GRU(v t , h t?1 )<label>(8)</label></formula><p>We then use an FC layer to map the sequence of hidden states to the frame-level quality scores {q t } T t=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blurry Poor Color Grainy</head><p>Underexposed Shaky Overexposed <ref type="figure">Fig. 2</ref>. Comparison with images from IQA databases and single frames of videos from VQA databases. The top row presents the images from IQA databases including BID <ref type="bibr" target="#b40">[41]</ref>, LIVE Challenge <ref type="bibr" target="#b41">[42]</ref>, KonIQ-10k <ref type="bibr" target="#b42">[43]</ref>, and SPAQ <ref type="bibr" target="#b43">[44]</ref>. And the bottom row presents the frames of videos sampled from VQA databases covering CVD2014 <ref type="bibr" target="#b44">[45]</ref>, KoNViD-1k <ref type="bibr" target="#b45">[46]</ref>, LIVE-Qualcomm <ref type="bibr" target="#b46">[47]</ref>, LIVE-VQC <ref type="bibr" target="#b47">[48]</ref>, YouTube-UGC <ref type="bibr" target="#b48">[49]</ref>, and LSVQ <ref type="bibr" target="#b35">[36]</ref>. In each subgraph, the top row presents the continuous frames of videos sampled from Kinetics-400 <ref type="bibr" target="#b59">[60]</ref>, while the bottom row presents that from VQA databases including KoNViD-1k <ref type="bibr" target="#b45">[46]</ref>, LIVE-VQC <ref type="bibr" target="#b47">[48]</ref>, and LSVQ <ref type="bibr" target="#b35">[36]</ref>.</p><p>We then adopt the HVS-inspired temporal hysteresis pooling <ref type="bibr" target="#b61">[62]</ref> to temporally aggregate the frame-level quality scores to an overall video quality score. Specifically, we use a differentiable hysteresis-based temporal pooling model. Let ? denotes the memory duration, a memory quality item m t at the t-th frame is defined with the worst quality case across the previous frames as:</p><formula xml:id="formula_8">m t = q t , t = 1 min(q k ) k ? {max(1, t ? ? ), ..., t ? 1}, t &gt; 1<label>(9)</label></formula><p>A current quality item c t is calculated with the next ? frames based on the fact that more rapid response will be paid into the drops in quality than the increase situation. This procedure can be established by a weighted quality combination using the softmin function as:</p><formula xml:id="formula_9">c t = k a k q k a k = e ?q k i e ?qi i, k ? {t, ..., min(t + ?, T )}<label>(10)</label></formula><p>Then the hysteresis effect is expressed by a linear combination of the memory and the current quality items as:</p><formula xml:id="formula_10">q t = ?m t + (1 ? ?)c t<label>(11)</label></formula><p>where ? is a contribution factor of different components. Finally, the entire video quality score is computed as the global average of the time-varying predicted scores: </p><formula xml:id="formula_11">Q p = 1 T T t=1 q t<label>(12)</label></formula><formula xml:id="formula_12">STRIDES ARE DENOTED AS {TEMPORAL STRIDE, SPATIAL STRIDE 2 } Layer Spatial Motion Output size Input - - T ? H ? W data layer - stride 2, 1 2 S: T ? H ? W M: T 2 ?H ? W Conv 1?7 2 , 64 stride 1, 2 2 5?7 2 , 8 stride 1, 2 2 S: T ? H 2 ? W 2 M: T 2 ? H 2 ? W 2 Pooling 1?3 2 max stride 1, 2 2 1?3 2 max stride 1, 2 2 S: T ? H 4 ? W 4 M: T 2 ? H 4 ? W 4 ResB 1 1?1 2 , 64 1?3 2 , 64 1?1 2 , 256 ?3 3?1 2 , 8 1?3 2 , 8 1?1 2 , 32 ?3 S: T ? H 4 ? W 4 M: T 2 ? H 4 ? W 4 ResB 2 1?1 2 , 128 1?3 2 , 128 1?1 2 , 512 ?4 3?1 2 , 16 1?3 2 , 16 1?1 2 , 64 ?4 S: T ? H 8 ? W 8 M: T 2 ? H 8 ? W 8 ResB 3 1?1 2 , 256 1?3 2 , 256 1?1 2 , 1024 ?6 3?1 2 , 32 1?3 2 , 32 1?1 2 , 128 ?6 S: T ? H 16 ? W 16 M: T 2 ? H 16 ? W 16 ResB 4 1?1 2 , 512 1?3 2 , 512 1?1 2 , 2048 ?3 3?1 2 , 64 1?3 2 , 64 1?1 2 , 256 ?3 S: T ? H 32 ? W 32 M: T 2 ? H 32 ? W 32</formula><p>Pooling average average S:</p><formula xml:id="formula_13">T ? 1 ? 1 M: T 2 ? 1 ? 1 FC (2,048)?2 256 S: T ? 2 M: #classes</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>An objective video quality model is expected to make quality predictions of videos consistently with subjective ratings. To this end, we employ two loss functions to encourage prediction monotonicity and precision, respectively. Following <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b33">[34]</ref>, we adopt the Pearson Linear Correlation Coefficient (PLCC) loss to optimize our model towards higher prediction precision. To better measure the degree of linear correlation against ground truths, a nonlinear mapping is commonly introduced before calculating PLCC <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Following the recommendation of the Video Quality Experts Group, this procedure can be implemented with a 4-parameter logistic function <ref type="bibr" target="#b64">[65]</ref> as:</p><formula xml:id="formula_14">Q m = ? 3 ? ? 4 1 + e ? Qp ?? 1 |? 2 | + ? 4<label>(13)</label></formula><p>where {? i |i ? {1, ..., 4}} are the learnable fitting parameters and Q m is the fitted quality score. As the reformulation in <ref type="bibr" target="#b15">[16]</ref>, the above 4-parameter logistic function can be designed as a network module of {Linear, Sigmoid, and Linear} layers, which is represented as:</p><formula xml:id="formula_15">Q m = ? 3 Sigmoid(? 1 Q p + ? 2 ) + ? 4<label>(14)</label></formula><p>where</p><formula xml:id="formula_16">? 1 = 1/|? 2 |, ? 2 = ?? 1 /|? 2 |, ? 3 = ? 3 ? ? 4 , ? 4 = ? 4 ,</formula><p>and Sigmoid(?) = 1/ 1 + e ?(?) . Given N training samples from a specific database, the differentiable PLCC loss then can be formulated as:</p><formula xml:id="formula_17">PLCC = (1 ? PLCC)/2 PLCC = i (Q i m ? Q m )(Q i ? Q) i (Q i m ? Q m ) 2 i (Q i ? Q) 2<label>(15)</label></formula><p>where Q m and Q denote the mean values of the fitted</p><formula xml:id="formula_18">predictions {Q i m } N i=1 and subjective quality opinions {Q i } N i=1</formula><p>. To the best of our knowledge, existing BVQA methods have not explored any optimization strategy to explicitly encourage the prediction monotonicity of models. This is mainly due to the non-differentiable operations of frequently used order statistics and ranking metrics. Inspired by <ref type="bibr" target="#b65">[66]</ref>, we adopt a differentiable proxy to boost the model prediction monotonicity, which is termed as a Spearman Rank-order Correlation Coefficient (SRCC) loss. In principle, the SRCC metric can be defined as the PLCC between ranks. We denote the ranks of the model predictions {Q i p } N i=1 and the ground-truth annotations</p><formula xml:id="formula_19">{Q i } N i=1 as {Q i pr } N i=1 and {Q i r } N i=1</formula><p>respectively, where we assume the elements of the original model predictions are ranked in a descending order. The differentiable SRCC loss can be formulated as:</p><formula xml:id="formula_20">SRCC = 1 ? SRCC SRCC = i (Q i pr ? Q pr )(Q i r ? Q r ) i (Q i pr ? Q pr ) 2 i (Q i r ? Q r ) 2<label>(16)</label></formula><p>where Q pr and Q r are computed from a differentiable ranking function. We refer readers to <ref type="bibr" target="#b65">[66]</ref> for details of the process of computing "soft" ranks and the proof of its differentiability. Finally, we have an overall loss function as:</p><formula xml:id="formula_21">= PLCC + ? SRCC<label>(17)</label></formula><p>where ? trades off the influence of the two elements. Note that both SRCC loss and PLCC loss are list-wise ranking loss functions, which can be used in either individual database training or mixed databases training <ref type="bibr" target="#b15">[16]</ref> settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first describe the experimental setups, including benchmarking databases, competing methods, performance criteria, and implementation details. We then present and analyze the results of three scenarios: individual, mixed, and cross databases. Finally, we verify the rationality of the proposed method through qualitative results, ablation study, and computational complexity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setups</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Benchmarking Databases:</head><p>We conduct experiments on six in-the-wild VQA databases, i.e., CVD2014 <ref type="bibr" target="#b44">[45]</ref>, KoNViD-1k <ref type="bibr" target="#b45">[46]</ref>, LIVE-Qualcomm <ref type="bibr" target="#b46">[47]</ref>, LIVE-VQC <ref type="bibr" target="#b47">[48]</ref>, YouTube-UGC <ref type="bibr" target="#b48">[49]</ref>, and LSVQ <ref type="bibr" target="#b35">[36]</ref>. The main information of these databases are summarized in <ref type="table" target="#tab_0">Table II</ref>. It is clear that they differ in content, resolution, time duration, and annotation scale, etc.   2) Competing Methods: We compare the proposed method against both adapted BIQA and BVQA models. Note that we evolve BIQA algorithms into the baselines for VQA by extracting frame-level features, followed by temporal average pooling to obtain the video-level features. The representative BIQA models are NIQE <ref type="bibr" target="#b24">[25]</ref>, IL-NIQE <ref type="bibr" target="#b66">[67]</ref>, BRISQUE <ref type="bibr" target="#b7">[8]</ref>, M3 <ref type="bibr" target="#b67">[68]</ref>, HIGRADE <ref type="bibr" target="#b68">[69]</ref>, FRIQUEE <ref type="bibr" target="#b25">[26]</ref>, CORNIA <ref type="bibr" target="#b29">[30]</ref>, HOSA <ref type="bibr" target="#b69">[70]</ref>, and pre-trained DNN models, VGG-19 <ref type="bibr" target="#b70">[71]</ref>, ResNet-50 <ref type="bibr" target="#b49">[50]</ref>. The compared BVQA methods can be roughly classified into three groups: 1), an opinion-unaware model, i.e., VIIDEO <ref type="bibr" target="#b9">[10]</ref>; 2), three hand-crafted models: V-BLIINDS <ref type="bibr" target="#b8">[9]</ref>, TLVQM <ref type="bibr" target="#b71">[72]</ref>, and VIDEVAL <ref type="bibr" target="#b72">[73]</ref>; and 3), four DNN methods: VSFA <ref type="bibr" target="#b10">[11]</ref>, MDTVSFA <ref type="bibr" target="#b15">[16]</ref>, RAPIQUE <ref type="bibr" target="#b16">[17]</ref>, and PVQ <ref type="bibr" target="#b35">[36]</ref>.</p><p>3) Performance Criteria: We use two criteria to benchmark all methods. Specifically, the SRCC is used to measure the prediction monotonicity. The PLCC is adopted to evaluate the prediction accuracy. Before calculating PLCC, a nonlinear logistic mapping is applied as suggested in <ref type="bibr" target="#b64">[65]</ref>. Here, we employ a four-parameter logistic function. Except for LSVQ, we split each database into 60% for training, 20% for validation, and 20% for testing with no overlap of video contents. Apart from three training-free methods NIQE, IL-NIQE, and VIIDEO, we re-train and validate the remaining methods on the same training/validation/testing splits. For the main experimental results summarized in <ref type="table" target="#tab_0">Table III</ref>, i.e., evaluation under the individual database training setting, we randomly repeat this procedure 100 times to prevent performance bias. The other experiments are conducted using the first ten seeds of the above 100 repetitions. Finally, the median results are recorded for comparison. As for LSVQ, we follow <ref type="bibr" target="#b35">[36]</ref> to train the model on a training set and evaluate it on two test subsets. We also provide database-size weighted average results (abbreviated as W.A.) to give insight into overall performance across different databases.</p><p>4) Implementation Details: To train the frame-level feature extractor, we use the pre-trained ResNet-50 on ImageNet <ref type="bibr" target="#b14">[15]</ref> to initialize the backbone network. Following <ref type="bibr" target="#b50">[51]</ref>, we set the parameter ? to 0.025 and ? to 1. We minimize s using 250,000 image pairs randomly sampled from the IQA databases with the resolution of 384?384?3. We train the model for 12 epochs with a learning rate decay factor of 10 for every 3 epochs from an initialization of 10 ?4 . For motion feature extraction, we apply the SlowFast pre-trained on Kinetics-400 <ref type="bibr" target="#b59">[60]</ref> as stated in Section III-C. During fine-tuning on the target VQA databases, the weights of the above two subnetworks are frozen. We set the hidden size of the GRU to 32. The duration ? and equilibrium factor ? in the hysteresisbased temporal pooling are set to 12 and 0.5, respectively. All the learnable parameters are optimized using Adam <ref type="bibr" target="#b73">[74]</ref> with a mini-batch of 32, an iteration of 40 epochs, and an initial learning rate of 5 ? 10 ?4 which decays with a ratio of 0.2 for every two epochs. And the balanced factor ? in the loss function is set to 1. The proposed method is implemented using PyTorch, and the source code is available at https://github.com/zwx8981/BVQA-2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance on Individual Databases</head><p>We compare the performance on each single database in <ref type="table" target="#tab_0">Table III and Table IV</ref>, from which we have several interesting observations. First, adapted BIQA methods attain meaningful performance on VQA databases, which validates their promises for VQA to some extent. VGG-19 and ResNet-50 demonstrate promising results, which suggest the advantage of the data-driven features learned from large-scale databases over knowledge-based features. Second, by incorporating temporal information, BVQA methods generally perform competitively against adapted BIQA models, which suggests that temporal modeling is valuable for BVQA. One exception is VIIDEO, which is calibrated to handle synthetic distortions. VSFA relies on frame-level features of pre-trained DNNs and models the temporal information using a RNN, yielding competitive performance on all databases. By incorporating local and global features in a unified framework and using a 3D network to capture temporal distortions, PVQ achieves significant performance improvement on LSVQ. Third, the proposed method learns more perceptually meaningful spatial features via a quality-aware pre-training process, transfers the motion-related knowledge from a 3D-CNN optimized on an action recognition task, and fine-tunes the entire model using a mixed list-wise ranking loss function. In general, our method achieves superior performance on all in-thewild VQA databases. Typically, the proposed method outperforms VSFA on LIVE-VQC by a large margin (+12.38% SRCC). It also presents remarkable performance on larger databases (KoNViD-1k, YouTube-UGC, and LSVQ), indicating its promising representation learning capacity on largescale applications.  <ref type="bibr" target="#b15">[16]</ref>, we adopt a database combination strategy to train the proposed method on mixed databases. We compare with MDTVSFA by mixing the four databases including CVD2014, KoNViD-1k, LIVE-Qualcomm, and LIVE-VQC for training. The results are summarized in <ref type="table" target="#tab_4">Table V</ref>. We also apply the naive linear re-scaling (LS) <ref type="bibr" target="#b71">[72]</ref> to integrate the subjective quality scores and use the L1 loss function for regression, termed as Proposed-LS. We observe that our BVQA metric consistently outperforms MDTVSFA. The mixed list-wise ranking method is superior to LS in terms of the overall performance on all databases, indicating favorable perceptual alignment across multiple databases with different scales of annotations. Taking a comparison of <ref type="table" target="#tab_0">Table V and Table III</ref>, we observe performance improvements on CVD2014 and LIVE-Qualcomm yet slight drops on KoNViD-1k and LIVE-VQC. This shows the mixed databases training strategy may help alleviating the over-fitting phenomenon on small databases while bringing acceptable disturbance to the larger ones.</p><p>2) Category-level Mixed Test: We follow the three categorical evaluation methodologies in <ref type="bibr" target="#b72">[73]</ref> to give insights into different aspects. To this end, we combine KoNViD-1k, LIVE-VQC, and YouTube-UGC calibrated via INLSA <ref type="bibr" target="#b74">[75]</ref> for experiments, termed as Combined U .  a) Resolution Subsets: According to resolution, the Combined U can be formed into three sets, i.e., 402 1080pvideos, 564 720p-videos, and 607 videos with resolution no more than 480p. We list the results in the "Resolution" column of <ref type="table" target="#tab_0">Table VI.</ref> b) Content Subsets: There is plenty of researches concentrating on different content-based scenarios. It is also interesting to observe the behaviors of compared models on different contents. To this end, we conduct experiments using three typical contents, i.e., 134 Screens, 70 Animations, and 180 Gamings. We report the performance in the "Content" column of <ref type="table" target="#tab_0">Table VI.</ref> c) Quality Subsets: The partition based on low and high quality is a valuable way to analyze the defects and success of the proposed model, which has been adopted as an evaluation methodology in both IQA <ref type="bibr" target="#b75">[76]</ref> and VQA <ref type="bibr" target="#b72">[73]</ref>. We use the threshold of 3.5537 <ref type="bibr" target="#b72">[73]</ref> to partition Combined U into 1,469 low quality and 1,458 high quality materials, and tabulate the comparisons in the "Quality" column of <ref type="table" target="#tab_0">Table VI.</ref> The results in <ref type="table" target="#tab_0">Table VI</ref> manifest the proposed method is effective and robust across different resolutions, contents, and quality levels. From the "Resolution" column of Table VI, we can see that learning-based features are more powerful than hand-crafted features. However, the "Content" and "Quality" columns of <ref type="table" target="#tab_0">Table VI</ref> demonstrate that pre-trained DNNs on an image classification task struggle to handle scenarios with different contents and quality levels. This further verifies the effectiveness of the proposed quality-aware pre-training and motion perception schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-database Evaluation</head><p>A BVQA model is expected to generalize well to unseen distortion scenarios. In this regard, we conduct cross-database evaluation by training BVQA models on one database and testing them on the other databases. We report the results in <ref type="table" target="#tab_0">Table VII and Table VIII</ref>, from which We observe that the proposed method performs well to unseen databases. Specifically, on small and medium scale databases, the improvement of average performance is over 9% (SRCC). For the largest database LSVQ, our method achieves a more than 4% (SRCC) improvement. These performance gains demonstrate the favorable generalizability of the proposed method, which we believe is mainly due to the effectiveness of the proposed knowledge transferring strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head><p>In this subsection, we present several successful and failure samples in <ref type="figure" target="#fig_3">Fig. 4 and Fig. 5</ref>, respectively. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, our method can distinguish quality levels even with small differences of MOSs. Then, we visualize several failure cases in <ref type="figure">Fig. 5</ref>. Among them, for the two comparisons of Video A against Video B and Video C against Video D, our method still makes reasonable quality predictions although their relative rankings are not consistent with the MOSs. Note that both cases are of relatively high perceptual quality, which poses a great challenge to BVQA models for quality discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ablation Study</head><p>To verify the rationality of each module of the proposed method, we conduct a series of ablation studies from the following six aspects.</p><p>Feature Ablation We evaluate the effectiveness of different features fusion strategies. We begin with a baseline which uses the pre-trained ResNet-50 on ImageNet as the framelevel feature extractor, which is equipped with a GRU to model the temporal information. We then replace the baseline frame-level feature extractor with a ResNet-50 trained using the proposed quality-aware pre-training scheme (S-Feature) or the motion features of SlowFast F (M-Feature). Note that the proposed method relies on both spatial and motion features (S+M-Feature), and all variants are trained with the mixed loss function as described in Section III-E. We report the results in the "Feature" section of <ref type="table" target="#tab_0">Table IX</ref>, where we have several interesting observations. First, S-Feature alone is able to introduce a 4.23% improvement of weighted average SRCC over the baseline, indicating the effectiveness of the     proposed quality-aware pre-training strategy. In contrast, M-Feature alone results in a significant performance drop on all databases, which suggests that spatial features play a vital role in perceiving the quality of videos. Combining S-Feature and M-Feature leads to the most promising results, demonstrating the complementarity between spatial and motion features for the VQA task. Typically, the motion features lead to a notable improvement (+6.6% SRCC) on LIVE-VQC, where motionrelated distortions are prevailing <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. We also replace GRU with average temporal pooling and then use SVR to learn a feature-quality mapping on S-Feature (S-Feature+SVR).</p><p>The unfavorable results highlight the importance of modeling temporal-memory effects in an appropriate way.</p><p>Interaction Ablation We also make two preliminary efforts to explore the spatio-temporal interaction effect. First, we integrate the spatial features generated from the slow pathway of SlowFast (dubbed as SlowFast S ) with our S+M-Feature (dubbed as S+M+SF S -Feature). Profiting from the lateral connections between the two pathways in the Slow-Fast network, incorporating the features of the slow pathway is expected to capture the interaction between spatial and temporal distortions. Specifically, we up-sample the features of SlowFast S to the same temporal dimension of our S+M-Feature, followed by the channel-wise feature concatenation. Second, similar to <ref type="bibr" target="#b35">[36]</ref>, we aim for combining 2D and 3D pre-trained models to learn complementary features. Specifically, we use a pre-trained 3D ResNet-18 <ref type="bibr" target="#b76">[77]</ref> to extract the spatio-temporal features (dubbed as ST 3D -Feature). We down-sample the proposed S-Feature to the same temporal dimension of ST 3D -Feature, and then concatenate them (dubbed as S+ST 3D -Feature). As shown in the "Interaction" section of <ref type="table" target="#tab_0">Table IX</ref>, both practices are capable of introducing performance improvement in terms of weighted average SRCC and PLCC results over the S-Feature baseline, which further confirms the importance of combining spatial and temporal information for BVQA. However, neither of them outperforms the proposed S+M-Feature, which we believe is because the spatial features of the pre-trained SlowFast and the 3D ResNet-18 do not match the BVQA task well, resulting in negative transfer phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-training Ablation</head><p>We experiment with different quality-aware pre-training schemes. Specifically, similar to <ref type="bibr" target="#b11">[12]</ref>, we generate pseudo-labels for KADIS-700k <ref type="bibr" target="#b77">[78]</ref> using MS-SSIM <ref type="bibr" target="#b78">[79]</ref>, and use them to pre-train a ResNet-50 in a regression manner (dubbed as S FR ). We also follow the practice of <ref type="bibr" target="#b36">[37]</ref> to train the frame-level feature extractor on KADIS-700k using a cross-entropy loss function and a pairwise hinge loss function, accounting for distortion types classification and degradation levels ranking. Note that we follow <ref type="bibr" target="#b36">[37]</ref> to exclude distortion types 13 and 23 due to the license issue. After that, we fine-tune the pre-trained feature extractor on KADID-10k <ref type="bibr" target="#b77">[78]</ref> with the above two loss functions and an L2 loss function for MOSs regression (dubbed as S KS+KD ). As a comparison, we further use authentically distorted KonIQ-10k <ref type="bibr" target="#b42">[43]</ref> database with 10,073 human-annotated for quality-aware pre-training, dubbed as S Ko Truth . The experimental results are summarized in the "Pretraining" section of <ref type="table" target="#tab_0">Table IX</ref>, from which we can observe that the proposed quality-aware pre-training strategy leads to the best performance, which we believe is due to the effective knowledge transfer from meaningful source domains with similar distortion scenarios (authentic distortions). Although S Ko Truth is also trained with authentic distortions, the proposed strategy obtains more powerful feature representation due to incorporating broader realistic contents and more diverse distortions for quality-aware pre-training. We also notice that S F R +M-Feature under-performs other competitors even with larger training samples, which suggests that the noisy proxy labels may mislead the representation learning, highlighting the importance of label precision.</p><p>Loss Ablation We evaluate the model trained with different loss functions, i.e., monotonicity-induced SRCC Loss, linearity-induced PLCC Loss, and the combination of them (Mixed). Note that the L1 loss function is taken here as the baseline for comparison. All the variants are based on the S+M-Feature. In the "Loss" section of <ref type="table" target="#tab_0">Table IX</ref>, we find that both SRCC Loss and PLCC Loss alone are able to yield promising results, outperforming the L1 loss baseline by clear margins. By combining SRCC and PLCC loss functions, we obtain a 0.93% additional gain in terms of the weighted SRCC. Notably, the Mixed Loss produces relatively significant improvements on CVD2014, LIVE-Qualcomm, and YouTube-UGC.</p><p>Ensemble Ablation As an alternative to the GRU, we use a Transformer <ref type="bibr" target="#b79">[80]</ref> encoder to model the temporal information, where the number of layers, the dimension of the feed-forward network, the number of heads, and the dropout ratio are set to 2, 2048, 2, and 0.2, respectively. We also explore a simple ensemble trick to further boost the performance as:</p><formula xml:id="formula_22">Q e = ?Q G f + (1 ? ?)Q T f<label>(18)</label></formula><p>where Q G f and Q T f are the predicted scores with GRU and Transformer, respectively. The parameter ? is the combination factor in the ensemble procedure. Concretely, we conduct experiments by varying ? from 0 to 1, stepped by 0.01. Q e is the final ensemble quality score. As shown in the "Ensemble" section of <ref type="table" target="#tab_0">Table IX</ref>, GRU slightly outperforms Transformer, and an ensemble of them leads to a 0.92% gain on the weighted average SRCC performance.</p><p>Distance Ablation It is interesting to explore whether we can rely on the CORAL <ref type="bibr" target="#b39">[40]</ref> distance to select source domains. To this end, we treat the mixed IQA databases as the source domain and compute the CORAL distances between it and VQA databases. The pairwise distance results are shown in <ref type="table" target="#tab_9">Table X</ref>. We have several useful observations. First, we can rely on CORAL distance to select better source domains when the difference of distances is significant enough. For example, the distances between ImageNet and VQA databases are larger than all IQA domains, resulting in worse final performances. Second, the distances are not entirely monotonic to the overall performance on the target domains. This is a reasonable phenomenon because we measure covariate shifts between two domains with an assumption that the conditional distributions (i.e., from input to quality predictions) and label distributions (i.e., MOSs) of the source and target domains are exactly the same, only by which we can simplify measuring covariate shifts into measuring the marginal distribution shifts between the source and target domains. Unfortunately, this assumption is difficult to hold in practice, where the quality spaces of IQA and VQA tasks are not entirely identical (label distribution), and the conditional distributions (i.e., the mapping functions from images or videos to their quality spaces) are also different. Empirically, we achieve promising results on the target VQA tasks by combining multiple diverse IQA databases as the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Computational Complexity</head><p>In practical applications, computational efficiency is desperately desired. We benchmark the computational complexity in this subsection. To make a fair comparison, all the methods are tested on the same machine, i.e., a Dell Precision 7920 Tower Workstation equipped with an Intel Xeon(R) Gold 5220R CPU?2 @2.20Ghz?96, 128G RAM, and NVIDIA Quadro RTX6000 24G GPU?2. We use the implementations of the compared methods released by their authors. All methods are tested with MATLAB R2020a or Python 3.8.8, both under the Ubuntu 18.04.5 LTS operating system. We test our method using CPU and GPU, respectively. Meanwhile, we adopt both serial (SEL) and parallel (PAL) modes for our proposed two groups of features. These test methods are briefly recorded as Proposed CPU SEL, Proposed CPU PAL, Proposed GPU SEL, and Proposed GPU PAL. First, by fixing the resolution at 1280?720, we plot the performance as a function of the runtime in <ref type="figure" target="#fig_5">Fig. 6(a)</ref>. Second, we evaluate the variation of runtime under different resolutions, i.e., 360p, 540p, 720p, 1080p, 1440p and 2160p, as displayed in <ref type="figure" target="#fig_5">Fig. 6(b)</ref>. All the results are derived from the average of ten repeated tests that aims to remove random bias. We select a video with a resolution of 1280?720 and temporal length of 467 frames from CVD2014, and all videos with different resolutions are transformed from it. Overall, we have two important conclusions. First, <ref type="figure" target="#fig_5">Fig. 6(a)</ref> shows that the proposed method (accelerated with GPU) achieves a favorable tradeoff between effectiveness and efficiency. Second, as shown in <ref type="figure" target="#fig_5">Fig. 6(b)</ref>, the proposed method delivers a larger advantage in efficiency as the resolution increases. For example, with the resolution growing from 360p to 2160p, our algorithm (Proposed GPU PAL) can achieve an increase in speed from 3 times to 16 times compared against TLVQM. Although the proposed method has a higher computational complexity on CPU, it delivers better prediction accuracy result in stark contrast to other methods. In addition, it can directly benefit from significant acceleration by GPU. In the future, it would be a promising topic to compress the model for better efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have proposed a DNN-based BVQA method for the in-the-wild scenario. We use model-based transfer learning methods to leverage knowledge from two types of source domains, corresponding to spatial appearance and temporal motion, respectively. Specifically, we conduct a quality-aware pre-training on multiple IQA databases to learn the framelevel feature extractor, which significantly enhances the feature representation without laborious efforts on video quality annotation. Similarly, we use a pre-trained DNN on action recognition to account for the motion perception of videos, which is complementary to the spatial features. We verify the promising performance of the proposed method through extensive experiments on six in-the-wild VQA databases. Besides, the merging of the differentiable PLCC and SRCC loss functions further boosts the performance.</p><p>As a limitation of the current model, it remains to be a challenging task to explore a more rational spatio-temporal interaction strategy. We believe the efficient joint optimization of the spatio-temporal representation is a promising direction. In addition, it is also important to incorporate the viewing conditions for making quality predictions <ref type="bibr" target="#b80">[81]</ref> of videos captured in varying environments. Another direction is developing effective continual learning methods <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b82">[83]</ref> for handling the BVQA where the data is streaming.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall framework of the proposed VQA model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a) Breakdancing (b) Riding with horse (c) Cooking on campfire (d) Playing basketball Comparison of videos with different motion patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>4447 0.4645 0.5972 0.5950 0.5092 0.5088 0.1770 0.3937 0.0945 0.4862 0.3200 0.3605 0.4347 0.4617 0.2986 0.3110 0.4067 0.4329 M3 0.4541 0.4958 0.5129 0.4933 0.4989 0.5180 0.2167 0.2387 0.2813 0.5212 0.2505 0.3262 0.3992 0.4840 0.2384 0.2328 0.3691 0.4033 HIGRADE 0.4574 0.5316 0.5447 0.5478 0.6147 0.6282 0.4603 0.5555 0.2703 0.4348 0.5692 0.6233 0.5463 0.5707 0.4659 0.4792 0.5178 0.5442 FRIQUEE 0.5513 0.6039 0.6130 0.6118 0.6735 0.6966 0.4779 0.5612 0.1846 0.4929 0.6527 0.6944 0.5319 0.5926 0.5004 0.5360 0.5490 0.5932 CORNIA 0.6140 0.7063 0.6176 0.6190 0.6667 0.7080 0.3304 0.4187 0.2461 0.4674 0.5241 0.6060 0.4867 0.5454 0.3619 0.3712 0.4911 0.5330 HOSA 0.5747 0.6423 0.6578 0.6489 0.7158 0.7302 0.3396 0.5127 0.1329 0.4591 0.5611 0.6057 0.5411 0.5874 0.4296 0.4402 0.5351 0.5696 VGG-19 0.6240 0.6546 0.6407 0.6370 0.7076 0.7138 0.4025 0.4689 0.1340 0.4182 0.5436 0.5841 0.5528 0.5969 0.4332 0.4375 0.5419 0.5667 ResNet-50 0.6373 0.6713 0.6548 0.6781 0.7591 0.7745 0.4213 0.5220 0.2549 0.4270 0.5639 0.5945 0.5944 0.6476 0.4621 0.4701 0.5751 0.6073 V-BLIINDS 0.4048 0.5063 0.5642 0.5758 0.5912 0.6096 0.1730 0.3036 -0.1560 0.4471 0.4138 0.5447 0.5033 0.5095 0.3503 0.3545 0.4457 0.4778 TLVQM 0.5530 0.6329 0.6464 0.6467 0.6113 0.6332 0.3266 0.4690 0.1274 0.3926 0.6014 0.6213 0.5026 0.5497 0.5064 0.5130 0.5314 0.5653 VIDEVAL 0.5536 0.6016 0.6312 0.6381 0.6106 0.6667 0.5307 0.6469 0.2109 0.4480 0.6971 0.7105 0.5816 0.6194 0.5558 0.5830 0.5785 0.6168 RAPIQUE 0.5262 0.6230 0.5911 0.6362 0.6820 0.6927 0.4392 0.4514 0.3120 0.4882 0.5471 0.6643 0.6349 0.6828 0.5250 0.5391 0.5807 0.6210 VSFA 0.6512 0.6697 0.6677 0.6650 0.7084 0.7277 0.4700 0.5493 -0.0021 0.4355 0.6774 0.7501 0.6169 0.6741 0.5286 0.5313 0.5999 0.6327 Proposed 0.6759 0.6902 0.7431 0.7333 0.7819 0.7972 0.5726 0.6845 0.5604 0.6635 0.7073 0.7655 0.6863 0.7118 0.6524 0.6694 0.6896 0.7110</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>4732 0.5141 0.2550 0.2567 0.6160 0.6492 0.5682 0.6052 0.5702 0.5726 0.1519 0.1666 0.5175 0.5065 0.4573 0.4841 0.4628 0.4775 TLVQM 0.4993 0.5238 0.3920 0.4255 0.5724 0.6293 0.7206 0.7530 0.6398 0.6310 0.2181 0.2501 0.5558 0.5783 0.4883 0.5463 0.5278 0.5548 VIDEVAL 0.4902 0.5016 0.5784 0.5811 0.5916 0.6289 0.7022 0.7153 0.6565 0.6533 0.2390 0.2612 0.6839 0.6863 0.4399 0.4839 0.5597 0.5740 VSFA 0.4129 0.4840 0.4306 0.4666 0.5934 0.6061 0.7175 0.7597 0.6949 0.7109 0.4221 0.4525 0.7167 0.7111 0.6257 0.6597 0.6187 0.6418 Proposed 0.6696 0.7007 0.4701 0.4807 0.6949 0.7120 0.7799 0.7803 0.7382 0.7210 0.6025 0.6029 0.7847 0.7818 0.6887 0.7274 0.7092 0.7121 (a) Three representative frames of Video A in YouTube-UGC (b) Three representative frames of Video B in YouTube-UGC (c) Three representative frames of Video C in YouTube-UGC (d) Three representative frames of Video D in YouTube-UGC (e) Three representative frames of Video E in YouTube-UGC (f) Three representative frames of Video F in YouTube-UGC Successful cases sampled from the YouTube-UGC [49] test set. The MOSs of A to F are 1.462, 1.706, 2.983, 3.033, 4.399, and 4.526, respectively. The predictions of our model are 1.760, 2.527, 3.185, 3.277, 4.311, and 4.453, respectively. Both groups of scores conform to the ranking of A&lt;B&lt;C&lt;D&lt;E&lt;F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature baseline 0 .</head><label>0</label><figDesc>8725 0.8860 0.7735 0.7891 0.7533 0.7968 0.7107 0.7517 0.7864 0.7861 0.7726 0.7888 S-Feature 0.8516 0.8674 0.8272 0.8290 0.8184 0.8311 0.7752 0.7877 0.8143 0.8127 0.8149 0.8191 S-Feature+SVR 0.7583 0.7890 0.8182 0.8175 0.7400 0.7936 0.7459 0.7781 0.8013 0.8033 0.7909 0.8024 M-Feature 0.7741 0.7724 0.6649 0.6631 0.6389 0.6465 0.7245 0.7217 0.6624 0.6566 0.6804 0.6776 S+M-Feature (Proposed) 0.8675 0.8717 0.8362 0.8335 0.8361 0.8389 0.8412 0.8415 0.8233 0.8228 0.8349 0.8342 Interaction S+M+SF S -Feature 0.8613 0.8823 0.8310 0.8326 0.8189 0.8244 0.8355 0.8408 0.8066 0.8076 0.8249 0.8285 S+ST 3D -Feature 0.8790 0.8917 0.8152 0.8171 0.8318 0.8471 0.7909 0.8027 0.8132 0.8120 0.8158 0.8199 S+M-Feature (Proposed) 0.8675 0.8717 0.8362 0.8335 0.8361 0.8389 0.8412 0.8415 0.8233 0.8228 0.8349 0.8342 Pre-training S F R +M-Feature 0.8175 0.8293 0.7390 0.7355 0.5930 0.6484 0.7705 0.7810 0.7484 0.7431 0.7441 0.7471 S KS+KD +M-Feature 0.8140 0.8405 0.8314 0.8279 0.7500 0.7829 0.8073 0.8096 0.7792 0.7783 0.8033 0.8060 S Ko T ruth +M-Feature 0.8534 0.8654 0.8258 0.8241 0.7768 0.7880 0.8248 0.8227 0.7837 0.7800 0.8102 0.8095 S+M-Feature (Proposed) 0.8675 0.8717 0.8362 0.8335 0.8361 0.8389 0.8412 0.8415 0.8233 0.8228 0.8349 0.8342 Loss L1 Loss baseline 0.8094 0.8205 0.8162 0.8136 0.7266 0.7379 0.8113 0.7984 0.7918 0.7811 0.8011 0.7957 SRCC Loss 0.8526 0.8632 0.8308 0.8296 0.7982 0.8120 0.8405 0.8428 0.7931 0.7821 0.8192 0.8170 PLCC Loss 0.8582 0.8563 0.8289 0.8275 0.8029 0.8226 0.8343 0.8423 0.8152 0.8104 0.8256 0.8260 Mixed Loss (Proposed) 0.8675 0.8717 0.8362 0.8335 0.8361 0.8389 0.8412 0.8415 0.8233 0.8228 0.8349 0.8342 Ensemble GRU 0.8675 0.8717 0.8362 0.8335 0.8361 0.8389 0.8412 0.8415 0.8233 0.8228 0.8349 0.8342 Transformer 0.8527 0.8690 0.8382 0.8358 0.8303 0.8275 0.8355 0.8272 0.8193 0.8162 0.8318 0.8295 GRU+Transformer 0.8703 0.8760 0.8441 0.8394 0.8529 0.8467 0.8457 0.8386 0.8362 0.8291 0.8441 0.8388 (Ensemble ratio) (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>(a) The weighted average SRCC results (collected from Table III) as a function of the running time in the logarithm space. (b) The running time as a function of the video resolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>ARCHITECTURES OF THE "SPATIAL" AND "MOTION" SUB-NETWORKS. THE KERNEL DIMENSIONS ARE DENOTED BY {T?(H?W),C} FOR TEMPORAL, SPATIAL, AND CHANNEL DIMENSIONS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II SUMMARY</head><label>II</label><figDesc>OF THE BENCHMARKING IN-THE-WILD VQA DATABASES</figDesc><table><row><cell>Database</cell><cell cols="2">Number of Videos Number of Scenes</cell><cell>Resolution</cell><cell cols="3">Format Time Duration Max Length</cell><cell>Annotation Range</cell></row><row><cell>CVD2014 [45]</cell><cell>234</cell><cell>5</cell><cell>480p, 720p</cell><cell>RGB</cell><cell>10-25s</cell><cell>830</cell><cell>[-6.50, 93.38]</cell></row><row><cell>KoNViD-1k [46]</cell><cell>1,200</cell><cell>1,200</cell><cell>540p</cell><cell>RGB</cell><cell>8s</cell><cell>240</cell><cell>[1.22, 4.64]</cell></row><row><cell>LIVE-Qualcomm [47]</cell><cell>208</cell><cell>54</cell><cell>1080p</cell><cell>YUV</cell><cell>15s</cell><cell>526</cell><cell>[16.5621, 73.6428]</cell></row><row><cell>LIVE-VQC [48]</cell><cell>585</cell><cell>585</cell><cell>240p-1080p</cell><cell>RGB</cell><cell>10s</cell><cell>1,202</cell><cell>[6.2237, 94.2865]</cell></row><row><cell>YouTube-UGC [49]</cell><cell>1,142 *</cell><cell>1,142</cell><cell>360p-4k</cell><cell>YUV</cell><cell>20s</cell><cell>2,819</cell><cell>[1.242, 4.698]</cell></row><row><cell>LSVQ [36]</cell><cell>39,072 **</cell><cell>39,072</cell><cell>99p-4k</cell><cell>RGB</cell><cell>5-12s</cell><cell>4,096</cell><cell>[2.4483, 91.4194]</cell></row></table><note>* Following [49], we exclude 57 grayscale videos in latest YouTube-UGC database, remaining 1,142 videos.** The number of existing ground truths (MOSs) in LSVQ is 39,072.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III MEDIAN</head><label>III</label><figDesc>SRCC AND PLCC RESULTS ON CVD2014, KONVID-1K, LIVE-QUALCOMM, LIVE-VQC, AND YOUTUBE-UGC UNDER THE INDIVIDUAL DATABASE TRAINING SETTING. THE STANDARD DEVIATION IS SHOWN IN GREY</figDesc><table><row><cell></cell><cell>Database</cell><cell>CVD2014</cell><cell>KoNViD-1k</cell><cell>LIVE-Qualcomm</cell><cell>LIVE-VQC</cell><cell>YouTube-UGC</cell><cell>W.A.</cell></row><row><cell></cell><cell>Criteria</cell><cell></cell><cell></cell><cell>SRCC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NIQE</cell><cell>0.4755 (? 0.1174)</cell><cell>0.5392 (? 0.0366)</cell><cell>0.4608 (? 0.1049)</cell><cell>0.5930 (? 0.0581)</cell><cell>0.2499 (? 0.0532)</cell><cell>0.4412</cell></row><row><cell></cell><cell>IL-NIQE</cell><cell>0.5295 (? 0.1039)</cell><cell>0.5199 (? 0.0377)</cell><cell>0.0556 (? 0.1489)</cell><cell>0.5019 (? 0.0669)</cell><cell>0.3198 (? 0.0468)</cell><cell>0.4209</cell></row><row><cell></cell><cell>BRISQUE</cell><cell>0.7900 (? 0.0570)</cell><cell>0.6493 (? 0.0416)</cell><cell>0.5527 (? 0.1029)</cell><cell>0.5936 (? 0.0634)</cell><cell>0.3932 (? 0.0613)</cell><cell>0.5566</cell></row><row><cell></cell><cell>M3</cell><cell>0.8009 (? 0.0482)</cell><cell>0.6422 (? 0.0408)</cell><cell>0.6272 (? 0.0948)</cell><cell>0.5876 (? 0.0649)</cell><cell>0.3450 (? 0.0528)</cell><cell>0.5421</cell></row><row><cell>BIQA</cell><cell>HIGRADE FRIQUEE</cell><cell>0.7096 (? 0.0768) 0.8212 (? 0.0430)</cell><cell>0.7062 (? 0.0333) 0.7352 (? 0.0275)</cell><cell>0.6326 (? 0.0843) 0.7158 (? 0.0842)</cell><cell>0.5959 (? 0.0653) 0.6502 (? 0.0553)</cell><cell>0.7252 (? 0.0340) 0.7538 (? 0.0286)</cell><cell>0.6892 0.7315</cell></row><row><cell></cell><cell>CORNIA</cell><cell>0.6277 (? 0.0846)</cell><cell>0.7351 (? 0.0274)</cell><cell>0.4551 (? 0.1155)</cell><cell>0.6808 (? 0.0460)</cell><cell>0.5671 (? 0.0422)</cell><cell>0.6440</cell></row><row><cell></cell><cell>HOSA</cell><cell>0.8478 (? 0.0349)</cell><cell>0.7606 (? 0.0239)</cell><cell>0.7300 (? 0.0793)</cell><cell>0.6784 (? 0.0476)</cell><cell>0.5961 (? 0.0427)</cell><cell>0.6947</cell></row><row><cell></cell><cell>VGG-19</cell><cell>0.8367 (? 0.0403)</cell><cell>0.7209 (? 0.0287)</cell><cell>0.7197 (? 0.0771)</cell><cell>0.6762 (? 0.0494)</cell><cell>0.6037 (? 0.0439)</cell><cell>0.6814</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>0.8492 (? 0.0412)</cell><cell>0.7651 (? 0.0249)</cell><cell>0.7561 (? 0.0688)</cell><cell>0.6814 (? 0.0482)</cell><cell>0.6542 (? 0.0374)</cell><cell>0.7183</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.0503 (? 0.1387)</cell><cell>0.2874 (? 0.0517)</cell><cell>0.0808 (? 0.1237)</cell><cell>0.0461 (? 0.0815)</cell><cell>0.0567 (? 0.0551)</cell><cell>0.1381</cell></row><row><cell></cell><cell>V-BLIINDS</cell><cell>0.7950 (? 0.0661)</cell><cell>0.7063 (? 0.0343)</cell><cell>0.5702 (? 0.0987)</cell><cell>0.6811 (? 0.0530)</cell><cell>0.5348 (? 0.0467)</cell><cell>0.6415</cell></row><row><cell></cell><cell>TLVQM</cell><cell>0.7799 (? 0.0494)</cell><cell>0.7588 (? 0.0260)</cell><cell>0.7849 (? 0.0650)</cell><cell>0.7878 (? 0.0341)</cell><cell>0.6568 (? 0.0418)</cell><cell>0.7323</cell></row><row><cell>BVQA</cell><cell>VIDEVAL</cell><cell>0.8144 (? 0.0462)</cell><cell>0.7704 (? 0.0242)</cell><cell>0.6706 (? 0.0975)</cell><cell>0.7438 (? 0.0455)</cell><cell>0.7763 (? 0.0280)</cell><cell>0.7647</cell></row><row><cell></cell><cell>RAPIQUE</cell><cell>0.8071 (? 0.0557)</cell><cell>0.7884 (? 0.0236)</cell><cell>0.6658 (? 0.0970)</cell><cell>0.7413 (? 0.0450)</cell><cell>0.7473 (? 0.0329)</cell><cell>0.7600</cell></row><row><cell></cell><cell>VSFA</cell><cell>0.8501 (? 0.0390)</cell><cell>0.7943 (? 0.0214)</cell><cell>0.7080 (? 0.0822)</cell><cell>0.7176 (? 0.0483)</cell><cell>0.7873 (? 0.0229)</cell><cell>0.7772</cell></row><row><cell></cell><cell>Proposed</cell><cell>0.8626 (? 0.0396)</cell><cell cols="2">0.8354 (? 0.0193) 0.8334 (? 0.0622)</cell><cell cols="3">0.8414 (? 0.0276) 0.8252 (? 0.0219) 0.8348</cell></row><row><cell></cell><cell>Criteria</cell><cell></cell><cell></cell><cell>PLCC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>NIQE</cell><cell>0.6070 (? 0.1042)</cell><cell>0.5513 (? 0.0348)</cell><cell>0.5336 (? 0.0985)</cell><cell>0.6312 (? 0.0504)</cell><cell>0.2982 (? 0.0484)</cell><cell>0.4822</cell></row><row><cell></cell><cell>IL-NIQE</cell><cell>0.5420 (? 0.0953)</cell><cell>0.5371 (? 0.0489)</cell><cell>0.2852 (? 0.1213)</cell><cell>0.5433 (? 0.0653)</cell><cell>0.3585 (? 0.0475)</cell><cell>0.4624</cell></row><row><cell></cell><cell>BRISQUE</cell><cell>0.8049 (? 0.0616)</cell><cell>0.6513 (? 0.0405)</cell><cell>0.5986 (? 0.1031)</cell><cell>0.6242 (? 0.0611)</cell><cell>0.4073 (? 0.0612)</cell><cell>0.5713</cell></row><row><cell></cell><cell>M3</cell><cell>0.8138 (? 0.0533)</cell><cell>0.6452 (? 0.0411)</cell><cell>0.6687 (? 0.0930)</cell><cell>0.6218 (? 0.0613)</cell><cell>0.3769 (? 0.0554)</cell><cell>0.5634</cell></row><row><cell>BIQA</cell><cell>HIGRADE FRIQUEE</cell><cell>0.7261 (? 0.0869) 0.8415 (? 0.0433)</cell><cell>0.7104 (? 0.0335) 0.7354 (? 0.0265)</cell><cell>0.6691 (? 0.0853) 0.7481 (? 0.0842)</cell><cell>0.6188 (? 0.0635) 0.6914 (? 0.0595)</cell><cell>0.7103 (? 0.0340) 0.7505 (? 0.0283)</cell><cell>0.6930 0.7410</cell></row><row><cell></cell><cell>CORNIA</cell><cell>0.6631 (? 0.0765)</cell><cell>0.7356 (? 0.0257)</cell><cell>0.5203 (? 0.0984)</cell><cell>0.7239 (? 0.0379)</cell><cell>0.5851 (? 0.0396)</cell><cell>0.6642</cell></row><row><cell></cell><cell>HOSA</cell><cell>0.8673 (? 0.0312)</cell><cell>0.7580 (? 0.0244)</cell><cell>0.7451 (? 0.0731)</cell><cell>0.7242 (? 0.0376)</cell><cell>0.6037 (? 0.0417)</cell><cell>0.7066</cell></row><row><cell></cell><cell>VGG-19</cell><cell>0.8505 (? 0.0406)</cell><cell>0.7385 (? 0.0263)</cell><cell>0.7594 (? 0.0705)</cell><cell>0.7281 (? 0.0446)</cell><cell>0.6074 (? 0.0441)</cell><cell>0.7013</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>0.8652 (? 0.0392)</cell><cell>0.7781 (? 0.0232)</cell><cell>0.7957 (? 0.0601)</cell><cell>0.7381 (? 0.0395)</cell><cell>0.6485 (? 0.0413)</cell><cell>0.7344</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.2479 (? 0.1035)</cell><cell>0.3083 (? 0.0480)</cell><cell>0.2301 (? 0.0980)</cell><cell>0.2100 (? 0.0720)</cell><cell>0.1497 (? 0.0544)</cell><cell>0.2284</cell></row><row><cell></cell><cell>V-BLIINDS</cell><cell>0.8067 (? 0.0761)</cell><cell>0.7011 (? 0.0342)</cell><cell>0.6269 (? 0.0881)</cell><cell>0.6997 (? 0.0499)</cell><cell>0.5409 (? 0.0461)</cell><cell>0.6493</cell></row><row><cell></cell><cell>TLVQM</cell><cell>0.7904 (? 0.0499)</cell><cell>0.7598 (? 0.0254)</cell><cell>0.8152 (? 0.0655)</cell><cell>0.7942 (? 0.0339)</cell><cell>0.6470 (? 0.0406)</cell><cell>0.7331</cell></row><row><cell>BVQA</cell><cell>VIDEVAL</cell><cell>0.8320 (? 0.0538)</cell><cell>0.7709 (? 0.0273)</cell><cell>0.7054 (? 0.1035)</cell><cell>0.7476 (? 0.0445)</cell><cell>0.7715 (? 0.0290)</cell><cell>0.7673</cell></row><row><cell></cell><cell>RAPIQUE</cell><cell>0.8232 (? 0.0529)</cell><cell>0.8051 (? 0.0217)</cell><cell>0.6913 (? 0.0916)</cell><cell>0.7618 (? 0.0408)</cell><cell>0.7569 (? 0.0310)</cell><cell>0.7755</cell></row><row><cell></cell><cell>VSFA</cell><cell>0.8690 (? 0.0379)</cell><cell>0.7985 (? 0.0207)</cell><cell>0.7741 (? 0.0724)</cell><cell>0.7707 (? 0.0379)</cell><cell cols="2">0.7888 (? 0.0225) 0.7938</cell></row><row><cell></cell><cell>Proposed</cell><cell>0.8826 (? 0.0372)</cell><cell cols="2">0.8339 (? 0.0178) 0.8371 (? 0.0517)</cell><cell>0.8394 (? 0.0284)</cell><cell>0.8178 (? 0.0260)</cell><cell>0.8330</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SRCC</head><label>IV</label><figDesc>AND PLCC RESULTS ON LSVQ [36] UNDER THE INDIVIDUAL DATABASE TRAINING SETTING. THE DATABASE SIZE IS SHOWN IN THE BRACKET</figDesc><table><row><cell></cell><cell>Database</cell><cell cols="2">Test(7.4k) Test-1080p(3.5k) W.A.(10.9k)</cell></row><row><cell></cell><cell>Criteria</cell><cell cols="2">SRCC PLCC SRCC PLCC SRCC PLCC</cell></row><row><cell cols="3">BIQA BRISQUE 0.579 0.576 0.497</cell><cell>0.531 0.5527 0.5616</cell></row><row><cell></cell><cell>TLVQM</cell><cell>0.772 0.774 0.589</cell><cell>0.616 0.7132 0.7233</cell></row><row><cell></cell><cell cols="2">VIDEVAL 0.794 0.783 0.545</cell><cell>0.554 0.7140 0.7095</cell></row><row><cell>BVQA</cell><cell cols="2">VSFA PVQ(w/o) 0.814 0.816 0.686 0.801 0.796 0.675</cell><cell>0.704 0.7605 0.7665 0.708 0.7729 0.7813</cell></row><row><cell></cell><cell>PVQ(w)</cell><cell>0.827 0.828 0.711</cell><cell>0.739 0.7898 0.7994</cell></row><row><cell></cell><cell cols="2">Proposed 0.852 0.854 0.772</cell><cell>0.788 0.8261 0.8324</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V MEDIAN</head><label>V</label><figDesc>SRCC AND PLCC RESULTS UNDER THE MIXED DATABASES</figDesc><table><row><cell></cell><cell cols="2">TRAINING SETTING</cell><cell></cell><cell></cell></row><row><cell>Database</cell><cell cols="4">Criteria MDTVSFA Proposed-LS Proposed</cell></row><row><cell>CVD2014</cell><cell>SRCC PLCC</cell><cell>0.8326 0.8347</cell><cell>0.8406 0.8519</cell><cell>0.8737 0.8793</cell></row><row><cell>KoNViD-1k</cell><cell>SRCC PLCC</cell><cell>0.7816 0.7786</cell><cell>0.7976 0.7896</cell><cell>0.8205 0.8142</cell></row><row><cell>LIVE-Qualcomm</cell><cell>SRCC PLCC</cell><cell>0.8136 0.8291</cell><cell>0.8041 0.8253</cell><cell>0.8387 0.8617</cell></row><row><cell>LIVE-VQC</cell><cell>SRCC PLCC</cell><cell>0.7277 0.7784</cell><cell>0.7889 0.7806</cell><cell>0.8162 0.8445</cell></row><row><cell>W.A.</cell><cell>SRCC PLCC</cell><cell>0.7779 0.7860</cell><cell>0.7993 0.7974</cell><cell>0.8307 0.8260</cell></row><row><cell cols="3">C. Performance on Mixed Databases</cell><cell></cell><cell></cell></row><row><cell cols="5">1) Database-level Mixed Test: In a more practical exper-</cell></row><row><cell cols="5">imental setting, a BVQA model is expected to perform well</cell></row><row><cell cols="5">across different data distributions. Similar to MDTVSFA</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI MEDIAN</head><label>VI</label><figDesc>SRCC AND PLCC RESULTS ON DIFFERENT CATEGORIES OF MIXED SUBSETS, INCLUDING RESOLUTION TYPES OF 1080P, 720P, AND ?480P, CONTENT TYPES OF SCREEN, ANIMATION, AND GAMING, AND QUALITY TYPES OF LOW QUALITY AND HIGH QUALITY FROM KONVID-1K [46], LIVE-VQC<ref type="bibr" target="#b47">[48]</ref>, AND YOUTUBE-UGC<ref type="bibr" target="#b48">[49]</ref> Criteria SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC BRISQUE 0.</figDesc><table><row><cell>Category</cell><cell></cell><cell>Resolution</cell><cell></cell><cell></cell><cell>Content</cell><cell></cell><cell>Quality</cell><cell></cell></row><row><cell>Subset</cell><cell>1080p</cell><cell>720p</cell><cell>?480p</cell><cell>Screen</cell><cell>Animation</cell><cell>Gaming</cell><cell>Low Quality High Quality</cell><cell>W.A.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII SRCC</head><label>VII</label><figDesc>AND PLCC RESULTS OF THE CROSS-DATABASE EVALUATION ON CVD2014<ref type="bibr" target="#b44">[45]</ref>, KONVID-1K<ref type="bibr" target="#b45">[46]</ref>, LIVE-QUALCOMM<ref type="bibr" target="#b46">[47]</ref>, LIVE-VQC<ref type="bibr" target="#b47">[48]</ref>, AND YOUTUBE-UGC<ref type="bibr" target="#b48">[49]</ref> Criteria SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC BRISUQE 0.</figDesc><table><row><cell>Training</cell><cell>CVD2014</cell><cell>LIVE-Qualcomm</cell><cell>KoNViD-1k</cell><cell>LIVE-VQC</cell><cell cols="2">YouTube-UGC</cell></row><row><cell>Testing</cell><cell>LIVE-Qualcomm</cell><cell>CVD2014</cell><cell cols="3">LIVE-VQC YouTube-UGC KoNViD-1k YouTube-UGC KoNViD-1k</cell><cell>LIVE-VQC</cell><cell>W.A.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell cols="7">SRCC AND PLCC RESULTS OF THE CROSS-DATABASE EVALUATION</cell></row><row><cell cols="7">WHERE THE MODEL IS TRAINED ON LSVQ [36], AND THEN TESTED ON</cell></row><row><cell></cell><cell cols="5">KONVID-1K [46] AND LIVE-VQC [48]</cell></row><row><cell>Testing</cell><cell cols="2">KoNViD-1k</cell><cell cols="2">LIVE-VQC</cell><cell cols="2">W.A.</cell></row><row><cell>Criteria</cell><cell cols="2">SRCC PLCC</cell><cell cols="2">SRCC PLCC</cell><cell>SRCC</cell><cell>PLCC</cell></row><row><cell>BRISQUE</cell><cell>0.646</cell><cell>0.647</cell><cell>0.524</cell><cell>0.536</cell><cell cols="2">0.6060 0.6106</cell></row><row><cell>TLVQM</cell><cell>0.732</cell><cell>0.724</cell><cell>0.670</cell><cell>0.691</cell><cell cols="2">0.7117 0.7132</cell></row><row><cell>VIDEVAL</cell><cell>0.751</cell><cell>0.741</cell><cell>0.630</cell><cell>0.640</cell><cell cols="2">0.7113 0.7079</cell></row><row><cell>VSFA</cell><cell>0.784</cell><cell>0.794</cell><cell>0.734</cell><cell>0.772</cell><cell cols="2">0.7676 0.7868</cell></row><row><cell>PVQ(w/o)</cell><cell>0.782</cell><cell>0.781</cell><cell>0.747</cell><cell>0.776</cell><cell cols="2">0.7705 0.7794</cell></row><row><cell>PVQ(w)</cell><cell>0.791</cell><cell>0.795</cell><cell>0.770</cell><cell>0.807</cell><cell cols="2">0.7841 0.7989</cell></row><row><cell>Proposed</cell><cell>0.839</cell><cell>0.830</cell><cell>0.816</cell><cell>0.824</cell><cell cols="2">0.8315 0.8280</cell></row><row><cell cols="7">(a) Three representative frames of Video A in YouTube-UGC</cell></row><row><cell cols="7">(b) Three representative frames of Video B in YouTube-UGC</cell></row><row><cell cols="7">(c) Three representative frames of Video C in KoNViD-1k</cell></row><row><cell cols="7">(d) Three representative frames of Video D in KoNViD-1k</cell></row><row><cell cols="7">Fig. 5. Failure cases sampled from the YouTube-UGC [49] and KoNViD-</cell></row><row><cell cols="7">1k [46] test sets. The MOSs of A and B are both 3.056 while the predictions</cell></row><row><cell cols="7">are 3.464 and 3.187, respectively. The MOSs of C and D are 3.88 and 4.40</cell></row><row><cell cols="5">while the predictions are 4.017 and 3.938, respectively.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX ABLATION</head><label>IX</label><figDesc>STUDY OF DIFFERENT MODEL DESIGNINGS</figDesc><table><row><cell>Database</cell><cell>CVD2014</cell><cell>KoNViD-1k</cell><cell cols="2">LIVE-Qualcomm</cell><cell>LIVE-VQC</cell><cell>YouTube-UGC</cell><cell>W.A.</cell></row><row><cell>Criteria</cell><cell cols="3">SRCC PLCC SRCC PLCC SRCC</cell><cell>PLCC</cell><cell cols="3">SRCC PLCC SRCC PLCC SRCC PLCC</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X THE</head><label>X</label><figDesc>CORAL<ref type="bibr" target="#b39">[40]</ref> DISTANCE BETWEEN THE SOURCE AND TARGET DOMAINS. A LOWER VALUE OF THE CORAL DISTANCE INDICATES THAT THE SOURCE AND TARGET DOMAINS ARE CLOSER IN THE FEATURE SPACE. NOTE THESE VALUES NEED TO MULTIPLY 10 ?5 . THE LAST TWO COLUMNS SHOW THE MEDIAN SRCC AND PLCC RESULTS FOR OVERALL PERFORMANCE</figDesc><table><row><cell>Database</cell><cell>CVD2014</cell><cell>KoNViD-1k</cell><cell>LIVE-Qualcomm</cell><cell>LIVE-VQC</cell><cell>YouTube-UGC</cell><cell>Mixed VQA</cell><cell cols="2">W.A. SRCC PLCC</cell></row><row><cell>ImageNet</cell><cell>6.6480</cell><cell>2.6163</cell><cell>5.5639</cell><cell>2.6648</cell><cell>2.2154</cell><cell>1.8917</cell><cell>0.7726</cell><cell>0.7888</cell></row><row><cell>BID</cell><cell>3.3920</cell><cell>1.7005</cell><cell>2.0990</cell><cell>1.4037</cell><cell>2.0297</cell><cell>1.4509</cell><cell>0.8126</cell><cell>0.8155</cell></row><row><cell>LIVEC</cell><cell>2.6709</cell><cell>1.5320</cell><cell>1.7367</cell><cell>1.0034</cell><cell>1.6620</cell><cell>1.1324</cell><cell>0.8026</cell><cell>0.8132</cell></row><row><cell>KonIQ-10k</cell><cell>3.1534</cell><cell>1.0601</cell><cell>1.7071</cell><cell>1.0335</cell><cell>1.3171</cell><cell>0.8833</cell><cell>0.8102</cell><cell>0.8095</cell></row><row><cell>SPAQ</cell><cell>3.1084</cell><cell>1.7760</cell><cell>1.8764</cell><cell>1.5239</cell><cell>1.3138</cell><cell>1.2749</cell><cell>0.8183</cell><cell>0.8239</cell></row><row><cell>Mixed IQA</cell><cell>3.6190</cell><cell>1.2398</cell><cell>1.8628</cell><cell>1.2534</cell><cell>1.1482</cell><cell>0.9828</cell><cell>0.8349</cell><cell>0.8342</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The global internet phenomena report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandvine</surname></persName>
		</author>
		<ptr target="https://www.sandvine.com/hubfs/SandvineRedesign2019/Downloads/Internet%20Phenomena/Internet%20Phenomena%" />
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vni</forename><surname>Cisco</surname></persName>
		</author>
		<ptr target="http://www.cisco.com/en/US/solutions/collateral/ns341/ns525/ns537/ns705/ns827/whitepaperc11-481360ns827NetworkingSolutionsWhitePaper.html" />
		<title level="m">Cisco visual networking index: Forecast and trends</title>
		<meeting><address><addrLine>San Jose, CA 95134 USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A quality-ofexperience index for streaming video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="166" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regression or classification? New methods to evaluate no-reference picture and video quality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2085" to="2089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and reliable structure-oriented video noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2005-01" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No-reference pixel video quality monitoring of channel-induced distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valenzise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Magni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tubaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012-04" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="605" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment using codec analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Forchhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1637" to="1650" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Blind video quality assessment with weakly supervised learning and resampling strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2244" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment with heterogeneous knowledge ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4174" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal representation learning for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unified quality assessment of in-the-wild videos with mixed datasets training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1238" to="1257" />
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RAPIQUE: Rapid and accurate video quality prediction of user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno>abs/2101.10955</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno>abs/2103.02503</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SlowFast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comparative evaluation of temporal pooling methods for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on natural redundancy statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blind image quality assessment in multiple bandpass and redundancy domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="37" to="47" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatiotemporal statistics for video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3329" to="3342" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment using natural spatiotemporal scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V R</forename><surname>Dendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5612" to="5624" />
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment via feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="491" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment with 3D shearlet transform and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-M</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1044" to="1057" />
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-toend blind image quality assessment using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1202" to="1213" />
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end blind quality assessment of compressed videos using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep neural networks for no-reference video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2349" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Patch-VQ: &apos;Patching Up&apos; the video quality problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="19" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rich features for perceptual quality assessment of ugc videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="435" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Introduction to transfer learning, 2021. [Online]. Available: jd92.wang/tlbook</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">No-reference blur assessment of digital pictures based on multifeature classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ciancio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L N T</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A B</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Said</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Obrador</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="75" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sziranyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4041" to="4056" />
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of smartphone photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CVD2014 -A database for evaluating no-reference video quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nuutinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaahteranoksa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oittinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>H?kkinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3073" to="3086" />
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The Konstanz natural video database (KoNViD-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">In-capture mobile video distortions: A study of subjective behavior and objective algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2061" to="2077" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">YouTube UGC dataset for video compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Multimedia Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Uncertainty-aware blind image quality assessment in the laboratory and wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3474" to="3486" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A law of comparative judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Thurstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="273" to="286" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">FRank: A ranking method with fidelity loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MotionSqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="345" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segregation of form, color, movement, and depth: anatomy, physiology, and perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hubel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="issue">4853</biblScope>
			<biblScope unit="page" from="740" to="749" />
			<date type="published" when="1988-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distributed hierarchical processing in the primate cerebral cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Felleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="1991-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural mechanisms of form and motion processing in the primate visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1994-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The Kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Temporal hysteresis model of time varying subjective video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1153" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using a deep bilinear convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020-01" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="36" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Perceptual video quality prediction emphasizing chroma distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1408" to="1422" />
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective models of video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<ptr target="http://www.vqeg.org" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Fast differentiable sorting and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="950" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4850" to="4862" />
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of tone-mapped HDR pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on high order statistics aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4444" to="4457" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">UGC-VQA: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4449" to="4464" />
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An objective method for combining multiple subjective data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Pinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<date type="published" when="2003-06" />
			<biblScope unit="volume">5150</biblScope>
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Predicting the quality of images compressed after distortion in two steps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5757" to="5770" />
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">KADID-10k: A large-scale artificially distorted IQA database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals</title>
		<imprint>
			<date type="published" when="2003-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Quantifying visual image quality: A bayesian view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duanmu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2102.00915</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Continual learning for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<idno>abs/2102.09717</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Task-specific normalization for continual learning of blind image quality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/2107.13429</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
