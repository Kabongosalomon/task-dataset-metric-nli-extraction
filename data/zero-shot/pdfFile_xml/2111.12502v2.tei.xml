<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TriStereoNet: A Trinocular Framework for Multi-baseline Disparity Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faranak</forename><surname>Shamsafar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
						</author>
						<title level="a" type="main">TriStereoNet: A Trinocular Framework for Multi-baseline Disparity Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While various deep learning-based approaches have been developed for stereo vision, binocular setups with fixed baselines present limited input data and face occlusion issues when covering a large depth range. We address this problem by using both narrow and wide baselines. Also, with increased evidence coming from the wide baseline, the recovered depth range can be extended. This scheme is beneficial for autonomous urban and highway driving applications. Thus, we present a model for processing the data from a trinocular setup with a narrow and a wide stereo pair. In this design, two pairs of binocular data with a common reference image are treated with one network in an end-to-end manner. We explore different methods and levels of fusion, and propose a Guided Addition module for a mid-level fusion of the two baseline data. In addition, a method is presented to iteratively optimize the network with sequential self-supervised and supervised learning on real and synthetic datasets. With this method, we can train the model on real-world data without needing the ground-truth disparity maps. Quantitative and qualitative results demonstrate that the multi-baseline network surpasses the model with a similar architecture but trained with individual baseline data. In particular, the trinocular setup outperforms the narrow and wide baselines by 13% and 24% in D1 error, respectively. Code and dataset: https://github.com/cogsystuebingen/tristereonet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While there are several methods for estimating depth, stereo matching is the most adaptable to various use-cases. Over the past three decades, the passive recovery of depth maps using stereo has attracted interest in the computer vision field, and numerous real-world applications, like autonomous driving and robot navigation, can benefit from estimating depth directly from images. Other depth estima-  tion technologies for autonomous vehicles, such as Laser Imaging Detection and Ranging (LiDAR) sensors, actively measure depth based on the travel time of a light beam emitted by the device. However, in addition to being expensive to set up, LiDAR produces a sparse depth map that is also susceptible to weather conditions. Estimating depth via stereo is done by calculating the disparities (displacements) between the matching points in the rectified images, which makes it simple to estimate the depth via triangulation. This is usually achieved with two images (i.e. a binocular setup) and different methods can be used to estimate the disparity between them.</p><p>Before deep learning, various hand-engineered features, like Sum of Absolute Difference or Census Transform <ref type="bibr" target="#b35">[36]</ref> were used for finding matching points and for computing a cost volume. This was followed by a regularization module, such as the well-known Semi Global Matching (SGM) method <ref type="bibr" target="#b12">[13]</ref>. Deep learning-based approaches, on the other hand, attempt to adapt a network for either some steps of stereo vision <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref> or the entire pipeline as an end-to-end technique <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39]</ref>. The latter group has significantly improved the accuracy of stereo vision. Nevertheless, the applicability of these approaches in real-world scenarios remains a concern due to their bias toward the content of the training images, which includes varying object distances from the camera. Using multiple images to calculate depth is a possible solution as it provides more visual clues about the scene. Additionally, in some applications like autonomous driving, having more than one fixed baseline is critical. The reason is that while close-range distances require obtaining depth information via a narrow baseline, such as in low/moderatespeed urban driving settings with nearby vehicles or pedestrians, a wide baseline is equally essential when driving at high speeds on a highway. Note that depth resolution decreases quadratically with depth in a fixed baseline <ref type="bibr" target="#b6">[7]</ref>. More specifically, each stereo camera (with a single baseline) has a blind range or occlusion, i.e. the region that cannot be seen by both cameras at the same time. The blind range of a stereo camera with a narrow baseline is smaller than that of a stereo camera with a wider baseline. A narrow stereo camera, however, is not able to accurately infer depth at a distance. By contrast, using a wider baseline leads to more accurate predictions for distant regions, but increases false matches due to the large disparity search range and severe occlusion problem. Therefore, depending on the scene and object distances, both narrow and wide baselines are needed.</p><p>To address this issue, we propose a three-view stereo setup ( <ref type="figure" target="#fig_1">Fig. 1</ref>) and a deep network ( <ref type="figure" target="#fig_2">Fig. 2)</ref> for estimating the disparity by three inputs. Our work considers two baselines of this trinocular setup with a shared reference image to obtain accurate disparity maps. To the best of our knowledge, the proposed model is the first that processes the multi-baseline trinocular setting in an end-to-end deep learning-based manner. Our motivation for using three cameras for stereo is to leverage both the narrow and the wide baselines for accurate depth estimation. Furthermore, in such a setup, the visual data can be recovered for objects in the blind range of the wider baseline by using the narrow baseline. This formulation is particularly necessary for autonomous navigation and driving scenarios where diversified and unpredictable near-and far-range objects appear.</p><p>The main contributions of this work are summarized as follows: 1) We design a horizontally-aligned multi-baseline trinocular stereo setup for more accurate depth estimation. Accordingly, an end-to-end deep learning-based model is proposed for processing the 3-tuple input. 2) We propose a new layer for merging the disparity-related data for a midlevel fusion. We also investigate other levels and methods of fusion. 3) An iterative sequential self-supervised and supervised learning is proposed to make the design applicable to new real-world scenarios where disparity annotations are unavailable. 4) We build a synthetic dataset for the trinocular design together with the ground-truth information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Classical algorithms for stereo vision can mainly be divided into three modules: matching cost computation, cost regularization, and disparity optimization. For cost regularization, local approaches calculate disparities based on the neighboring pixels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref> with the drawback of the sensitivity to occlusion and uniform texture. Alternatively, the global methods consider disparity changes globally to increase accuracy, but at the cost of higher computational complexity. The semi-global approaches examines both locally for predicting better disparities for small regions, as well as globally for estimating based on the overall content of the images <ref type="bibr" target="#b12">[13]</ref>. After the introduction of semi-global solution, classical work mainly focused on improving its accuracy and speed <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>With the rise of deep learning, stereo matching continued to be reformed by these modern techniques. There are two categories of deep models that follow the general paradigm for stereo reconstruction: the methods that formulate one or some of the steps with a deep learning framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>, and the approaches that transfer the full process in an end-to-end scheme <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. Based on recent advances, our model is end-to-end, but processes a 3-tuple sample.</p><p>The concept of multi-baseline stereo was first explored back in 1993 <ref type="bibr" target="#b24">[25]</ref> to benefit from narrow and wide baselines. In <ref type="bibr" target="#b24">[25]</ref>, the fusion was applied after computing the Sum of Squared Distances (SSD) of images. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> studied a three-view setup by adding a camera on top of the left one. In <ref type="bibr" target="#b22">[23]</ref>, the authors validate the performance of a multi-baseline system for autonomous navigation in agricultural and off-road environments. The system integrates the point clouds obtained from the wide and narrow baselines. An FPGA-based multi-baseline stereo system with four cameras was developed in <ref type="bibr" target="#b13">[14]</ref> via Census Transform and without regularization like SGM. The authors showed that their setup is superior to binoculars with SGM in recovering fine structures. In a different work <ref type="bibr" target="#b30">[31]</ref>, multi-baseline is used to accelerate the SGM-based disparity estimation. In contrast to these studies, which are all based on classical stereo methods, our multi-baseline design combines information from the wide and narrow baselines in a full deep learning-based framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Trinocular Stereo Setup</head><p>The schematic of our multi-baseline trinocular setup with horizontally-aligned cameras is illustrated in <ref type="figure" target="#fig_1">Fig.1</ref>. Due to the fact that identical cameras are positioned in parallel with known displacements, not only can we perform stereo matching between different pairs of cameras, but we can also fuse the data for accurate and robust prediction. In our formulation, we consider two left-middle (b LM /b narrow ) and left-right (b LR /b wide ) baselines with the left image as reference. Note that it is possible to consider other stereo pairs or reference images. We fuse the information from the driver's perspective (for right-hand traffic) to evaluate our method in driving scenarios.</p><p>This setup is an array of axis-aligned cameras, where the matching pixels are located on the same horizontal line, allowing the fusion to take place, i.e. both the LM and LR pairs contribute to disparity estimation. Theoretically, the disparities between the matching points in LM and LR pairs depend on the baselines. That is, via triangulation for a fixed object and given b LR = r?b LM , we get d LR = r?d LM , with d as the notation for disparity and r = 2 as the ratio between the baselines. As a result, by fusing the LM and LR pairs, more constraints are introduced since d LR = r ? d LM must be satisfied. Hence, this setup benefits from both the wide (for accurate depth estimation of distant objects) and narrow baseline (for depth estimation of close objects that lie in the blind range of the wide baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Structure</head><p>Backbone. Our TriStereoNet architecture is depicted in <ref type="figure" target="#fig_2">Fig. 2</ref>, which is based on GwcNet <ref type="bibr" target="#b9">[10]</ref>. GwcNet was orig-inally proposed for standard binocular stereo and is divided into four main modules: feature extraction, cost volume construction (which creates 4D data as the output), prehourglass, and hourglasses (encoder-decoder). Since the hourglass modules operate on 4D data, they are built with heavy 3D convolutions. Thus, we use only a single hourglass for efficiency.</p><p>In our modification for three inputs, the ResNet-like feature extraction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> is shared for three rectified images (I L , I M , I R ). Given the images of size 3 ? H ? W , the feature data for each image is 320 ? H/4 ? W/4. After computing the three feature data (F L , F M , F R ) for the 3-tuple sample, two cost volumes are computed for the LM and LR pairs (cost LM and cost LR ) by group-wise correlation <ref type="bibr" target="#b9">[10]</ref>. These 4D cost data are of size F ? D ? H/4 ? W/4, where F = 40 is the number of groups in group-wise correlation and D = d max /4 is the maximum disparity range (assumed to be d max = 192 for the input image size). Aligning the Cost Volumes.</p><p>One notable point is that the baselines of the LM and LR pairs are different. Thus, the disparity range of the two cost volumes must be aligned for fusion. Namely, we need to consider cost LM (f, d/r, i, j) to align the disparity values with cost LR (f, d, i, j). Here, i, j stand for spatial dimension. To compute cost LM (f, d/r, i, j) at non-integer values, we should interpolate the values across the disparity dimension. To this end, we utilize natural cubic splines with a function as eq. 1 for each sub-interval [x j?1 , x j ] (j = 2, 3, ...n). For more details on how the related coefficients can be computed, we refer the reader to <ref type="bibr" target="#b19">[20]</ref>.</p><formula xml:id="formula_0">S j (x) = a j + b j (x ? x j ) + c j (x ? x j ) 2 + d j (x ? x j ) 3</formula><p>(1) Fusion of Narrow-&amp; Wide-Baseline Data. At this stage, the main questions are where and how to apply the fusion of the two baseline data. Note that an important assumption for making this fusion feasible is that all three images must be rectified, and this can be obtained by our trinocular setup. There are mainly three levels of fusion in the network ( <ref type="figure" target="#fig_3">Fig. 3</ref>): 1) after cost volume computation, 2) after pre-hourglass, and 3) after hourglass. We find that, after cost volume computation, if data are processed with a few more convolutions, i.e. with pre-hourglass module, their aggregation obtains higher accuracy. Fusion after hourglass also outperforms direct fusion of the cost volumes; however, the complexity increases as the network operates two 4D data (instead of one) in the hourglass with all heavy 3D convolutions. We propose a Guided Addition (GA) module ( <ref type="figure" target="#fig_4">Fig. 4</ref>) for merging two streams of the 4D disparity data after pre-hourglass. This module applies depth-wise 3D convolution across the feature (and not disparity) dimension to each 4D data. After 3D batch normalization, the layer merges the data by addition. This simple yet effective sub-network shows superior performance to methods  like direct addition. Also, it retains the data size with kernel size 3, stride 1, and the same number of channels.</p><p>We investigated other manners and other levels of fusion. Namely, for cost volume fusion, we applied addition, average, concatenation, maximization and top feature selection (i.e. getting the largest elements from the two data across the feature dimension). Also, we examined average and top feature selection for pre-hourglass and hourglass fusion. However, for accuracy and efficiency, we adopted Guided Addition after pre-hourglass (pre hg ga , c.f. Tab. 4). We also observed that fusion by addition and average in cost level, and top feature selection in pre-hourglass and hourglass levels do not outperform the single LR baseline. This indicates the necessity of the appropriate level and method of fusion.</p><p>It should be noted that although our network processes in-line images, it can easily be adapted for a vertical baseline. For instance, given an L-shape trinocular setup (Top, Left, Right), with the left image as reference, we only need to rotate the top and left image features (which correspond to the vertical baseline) by 90 ? before cost volume, and then rotate back the resulted cost for fusion with the horizontal baseline. As the main focus of our paper is designing a deep end-to-end model for multi-baseline processing, it can be generalized to any other camera configuration as long as the images are rectified in a specific direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Mechanism</head><p>Supervised Learning. By generating a trinocular synthetic dataset with ground-truth information, we can apply supervised learning via a loss function between the estimated and the ground-truth disparity maps. To this end, we employ Huber loss as eq. 2, with d andd as the ground-truth and estimated disparity maps, and n as the number of image pixels. ? is the threshold for the scaled L1 and L2 loss. We show in Sec. 4 that the value of ? affects the performance. Previous works mostly use smooth L1 loss for disparity loss <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>.</p><formula xml:id="formula_1">L d (d,d) = 1 n i 0.5(d i ?d i ) 2 , if |d i ?d i | &lt; ? ? ? (|d i ?d i | ? 0.5 ? ?), otherwise</formula><p>(2) Self-supervised Learning. To demonstrate the performance of TriStereoNet on real-world data, and because providing the ground-truth labels for a real dataset is costly, unsupervised learning by a self-supervision loss is utilized. For this purpose, we consider photometric and disparity smoothness losses. More specifically, we reconstruct the reference image by warping the target image with the estimated disparity. We then use a combination of SSIM <ref type="bibr" target="#b31">[32]</ref> and L1 loss as in <ref type="bibr" target="#b8">[9]</ref> (eq. 3) to measure image discrepancy between the reconstructed and the original image.</p><formula xml:id="formula_2">L p (I,?) = 1 n i ? 1 ? SSIM I,? 2 + (1 ? ?) I ??</formula><p>(3) Here, we set ? = 0.85 and use a SSIM with a 3 ? 3 block filter.? is the reconstructed image computed given the disparity map and the baseline. Thus, our multi-baseline setup makes it feasible to consider two photometric losses, which correspond to the reconstructed left image with narrow and wide baselines, as in eq. 4. Note that for warping the middle image to reconstruct the left image, we need to used/r because of d LR = r ? d LM .</p><formula xml:id="formula_3">I M L = WARP(I M ,d/r),? R L = WARP(I R ,d)<label>(4)</label></formula><p>For disparity smoothness loss, we adopt the edge-aware smoothness loss in <ref type="bibr" target="#b10">[11]</ref> (eq. 5) to encourage the disparity to be locally smooth. This is an L1 loss on the disparity gradients weighted by image gradients.</p><formula xml:id="formula_4">L s (d, I L ) = 1 n i ? xd e ? ?xI L + ? yd e ? ?yI L (5)</formula><p>Final Loss. The final loss is as follows:</p><formula xml:id="formula_5">L = ? d ? L d + ? p ? L p + ? s ? L s ,<label>(6)</label></formula><p>where (? d , ? p , ? s ) are loss coefficients. We formulate this loss for a hybrid supervised and self-supervised training. Namely, when training on the synthetic dataset, the photometric loss is ignored, i.e. ? p = 0, and when the real dataset is used, ? d = 0.</p><p>Iterative Sequential Learning Scheme. Here, we propose a training approach that is highly efficient with no need to ground-truth disparity of the real dataset. To achieve this, we begin by training the model by self-supervised learning on a real dataset with L s and L p . Then, proceeding with the synthetic dataset, we employ L s and L d losses for supervised learning. This way, we optimize the network iteratively with sequential self-supervised and supervised learning. This technique has three advantages: 1) In the absence of ground-truth disparity, we can train the model on realworld data. 2) Synthetic dataset supervision assists the network in learning finer details, since considering only image reconstruction loss (L p ) yields blurry results. 3) Using selfsupervision without ground-truth helps the network learn the underlying principles of the trinocular setup, preventing it from overfitting to ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussion</head><p>Synthetic Dataset. For supervised training of our learning regime, we require the ground-truth data to be available. To the best of our knowledge, there is no publicly available dataset which aims at multi-baseline stereo fusion with inline cameras and with a high depth range application, like driving. Also, we need a large number of samples for a deep model. Thus, with the help of CARLA <ref type="bibr" target="#b4">[5]</ref>, we generated a synthetic dataset that consists of RGB images of three cameras on an axis with the middle camera centered in the leftright baseline (38.6 cm). In order to represent diversified content, we defined 25 configurations based on CARLA's features, such as weather condition, day time, traffic, location, and the number of pedestrians. Accordingly, we created a total number of 9649/2413 training/test samples with images of size 720 ? 1280. Note that the images of each triple sample are rectified, and thus, stereo matching and multi-baseline fusion can be performed using any pairs of viewpoints. <ref type="figure">Figure 5</ref> shows a 3-tuple sample of this dataset embedded with horizontal lines. Real Dataset. For the real dataset, we use the trinocular set of images collected by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. This dataset is a collection of tricamera stereo sequences with 10 sets as Harbour bridge, Barriers, Dusk, Queen street, People, Midday, Night, Wiper, Dusk and Night. The last two sets are ignored due to their too bright/dark illumination. The images are 10-bit gray-scale, with a resolution of 480 ? 640. Like our CARLA dataset, each 3-tuple sample satisfies the standard epipolar geometry <ref type="figure">(Fig. 5)</ref>  <ref type="figure">Figure 5</ref>. 1st Row: Our synthetic trinocular dataset generated by CARLA. 2nd Row: The real trinocular images collected by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. As seen through the horizontal lines, the 3-tuple sets in both real and synthetic datasets are rectified. <ref type="figure">Figure 6</ref>. EPE error on the CARLA test set, when the network is trained from scratch with random weights (no self-supervision by the real dataset) and when trained iteratively with self-supervision.</p><p>parity. ? px-re-1: We define this metric as the average of BMPRE metric, defined in <ref type="bibr" target="#b1">[2]</ref>, i.e. px-re-1 is the percentage of pixels for which MRE is ? 1. This metric integrates the benefits of px-1 and MRE. Note that MRE and px-re-1 consider depth (and not disparity). While in recent studies for stereo matching, mainly EPE and D1 measures are evaluated, we believe considering MRE and px-re-1 metrics are equally important as they better represent the actual estimation error in terms of depth, which is the ultimate goal of stereo matching. EPE, D1, and px-1 are incompetent in yielding higher error values for larger triangulation errors <ref type="bibr" target="#b29">[30]</ref>. Implementation Details. For training, random crops of 256 ? 512 are used for both of the datasets. Testing is conducted on crops of 512 ? 960 of the synthetic images. For this dataset, the learning rate starts from 0.001 and is downscaled by a factor of 2 after epochs 10, 12, 14, 16 (in 30 epochs) with Adam optimizer <ref type="bibr" target="#b17">[18]</ref>. As for the real dataset, we train the model for 60 epochs with a learning rate downscaled by 2 after epochs 40, 50. We also set ? = 0.25 for the Huber loss. Quantitative and Qualitative Results. <ref type="table">Table 1</ref> presents the evaluation results on the synthetic dataset. We applied iterative sequential learning four times; namely, each it-   eration started by learning from the real dataset (via selfsupervision) and then from the synthetic dataset (via supervised training). From the first iteration to the fourth round, performance improves by approximately 12%, 15%, 13%, 16%, 20% reduction in EPE, D1, px-1, MRE, and px-re-1, respectively. With the subsequent iterations, this gain decreases. In <ref type="figure">Fig. 6</ref>, the EPE evaluation for different iterations of sequential training is plotted. Note how this learn- <ref type="bibr" target="#b2">[3]</ref> 0.88 2.00 256.66 5.22 GA-Ne-deep <ref type="bibr" target="#b38">[39]</ref> 0.63 1.61 670.25 6.58 GA-Net-11 <ref type="bibr" target="#b38">[39]</ref> 0.67 1.92 383.42 4.48 GwcNet-gc <ref type="bibr" target="#b9">[10]</ref> 0.63 1.55 260.49 6.82 GwcNet-g <ref type="bibr" target="#b9">[10]</ref> 0  <ref type="table">Table 2</ref>. Error metrics on the KITTI 2015 validation set and the computational complexity. For TriStereoNet, we assume the middle image is missed and is replaced by the right image. For computing the complexity in terms of MACs, the input resolution is 256 ? 512.</p><formula xml:id="formula_6">Method EPE(px) ? D1(%) ? MACs(G) ? Params(M ) ? PSMNet</formula><p>ing mechanism surpasses vanilla training, where no selfsupervision is carried out. Besides, while the learning process with self-supervision leads to a reasonable convergence in each case, restarting the training with the real dataset further improves the supervised learning. <ref type="figure" target="#fig_5">Figure 7</ref> depicts some qualitative results together with the error maps on the synthetic dataset. We can see that the mechanism of iterative sequential training reduces the error. The results on the real dataset together with the 3D reprojections are presented in <ref type="figure" target="#fig_6">Fig. 8</ref>. Note that these images are completely different from the synthetic dataset, except that they are both driving scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Binocular Deployment</head><p>The performance of TriStereoNet is evaluated on the binocular setup as well. For this, ignoring the viewpoint of the middle camera, we finetune the network on 159/40 training/validation split from the KITTI 2015 dataset <ref type="bibr" target="#b20">[21]</ref>. The results are tabulated in Tab. 2, including the error rates as well as the computational complexity in terms of the number of operations (MAC) and parameters. Accordingly, TriStereoNet is also capable of estimating the disparity in binocular setups with similar accuracy but with less complexity than binocular models. Notably, it surpasses GA-Net-11 with 40% fewer GigaMACs and is competitive with GA-Net-deep with 66%/36% fewer operations/parameters. The reason may be due to multi-baseline pre-training and self-supervision, which help the model learn the principles of stereo matching with more constraints from the setup.</p><p>The results on the KITTI 2015 benchmark (200 test images) are reported in Tab. 3, which also confirm that when TriStereoNet is deployed in a binocular setting, it is still capable of estimating the disparity with superior or competitive accuracy to the inherent binocular models, despite that it has been pretrained with a 3-tuple input. <ref type="figure">Fig. 9</ref> illustrates some qualitative comparisons. By visual inspection, we noticed that TriStetreoNet performs more robust in difficult areas, i.e. in rich-textured and detailed segments (e.g. trees), distant objects/regions (e.g. distant cars, trees or sky), and challenging illuminated areas (e.g. very bright regions).</p><p>GCNet <ref type="bibr" target="#b16">[17]</ref> PSMNet <ref type="bibr" target="#b2">[3]</ref> GwcNet-g <ref type="bibr" target="#b9">[10]</ref> TriStereoNet <ref type="figure">Figure 9</ref>. Qualitative performance on KITTI 2015 benchmark. The first row shows sample left images. The disparity images together with their error maps are estimated by GCNet <ref type="bibr" target="#b16">[17]</ref>, PSMNet <ref type="bibr" target="#b2">[3]</ref>, GwcNet-g <ref type="bibr" target="#b9">[10]</ref> and TriStereoNet in order.</p><p>Note that almost the upper half of the KITTI images do not have the ground-truth annotations of LiDAR. This causes other models to estimate inaccurate or erroneous disparity values in these regions as they cannot learn due to a lack of supervision in those areas. The images' upper half includes trees, buildings, street-covered areas, or the sky. As an example, with other approaches, some sky-related areas are wrongly estimated with very high disparity values, meaning very close objects. TriStereoNet, however, performs with great superiority in these regions as a result of its selfsupervised pre-training with three cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Fusion Level and Fusion Method. Our first analysis examines the effect of the fusion level (where fusion occurs) and the fusion method for combining narrow-and widebaseline data. <ref type="table">Table 4</ref> shows the related results when models are only trained on synthetic data. In all of these experiments, the models are trained with 30 epochs, and the best checkpoint is selected based on the least EPE on the test set.</p><p>On a cost-level fusion, the Guided Addition layer outperforms the other fusion methods (except for EPE). At this level, we also examined average and simple addition, but  <ref type="figure" target="#fig_1">Figure 10</ref>. Disparity error maps for self-supervised initialization (warmer colors denote larger error values). 2nd Column: the model is trained from scratch on the synthetic dataset. 3rd &amp; 4th Columns: self-supervision with the real dataset is performed before the supervised training on the synthetic dataset, using the photometric losses of? M L and? R L , respectively.  discarded them since their results were similar to the single wide baseline. It is possible to achieve a greater gain in performance by applying fusion after both the pre-hourglass and hourglass. As compared with average fusion, we can see that not only the level of fusion is important, but the fusion method is similarly affecting the performance. The hg avg method yields more accurate results in terms of MRE and px-re-1. We believe this is because the hourglass module further processes two data streams. In essence, it works as a regularization step, aggregating disparity information regarding pixels' localities to eliminate unwanted spurious values. Nevertheless, we selected pre hg ga for merging the data as it involves less computational complexity than in hg avg . Note that hg avg requires processing two data flows in a heavy encoder-decoder with 3D convolutions. Trinocular vs. Binocular. To demonstrate the trinocular setup's superior performance over the binocular setup, we exploited a similar network architecture for either the leftmiddle (LM) or the left-right (LR) image pair. The results are shown in Tab. 4. TriStereoNet, which fuses the two baseline streams, outperforms single pairs on all metrics. Learning Scheme. <ref type="table">Table 5</ref> presents the impact of selfsupervised initialization with the real dataset before training on the synthetic dataset in a supervised manner. It also  <ref type="table">Table 4</ref>. Comparison of the standard stereo with narrow (LM) and wide (LR) baselines (trained with binocular backbone adapted from GwcNet <ref type="bibr" target="#b9">[10]</ref>) and the proposed trinocular stereo (LMR). Furthermore, different levels and methods of fusion are included. "ga" indicates the proposed Guided Addition fusion method.  <ref type="table">Table 5</ref>. Effect of self-supervised initialization with the real dataset and different photometric losses using? M L and? R L . While in the first experiment, the model has been trained from scratch, the other two are trained after the self-supervised pre-training.  <ref type="table">Table 6</ref>. Effect of the ? value in the Huber loss function. Huber loss with ? = 1 is equivalent to smooth L1 loss. compares the two methods by which we can reconstruct the reference image. We can see that self-supervised initialization improves the results, particularly when? R L is used for self-supervision. <ref type="figure" target="#fig_1">Figure 10</ref> shows the qualitative comparison of these approaches. Disparity Loss. Last but not least, we evaluated the effect of the Huber loss for comparing disparity maps. <ref type="table">Table 6</ref> shows that Huber loss is outperforming smooth L1 loss for both binocular and trinocular settings. Note that Huber loss is equivalent to smooth L1 loss when ? = 1. <ref type="table">Table 6</ref> also confirms our choice of ? as 0.25.</p><formula xml:id="formula_7">Methods All(%) Noc(%) D1 bg D1 f g D1 all D1 bg D1 f g D1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>The paper proposes a deep end-to-end network for disparity estimation in a multi-baseline trinocular setup. The pipeline processes two pairs of images from an axis-aligned three-camera configuration with narrow and wide baselines. In this regard, we introduced a new layer for effectively merging the information from the two baselines. A synthetic dataset was generated to help with a proposed it-erative sequential learning of real and synthetic datasets. With this learning mechanism, we can train on a real dataset with no ground-truth information. Experiments show that the proposed method outperforms the disparity map estimated by each image pair. By providing improved flexibility and scalability, this multi-baseline deep model is promising for long range visual perception and autonomous navigation, where the image content is diversified and changeable in terms of the distance to the camera.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work is funded by the Federal Ministry of Education and Research (BMBF) and the Baden-W?rttemberg Ministry of Science as part of the Excellence Strategy of the German Federal and State Governments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Multi-baseline stereo with three cameras (Left, Middle, Right). Note that disparity (displacement of matching points) is directly dependent on the baseline. The left image is the reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview. After extracting the feature of the 3-tuple rectified input images ((IL, IM , IR) ? (FL, FM , FR)), cost volumes are computed for the narrow ((FL, FM ) ? costLM ) and wide ((FL, FR) ? costLR) baselines. "Int" applies the interpolation to stretch costLM across the disparity dimension. "GA" or Guided Addition fuses the two streams of data. While L d and Ls losses are used in supervised learning, Lp and Ls are exploited for self-supervision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Different levels for merging the narrow-and widebaseline data: after cost volume (a), after pre-hourglass (b), and after hourglass (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The proposed Guided Addition (GA) layer to merge two 4D data. F , D, H-W denote the feature, disparity, and spatial size, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Disparity estimation and their error maps on the synthetic test set. In the error maps, warmer colors indicate higher error values. 1st Row: Two input images and their ground-truth disparity map. 2nd Row: Disparity estimation with vanilla training with no self-supervision. 3rd Row: Disparity estimation with the proposed sequential learning after four iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Disparity estimations on the real dataset. 1st Col.: Input images (Note that the only similarity of these images with the synthetic dataset is "being recorded in a driving scenario"). 2nd Col.: Estimated disparity maps. 3rd Col.: Reprojectd disparity maps as 3D point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Percentage of pixels whose estimation error is ? 3px or ? 5% of the ground-truth disparity.? px-1: Portion of pixels for which absolute error is ? 1px.? MRE (Mean Relative Error)<ref type="bibr" target="#b29">[30]</ref>: Mean of absolute error among the valid pixels divided by the ground-truth dis-</figDesc><table><row><cell>. We split the dataset into 1920/480 training/test samples. This dataset does not include ground-truth disparity maps. Evaluation Metrics. We evaluate the performance of TriS-tereoNet in terms of: ? EPE (End-point Error): Mean of absolute error among valid pixels. ? D1: Left image</cell><cell>Middle image</cell><cell>Right image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Evaluation results on the KITTI 2015 benchmark with D1 measure for non-occluded and all pixels in background, foreground and all areas. The models are sorted according to D1 all ; the lower, the better.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CBMV: A coalesced bidirectional matching volume for disparity estimation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<editor>Konstantinos Batsos, Changjiang Cai, and Philippos Mordohai</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2060" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BMPRE: An error measure for evaluating disparity maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Trujillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Sign. Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1051" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast block matching algorithm based on the winner-update strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ping</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiou-Shann</forename><surname>Fuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1212" to="1222" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03938</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeppruner: Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="4384" to="4393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variable baseline/resolution stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving stereo subpixel accuracy for long range stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3273" to="3282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pm-huber: Patchmatch with huber regularization for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Heise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Half-resolution semi-global stereo match</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandino</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embedded real-time multi-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Honegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5245" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Triple-SGM: Stereo processing using semi-global matching with cost fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kallwies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Forkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Joachim</forename><surname>Wuensche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on App. of Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="192" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective combination of vertical and horizontal stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kallwies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Joachim</forename><surname>Wuensche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on App. of Comput. Vis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1992" to="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cubic spline interpolation. College of the Redwoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sky</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1049" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time stereo vision: Optimizing semiglobal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1197" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d reconstruction and classification of natural environments by an autonomous vehicle using multi-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalisa</forename><surname>Milella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Reina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Service Robotics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">J?rgen Hesser, and Reinhard M?nner. Calculating dense disparity maps from color stereo images, an efficient implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>M?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparative study of stereomatching algorithms for road-modeling in the presence of windscreen wipers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Schauwecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandino</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SGM-Nets: Semi-global matching with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihito</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faranak</forename><surname>Shamsafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Woerz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.09770</idno>
		<title level="m">Rafia Rahim, and Andreas Zell. MobileStereoNet: Towards lightweight deep networks for stereo matching</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CFNet: Cascade and fused cost volume for robust stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhelun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13906" to="13915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time dense stereo for intelligent vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wannes</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Der</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="50" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stereo matching optimization with multibaseline trinocular camera model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)</title>
		<imprint/>
	</monogr>
	<note>pages 1-4. IEEE, 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mabnet: A lightweight stereo network based on multibranch adjustable bottleneck module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiabin</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiying</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="340" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bilateral grid learning for stereo matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12497" to="12506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aanet: Adaptive aggregation network for efficient stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1959" to="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-parametric local transforms for computing visual correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Woodfill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jure?bontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2287" to="2318" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GA-Net: Guided aggregation net for endto-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
