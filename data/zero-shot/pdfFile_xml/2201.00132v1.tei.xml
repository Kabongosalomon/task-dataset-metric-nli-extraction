<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAFL: A Self-Attention Scene Text Recognizer with Focal Loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao</forename><forename type="middle">Hieu</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Le-Cong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Duc</roleName><forename type="first">Huu</forename><forename type="middle">Manh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Hung</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phi</forename><surname>Le Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology Hanoi</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAFL: A Self-Attention Scene Text Recognizer with Focal Loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Scene Text Recognition</term>
					<term>Self-attention</term>
					<term>Focal loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the last decades, scene text recognition has gained worldwide attention from both the academic community and actual users due to its importance in a wide range of applications. Despite achievements in optical character recognition, scene text recognition remains challenging due to inherent problems such as distortions or irregular layout. Most of the existing approaches mainly leverage recurrence or convolution-based neural networks. However, while recurrent neural networks (RNNs) usually suffer from slow training speed due to sequential computation and encounter problems as vanishing gradient or bottleneck, CNN endures a trade-off between complexity and performance. In this paper, we introduce SAFL, a self-attentionbased neural network model with the focal loss for scene text recognition, to overcome the limitation of the existing approaches. The use of focal loss instead of negative log-likelihood helps the model focus more on low-frequency samples training. Moreover, to deal with the distortions and irregular texts, we exploit Spatial TransformerNetwork (STN) to rectify text before passing to the recognition network. We perform experiments to compare the performance of the proposed model with seven benchmarks. The numerical results show that our model achieves the best performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, text recognition has attracted the attention of both academia and actual users due to its application on various domains such as translation in mixed reality, autonomous driving, or assistive technology for the blind. Text recognition can be classified into two main categories: scanned document recognition and scene text recognition. While the former has achieved significant advancements, the latter remains challenging due to scene texts' inherent characteristics such as the distortion and irregular shapes of the texts. Recent methods in scene text recognition are inspired by the success of deep learning-based recognition models. Generally, these methods can be classified in two approaches: recurrent neural networks (RNN) based and convolutional neural networks (CNN) based. RNN-based models have shown their effectiveness, thanks to capturing contextual information and dependencies between different patches. However, RNNs typically compute along with the symbol positions of the input and output sequences, which cannot be performed in * Authors contribute equally ? Corresponding author parallel fashion, thus leads to high training time. Furthermore, RNNs also encounter problems such as vanishing gradient <ref type="bibr" target="#b0">[1]</ref> or bottleneck <ref type="bibr" target="#b1">[2]</ref>. CNN-based approach, which allows computing the hidden representation parallelly, have been proposed to speed up the training procedure. However, to capture the dependencies between distant patches in long input sequences, CNN models require stacking more convolutional layers, which significantly increases the network's complexity. Therefore, CNN-based methods suffer the trade-off between complexity and accuracy. To remedy these limitations, in natural language processing (NLP) fields, a self-attention based mechanism named transformer <ref type="bibr" target="#b2">[3]</ref> has been proposed. In the transformer, dependencies between different input and output positions are captured using a self-attention mechanism instead of sequential procedures in RNN. This mechanism allows more computation parallelization with higher performance. In the computer vision domain, some research have leveraged the transformer architecture and showed the effectiveness of some problems <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b4">[5]</ref> Inspired by the transformer network, in this paper, we propose a self-attention based scene text recognizer with focal loss, namely as SAFL. Moreover, to tackle irregular shapes of scene texts, we also exploit a text rectification named Spatial Transformer Network (STN) to enhance the quality of text before passing to the recognition network. SAFL, as depicted in <ref type="figure">Figure 1</ref>, contains three components: rectification, feature extraction, and recognition. First, given an input image, the rectification network, built based on the Spatial Transformer Network (STN) <ref type="bibr" target="#b5">[6]</ref>, transforms the image to rectify its text. Then, the features of the rectified image are extracted using a convolutional neural network. Finally, a self-attention based recognition network is applied to predict the output character sequence. Specifically, the recognition network is an encoderdecoder model, where the encoder utilizes multi-head selfattention to transform input sequence to hidden feature representation, then the decoder applies another multi-head selfattention to output character sequence. To balance the training data for improving the prediction accuracy, we exploit focal loss instead of negative log-likelihood as in most recent works <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>.</p><p>To evaluate our proposed model's performance, we train SAFL with two synthetic datasets: Synth90k <ref type="bibr" target="#b8">[9]</ref> and SynthText <ref type="bibr" target="#b9">[10]</ref>, and compare its accuracy with standard benchmarks, <ref type="figure">Fig. 1</ref>. Overview of SAFL on both regular and irregular datasets. The experiment results show that our method outperforms the state-of-the-art on all datasets. Furthermore, we also perform experiments to study the effectiveness of focal loss. The numerical results show the superiority of focal loss over the negative log-likelihood loss on all datasets.</p><p>The remainder of the paper is organized as follows. Section II introduces related works. We describe the details of the proposed model in Section III and present the evaluation results in Section IV. Finally, we conclude the paper and discuss the future works in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Scene text recognition has attracted great interest over the past few years. Comprehensive surveys for scene text recognition may be found in <ref type="bibr" target="#b10">[11]</ref> [12] <ref type="bibr" target="#b12">[13]</ref>. As categorized by previous works <ref type="bibr" target="#b7">[8]</ref> [14] <ref type="bibr" target="#b14">[15]</ref>, scene text may be divided into two categories: regular and irregular text. The regular text usually has a nearly horizontal shape, while the irregular text has an arbitrary shape, which may be distorted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Regular text recognition</head><p>Early work mainly focused on regular text and used a bottom-up scheme, which first detects individual characters using a sliding window, then recognizing the characters using dynamic programming or lexicon search <ref type="bibr" target="#b15">[16]</ref> [17] <ref type="bibr" target="#b17">[18]</ref>. However, these methods have an inherent limitation, which is ignoring contextual dependencies between characters. Shi et al. <ref type="bibr" target="#b18">[19]</ref> and He et al. <ref type="bibr" target="#b19">[20]</ref> typically regard text recognition as a sequence-to-sequence problem. Input images and output texts are typically represented as patch sequences and character sequences, respectively. This technique allows leveraging deep learning techniques such as RNNs or CNNs to capture contextual dependencies between characters <ref type="bibr" target="#b6">[7]</ref> [19] <ref type="bibr" target="#b19">[20]</ref>, lead to significant improvements in accuracy on standard benchmarks. Therefore, recent work has shifted focus to the irregular text, a more challenging problem of scene text recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Irregular text recognition</head><p>Irregular text is a recent challenging problem of scene text recognition, which refers to texts with perspective distortions and arbitrary shape. The early works correct perspective distortions by using hand-craft features. However, these approaches require correct tunning by expert knowledge for achieving the best results because of a large variety of hyperparameters. Recently, Yang et al. <ref type="bibr" target="#b20">[21]</ref> proposed an auxiliary dense character detection model and an alignment loss to effectively solve irregular text problems. Liu et al. <ref type="bibr" target="#b21">[22]</ref> introduced a Character-Aware Neural Network (Char-Net) to detect and rectify individual characters. Shi et al. <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref> addressed irregular text problems with a rectification network based on Spatial Transformer Network (STN), which transform input image for better recognition. Zhan et al. <ref type="bibr" target="#b22">[23]</ref> proposed a rectification network employing a novel line-fitting transformation and an iterative rectification pipeline for correction of perspective and curvature distortions of irregular texts. <ref type="figure">Figure 1</ref> shows the structure of SAFL, which is comprised of three main components: rectification, feature extraction, and recognition. The rectification module is a Spatial Transformer Network (STN) <ref type="bibr" target="#b5">[6]</ref>, which receives the original image and rectifies the text to enhance the quality. The feature extraction module is a convolution neural network that extracts the information of the rectified image and represents it into a vector sequence. The final module, i.e., recognition, is based on the self-attention mechanism and the transformer network architecture <ref type="bibr" target="#b2">[3]</ref>, to predict character sequence from the feature sequence. In the following, we first present the details of the three components in Section III-A, III-B and III-C, respectively. Then, we describe the training strategy using focal loss in Section III-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rectification</head><p>In this module, we leverage a Thin Plate Spline (TPS) transformation <ref type="bibr" target="#b7">[8]</ref>, a variant of STN, to construct a rectification network. Given the input image I with an arbitrary size, the rectification module first resizes I into a predefined fixed size. Then the module detects several control points along the top and bottom of the text's bounding. Finally, TPS applies a smooth spline interpolation between a set of control points to rectify the predicted region to obtain a fixed-size image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>We exploit the convolution neural network (CNN) to extract the features of the rectified image (obtained from a rectification network) into a sequence of vectors. Specifically, the input image is passed through convolution layers (ConvNet) to produce a feature map. Then, the model separates the feature map by rows. The output received after separating the feature map are feature vectors arranged in sequences. The scene text recognition problem then becomes a sequence-to-sequence problem whose input is a sequence of characteristic vectors, and whose output is a sequence of characters predicted. Based on the proposal in <ref type="bibr" target="#b2">[3]</ref>, we further improve information about the position of the text in the input image by using positional encoding. Each position pos is represented by a vector whose value of the i th dimension, i.e., P E (pos,i) , is defined as</p><formula xml:id="formula_0">P E (pos,i) = ? ? ? sin pos 10000 2i d model , if 0 ? i ? d model 2 cos pos 10000 2i d model , if d model 2 ? i ? d model ,<label>(1)</label></formula><p>where d model is the vector size. The position information is added into the encoding vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-attention based recognition network</head><p>The architecture of the recognition network follows the encoder-decoder model. Both encoder blocks and decoder blocks are built based on the self-attention mechanism. We will briefly review this mechanism before describing each network's details. 1) Self-attention mechanism: Self-attention is a mechanism that extracts the correlation between different positions of a single sequence to compute a representation of the sequence. In this paper, we utilize the scaled dot-product attention proposed in <ref type="bibr" target="#b2">[3]</ref>. This mechanism consists of queries and keys of dimension d k , and values of dimension d v . Each query performs the dot product of all keys to obtain their correlation. Then, we obtain the weights on the values by using the softmax function. In practice, the keys, values, and queries are also packed together into matrices K, V and Q. The matrix of the outputs is computed as follow:</p><formula xml:id="formula_1">Attention(Q, K, V ) = softmax QK T ? d k V<label>(2)</label></formula><p>The dot product is scaled by 1</p><formula xml:id="formula_2">? d k</formula><p>to alleviate the small softmax values which lead to extremely small gradients with large values of d k . <ref type="bibr" target="#b2">[3]</ref>.</p><p>2) Encoder: Encoder is a stack of N e blocks. Each block consists of two main layers. The first layer is a multi-head attention layer, and the second layer is a fully-connected feedforward layer. The multi-head attention layer is the combination of multiple outputs of the scale dot product attention. Each scale-dot product attention returns a matrix representing the feature sequences, which is called head attention. The combination of multiple head attentions to the multi-head Multi-head attention can be formulated as follows:</p><formula xml:id="formula_3">M ultiHead(Q, K, V ) = Concat (head 1 , . . . , head h ) W O (3) where head i = Attention QW Q i , KW K i , V W V i , h is the number of heads, W Q i ? R dmodel ?d k , W K i ? R dmodel ?d k , W V i ? R dmodel ?dv , W O ? R hdv?dmodel are weight matrices. d k , d v</formula><p>and d model are set to the same value. Layer normalization <ref type="bibr" target="#b23">[24]</ref> and residual connection <ref type="bibr" target="#b24">[25]</ref> are added into each main layer (i.e., multi-head attention layer and fully-connected layer) to improve the training effect. Specifically, the residual connections helps to decrease the loss of information in the backpropagation process, while the normalization makes the training process more stable. Consequently, the output of each main layer with the input x can be represented as LayerN orm(x + Layer(x)), where Layer(x) is the function implemented by the layer itself, and LayerN orm() represents the normalization operation. The blocks of the encoder are stacked sequentially, i.e., the output of the previous block is the input of the following block.</p><p>3) Decoder: The decoding process predicts the words in a sentence from left to right, starting with the start tag until encountering the end tag. The decoder is comprised of N d decoder blocks. Each block is also built based on multihead attention and a fully connected layer. The multi-head attention in the decoder does not consider words that have not been predicted by weighting these positions with ??. Furthermore, the decoder uses additional multi-head attention that receives keys and values from the encoder and queries from the decoder. Finally, the decoder's output is converted into a probability distribution through a linear transformation and softmax function. <ref type="figure" target="#fig_0">Figure 2</ref> shows that the lexicon of training datasets suffers from an unbalanced sample distribution. The unbalance may lead to severe overfitting for high-frequency samples and underfitting for low-frequency samples. To this end, we propose to use focal loss <ref type="bibr" target="#b25">[26]</ref> instead of negative log-likelihood as in most of recent methods <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b7">[8]</ref>. By exploiting focal loss, the model will not encounter the phenomenon of ignoring to train low-frequency samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>Focal loss is known as an effective loss function to address the unbalance of datasets. By reshaping the standard crossentropy loss, focal loss reduces the impacts of high-frequency samples and thus focus training on low-frequency ones <ref type="bibr" target="#b25">[26]</ref>. The focal loss is defined as follows:</p><formula xml:id="formula_4">F L (p t ) = ?? t (1 ? p t ) ? log (p t ) ,<label>(4)</label></formula><p>where, p t is the probability of the predicted value, computed using softmax function, ? and ? are tunable hyperparameters used to balance the loss. Intuitively, focal loss is obtained by multiplying cross entropy by ? t (1 ? p t ) ? . Note that the</p><formula xml:id="formula_5">weight ? t (1 ? p t ) ? is inversely proportional with p t , thus</formula><p>the focal loss helps to reduce the impact of high-frequency samples (whose value of p t is usually high) and focus more on low-frequency ones (which usually have low value of p t ).</p><p>Based on focal loss, we define our training objective as follows:</p><formula xml:id="formula_6">L = ? T t=1 (? t (1 ? p (y t | I)) ? log p (y t | I))))<label>(5)</label></formula><p>where y t are the predicted characters, T is the length of the predicted sequence, and I is the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PERFORMANCE EVALUATION</head><p>In this section, we conduct experiments to demonstrate the effectiveness of our proposed model. We first briefly introduce datasets used for training and testing, then we describe our implementation details. Next, we analyze the effect of focal loss on our model. Finally, we compare our model against state-of-the-art techniques on seven public benchmark datasets, including regular and irregular text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The training datasets contains two datasets: Synth90k and SynthText. Synth90k is a synthetic dataset introduced in <ref type="bibr" target="#b8">[9]</ref>. This dataset contains 9 million images created by combining 90.000 common English words and random variations and effects. SynthText is a synthetic dataset introduced in <ref type="bibr" target="#b9">[10]</ref>, which contains 7 million samples by the same generation process as Synth90k <ref type="bibr" target="#b8">[9]</ref>. However, SynthText is targeted for text detection so that an image may contain several words. All experiments are evaluated on seven well-known public benchmarks described, which can be divided into two categories: regular text and irregular text. Regular text datasets include IIIT5K, SVT, ICDAR03, ICDAR13.</p><p>? IIIT5K <ref type="bibr" target="#b26">[27]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Configurations 1) Implementation Detail:</head><p>We implement the proposed model by Pytorch library and Python programming language.</p><p>The model is trained and tested on an NDIVIA RTX 2080 Ti GPU with 12 GB memory. We train the model from scratch using Adam optimizer with the learning rate of 0.00002.</p><p>To evaluate the trained model, we use dataset III5K. The pretrained model and code are available at <ref type="bibr">[33]</ref> 2) Rectification Network: All input images are resized to 64 ? 256 before applying the rectification network. The rectification network consists of three components: a localization network, a thin plate spline (TPS) transformation, and a sampler. The localization network consists of 6 convolutional layers with the kernel size of 3 ? 3 and two fully-connected (FCN) layers. Each FCN is followed by a 2 ? 2 max-pooling layer. The number of the output filters is 32, 64, 128, 256, and 256. The number of output units of FCN is 512 and 2K, respectively, where K is the number of the control points. In all experiments, we set K to 20, as suggested by <ref type="bibr" target="#b7">[8]</ref>. The sampler generates the rectified image with a size of 32 ? 100. The size of the rectified image is also the input size of the feature extraction module.</p><p>3) Feature Extraction: We construct the feature extraction module based on Resnet architecture <ref type="bibr" target="#b24">[25]</ref>. The configurations of the feature extraction network are listed in <ref type="table">Table I</ref>. Our feature extraction network contains five blocks of 45 residual layers. Each residual unit consists of a 1 ? 1 convolutional layer, followed by a 3 ? 3 convolution layer. In the first two blocks, we use 2 ? 2 stride to reduce the feature map dimension. In the next blocks, we use 2 ? 1 stride to downsampled feature maps. The 2 ? 1 stride also allows us to retain more information horizontally to distinguish neighbor characters effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Recognition:</head><p>The number of blocks in the encoder and the decoder are set both to 4. In each block of the encoder and the decoder, the dimension of the feed forward vector and the ouput vector are set to 2048 and 512, respectively. The number of head attention layers is set to 8. The decoder recognizes 94 different characters, including numbers, alphabet characters, uppercase, lowercase, and 32 punctuation in ASCII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Result and Discussion</head><p>1) Impact of focal loss: To analyze the effect of focal loss, we study two variants of the proposed model. The first variant uses negative log-likelihood, and the second one leverages focal loss. As shown in <ref type="table" target="#tab_1">Table II</ref>, the model with focal loss outperforms the one with log-likelihood on all datasets. Notably, on average, focal loss improves the accuracy by 2.3 % compared to log-likelihood. For the best case, i.e., CUTE, the performance gap between the two variants is 4.8 % 2) Impact of rectification network: In this section, we study the effect of text rectification by comparing SAFL and a variant which does not include the rectification module.  <ref type="table" target="#tab_1">Table III</ref> depicts the recognition accuracies of the two models over seven datasets. It can be observed that the rectification module increases the accuracy significantly. Specifically, the performance gap between SAFL and the one without the rectification module is 4.1% on average. In the best cases, SAFL improves the accuracy by 10.1% and 7% compared to the other on the datasets SVT-P and CUTE, respectively. The reason is that both SVT-P and CUTE contains many both irregular texts such as perspective texts or curved texts.</p><p>3) Comparison with State-of-the-art: In this section, we compare the performance of SAFL with the latest approaches in scene text recognition. The evaluation results are shown in <ref type="table" target="#tab_3">Table IV</ref>. In each column, the best value is bolded. the "Avarage" column is the weighted average over all the data sets. Concerning the irregular text, it can be observed that SAFL achieves the best performance on 3 data sets. Particularly, SAFL outperforms the current state-of-the-art, ESIR <ref type="bibr" target="#b22">[23]</ref>, by a margin of 1.2% on average, particulary on CUTE (+2.1%) and SVT-P (+2.1%). Concerning the regular datasets, SAFL outperforms the other methods on two datasets IIIT5K and ICDAR03. Moreover, SAFL also shows the highest average accuracy over all the regular text datasets. To summarize, SAFL achieves the best performance on 5 of 7 datasets and the highest average accuracy on both irregular and regular texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed SAFL, a deep learning model for scene text recognition, which exploits self-attention mechanism and focal loss. The experiment results showed that SAFL achieves the highest average accuracy on both the regular datasets and irregular datasets. Moreover, SAFL outperforms the state-of-the-art on CUTE dataset by a margin of 2.1%. Summary, SAFL shows superior performance on 5 out of 7 benchmarks, including IIIT5k, ICDAR 2003, ICDAR 2015, SVT-P and CUTE. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Frenquency of characters in training lexicon attention allows our model to learn more representations of feature sequences, thereby increasing the diversity of the extracted information, and thereby enhance the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>contains 3000 test images collected from Google image searches. ICDAR03 [28] contains 860 word-box cropped images. SVT contains 647 testing word-box collected from Google Street View. Irregular text datasets include ICDAR15, SVT-P, CUTE. SVT-P [31] contains 645 testing word-box cropped images collected from Google Street View. Most of them are heavily distorted by the non-frontal view angle. ? CUTE [32] contains 288 word-box cropped images, which are curved text images.</figDesc><table><row><cell>? ICDAR15 [30] contains 1811 testing word-box cropped</cell></row><row><cell>images collected from Google Glass without careful</cell></row><row><cell>positioning and focusing.</cell></row></table><note>?? ICDAR13 [29] contains 1015 word-box cropped images.??</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">RECOGNITION ACCURACIES WITH NEGATIVE LOG-LIKELIHOOD AND</cell></row><row><cell></cell><cell>FOCAL LOSS</cell><cell></cell></row><row><cell>Variant</cell><cell cols="2">Negative log-likelihood Focal Loss</cell></row><row><cell>IIIT5K</cell><cell>92.6</cell><cell>93.9</cell></row><row><cell>SVT</cell><cell>85.8</cell><cell>88.6</cell></row><row><cell>ICDAR03</cell><cell>94.1</cell><cell>95</cell></row><row><cell>ICDAR13</cell><cell>92</cell><cell>92.8</cell></row><row><cell>ICDAR15</cell><cell>76.1</cell><cell>77.5</cell></row><row><cell>SVT-P</cell><cell>79.4</cell><cell>81.7</cell></row><row><cell>CUTE</cell><cell>80.6</cell><cell>85.4</cell></row><row><cell>Avarage</cell><cell>86.9</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III RECOGNITION</head><label>III</label><figDesc>ACCURACIES WITH AND WITHOUT RECTIFICATION</figDesc><table><row><cell>Variant</cell><cell cols="2">SAFL w/o text rectification SAFL</cell></row><row><cell>IIIT5K</cell><cell>90.7</cell><cell>93.9</cell></row><row><cell>SVT</cell><cell>83.3</cell><cell>88.6</cell></row><row><cell>ICDAR03</cell><cell>93</cell><cell>95</cell></row><row><cell>ICDAR13</cell><cell>90.7</cell><cell>92.8</cell></row><row><cell>ICDAR15</cell><cell>72.9</cell><cell>77.5</cell></row><row><cell>SVT-P</cell><cell>71.6</cell><cell>81.7</cell></row><row><cell>CUTE</cell><cell>77.4</cell><cell>85.4</cell></row><row><cell>Avarage</cell><cell>84.1</cell><cell>88.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SCENE</head><label>IV</label><figDesc>TEXT ACCURANCIES (%) OVER SEVEN PUBLIC BENCHMARK TEST DATASETS.</figDesc><table><row><cell>Method</cell><cell cols="4">Regular test dataset IIIT5k SVT ICDAR03 ICDAR13</cell><cell>Average</cell><cell cols="3">Irregular test dataset ICDAR15 SVT-P CUTE</cell><cell>Average</cell></row><row><cell>Jaderberg et al. [34]</cell><cell>-</cell><cell>80.7</cell><cell>93.1</cell><cell>90.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CRNN [19]</cell><cell>78.2</cell><cell>80.8</cell><cell>89.4</cell><cell>86.7</cell><cell>81.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RARE [7]</cell><cell>81.9</cell><cell>81.9</cell><cell>90.1</cell><cell>88.6</cell><cell>85.3</cell><cell>-</cell><cell>71.8</cell><cell>59.2</cell><cell>-</cell></row><row><cell>Lee et al.</cell><cell>78.4</cell><cell>80.7</cell><cell>88.7</cell><cell>90.0</cell><cell>82.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al. [21]</cell><cell>-</cell><cell>75.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.8</cell><cell>69.3</cell><cell>-</cell></row><row><cell>FAN [35]</cell><cell>87.4</cell><cell>85.9</cell><cell>94.2</cell><cell>93.3</cell><cell>89.4</cell><cell>70.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Shi et al. [7]</cell><cell>81.2</cell><cell>82.7</cell><cell>91.9</cell><cell>89.6</cell><cell>84.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al. [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.8</cell><cell>69.3</cell><cell>-</cell></row><row><cell>Char-Net [22]</cell><cell>83.6</cell><cell>84.4</cell><cell>91.5</cell><cell>90.8</cell><cell>86.2</cell><cell>60.0</cell><cell>73.5</cell><cell>-</cell><cell>-</cell></row><row><cell>AON [36]</cell><cell>87.0</cell><cell>82.8</cell><cell>91.5</cell><cell>-</cell><cell>-</cell><cell>68.2</cell><cell>73.0</cell><cell>76.8</cell><cell>70.0</cell></row><row><cell>EP [37]</cell><cell>88.3</cell><cell>87.5</cell><cell>94.6</cell><cell>94.4</cell><cell>90.3</cell><cell>73.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Liao et al. [38]</cell><cell>91.9</cell><cell>86.4</cell><cell>-</cell><cell>86.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.9</cell><cell>-</cell></row><row><cell>Baek et al. [14]</cell><cell>87.9</cell><cell>87.5</cell><cell>94.9</cell><cell>92.3</cell><cell>89.8</cell><cell>71.8</cell><cell>79.2</cell><cell>74.0</cell><cell>73.6</cell></row><row><cell>ASTER [8]</cell><cell>93.4</cell><cell>89.5</cell><cell>94.5</cell><cell>91.8</cell><cell>92.8</cell><cell>76.1</cell><cell>78.5</cell><cell>79.5</cell><cell>76.9</cell></row><row><cell>SAR [39]</cell><cell>91.5</cell><cell>84.5</cell><cell>-</cell><cell>91.0</cell><cell>-</cell><cell>69.2</cell><cell>76.4</cell><cell>83.3</cell><cell>72.1</cell></row><row><cell>ESIR [23]</cell><cell>93.3</cell><cell>90.2</cell><cell>-</cell><cell>91.3</cell><cell>-</cell><cell>76.9</cell><cell>79.6</cell><cell>83.3</cell><cell>78.1</cell></row><row><cell>SAFL</cell><cell>93.9</cell><cell>88.6</cell><cell>95</cell><cell>92.8</cell><cell>93.3</cell><cell>77.5</cell><cell>81.7</cell><cell>85.4</cell><cell>79.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank AIMENEXT Co., Ltd. for supporting our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust scene text recognition with automatic rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4168" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scene text detection and recognition: Recent advances and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text detection and recognition in imagery: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1480" to="1500" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A simple and robust convolutional-attention network for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01375</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Strokelets: A learned multi-scale representation for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4042" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reading scene text in deep convolutional sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to read irregular text with attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Char-net: A character-aware neural network for distorted scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2059" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Top-down and bottom-up cues for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2687" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions: entries, results, and future directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nagai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Document Analysis and Recognition (IJDAR)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="105" to="122" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Icdar 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep structured output learning for unconstrained text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5903</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Edit probability for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1508" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
