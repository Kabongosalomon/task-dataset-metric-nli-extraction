<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relation Network for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
							<email>hyunpark@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
							<email>bumsub.ham@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relation Network for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model outperforms the state of the art on the Market1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Person re-identification (reID) is one of fundamental tasks in computer vision, with the purpose of retrieving a particular person from a set of pedestrian images, captured by multiple cameras. It has been getting a lot of attention in recent years, due to the wide range of applications including pedestrian detection ) and multi-person tracking <ref type="bibr" target="#b28">(Tang et al. 2017</ref>). This problem is very challenging, since pedestrians have different attributes (e.g., clothing, gender, hair), and the pictures of them are taken under different conditions, such as illumination, occlusion, background clutter, and camera types. Remarkable advances in convolutional neural networks (CNNs) over the last decade allow to obtain person representations <ref type="bibr" target="#b16">(Lin et al. 2017a;</ref><ref type="bibr" target="#b8">Ge et al. 2018)</ref> robust to these factors of variations, especially for human pose, and they also enables learning met-Person reID methods using CNNs typically focus on extracting a global feature of a person image <ref type="bibr" target="#b16">(Lin et al. 2017a;</ref><ref type="bibr" target="#b8">Ge et al. 2018;</ref><ref type="bibr" target="#b31">Zhang, Xiang, and Gong 2016;</ref><ref type="bibr" target="#b3">Chen et al. 2017</ref>) to obtain a compact descriptor for an efficient retrieval. This, however, gives a limited representation, as the global feature may not account for intra-class variations (e.g., human pose, occlusion, background clutter). To address this problem, part-based methods <ref type="bibr" target="#b32">(Zhao et al. 2017a;</ref><ref type="bibr" target="#b23">Su et al. 2017;</ref><ref type="bibr" target="#b36">Zheng et al. 2017a;</ref><ref type="bibr" target="#b15">Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b18">Liu et al. 2017;</ref><ref type="bibr" target="#b33">Zhao et al. 2017b;</ref><ref type="bibr" target="#b26">Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019</ref>) have been proposed. They extract local features from body parts (e.g., arms, legs, torso), often together with the global feature of a person image itself, and aggregate them for an effective person reID. To leverage body parts, these approaches extract pose maps from offthe-shelf pose estimators <ref type="bibr" target="#b32">(Zhao et al. 2017a;</ref><ref type="bibr" target="#b23">Su et al. 2017;</ref><ref type="bibr" target="#b36">Zheng et al. 2017a)</ref>, compute attention maps to consider discriminative regions of interest <ref type="bibr" target="#b15">(Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b18">Liu et al. 2017;</ref><ref type="bibr" target="#b33">Zhao et al. 2017b</ref>), or slice person images into horizontal grids <ref type="bibr" target="#b26">(Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019</ref>). Part-level features provide better person representations than a global one, but aggregating the individual local features e.g., by concatenating them without considering relations between body parts, is limited to represent an identity of a person discriminatively. In particular, this does not differentiate the identities of different persons that have similar attributes in corresponding parts between images, since part-based methods compute the similarity of corresponding part-level features independently.</p><p>In this paper, we propose to make each part-level feature incorporate information of other body parts to obtain discriminative person representations for an effective person reID. To this end, we introduce a new relation module exploiting one-vs.-rest relations of body parts. It accounts for the relations between individual body parts and the rest of them, so that each part-level feature contains information of the corresponding part itself and other body parts, supporting it to be more discriminative. As will be seen in our experiments, considering the relation between body parts provides better part-level features with a clear ad-vantage over current part-based methods. We have observed that 1) directly using both global average and max pooling techniques (GAP and GMP) to obtain a global feature of a person image does not provide a performance gain, and 2) GMP gives better results than GAP. Based on this, we also present a global contrastive pooling (GCP) method to obtain better feature representations based on GMP, which adaptively aggregates GAP and GMP results of the entire partlevel features. Specifically, it uses the discrepancy between the pooling results, and distill the complementary information to max pooled features in a residual manner. Experimental results on standard benchmarks, including the Mar-ket1501 <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>, DukeMTMC-reID <ref type="bibr" target="#b21">(Ristani et al. 2016)</ref>, and CUHK03 <ref type="bibr" target="#b13">(Li et al. 2014)</ref>, demonstrate the advantage of our approach for person reID. To encourage comparison and future work, our code and models are available online: https://cvlab-yonsei.github.io/projects/RRID/.</p><p>The main contributions of this paper can be summarized as follows: 1) We introduce a relation network for part-based person reID to obtain discriminative local features. 2) We propose a new pooling method exploiting contrastive features, GCP, to extract a global feature of a person image. 3) We achieve a new state of the art, outperforming other partbased reID methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Person reID. Several person reID methods based on CNNs have recently been proposed. They typically formulate the reID task as a multi-class classification problem <ref type="bibr" target="#b37">(Zheng, Zheng, and Yang 2018)</ref>, where person images of the same identity belong to the same category. A classification loss encourages the images of the same identity to be embedded nearby in feature space. Other reID methods additionally use person images of different identities for training, and enforces the feature distance of person images with the same identity to be smaller than that with different identities by a ranking loss. There are many attempts to obtain discriminative feature representations, e.g., leveraging generative adversarial networks (GANs) to distill identityrelated features <ref type="bibr" target="#b8">(Ge et al. 2018)</ref>, using attributes to offer complementary information <ref type="bibr" target="#b17">(Lin et al. 2017b</ref>), or exploiting body parts to extract diverse person features <ref type="bibr" target="#b32">(Zhao et al. 2017a;</ref><ref type="bibr" target="#b36">Zheng et al. 2017a;</ref><ref type="bibr" target="#b23">Su et al. 2017;</ref><ref type="bibr" target="#b18">Liu et al. 2017;</ref><ref type="bibr" target="#b15">Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b30">Yao et al. 2019;</ref><ref type="bibr" target="#b33">Zhao et al. 2017b;</ref><ref type="bibr" target="#b26">Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019;</ref><ref type="bibr" target="#b29">Wang et al. 2018)</ref>.</p><p>Part-based methods enhance the discriminative capabilities of various body parts. We classify them into three categories: The first approach uses a pose estimator (or a landmark detector) to extract a pose map <ref type="bibr" target="#b32">(Zhao et al. 2017a;</ref><ref type="bibr" target="#b36">Zheng et al. 2017a;</ref><ref type="bibr" target="#b23">Su et al. 2017)</ref>. This requires an additional data with landmark annotations to train a pose estimator, and the retrieval accuracy of reID largely depends on the performance of the estimator. The second approach leverages body parts implicitly using an attention map <ref type="bibr" target="#b15">(Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b18">Liu et al. 2017;</ref><ref type="bibr" target="#b33">Zhao et al. 2017b;</ref><ref type="bibr" target="#b30">Yao et al. 2019)</ref>, which can be achieved without auxiliary supervisory signals (i.e., pose annotations). It provides a feature representation robust to background clutter, focusing on the regions of interest, but the attended regions may not contain discriminative body parts. The third approach also exploits body parts implicitly dividing person images into horizontal grids of multiple scales <ref type="bibr" target="#b26">(Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019;</ref><ref type="bibr" target="#b29">Wang et al. 2018)</ref>. It assumes that person pictures, localized by off-the-shelf object detectors <ref type="bibr">(Felzenszwalb et al. 2008)</ref>, generally have the same body parts for particular grids (e.g., legs on the lower parts of person images). This is, however, problematic when the detectors do not localize the persons tightly. Our method belongs to the third category. In contrast to other methods, we aggregate local features while considering relations between body parts, rather than exploiting them directly. Furthermore, we introduce a GCP method to obtain a global feature of a person image, providing discriminative person representations.</p><p>Relation network. Exploiting relational reasoning <ref type="bibr" target="#b22">(Santoro et al. 2017;</ref><ref type="bibr" target="#b1">Baradel et al. 2018;</ref><ref type="bibr" target="#b25">Sun et al. 2018a;</ref><ref type="bibr" target="#b27">Sung et al. 2018</ref>) is important for many tasks requiring the capacity to reason about dependencies between different entities (e.g., objects, actors, scene elements). Many works have been proposed to support relation-centric computation including interaction networks <ref type="bibr" target="#b2">(Battaglia et al. 2016</ref>) and gated graph sequence networks <ref type="bibr" target="#b14">(Li et al. 2016)</ref>. The relation network <ref type="bibr" target="#b22">(Santoro et al. 2017</ref>) is a representative method that has been successfully adapted to computer vision problems, including visual question answering <ref type="bibr" target="#b22">(Santoro et al. 2017)</ref>, object detection <ref type="bibr" target="#b1">(Baradel et al. 2018)</ref>, action recognition <ref type="bibr" target="#b25">(Sun et al. 2018a)</ref>, and few-shot learning <ref type="bibr" target="#b27">(Sung et al. 2018)</ref>. The basic idea behind the relation network is to consider all pairs of entities and to integrate all these relations e.g., to answer the question <ref type="bibr" target="#b22">(Santoro et al. 2017)</ref> or to localize the objects of interest <ref type="bibr" target="#b1">(Baradel et al. 2018)</ref>. Motivated by this work, we leverage relations of body parts to obtain better person representations for part-based person reID. Differently, we exploit the relations between individual body parts and the rest of them, rather than considering all pairs of the parts. This encourages each part-level feature to incorporate information of other body parts as well, making it more discriminative, while retaining compact feature representations for person reID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our Approach</head><p>We show in <ref type="figure" target="#fig_0">Fig. 1</ref> an overview of our framework. We extract a feature map of size H ? W ? C from a person image, where H, W , C are height, width, and the number of channels, respectively. The resulting feature map is divided equally into six horizontal grids. We then apply GMP to each feature map, and obtain part-level features of size 1 ? 1 ? C. We feed these features through two modules in order to extract novel local and global person representations: One-vs.rest relation module and GCP. The first module makes each part-level feature more discriminative by considering the relations between individual body parts and the rest of them, and outputs local relational features of size 1 ? 1 ? c where c &lt; C. The second module provides a global contrastive feature of size 1 ? 1 ? c representing the person image itself. We concatenate global contrastive and local relational features along the channel dimension, and use the feature of size 1 ? 1 ? 7c as a person representation for reID. We train our model end-to-end using cross-entropy and triplet losses, with triplets of anchor, positive and negative person images, where the anchor image has the same identity as a positive one while having a different identity from a negative one. At test time, we extract features of person images, and compute the Euclidean distance between them to determine the identities of persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation networks for part-based reID</head><p>Part-level features. We exploit a ResNet-50 <ref type="bibr" target="#b9">(He et al. 2016</ref>) trained for ImageNet classification <ref type="bibr" target="#b4">(Deng et al. 2009</ref>) as our backbone network to extract an initial feature map from an input person image. Specifically, following the work of <ref type="bibr" target="#b26">(Sun et al. 2018b</ref>), we remove the GAP and fully connected layers from the ResNet-50 architecture, and set stride of the last convolutional layer to 1. Similar to other partbased reID methods <ref type="bibr" target="#b26">(Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019)</ref>, we split the initial feature map into multiple horizontal grids of size H/6 ? W ? C. We apply GMP to each of them, and obtain part-level features of size 1 ? 1 ? C.</p><p>One-vs.-rest relational module. Extracting part-level features from the horizontal grids allows to leverage body parts implicitly for diverse person representations. Existing reID methods <ref type="bibr" target="#b26">(Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019;</ref>) use these local features independently for person retrieval. They concatenate all local features in a particular order, considering rough geometric correspondences between person images. Although this gives a structural person representation robust to geometric variations and occlusion, the local features cover small parts of an image only, and more importantly they do not account for the relations between body parts. That is, individual parts are isolated, and do not communicate with other ones, which distracts computing the similarity between different persons with similar attributes in corresponding parts. To alleviate this problem, we propose to leverage the relations between body parts for person representations. Specifically, we introduce a new relation network <ref type="figure">(Fig. 2(a)</ref>) that exploits one-vs-rest relation of body parts, making it possible for each part-level feature to contain information of the corresponding part itself and other body parts.</p><p>Concretely, we denote by p i (i = 1, . . . , 6) each partlevel feature of size 1 ? 1 ? C. We apply an average pooling to all part-level features, except the one of the particular part p i , aggregating the information from other body parts as follows: r i = 1 5 j =i p j . We then add a 1 ? 1 convolutional layer separately for each p i and r i , giving feature mapsp i andr i of size 1 ? 1 ? c, respectively. The relation network concatenates the featuresp i andr i , and outputs a local relational feature q i for each p i . We depict in <ref type="figure">Fig. 2(a)</ref> an example of extracting the local relational feature q 1 . Here, we assume that the feature q i contains information of the original onep i itself and other body parts. We thus use a skip-connection <ref type="bibr" target="#b9">(He et al. 2016)</ref> to transfer the relational information ofp i andr i top i , as follows:</p><formula xml:id="formula_0">q i =p i + R p (T (p i ,r i )), (i = 1, . . . , 6),<label>(1)</label></formula><p>where R p is a sub-network consisting of a 1 ? 1 convolution, batch normalization <ref type="bibr" target="#b11">(Ioffe and Szegedy 2015)</ref>, and ReLU (Krizhevsky, Sutskever, and Hinton 2012) layers.</p><p>We denote by T a concatenation of features. The residual R p (T (p i ,r i )) supports the part-level featurep i , making it more discriminative and robust to occlusion. We may leverage all pairwise relations between the featuresp i similar to <ref type="bibr" target="#b2">(Battaglia et al. 2016)</ref>, but this requires a large computational cost and increases the dimension of features drastically. In contrast, our one-vs.-rest relation module computes the feature q i in linear time, and also retains a compact feature representation.</p><p>C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part-level features</head><p>The rest of the feature GCP. To represent an entire person image, previous reID methods use GAP <ref type="bibr" target="#b26">(Sun et al. 2018b</ref>), GMP , or both <ref type="bibr" target="#b7">(Fu et al. 2019)</ref>. GAP covers the whole body parts of the person image ( <ref type="figure">Fig. 3(a)</ref>), but it is easily distracted by background clutter and occlusion. GMP overcomes this problem by aggregating the feature from the most discriminative part useful for reID while discarding background clutter <ref type="figure">(Fig. 3(b)</ref>). This, however, does not contain information from the whole body parts. A hybrid approach to exploiting both GAP and GMP <ref type="bibr" target="#b7">(Fu et al. 2019</ref>) may perform better, but it is also influenced by background clutter ( <ref type="figure">Fig. 3(c)</ref>). It has been proven that GMP is more effective than GAP <ref type="bibr" target="#b7">(Fu et al. 2019</ref>), which will be also verified once more in our experiment. Motivated by this, we propose a novel GCP method based on GMP to extract a global feature map from the whole body parts <ref type="figure">(Fig. 2(b)</ref>). Rather than applying GAP or GMP to the initial feature map from the input person image <ref type="bibr" target="#b26">(Sun et al. 2018b;</ref><ref type="bibr" target="#b7">Fu et al. 2019)</ref>, we first perform average and max pooling with all part-level features. We denote by p avg and p max resulting feature maps obtained by average and max pooling, respectively. Note that p avg and p max are robust to background clutter, as we use a GMP method to obtain the initial part-level features <ref type="figure" target="#fig_0">(Fig. 1)</ref>. That is, we aggregate the features from the most discriminative parts for every horizontal regions. In particular, p max corresponds to the result of GMP with respect to the initial feature map from the backbone network. We then compute a contrastive feature p cont by subtracting p max from p avg , namely, the discrepancy between them. It aggregates most discriminative information from in-dividual body parts (e.g., green boxes in <ref type="figure">Fig. 3(d)</ref>) except the one for p max (e.g., the red box in <ref type="figure">Fig. 3(d)</ref>). We add bottleneck layers to reduce the number of channels of p cont and p max from C to c, denoted byp cont andp max , respectively, and finally transfer the complementary information of the contrastive featurep cont top max <ref type="figure">(Fig. 3(d)</ref>). Formally, we obtain a global contrastive feature q 0 of the input image as follows:</p><formula xml:id="formula_1">C Local relational A q 1 Conv. C ? c Conv. C ? c Conv. 2c ? c C A M C Conv. C ? c Conv. C ? c Conv. 2c ? c Global contrastive p 1 feature feature c c Part-level features p max p cont q 0 r 1 ? p cont ? p 2 p 3 p 4 p 5 p 6 r 1 p 1 ? p 1 p 2 p 3 p 4 p 5 p 6 p max ? (a) One-vs.-rest relation module 22 C M C Conv. C ? c Conv. 2c ? c c Part-level features p max p cont p cont ? p max ? Conv. C ? c Global contrastive feature q 0 p 1 p 2 p 3 p 4 p 5 p 6 p avg A (b) GCP</formula><formula xml:id="formula_2">q 0 =p max + R g (T (p max ,p cont )),<label>(2)</label></formula><p>where R g is a sub-network that consists of a 1 ? 1 convolutional, batch normalization <ref type="bibr" target="#b11">(Ioffe and Szegedy 2015)</ref>, and ReLU (Krizhevsky, Sutskever, and Hinton 2012) layers. The global feature q 0 is based onp max , and aggregates the complementary information from the contrastive featurep cont with reference top max . It thus inherits the advantages of GMP such as the robustness to background clutter while covering the whole body parts. We concatenate the global contrastive feature q 0 in (2) and local relational ones q i (i = 1, . . . , 6) in (1), and use it as a person representation for reID.</p><p>Training loss. We exploit ground-truth identification labels of person images to learn the person representation. To train our model, we use cross-entropy and triplet losses, balanced by the parameter ? as follows:</p><formula xml:id="formula_3">L = L triplet + ?L ce ,<label>(3)</label></formula><p>where we denote by L triplet and L ce triplet and crossentropy losses, respectively. The cross-entropy loss is de-  <ref type="bibr" target="#b21">(Ristani et al. 2016</ref>) datasets. We denote suffixes "-S" and "-F" by our models using q P6 and T (q P2 , q P4 , q P6 ), respectively.</p><p>fines as</p><formula xml:id="formula_4">L ce = ? N n=1 i y n log? n i ,<label>(4)</label></formula><p>where we denote by N and y n the number of images in minibatch and a ground-truth identification label, respectively. y n i is a predicted identification label for each feature q i in the person representation, defined a?</p><formula xml:id="formula_5">y n i = argmax c?K exp((w c i ) T q i ) K k=1 exp((w k i ) T q i ) .<label>(5)</label></formula><p>K is the number of identification labels, and w k i is classifier for the feature q i and the label k. We use a fully connected layer for the classifier. To enhance the ranking performance, we use the batch-hard triplet loss <ref type="bibr" target="#b10">(Hermans, Beyer, and Leibe 2017)</ref>, formulated as follows:</p><formula xml:id="formula_6">L triplet = N K k=1 N M m=1 [? + max n=1...M q A k,m ? q P k,n 2 ? min l=1...K n=1...N l =k q A k,m ? q N l,n 2 ] + ,<label>(6)</label></formula><p>where N K is the number of identities in mini-batch, and N M is the number of images for each identification label in minibatch (N = N K N M ). ? is a margin parameter to control the distances between positive and negative pairs in feature space. we denote by q A i,j, , q P i,j, , q N i,j, person representations of anchor, positive, and negative images, respectively, where i, j correspond to identification and image indexes.</p><p>Extension to different numbers of grids. We so far describe our model using global and local features, i.e., T (q 0 . . . q 6 ), for a person representation, which is denoted by q P6 hereafter. Without loss of generality, we can use different numbers of horizontal grids for the person representation, to consider various parts of multiple scales <ref type="bibr" target="#b7">(Fu et al. 2019)</ref>, such as q P2 and q P4 that splits the initial feature map into two and four horizontal regions, respectively. Accordingly, we concatenate the features of q P2 , q P4 and q P6 , i.e., T (q P2 , q P4 , q P6 ), and use it as a final person representation for an effective reID. Note that q P2 , q P4 , and q P6 contain different local relational features, and thus have different global contrastive features. Note also that these features share the same backbone network with the same parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>Dataset. We test our method on the following datasets and compare its performance with the state of the art. 1) The Market1501 dataset <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref> contains 32,668 person images of 1,501 identities captured by six cameras. We use the training/test split provided by <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref> where it consists of 12,936 images of 751 identities for training and 3,368 query and 19,732 gallery images of 750 identities for testing. 2) The CUHK03 dataset <ref type="bibr" target="#b13">(Li et al. 2014</ref>) provides 14,097 images of 1,467 identities observed by two cameras, and it offers two types of person images: Manually labeled and detected ones by the DPM method <ref type="bibr">(Felzenszwalb et al. 2008)</ref>. Following the training/test split of <ref type="bibr" target="#b38">(Zhong et al. 2017a</ref>), we divide it into 7,365 images of 767 identities for training, and 1,400 query and 5,332 gallery images of 700 identities. 3) The DukeMTMC-reID <ref type="bibr" target="#b21">(Ristani et al. 2016</ref>) offers 16,522 training images of 702 identities, 2,228 query and 17,661 gallery images of 702 identities.</p><p>Training. We resize all images into 384 ? 128 for training. We set the numbers of feature channels C to 2,048 and c to 256. This results in 1,792-and 3,840-dimensional features for q P6 and T (q P2 , q P4 , q P6 ), respectively. We augment the training datasets with horizontal flipping and random erasing <ref type="bibr" target="#b38">(Zhong et al. 2017b)</ref>. We use the stochastic gradient descent (SGD) as the optimizer with momentum of 0.9 and weight decay of 5e-4. We train our model with a batch size N of 64 for 80 epochs, where we randomly choose 16 identities and sample 4 person images for each identity (N K = 16, N M = 4). A learning rate initially set to 1e-3 and 1e-2 for the backbone network and other parts, respectively, until 40 epochs is divided by 10 every 20 epochs. We empirically set the weight parameter ? to 2, and fix it to all experiments. All networks are trained end-to-end using PyTorch <ref type="bibr" target="#b20">(Paszke et al. 2017</ref>). Training our model takes about six, three and eight hours with two NVIDIA Titan Xps for the Market1501, CUHK03, and DukeMTMC-reID datasets, respectively.</p><p>Comparison with the state of the art Quantitative results. We compare in <ref type="table">Table our</ref> models with the state of the arts including part-based person reID methods. We measure mean average precision (mAP) (%) and rank-1 accuracy (%) on the Market1501 <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>, CUHK03 <ref type="bibr" target="#b13">(Li et al. 2014)</ref>, and DukeMTMC-reID <ref type="bibr" target="#b21">(Ristani et al. 2016)</ref> datasets. We report reID results for a single query for a fair comparison. We denote suffixes "-S" and "-F" by our models using q P6 and T (q P2 , q P4 , q P6 ), respectively, for final person representations. <ref type="table">Table shows</ref> that Ours-S outperforms state-of-the-art reID methods in terms of mAP and rank-1 accuracy for all datasets, except MGN ). This demonstrates the effectiveness of our one-vs.-rest relation module and GCP. Moreover, Ours-S uses 1,792-dimensional features, allowing to an efficient person retrieval, while providing state-of-the-art re-sults. Ours-F gives the best results again on all datasets in terms of mAP. We achieve mAP of 88.9% and rank-1 accuracy of 95.2% for the Market1501, mAP of 69.6%/75.6% and rank-1 accuracy of 74.4%/77.9% with detected/labeled images for the CUHK03, and mAP of 78.6% and rank-1 accuracy of 89.7% for the DukeMTMC-reID. The rank-1 accuracy of Ours-F is slightly lower than that of MGN ) on the Market1501, but Ours-F outperforms MGN on other datasets by a significant margin. Note that person images in the CUHK03 and DukeMTMC-reID datasets are much more difficult to retrieve, as they are typically with large pose variations, background clutter, occlusion, and confusing attributes.</p><p>Qualitative results. <ref type="figure" target="#fig_2">Figure 4</ref> shows a visual comparison of person retrieval results with the state of the art <ref type="bibr" target="#b19">(Luo et al. 2019;</ref><ref type="bibr" target="#b0">Bai et al. 2017;</ref><ref type="bibr" target="#b26">Sun et al. 2018b</ref>) on the Market1501 dataset <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>. We show the top-5 retrieval results for the query image. The results for all comparisons have been obtained from the official models provided by the authors. We can see that our method retrieves correct person images, and in particular it is robust to attribute variations (e.g., bicycles) and background clutter (e.g., grass).</p><p>Other part-based methods including AlignedReID and PCB try to match local features between images for retrieval. For example, they focus on finding the correspondences for bicycles or grass in the query image, giving many falsepositive results. Note that the gallery set contain many images of persons riding a bicycle, but they have different identities from the person in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Ablation study. We show an ablation analysis on different components of our model in <ref type="table">Table .</ref> We compare the performance for several variants of our model in terms of mAP and rank-1 accuracy. From the first three rows, we can see that using both global and local features improves the retrieval performance, which confirms the finding in part-based reID methods. The fourth row shows that GMP gives better results than GAP, as GMP avoids background clutter. The results in the next row demonstrates the effect of local features obtained using our relation module. For example, this gives the performance gains of 3% and 1% for mAP and rank-1 accuracy, respectively, for the Market1501 dataset, which is quite significant. From the fifth and eighth rows, we show a comparison of the retrieval performance according to pooling methods for the global feature, and we can see that GCP performs better than GMP, GAP and GMP+GAP in terms of mAP and rank-1 accuracy. For example, compared with GMP+GAP, our GCP improves mAP from 86.6% to 88.0% on the Market1501 dataset. The last four rows suggest that exploiting part-level features of multiple scales is important, and using all components performs best. We show in <ref type="figure" target="#fig_3">Fig. 5(a)</ref> retrieval results with/with a relational module. We can see that person representations obtained from the relation module successfully discriminate the same attribute (e.g., violet shirts) for the person images of different identities (e.g., gender), and they are robust to   occlusion (e.g., the person occluded by a bag or a bicycle). We compare in <ref type="figure" target="#fig_3">Fig. 5(b)</ref> retrieval results for different pooling methods. We confirm once again that GAP is not robust to background clutter and GMP sees the most discriminative region (e.g., bicycle) only rather than the person. We observe that GCP alleviates these problems while maintaining the advantage of GMP.</p><p>One-vs.-rest relation. We consider relations between each part-level featurep i and its rest featurer i . To show how the relation module works, we train a model using the combined rest featurer i instead of T (p i ,r i ) in (1). In this case, we do not use a multi-scale extension. We visualize activation maps of R p (r i ) and R p (T (p i ,r i )) in <ref type="figure">Fig. 6</ref>. We observe that R p (T (p i ,r i )) focuses more on the regions whose attributes are different from those ofp i , compared to R p (r i ).  <ref type="figure">Figure 6</ref>: Examples of activation maps for the models using R p (r 3 ) (top) and R p (T (p 3 ,r 3 )) (bottom). We also show top-3 retrieval results for each model. We can see that the rest regions (e.g., the fifth and sixth horizontal grids) are highly activated by the person representation using R p (T (p 3 ,r 3 )), compared to that using R p (r 3 ), indicating that our one-vs.-rest relation allows each part-level feature to see the rest regions effectively.</p><p>This demonstrates that R p (T (p i ,r i )) extracts the complementary features ofp i , that are helpful for person reID but not contained inp i , from the rest of the parts. This also verifies that using the feature ofp i only is not enough to discriminate the identities of different persons having similar attributes in corresponding parts between images. The mAP/rank-1 forr i on Market1501 are 84.5/93.4 which are lower than 86.7/94.2 obtained using T (p i ,r i ) in <ref type="table">Table .</ref> Performance comparison of local features. We demonstrate the capabilities of providing discriminative features of   <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>. RM: One-vs.rest relational module.</p><p>the one-vs.-rest relation module. We extract a local feature of size 1 ? 1 ? 256 for each horizontal region with and without using the relation module. Given a query image, we then retrieve person images using a single local feature in the person representation of q P6 . We report mAP and rank-1 accuracy for individual local features extracted from different horizontal regions (1: top, 6: bottom), on the Market1501 dataset <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref> in <ref type="table">Table .</ref> From this table, we can observe two things: (1) The relation module improves mAP and rank-1 accuracy drastically. The improvements in mAP and rank-1 accuracy are 21.7% and 19.6%, respectively, on average. The rank-1 accuracy measures the performance of retrieval results for the easiest match, while mAP characterizes the ability to retrieve all person images of the same identity, indicating that the relation module is beneficial especially for retrieving challenging person images.</p><p>(2) The local features from the third and last horizontal regions give the best and the worst results, respectively. This suggests that the middle part typically corresponding to the torso of a person provides the most discriminative feature for specifying a particular person, and the bottom part (e.g., legs or sometimes background due to the incorrect localization of the person detector) gives the least discriminative feature for person reID.</p><p>Performance comparison of global features. <ref type="table">Table  compares</ref> the reID performance of single global features in terms of mAP and rank-1 accuracy on the Mar-ket1501 dataset <ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>. We use only the 256-dimensional global feature in the person representation of q P6 for person retrieval but obtained by different pooling methods. Note that the size of the global feature is much smaller than typical person representations in <ref type="table">Table .</ref> We can see from Table that GCP gives the best retrieval results in terms of both mAP and rank-1 accuracy, outperforming GAP, GMP, and GAP+GMP by a large margin. Compared with other reID methods in <ref type="table">Table ,</ref> GCP offers a good compromise in terms of the accuracy and the size of features. For example, our global contrastive feature of size 1 ? 1 ? 256 achieves rank-1 accuracy of 93.4%, which is comparable with 93.1% for PCB+RPP <ref type="bibr" target="#b26">(Sun et al. 2018b</ref>) using 1,536dimensional features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have presented a relation network for person reID considering the relations between individual body parts and the rest of them, making each part-level feature more discriminative. We have also proposed to use contrastive features for a global person representation. We set a new state of the art on person reID, outperforming other reID methods by a significant margin. The ablation analysis clearly demonstrates the effectiveness of each component in our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our framework. The proposed reID model mainly consists of three parts: We first extract part-level features by applying GMP to individual horizontal slices of the feature map from the backbone network. We then input the local features into separate modules, a one-vs.-rest relation module and GCP, that give local relational and global contrastive features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Illustration of (a) a one-vs.rest relational module and (b) GCP. The relation module gives a local relational feature q i for each p i . Here, we show a process of extracting the feature q 1 . Other relational local features are similarly computed. The GCP outputs a global contrastive feature q 0 considering all part-level features simultaneously. We do not share weight parameters of convolutional layers for all part-level features. See text for details. Illustration of various pooling methods: (a) GAP; (b) GMP; (c) GAP + GMP; (d) GCP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparison of person reID results on the Market1501 dataset<ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>. We show the top-5 retrieval results (left: rank-1, right: rank-5) for Aligne-dReID<ref type="bibr" target="#b19">(Luo et al. 2019</ref>), Deep-Person<ref type="bibr" target="#b0">(Bai et al. 2017)</ref>, PCB<ref type="bibr" target="#b26">(Sun et al. 2018b</ref>) and Ours-F. Retrieved images with green and red boxes are correct and incorrect results, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparison of retrieval results: (a) a relational module and (b) pooling methods. We show top-1 results. The relation module discriminates the same attribute for the person images of different identities. GCP allows to aggregate features from discriminative regions, and provides a person representation robust to background clutter, overcoming the drawbacks of GAP and GMP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Quantitative comparison of different network architectures. We measure mAP(%) and rank-1 accuracy(%) on the</cell></row><row><cell>Market1501 (Zheng et al. 2015), CUHK03 (Li et al. 2014) and DukeMTMC-reID (Ristani et al. 2016) datasets. Numbers</cell></row><row><cell>in bold indicate the best performance and underscored ones are the second best. GF: Global features; LF: Local features;</cell></row><row><cell>RM: One-vs.-rest relational module; Ext.: An extension to multiple scales.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of single local features on the Market1501 dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison of global features on the Market1501 dataset<ref type="bibr" target="#b35">(Zheng et al. 2015)</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research was supported by R&amp;D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation of KOREA (NRF) funded by Ministry of Science and ICT (NRF-2018M3E3A1057289).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10658</idno>
		<title level="m">Deep-Person: Learning discriminative deep features for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object level visual reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Horizontal pyramid matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FD-GAN: Pose-guided feature distilling gan for robust person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ima-geNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consistentaware deep learning for person re-identification in a camera network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving person re-identification by attribute and identity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07220</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HydraPlus-Net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">AlignedReID++: Dynamically matching local information for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>De-Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose-driven deep convolutional model for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SVDNet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IEEE TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning a discriminative null space for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spindle Net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pose invariant embedding for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE TIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A discriminatively learned CNN embedding for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">; L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. Zheng, Z.; Zheng</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ACM TOMM</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">; L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
	</analytic>
	<monogr>
		<title level="m">Random erasing data augmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR. Zhong, Z.; Zheng,</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
