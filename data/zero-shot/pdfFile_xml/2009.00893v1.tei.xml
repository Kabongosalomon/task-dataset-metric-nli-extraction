<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaotian</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
								<orgName type="laboratory">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
								<orgName type="laboratory">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
							<email>jianqiang.hjq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
								<orgName type="laboratory">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University Embedded System Engineering Research Center</orgName>
								<address>
									<country>Ministry of Education of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Zhejiang Provincial Key Laboratory for Network Multimedia Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>huaxiansheng@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
								<orgName type="laboratory">DAMO Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaotian</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxin</forename><surname>Jiang</surname></persName>
						</author>
						<title level="a" type="main">PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation</title>
					</analytic>
					<monogr>
						<title level="m">28th ACM International Conference on Multimedia (MM 2020)</title>
						<meeting> <address><addrLine>Seattle, WA, USA; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394171.3413722</idno>
					<note>ACM Reference Format: Yaowu Chen, Xian-Sheng Hua. 2020. PCPL: Predicate-Correlation Percep-tion Learning for Unbiased Scene Graph Generation. In Proceedings of the ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00. ACM, New York, NY, USA, 9 pages.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Scene understanding KEYWORDS Scene Graph Generation</term>
					<term>Long-tailed Bias</term>
					<term>Correlation Perception</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today, scene graph generation (SGG) task is largely limited in realistic scenarios, mainly due to the extremely long-tailed bias of predicate annotation distribution. Thus, tackling the class imbalance trouble of SGG is critical and challenging. In this paper, we first discover that when predicate labels have strong correlation with each other, prevalent re-balancing strategies (e.g., re-sampling and re-weighting) will give rise to either over-fitting the tail data (e.g., bench sitting on sidewalk rather than on), or still suffering the adverse effect from the original uneven distribution (e.g., aggregating varied parked on/standing on/sitting on into on). We argue the principal reason is that re-balancing strategies are sensitive to the frequencies of predicates yet blind to their relatedness, which may play a more important role to promote the learning of predicate features. Therefore, we propose a novel Predicate-Correlation Perception Learning (PCPL for short) scheme to adaptively seek out appropriate loss weights by directly perceiving and utilizing the correlation among predicate classes. Moreover, our PCPL framework is further equipped with a graph encoder module to better extract context features. Extensive experiments on the benchmark VG150 dataset show that the proposed PCPL performs markedly better on tail classes while well-preserving the performance on head ones, which significantly outperforms previous state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: A comparison of re-balancing(e.g., re-weighting) methods and our PCPL. The training weight of re-balancing is set to nth power of the inverse of class sample frequencies where n is the value of re-balancing parameter. (a) An input image with bounding boxes and object labels. (b) The blue curve depicts the mean recall@100 of re-balancing for "on" and "sitting on" under different settings of parameter while the orange curve indicates the performance of our PCPL. It should be noted that there is no intersection between the PCPL and re-balancing training process. (c) SGG with re-balancing parameter = 0. (d) SGG from the proposed PCPL. (e) SGG with re-balancing parameter = 1. Red boxes in (c) and (e) denote wrong predication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Scene graph generation (SGG) <ref type="bibr" target="#b12">[13]</ref>, which is a visual task to detect objects and recognize semantic relationships between different objects in an image, can serve as a powerful structural representation of images and benefit other high-level Vision-and-Language tasks such as image generation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, image retrieval <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>, visual question answering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref> and image captioning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>. Taking advantage of the remarkable feature representations of convolutional neural networks (CNNs) <ref type="bibr" target="#b15">[16]</ref> and diverse contextual feature fusion strategies (e.g., message passing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>, lstm <ref type="bibr" target="#b40">[41]</ref>), a variety of methods have made significant progress to improve the recall evaluation metric performance of SGG tasks. Some other works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref> further utilize the co-occurring language regularity of typical subject-predicate-object relationship triplets as prior information to enhance overall performance. However, in practice the SGG benchmark datasets such as Visual Genome 150 (VG150) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37]</ref> always have extremely long-tailed predicate label distributions (i.e., imbalanced annotation bias in training data, dominating by a few classes which occupy most of the data). Although achieving encouraging performance on head classes (e.g., on/has), these previous efforts are not feasible to obtain outstanding accuracy on predicting fewer but more meaningful predicate samples(e.g., sitting on/riding/looking at/eating/parked on), making them largely limited for supporting high-level tasks in real-world scenarios.</p><p>Prevalent class re-balancing strategies are introduced into SGG recently, examined by Tang et al. <ref type="bibr" target="#b30">[31]</ref> at training stage in order to tackle the challenging long-tailed training data bias problems. In general, the prominent class re-balancing methods are roughly summarized as two types, which are adjusting the sample proportion within a mini-batch (i.e., re-sampling) or assigning relatively higher costs to tail samples (i.e., re-weighting). These two categories share the same connotation of manually tuning sampling frequencies or classifier weights based on the numbers of different class samples during training process to simulate the test distributions. These effective strategies indeed promote the overall mean recall evaluation metric for SGG benchmark datasets, however, when going deeper to examine specific predicate cases, we unexpectedly find that the performance is not satisfactory under the circumstances that predicate labels have strong correlation with each other. <ref type="figure">Fig. 1</ref> comprehensively illustrates our observation. Taking a semantically closely related predicate pairon (i.e., head class, occupying a large proportion of annotations) and sitting on (i.e., tail class, having rarely few samples) -as example, it can be seen from <ref type="figure">Fig. 1</ref> that, class re-balancing strategies, which merely rely on the manual tuned classifier weights based on the numbers of samples, give rise to either over-fitting the tail data when re-balancing parameter is relative high (bench sitting on sidewalk rather than on, shown in <ref type="figure">Fig. 1(e)</ref>) or still suffering the side effect from the original uneven distribution when re-balancing parameter is relative low (aggregating man sitting on bench into man on bench). As shown in <ref type="figure">Fig. 1(b)</ref>, the optimal point that maximizes the recall of both classes is hardly reachable by manually tuning the re-balancing parameter. We argue the principal reason is that re-balancing strategies merely utilize the frequencies of classes yet neglect their semantic relatedness, which may play a more important role to catalyze the learning of predicate features. To the contrary of other classification tasks, SGG essentially involves complex semantic correlations among different ground truth predicate annotations, which are insensitive to the class frequencies.</p><p>Consequently, we naturally put forward an assumption that the performance of predicates having strong correlations with multi classes will benefit from the learning of correlated ones, as a consequence of which, smaller loss weights are acceptable, otherwise those predicates tend to dominate other classes by severely degrading their recall, and vice versa. In view of that, we propose a novel Predicate-Correlation Perception Learning (PCPL for short) scheme aiming to tackle the class imbalance trouble of SGG, having the benefit of adaptively seeking out optimal loss weights by directly perceiving and explicitly utilizing the implicit correlations among predicates. Equipped with PCPL, the model is able to markedly improve the predicting results on tail classes and well preserve the performance on head predicates simultaneously, thus the optimum mean recall can be obtained as illustrated in <ref type="figure">Fig. 1(b)</ref>. Specifically, we construct an iteratively updated class graph to perceive the correlations between predicates and the loss weights of classes are appropriately inversed with their relatedness derived from the graph.</p><p>Morever, we propose a graph encoding module (GE for short) to encode global context through a series of stacked encoders in a graph manner. A variety of methods have been adopted by previous works to fuse global context into relationship representations. Dual graph message passing <ref type="bibr" target="#b36">[37]</ref> is relatively out-of-date while BiLSTM <ref type="bibr" target="#b8">[9]</ref> employed by Neural Motifs <ref type="bibr" target="#b40">[41]</ref> achieves better results yet suffers a drawback that different input orders will bring different results. Without introducing additional information, the performance of GGNN <ref type="bibr" target="#b2">[3]</ref> is not satisfying. Compared with previous methods, our graph encoding module can better capture the relationships between object classes and benefit from being permutation invariant, thus can obtain more robust contextual features, setting a higher baseline.</p><p>In summary, the contributions of this paper are threefold:</p><p>? We propose a novel Predicate-Correlation Perception Learning (PCPL for short) scheme that is able to alleviate the long-tailed bias of SGG by directly perceiving and explicitly utilizing the implicit correlations between predicate classes, opening up new ideas to tackle the imbalance issue of SGG or other tasks involving correlated classes. ? PCPL can significantly promote the predicting results of tail classes while well preserving the performance of head predicates, by adaptively assigning optimal loss weights, which are appropriately inversed with the degrees of relatedness, to different predicates. ? Extensive experiments show the effectiveness of PCPL and demonstrate that PCPL achieves a new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Scene Graph Methods</head><p>Reasoning about relationships is the major challenge for generating scene graph. There are mainly two approaches in previous works making efforts to improve the performance of relationship prediction. The first approach is to make better use of visual features. Xu et al. <ref type="bibr" target="#b36">[37]</ref> finds that relationship prediction can be greatly improved by jointly reasoning with contextual information. Message Passing model proposed by Xu iteratively refines its prediction by passing contextual messages along the topological structure of a scene graph. Zellers et al. <ref type="bibr" target="#b40">[41]</ref> emphasizes the importance of context by introducing BiLSTM to encode global context that can directly inform the local predictors. Second approach is to involve additional information such as semantic labels and statistical correlations to help prediction. Zellers et al. <ref type="bibr" target="#b40">[41]</ref> embeds GloVe word vectors, statistical correlations of object pairs and relationships to visual features to obtain better results. Chen et al. <ref type="bibr" target="#b2">[3]</ref> makes further use of statistical correlations. They facilitate scene graph generation by explicitly unifying the statistical knowledge with the architecture of graph neural network. Chen et al. <ref type="bibr" target="#b2">[3]</ref>, Tang et al. <ref type="bibr" target="#b31">[32]</ref> both take a notice on the class imbalanced issue of SGG by proposing the mean recall@K metric but their works are still confined to better feature extracting. In recent works, the criticalness of long-tailed bias of SGG is addressed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>. Tang et al. <ref type="bibr" target="#b30">[31]</ref> employ causal inference in the prediction stage in an effort to remove the training bias while Wen et al. <ref type="bibr" target="#b35">[36]</ref> make use of a pseudo-siamese network to pursue extracting balanced visual features. In contrast, our method perceives and utilizes the implicit correlations among predicate classes based on an innovative observation, aiming to generate unbiased scene graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Class Imbalance</head><p>Real-world large-scale datasets often have long-tailed data distributions. Neural networks trained on these datasets tend to perform poorly on less presented classes. It has become a critical issue for model training. A lot of works has been done to resolve the class imbalance problem. Existing methods can be categorized as re-sampling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42]</ref> and re-weighting <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. Resampling methods are simple yet effective. They often over-sample (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>) less presented classes or under-sample (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>) frequent classes to make the data distribution more balanced. However, they have their downsides. Under-sampling frequent classes will discard a large amount of data, causing waste of data. And it is not practicable when the dataset is extremely imbalanced. Over-sampling less presented classes can lead to over-fitting of the repeatedly sampled classes. Re-weighting methods assign different weights for different classes to balance the loss. The simplest way of re-weighting is to set weights of classes as the inverse of their frequency <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>, but this causes poor performance on frequent classes. Cui et al. <ref type="bibr" target="#b3">[4]</ref> proposes the definition of effective number of samples and reweights the loss by the inverse of effective number to address this issue. Another widely used re-weighting method is focal loss proposed by Lin et al. <ref type="bibr" target="#b21">[22]</ref>. Focal loss down-weights the loss assigned to well-classified examples and focuses training on a sparse set of hard examples.</p><p>While most of traditional re-balancing methods merely rely on sample frequencies to manually tune the loss weights or sample ratios of different classes, our proposed method is able to adaptively assign optimal training costs to classes based on their relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS 3.1 Problem definition</head><p>Scene graph <ref type="bibr" target="#b12">[13]</ref>, representing a visual scene's detailed semantics, is generated with:</p><formula xml:id="formula_0">? a set of bounding boxes B = {b 1 , b 2 , fi, b n }, referring to the spatial locations of detected regions, ? a set of labels O = {o 1 , o 2 , fi, o n }, containing object label o i of the corresponding bounding box b i , ? and R = {r 1?&gt;2 , r 1?&gt;3 , fi, r n?&gt;n?1 }, denoting the relation- ships of object pairs. A triplet of a start object (o i , b i ), an end object (o j , b j )</formula><p>and a predicate label p i?&gt;j connecting the former to the latter make up</p><formula xml:id="formula_1">r i?&gt;j ? R.</formula><p>As shown in <ref type="figure" target="#fig_1">Fig. 3(d)</ref>, we conduct a conventional two-stage pipeline which detects the locations and labels of objects first and then outputs the relationship representations. Given an input image containing two strong correlated predicates with great disparity in sample frequencies (i.e.,parked on and on), the ubiquitous cross entropy loss, employed by most of SGG methods to optimize the framework, causes aggregating parked on into on, as can be seen in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. At the other extreme, <ref type="figure" target="#fig_1">Fig. 3</ref>(b) illustrates that a typical re-balancing strategy, which assign the fixed inverse of frequencies to the sample weights of predicate classes, surprisingly give rise to over-fitting parked on. Both strategies fail to achieve satisfactory performance when there exists strong correlation between predicate classes with long-tailed distribution. In view of that, we propose a novel PCPL scheme aiming to tackle the class imbalance trouble of SGG by directly perceiving and explicitly making use of the implicit correlations among predicates. <ref type="figure" target="#fig_1">Fig. 3</ref>(c) presents an overview of PCPL. We construct an iteratively updated class graph to represent the correlations between predicates. By utilizing the relatedness derived from the graph, PCPL has an advantage of adaptively seeking out optimal loss weights instead of manually tuning. Equipped with PCPL, the model is able to markedly improve the predicting results on tail classes and well preserve the performance on head predicates simultaneously. In subsequent sections, we will start with an innovative observation of re-balancing on SGG and then describe our method in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An observation of re-balancing</head><p>As mentioned above, we find that re-balancing methods are of no avail when predicates are closely correlated to each other. In an effort to further explore the origins of this phenomenon, this section will discuss the influence of class correlations on the effectiveness of re-balancing for SGG before diving into our method. Considering that it is hard to judge the degree of correlations by human intuition, we utilize the distance between class centers in feature space as a quantitatively measurement for the relatednesses, as the feature clusters of two correlated classes tend to be closer than two independent classes. Concretely, we employ a learnable variable v k , jointly trained with the SGG model, to represent the center of predicate class k in the feature space. The dimension of v k is the same as the output relationship feature before the last fully connected layer of SGG model. For output features { f 1 , f 2 ...f N } and predicate labels {l 1 , l 2 ...l N }, the loss to update v is defined as:</p><formula xml:id="formula_2">L cent er = 1 N N i=1 (f i ? v l i ) 2<label>(1)</label></formula><p>where N is the count of ground truth predicate annotations in the mini-batch and v l i is the corresponding center variable for output feature f i . Notably, the gradient of v will not pass to feature f in backward propagation in order not to mess up the training procedure of SGG model. Directly following the training process, we acquire the correlations between predicate classes:</p><formula xml:id="formula_3">e k j = ||v j ? v k || 2<label>(2)</label></formula><p>Classes with larger e are more independent while smaller e means stronger correlation. e between class k and it self equals 0.</p><p>To provide a more intuitive illustration, comparative experiments are designed for two observation predicate groups with opposed level of correlations. Specifically, The first group consists of two predicates, a primary class, occupying a large proportion of annotations, and a strong correlation class, with far fewer samples but closely correlated with the primary one. The same primary class and a weak correlation class, having the same sample frequency with the strong one yet relatively independent, make up the other group. To get rid of the influence of other classes, we remove irrelevant annotations from the dataset for each group separately, thus to make the results clearer and more concise. Here we regard "has", "with" and "looking at" as the primary predicate, strong correlation predicate and weak correlation predicate respectively. Obtained from Eq. 2, e between has and with is 4.57 while that between has and looking at is 20.96, which means that the relatedness between has and with is strong and that between has and looking at is weak. For a fair comparison, we randomly down sample the frequency of with to the same scale of looking at.</p><p>Examined with a same baseline model on each group, the constrained and unconstrained R@100 improvements of re-weighting over cross-entropy are revealed in <ref type="figure" target="#fig_0">Fig. 2</ref>. The results demonstrate that both the constrained and unconstrained R@100 of the weak correlation predicate increase notably with almost no impact on the primary predicate. In the other group, though the constrained R@100 of the strong correlation predicate occurs a minor rise, that of the primary predicate happens a relatively significant decrease, while there is no obvious change on the unconstrained R@100 of both predicates. The contrast results of the two groups indicate that re-balancing, to some extent, is able to alleviate the class imbalance trouble when classes are independent. However, when it comes to classes closely correlated with each other, these strategies, sensitive to class frequency but blind to the correlations between classes, result in over-fitting to tail classes. Scene graph generation task, predicating relationship between instances, involves critically complex correlations between predicates, which fully exposes the shortcoming of re-balancing. In stark contrast, our proposed PCPL scheme can achieve a satisfying performance on both head and tail classes by adaptively assigning optimal training costs, which are appropriately inversed with the degrees of relatedness, to predicates, opening up new ideas to tackle the imbalance issue of SGG or other tasks involving correlated classes. Although predicates having strong correlations with multi classes are assigned with relatively smaller loss weights, their performance will benefit from the learning process of correlated ones, while other predicates gain improvements on account of higher training costs. The detailed process of PCPL will be described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Process</head><p>The class correlations are dynamically changing along with the optimization of the feature extracting network. For this reason, as is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we construct a learnable class graph to dynamically perceive the relatedness between predicates. As the network gradually converges, the graph we built is also achieving a relatively stable state. In this way, we are able to guide the learning of the model throughout the whole training process. The graph consists of a set of nodes and edges connecting every pair of them. Each node represents the center of one predicate class while the edges connecting nodes represent their degrees of correlation. Given output features, we first update the corresponding v i using Eq. 1 and update the edges with Eq. 2 afterwards, as presented in <ref type="figure" target="#fig_2">Fig. 4(b,c,d)</ref>. Then the global correlations u i of predicate class i is defined as:</p><formula xml:id="formula_4">u i = N j=1 e ? i j<label>(3)</label></formula><p>where N is the number of predicate classes in the dataset and e ? i j is the updated value of edge connecting node i and node j. Following this, we perform a normalization for u i to obtain the correlation factor ? :</p><formula xml:id="formula_5">? i = u i ? min(u) + ? max(u) ? min(u)<label>(4)</label></formula><p>where ? denotes a minimal value to prevent ? i from being zero. The correlation factor ? i can be seen as a measure for the independence degree of class i. After that, ? i is assigned to the classification loss weight of the SGG network, thus to correct the learning process and alleviate the training bias:</p><formula xml:id="formula_6">p ? l i = e p l i N j=1 e p j ,<label>(5)</label></formula><formula xml:id="formula_7">L = ? N i=1 ? l i N r k=1 (? l k ) * log p ? l i<label>(6)</label></formula><p>where N r is the count of ground truth predicate classes present at the current mini-batch, p is the probability of each predicate output by the model and l i is the ground truth label of feature i. Moreover, the dynamic graph makes it possible to alleviate the influence of noisy labels. Models are easy to be distracted by noisy labels because their losses are usually higher than normal samples. With the graph, we are able to distinguish and abandon noisy labels, thus to make the learning process more stable to some extent.</p><p>Given an output feature f i with ground-truth label i, we distinguish whether it is noisy or not by D dr op :</p><formula xml:id="formula_8">D dr op j = || f i ? v i || 2 ? || f i ? v j || 2 ? e i j ?<label>(7)</label></formula><p>where ? is a hyper-parameter and D dr op j means D dr op with class j.</p><p>Here we set ? as 2. If any of D dr op j is great than zero, we consider f i as noisy and abandon the corresponding sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Context Encoding</head><p>Given an image I , bounding boxes are first detected using Faster R-CNN as described in <ref type="figure" target="#fig_1">Fig. 3(d)</ref>. Besides, for each b i in the proposal region set B, it also outputs a corresponding feature vector and a possible label l i , which are of non-context, causing relatively low performance in object and predicate classification. Thus, as shown in <ref type="figure">Fig. 5</ref>, we design a graph encoding module to obtain contextualized representations. Taking the pooled feature vectors as a set of nodes, we use an input network implemented by fully connected layers to expand bounding box coordinates to the same dimension of node features. Following that, we perform an element-wise sum to acquire new representations of nodes, containing spatial information which is also crucial when inferring relationships. Afterwards, we can construct a fully connected undirected graph G by connecting all the nodes together. The edges between nodes represent to what extent nodes can interact with its neighbors. Then we iteratively process the graph with stacked encoders. As <ref type="figure">Fig. 5</ref> illustrates, each encoder consists of a self-attention layer and a Feed Forward network (FF). Every encoder calculates the attention coefficients between nodes and obtains the hidden state of each node by attending over its neighbors:</p><formula xml:id="formula_9">H i?1 = H i?1 + Attention(H i?1 )<label>(8)</label></formula><formula xml:id="formula_10">H i =? i?1 + FF(? i?1 )<label>(9)</label></formula><p>where H i is the hidden state of graph G output by ith encoder. In this way, messages can be propagated through the whole graph. Eventually, we obtain the final contextual representation of each region after processing several encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Implementation Details.</head><p>To keep consistent with previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, we adopt the Faster-RCNN detector <ref type="bibr" target="#b25">[26]</ref>, pretrained on ImageNet <ref type="bibr" target="#b26">[27]</ref> and refined on VG150 <ref type="bibr" target="#b36">[37]</ref>, with VGG16 <ref type="bibr" target="#b29">[30]</ref> being the backbone to generate region proposals. The numbers of stacked encoders and attention heads in graph encoding modules are set to 6 and 12 respectively. All our experiments are conducted using a NVIDIA P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Dataset.</head><p>We evaluate our methods and all the comparison models on Visual Genome <ref type="bibr" target="#b14">[15]</ref>, a large-scale dataset commonly used in vision-and-language tasks. Following prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>, we adopt the most popular preprocessed split, VG150 <ref type="bibr" target="#b36">[37]</ref>, which contains the most frequent 150 object categories and 50 predicate classes.   <ref type="figure">Figure 5</ref>: A diagram of the Graph Encoding Module (GE). We fuse object features as well as their corresponding spatial information to construct a graph and obtain contextual features by processing the graph with stacked encoders. Each encoder is permutation invariant by consisting of a selfattention layer and a Feed Forward network (FF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Metrics.</head><p>Most of previous works adopt the re-call@K (R@K for short) metric that measures the fraction of groundtruth relationship triplets(subject-predicate-object) that appear among the top K most confident predictions in an image <ref type="bibr" target="#b36">[37]</ref>. However, this metric is easily dominated by a few predicate classes accounting for absolute proportion of data due to the long-tail distribution of annotations. Thus we abandon R@K on most experiments and evaluate all the methods using the mean rcall@K (mR@K for short), proposed by Chen et al. <ref type="bibr" target="#b2">[3]</ref> and Tang et al. <ref type="bibr" target="#b31">[32]</ref>, to give a more comprehensive assessment. It is defined as the average R@K of all the predicate classes, which gives a fair performance appraisal for both head and tail classes. Notably, we report R@K in <ref type="table" target="#tab_1">Table 2</ref>, which compares different debiasing methods, to avoid over-fitting to tail classes. Both unconstrained and constrained <ref type="bibr" target="#b40">[41]</ref> mR@K are presented on all experiments, which obtained from multi and single output relationships respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Tasks.</head><p>To comprehensively evaluate the performance on different stages of SGG, we adopt the following three tasks: Predicate classification (PredCls) predicts the predicate classes of a set of given object pairs with ground truth bounding boxes and object labels. Scene graph classification (SGCls) predicts the object classes for ground truth bounding boxes and predicts the predicate labels of each object pairs. Scene graph generation (SGGen) only takes <ref type="figure">Figure 6</ref>: Performance comparison between re-weighting and our method on the VG150 dataset. The unconstrained R@100 for each predicate class on the PredCls task is presented.</p><p>the original image as input and sequentially detects the bounding boxes, object labels and then predicts the relationships between object pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared methods</head><p>In this section, we first perform a thorough comparison between our proposed method and the existing state-of-the-art methods of scene graph generation, including Iterative Message Passing (IMP+) <ref type="bibr" target="#b36">[37]</ref>, Frequency baseline (FREQ) <ref type="bibr" target="#b40">[41]</ref>, Stacked Motif Networks (SMN) <ref type="bibr" target="#b40">[41]</ref>, Knowledge-Embedded Routing Network (KERN) <ref type="bibr" target="#b2">[3]</ref>, Visual Contexts Tree (VCTree) <ref type="bibr" target="#b31">[32]</ref> and Stacked Motif Networks with TDE (SMN + TDE) <ref type="bibr" target="#b30">[31]</ref>. As shown in <ref type="table">Table.</ref> 1, we present the unconstrained and constrained mR@K on three tasks on the VG150 benchmark. KERN, which explicitly uses the statistical co-occurring prior outperforms SMN by 5.9% and 2.7% of the mean mR while VC-Tree, which mines the implicit relatedness between object pairs by learning a score matrix, gains a slight improvement over KERN. Though these methods achieve significant progress, TDE, as stateof-the-art on mR@K, is the first method focusing on the long-tailed trouble of SGG. While TDE is a prediction strategy, our proposed PCPL is a training scheme. Our method achieves higher constrained mR@K than others on all three tasks, gaining the mean mR of 22.1% and 32.9%, with a relative improvement of 31.5% and 24.2% compared with the previous state-of-the-art methods (i.e., SMN+TDE and KERN). Our model evidently outperforms others on PredCls and SGCls in the unconstrained mR@K metric, only slightly lower than KERN on the SGGen task, principally due to the statistical co-occurrence of objects KERN uses to promote the performance of object detection which is not the main concern of our discussion. Secondly, we perform a more in-depth comparison between our method and several commonly used class imbalance handling strategies as well as the previous state-of-the-art debiasing method of SGG (SMN+TDE) to further demonstrate the effectiveness of PCPL. We retrain our baseline model (GE) with focal loss <ref type="bibr" target="#b21">[22]</ref>, class balanced loss <ref type="bibr" target="#b3">[4]</ref> and weighted cross entropy loss respectively. The results of R@50/100 and mR@50/100 on three tasks are listed in <ref type="table">Table.</ref> 2. Focal loss, which assigns larger training costs to hard samples, leads to a decline of mR@50/100 from GE. Re-balancing methods, i.e. class-balanced loss and re-weighting gain significant improvements on mR@50/100 but occur huge decrease on R@50/100, indicating over-fitting to tail classes. SMN+TDE <ref type="bibr" target="#b30">[31]</ref> achieves a relatively balanced performance, promopting the mR@50/100 while preferablely keeping the performance of R50/100. Our method gains further increase on mR@50/100 from re-weighting and acquire comparable R@50/100 with SMN+TDE, though their detector is equipped with resnet <ref type="bibr" target="#b20">[21]</ref>, which is a more powerful backbone than the VGG16 we use. The comparison suggests that our proposed PCPL can supervise the model to learn a more unbiased representation of scene graph. <ref type="figure">Fig. 6</ref> presents a comparison between our method and re-weighting of the detailed recall@100 of PredCls task on each predicate class ranking by sample frequencies. PCPL performs better than reweighting on almost all the predicate classes. While evidently promoting the performance of tail classes with few training samples like walking in, mounted on and painted on, PCPL obtains recall@100 on the four head predicates, which accounting of nearly 70% of the training data, as 83%, 93%, 97.5% and 90.2%, with improvements of 29.9%, 13.8%, 8.6% and 8.1% over those of re-weighting. Re-balancing strategies blindly restrain the training of head classes and encourage tail predicates while disregarding the correlations between them, gaining unworthy improvements on tail classes at the cost of massive decrease of the results on head predicates (e.g., on). On the contrary, PCPL adaptively assigns optimal loss weights appropriately inversed with their relatedness to predicate classes during the training process. The performance of predicates with weak correlations (e.g., looking at,belonging to and playing) improves on account of higher training costs. Although predicate classes having strong correlations with multi classes (e.g., on) are assigned with relatively smaller loss weights, their learning benefits from the training process of correlated ones (e.g., parked on,standing on and walking on), thus we are able to obtain relatively unbiased results on all predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>We consider several ablations in <ref type="table" target="#tab_2">Table. 3 and Table. 4. Table.</ref> 3 reveals the different ways to obtain class center representations (i.e., AvgCenter and LearntCenter, using the average of all features of a class to represent the center in every epoch and learning a class center end-to-end,respectively) and normalize the correlation factor(i.e., SoftmaxNorm, ScalingNorm and MinMaxNorm, using softmax function, divided with the maximum value and employing Eq. 4 to obtain correlation factor ? from global correlation u, respectively). Results show that the composition we use (i.e., LearntCenter + MinMaxNorm) acquires best performance. An explanation for the low performance of SoftmaxNorm and ScalingNorm is that the global correlation u of each predicate is roughly at the same scale, causing the distribution of correlation factor ? obtained using Soft-maxNorm or ScalingNorm is too smooth to make enough impact on the loss weights while MinMaxNorm magnifies the difference.</p><p>The contributes of this paper can be summarized as PCPL, the graph encoder and the noisy label dropping method. To better verify the effectiveness of each components, we perform an ablation study as listed in <ref type="table">Table.</ref> 4. The performance of model with PCPL on all three tasks occurs an evident rise from GE, which clearly shows that our proposed PCPL greatly improves the generalization ability of the model. Meanwhile, GE still markedly outperforms IMP+, FREQ and SMN, indicating the effectiveness of the graph encoders in encoding context and extracting better visual features. Equipped   with the noisy label dropping schema, the performance of model gains a sight further improvement, demonstrating its efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we discover that the key challenge for generating unbiased scene graph lies in the complex relatedness among predicate classes. Thus, we propose a novel PCPL framework which can adaptively assign optimal loss weights to predicates by directly perceiving and explicitly utilizing the correlations among classes. PCPL is further equipped with a graph encoder module to better extract context features. Extensive experiments on the benchmark VG150 dataset show that PCPL performs markedly better on tail classes while well-preserving the performance on head ones, which significantly outperforms previous state-of-the-art methods in mean recall evaluation metric, demonstrating its effectiveness in removing the long-tailed bias of SGG.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The improvements of constrained and unconstrained recall@100 of re-balancing over cross-entropy in %. (a) The results of weakly correlated group. (b) The results of strongly correlated group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of how the baseline method (a), the re-balancing methods (b) and PCPL (c) generate scene graph from relationship representation and the corresponding output scene graph. Red boxes in (a) and (b) denote wrong predictions. (d) The pipeline used to acquire relationship representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the proposed Predicate-Correlation Perception Learning (PCPL) scheme. (a) A learnable class graph is constructed with each node representing the center of one predicate class and edges representing their correlations. (b) (c) (d) The graph is jointly trained with SGG model. (e) Correlation factors are derived from the graph. (f) We utilize correlation factors to adaptively assign optimal loss weights to predicate classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with state-of-the-art methods on VG150 dataset. The constrained and unconstrained mR@50/100 in % on PredCls, SGCls and SGGen tasks are presented. As VCTree and TDE do not report the unconstrained mR@K metric, they are not listed in unconstrained results.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="7">PredCls mR@50 mR@100 mR@50 mR@100 mR@50 mR@100 Mean SGCls SGGen</cell></row><row><cell></cell><cell>IMP+[37]</cell><cell>20.3</cell><cell>28.9</cell><cell>12.1</cell><cell>16.9</cell><cell>5.4</cell><cell>8.0</cell><cell>15.3</cell></row><row><cell></cell><cell>FREQ[41]</cell><cell>24.8</cell><cell>37.3</cell><cell>13.5</cell><cell>19.6</cell><cell>5.9</cell><cell>8.9</cell><cell>18.3</cell></row><row><cell>unconstrained</cell><cell>SMN[41]</cell><cell>27.5</cell><cell>37.9</cell><cell>15.4</cell><cell>20.6</cell><cell>9.3</cell><cell>12.9</cell><cell>20.6</cell></row><row><cell></cell><cell>KERN[3]</cell><cell>36.3</cell><cell>49.0</cell><cell>19.8</cell><cell>26.2</cell><cell>11.7</cell><cell>16.0</cell><cell>26.5</cell></row><row><cell></cell><cell>Ours</cell><cell>50.6</cell><cell>62.6</cell><cell>26.8</cell><cell>32.8</cell><cell>10.4</cell><cell>14.4</cell><cell>32.9</cell></row><row><cell></cell><cell>IMP+[37]</cell><cell>9.8</cell><cell>10.5</cell><cell>5.8</cell><cell>6.0</cell><cell>3.8</cell><cell>4.8</cell><cell>6.8</cell></row><row><cell></cell><cell>FREQ[41]</cell><cell>13.3</cell><cell>15.8</cell><cell>6.8</cell><cell>7.8</cell><cell>4.3</cell><cell>5.6</cell><cell>8.9</cell></row><row><cell></cell><cell>SMN[41]</cell><cell>13.3</cell><cell>14.4</cell><cell>7.1</cell><cell>7.6</cell><cell>5.3</cell><cell>6.1</cell><cell>9.0</cell></row><row><cell>constrained</cell><cell>KERN[3]</cell><cell>17.7</cell><cell>19.2</cell><cell>9.4</cell><cell>10.0</cell><cell>6.4</cell><cell>7.3</cell><cell>11.7</cell></row><row><cell></cell><cell>VCTree[31]</cell><cell>17.9</cell><cell>19.4</cell><cell>10.1</cell><cell>10.8</cell><cell>6.9</cell><cell>8.0</cell><cell>12.2</cell></row><row><cell></cell><cell>SMN+TDE[31]</cell><cell>25.5</cell><cell>29.1</cell><cell>13.1</cell><cell>14.9</cell><cell>8.2</cell><cell>9.8</cell><cell>16.8</cell></row><row><cell></cell><cell>Ours</cell><cell>35.2</cell><cell>37.8</cell><cell>18.6</cell><cell>19.6</cell><cell>9.5</cell><cell>11.7</cell><cell>22.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with other debiasing methods on VG150 dataset. The R@50/100 and mR@50/100 in % with and without constraints on PredCls, SGCls and SGGen tasks are presented.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="7">PredCls R@50/100 mR@50/100 R@50/100 mR@50/100 R@50/100 mR@50/100 Mean SGCls SGGen</cell></row><row><cell></cell><cell>GE + focal loss[22]</cell><cell>77.3/85.4</cell><cell>26.4/36.2</cell><cell>42.3/46.1</cell><cell>14.8/19.8</cell><cell>18.3/23.7</cell><cell>3.6/5.4</cell><cell>33.3</cell></row><row><cell>unconstrained</cell><cell cols="2">GE + class-balanced loss[4] 57.0/70.8 GE + re-weighting 56.5/70.7</cell><cell>35.1/44.9 39.0/49.6</cell><cell>33.2/39.9 32.0/38.9</cell><cell>19.1/24.0 20.6/25.8</cell><cell>8.4/12.8 8.1/12.1</cell><cell>6.1/8.9 6.5/9.4</cell><cell>30.0 30.8</cell></row><row><cell></cell><cell>Ours</cell><cell>72.1/81.5</cell><cell>50.6/62.6</cell><cell>39.9/44.5</cell><cell>26.8/32.8</cell><cell>15.2/20.6</cell><cell>10.4/14.4</cell><cell>38.4</cell></row><row><cell></cell><cell>GE + focal loss[22]</cell><cell>64.4/66.8</cell><cell>16.7/18.4</cell><cell>35.0/36.0</cell><cell>8.7/9.4</cell><cell>18.1/22.9</cell><cell>3.5/4.9</cell><cell>25.4</cell></row><row><cell></cell><cell>SMN+TDE[31]</cell><cell>46.2/51.4</cell><cell>25.5/29.1</cell><cell>27.7/29.9</cell><cell>13.1/14.9</cell><cell>16.9/20.3</cell><cell>8.2/9.8</cell><cell>24.4</cell></row><row><cell>constrained</cell><cell cols="2">GE + class-balanced loss[4] 43.4/48.1</cell><cell>29.7/33.6</cell><cell>24.9/26.8</cell><cell>15.9/17.9</cell><cell>8.4/12.6</cell><cell>6.0/8.8</cell><cell>23.0</cell></row><row><cell></cell><cell>GE + re-weighting</cell><cell>40.4/44.6</cell><cell>32.1/35.9</cell><cell>22.4/24.2</cell><cell>16.5/18.3</cell><cell>8.1/11.9</cell><cell>6.5/9.3</cell><cell>22.5</cell></row><row><cell></cell><cell>Ours</cell><cell>50.8/52.6</cell><cell>35.2/37.8</cell><cell>27.6/28.4</cell><cell>18.6/19.6</cell><cell>14.6/18.6</cell><cell>9.5/11.7</cell><cell>27.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of different compositions of PCPL on VG150. The constrained and unconstrained mR@50/100 in % on PredCls, SGCls and SGGen tasks are presented.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="7">PredCls mR@50 mR@100 mR@50 mR@100 mR@50 mR@100 Mean SGCls SGGen</cell></row><row><cell></cell><cell>LearntCenter + SoftmaxNorm</cell><cell>34.1</cell><cell>45.8</cell><cell>18.7</cell><cell>24.5</cell><cell>4.4</cell><cell>6.7</cell><cell>22.4</cell></row><row><cell>unconstrained</cell><cell>LearntCenter + ScalingNorm AvgCenter + MinMaxNorm</cell><cell>37.2 49.7</cell><cell>49.1 61.9</cell><cell>20.6 25.4</cell><cell>26.4 31.8</cell><cell>5.1 9.2</cell><cell>7.4 12.0</cell><cell>24.3 31.7</cell></row><row><cell></cell><cell>LearntCenter + MinMaxNorm</cell><cell>50.6</cell><cell>62.6</cell><cell>26.8</cell><cell>32.8</cell><cell>10.4</cell><cell>14.4</cell><cell>32.9</cell></row><row><cell></cell><cell>LearntCenter + SoftmaxNorm</cell><cell>17.4</cell><cell>18.9</cell><cell>9.1</cell><cell>9.7</cell><cell>3.9</cell><cell>5.3</cell><cell>10.7</cell></row><row><cell>constrained</cell><cell>LearntCenter + ScalingNorm AvgCenter + MinMaxNorm</cell><cell>19.0 34.1</cell><cell>20.5 36.9</cell><cell>10.1 17.8</cell><cell>10.7 18.9</cell><cell>4.5 8.6</cell><cell>5.8 10.6</cell><cell>11.8 21.2</cell></row><row><cell></cell><cell>LearntCenter + MinMaxNorm</cell><cell>35.2</cell><cell>37.8</cell><cell>18.6</cell><cell>19.6</cell><cell>9.5</cell><cell>11.7</cell><cell>22.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of our method.The constrained and unconstrained mR@50/100 in % on PredCls, SGCls and SGGen tasks are presented.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="7">PredCls mR@50 mR@100 mR@50 mR@100 mR@50 mR@100 Mean SGCls SGGen</cell></row><row><cell></cell><cell>GE</cell><cell>32.7</cell><cell>44.0</cell><cell>18.3</cell><cell>23.8</cell><cell>8.3</cell><cell>11.6</cell><cell>23.1</cell></row><row><cell>unconstrained</cell><cell>GE+PCPL</cell><cell>50.1</cell><cell>61.9</cell><cell>26.1</cell><cell>32.3</cell><cell>10.1</cell><cell>14.2</cell><cell>32.5</cell></row><row><cell></cell><cell>Ours</cell><cell>50.6</cell><cell>62.6</cell><cell>26.8</cell><cell>32.8</cell><cell>10.4</cell><cell>14.4</cell><cell>32.9</cell></row><row><cell></cell><cell>GE</cell><cell>17.3</cell><cell>18.7</cell><cell>9.3</cell><cell>9.8</cell><cell>5.5</cell><cell>6.5</cell><cell>11.2</cell></row><row><cell>constrained</cell><cell>GE+PCPL</cell><cell>34.5</cell><cell>37.4</cell><cell>18.1</cell><cell>19.2</cell><cell>9.3</cell><cell>11.2</cell><cell>21.6</cell></row><row><cell></cell><cell>Ours</cell><cell>35.2</cell><cell>37.8</cell><cell>18.6</cell><cell>19.6</cell><cell>9.5</cell><cell>11.7</cell><cell>22.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This paper is partially supported by NSFC (No.31627802) and the Fundamental Research Funds for the Central Universities.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is the Effect of Importance Weighting in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="872" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledgeembedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep active learning over the long tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Geifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00941</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating natural language explanations for visual question answering using scene graphs and visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giedrius</forename><surname>Burachas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Ziskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05715</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unpaired image captioning via scene graph alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10323" to="10332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from class-imbalanced data: Review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Haixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yijing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Mingyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Huang Yuanyue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="220" to="239" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The class imbalance problem: A systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaju</forename><surname>Stephen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent data analysis</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="429" to="449" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3573" to="3587" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Plankton classification on imbalanced large scale database via convolutional neural networks with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3713" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual Question Answering over Scene Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soohyeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju-Whan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><surname>Hyuk Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 First International Conference on Graph Computing (GC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Know more say less: Image captioning based on scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2117" to="2130" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SMOTE-RSB*: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enislay</forename><surname>Ramentol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yail?</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="245" to="265" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahana</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh M</forename><surname>Khapra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00850</idno>
		<title level="m">Scene Graph based Image Retrieval-A case study on the CLEVR Dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Bin Yang, and Raquel Urtasun</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Structured Query-Based Image Retrieval Using Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigit</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06653</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11949</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6619" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Using scene graph context to improve image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anahita</forename><surname>Bhiwandiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Bastidas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03762</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-modal scene graph matching for relationship-aware image-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1508" to="1517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unbiased Scene Graph Generation via Rich and Fair Semantic Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00176</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scene Graph Reasoning with Prior Visual Relationship for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09681</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pastegan: A semi-parametric method to generate image from scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li Yikang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sining</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3950" to="3960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV</title>
		<meeting>the European conference on computer vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
