<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Amazon</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
							<email>jwang@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reconstructing multi-human body mesh from a single monocular image is an important but challenging computer vision problem. In addition to the individual body mesh models, we need to estimate relative 3D positions among subjects to generate a coherent representation. In this work, through a single graph neural network, named MUG (Multi-hUman Graph network), we construct coherent multi-human meshes using only multi-human 2D pose as input. Compared with existing methods, which adopt a detection-style pipeline (i.e., extracting image features and then locating human instances and recovering body meshes from that) and suffer from the significant domain gap between labcollected training datasets and in-the-wild testing datasets, our method benefits from the 2D pose which has a relatively consistent geometric property across datasets. Our method works like the following: First, to model the multi-human environment, it processes multi-human 2D poses and builds a novel heterogeneous graph, where nodes across people and within one person are connected to capture inter-human interactions and draw the body geometry (i.e., skeleton and mesh structure). Second, it employs a dual-branch graph neural network structure -one for predicting inter-human depth relation and the other one for predicting root-joint-relative mesh coordinates. Finally, the entire multi-human 3D meshes are constructed by combining the output from both branches. Extensive experiments demonstrate that MUG outperforms previous multihuman mesh estimation methods on standard 3D human benchmarks -Panoptic, MuPoTS-3D and 3DPW. 2 Wu et al. Locate Branch Mesh Branch Image feature Backbone Pose Network 2D Pose Previous Pipeline Ours Human Meshes Image MUG</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods leveraging large-scale datasets have shown tremendous progress on tasks in 3D human mesh recovery, which is often an important initial step in computer vision and robotics applications on human behaviors, such as activity recognition <ref type="bibr" target="#b14">[15]</ref> and bodily expressed emotion understanding <ref type="bibr" target="#b43">[44]</ref>. For single-human mesh <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b29">30]</ref> estimation, recent works achieve admirable results. Naturally, it is desirable to extend the success to multi-human scenarios to provide a holistic understanding of a scene. Multi-human mesh estimation requires generating body mesh models and providing comprehensive 3D positions for each instance. Most recent methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b39">40]</ref> adopt a detection-style pipeline with whole images as input. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, they first feed the images into a backbone to extract features. Then one branch locates human instances. The other branch (or branches) will generate body meshes on top of the image feature and instances' locations. Particularly, <ref type="bibr" target="#b11">[12]</ref> leverages the region proposal network (RPN) <ref type="bibr" target="#b34">[35]</ref> to propose human body boxes and <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b39">40]</ref> locate body center points in their locate branches.</p><p>However, this pipeline, using images as input, suffers from the domain gap between in-the-wild and lab-collected datasets, which is a widely known problem in 3D-related works <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. The primary contradiction is that lab-collected datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref> are crucial for providing accurate 3D labels in training; While, these images have very different appearance from the in-the-wild testing images. For example, backgrounds and lighting conditions of the images collected in the monotonous laboratory environment are distinct from the images captured from in-the-wild environment. In addition, the domain gap can be more prominent in multi-human mesh estimation. Single-human mesh networks commonly take cropped-and-centered human images as input <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>. Human bodies tend to take up most of the image area and they share more similarities across lab-collected and in-the-wild datasets, whereas there are dramatic differences between the surrounding backgrounds of humans in these two kinds of datasets. In contrast, the multi-human mesh networks take whole images as input <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b39">40]</ref>, where the backgrounds occupy much more areas. That means taking whole images as input may lead to a more significant domain gap, thus preventing multi-human mesh methods from fully utilizing the training data.</p><p>To tackle the aforementioned domain gap issue, we hope to build up a new pipeline with the 2D pose as input. Compared with the image features, the 2D pose has relatively consistent geometric properties across different datasets. No matter how environmental characteristics (e.g., lighting and weather) change, the 2D pose remains the body joint coordinates in the images. Also, accurate 2D poses can be obtained through cutting-edge keypoint-detection networks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref> trained on large-scale in-the-wild datasets.</p><p>To reconstruct multi-human meshes from the 2D pose, we need to design a network to accomplish the following two goals (1) accurately predict body meshes from the 2D pose, and (2) estimate comprehensive relative 3D body positions from the 2D pose. Regarding the first goal, each body mesh vertex needs to get the coordinate information from surrounding joints and mesh vertices within the human. Therefore, we need to take advantage of the intra-human skeleton and mesh topology. For the second goal, capturing the inter-human dependencies is crucial to obtain the correct relative body positions. In short, we need to model intra-and inter-human relations simultaneously.</p><p>In this work, we propose a simple yet effective heterogeneous graph convolutional network MUG (Multi-hUman Graph network). First, we construct a novel heterogeneous graph to represent the holistic multi-human environment. Body joints and mesh vertices are represented as two different types of nodes. Within one human, we connect joint and mesh nodes to form intra-human edges according to the body skeleton structure and mesh topology. To represent interhuman relations, we pick and then connect appropriate joint nodes across humans as edges. Through different kinds of edges, the coordinate information can be transmitted among nodes within a human or across humans. Next, we design a dual branch graph convolutional network structure to simultaneously output root-camera depth and root-relative body mesh coordinates. After feeding the heterogeneous graph and 2D pose input into the neural network, we can construct multi-human meshes by combining the output of the two branches. The entire framework is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Extensive experiments show that our method significantly outperforms the previous SOTA <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b39">40]</ref> on multi-person 3D human datasets-Panoptic <ref type="bibr" target="#b12">[13]</ref>, MuPoTS-3D <ref type="bibr" target="#b26">[27]</ref> and 3DPW <ref type="bibr" target="#b23">[24]</ref>, where MuPoTS-3D and 3DPW are both inthe-wild datasets. Then we conduct ablation studies to help analyze the effectiveness of the graph construction and the influence of different 2D pose input. We also provide qualitative results.</p><p>Our contributions are summarized as follows.</p><p>-We design a new pipeline for multi-human reconstruction. With the 2D pose as input, we use one single graph network to output multi-human meshes and 3D locations simultaneously. The consistent geometric property of the 2D pose makes our method more robust to the domain gap. -We propose a novel graph neural network, named MUG, that leverages the heterogeneous graph, including joint nodes, vertex nodes, and different types of edges, to represent the intra-and inter-human relations. -Our method significantly outperforms previous approaches on standard multiperson 3D human datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-human mesh estimation</head><p>Single-human mesh estimation aims at reconstructing body mesh in the cropped human image. The body mesh can be represented by the 3D vertex coordinates or the human body models (such as SMPL <ref type="bibr" target="#b22">[23]</ref>). According to the method's output is body mesh model parameters or 3D vertex coordinates, human mesh estimation methods can be divided into two types: model-based and model-free. Bogo et al. pioneered a model-based method and demonstrated how to fit SMPL parameters by minimizing the error between predicting and input pose <ref type="bibr" target="#b1">[2]</ref>. Then the following works mainly adopt an end-to-end neural network to train images and regress SMPL parameters. HMR <ref type="bibr" target="#b13">[14]</ref> utilized adversarial loss for its regression network. Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref> exploited 2D joint heatmaps and silhouettes as a cue. SPIN <ref type="bibr" target="#b17">[18]</ref> introduced optimization methods into the regression network.</p><p>Despite the success of these model-based approaches, recent works argue that SMPL parameters, which are in the form of 3D rotation, might not be accessible to learn <ref type="bibr" target="#b18">[19]</ref>. As such, more and more recent researches focus on model-free methods. GraphCMR <ref type="bibr" target="#b18">[19]</ref> used ResNet to extract global features and GNN to regress 3D vertex coordinates. Inspired by the heatmap representation in 2D pose estimation, Moon et al. <ref type="bibr" target="#b29">[30]</ref> developed a new representation, called lixel, to help regress 3D coordinates. Lin et al. <ref type="bibr" target="#b20">[21]</ref> built up a transformer network. Choi et al. <ref type="bibr" target="#b3">[4]</ref> took the 2D pose as input and exploited 3D pose as intermediate features, then used GNN to estimate body mesh coordinates. Our work also uses 2D pose as input and adopts GNN. However, the single-human problem does not need to estimate the human depth and analyze the human iteration. <ref type="bibr" target="#b3">[4]</ref> only outputs root-relative coordinates. When reprojecting body meshes to the original image, <ref type="bibr" target="#b3">[4]</ref> cannot distinguish human body orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-human mesh estimation</head><p>Multi-human mesh estimation is a relatively new field <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b39">40]</ref>. Zanfir et al. <ref type="bibr" target="#b49">[50]</ref> published a pioneering work in 2018 in which they implemented a topdown approach. First, a multi-task single-human network was used to estimate 2D pose, 3D mesh, and other scene constraints. Then these outputs were jointly optimized to generate the final prediction. In another work in 2018, Zanfir et al. <ref type="bibr" target="#b50">[51]</ref> proposed a bottom-up method that first performed 3D pose estimation then used an unsupervised auto-encoder to estimate SMPL parameters. Recent works adopt detection-style pipelines to solve this task and achieve stunning performance. Jiang et al. <ref type="bibr" target="#b11">[12]</ref> follows the two-stage detection framework Faster RCNN <ref type="bibr" target="#b34">[35]</ref> to use RPN to propose human body boxes. Then it uses SMPL-head to regress SMPL parameters according to proposed boxes. Zhang et al. <ref type="bibr" target="#b52">[53]</ref> and ROMP <ref type="bibr" target="#b39">[40]</ref> adopt an one-stage solution -using one head to locate the center point of the human, and the other head outputs SMPL parameters according to the center point. As aforementioned, these methods suffer from the domain shift problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph neural networks</head><p>Graph neural networks (GNNs) learn node embeddings that preserve the graph structure and the original node features <ref type="bibr" target="#b15">[16]</ref>. Recently, GNNs have achieved <ref type="figure">Fig. 2</ref>. Illustration of the framework. 2D multi-human pose is the input. To explicitly model the inter-and intra-human relation, we build a heterogeneous graph containing joint nodes (blue circles) and mesh nodes (green circles). On top of the graph, we construct node features using 2D pose. The graph network adopts a dual-branch structure. Joint nodes with joint features go through the upper branch (blue blocks), meanwhile mesh nodes with mesh features go through the lower branch (green blocks). One GCN block (orange block) connects these two branches. Finally, the upper branch outputs root-camera depth and the lower branch outputs root-relative mesh coordinates. By combining the outcomes, we can recover the full multi-human meshes. The upper branch also outputs joint coordinates, which plays a supporting role in learning. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b48">49]</ref>. According to the type of graphs, GNNs can be divided into two categories: homogeneous GNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b7">8]</ref> and heterogeneous GNNs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>. The homogeneous GNNs assume all nodes and edges in the graph are of the same type, where most existing work focus on modeling such relations via different approaches, such as graph convolution <ref type="bibr" target="#b15">[16]</ref>, attention network <ref type="bibr" target="#b41">[42]</ref>, recurrent neural networks <ref type="bibr" target="#b7">[8]</ref>, etc. On the other hand, heterogeneous GNNs are proposed for graphs that contain different types of nodes and edges. The diversity of node types and edge types require GNNs to capture the characteristics of each node and edge type. For example, <ref type="bibr" target="#b35">[36]</ref> proposes R-GCN where every type of node and edge is modeled by an individual graph convolutional network <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Coordinates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Root Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Coordinates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Pose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Construct Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Human Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GCN Blocks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SOTA performance on various computer vision tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe our technical approach. The entire framework is illustrated in <ref type="figure">Figure 2</ref>. We describe our proposed heterogeneous multi-human graph in Subsection 3.1. Then we demonstrate the node feature construction in Subsection 3.2. The neural network structure is illustrated in Subsection 3.3. In Subsection 3.4, we analyze the root-camera depth computing. Finally, Subsection 3.5 introduces how to reconstruct multiple humans with network outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph construction</head><p>This subsection introduces how to construct the graph of MUG, <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the graph structure. Assuming there are K humans in total, we use G * to denote the entire multi-human graph and G 1 , ? ? ? , G K to denote the subgraph of <ref type="figure" target="#fig_2">Fig. 3</ref>. Structure of the graph. The graph consists of two kinds of nodes, joint nodes V Joint (blue circles) and mesh nodes V Mesh . Each human corresponds to one subgraph of the whole graph. Within one human, joint nodes are connected to get edge E Joint -Joint (green lines) according to body skeleton structure; following body mesh topology, mesh nodes are connected to get edge E Mesh -Mesh (orange lines); each mesh node connects to its two nearest joint nodes, generating edge E Joint -Mesh (red lines). To represent inter-human relation, we connect joint nodes across humans to get edge E Inter (purple lines). (b-c) shows the graph structure for image (a). For better display, we do not draw mesh nodes and only draw part of edge E Joint -Mesh . each human. Each subgraph contains two types of nodes, one is joint node V Joint i , the other is mesh node V Mesh i . Joint nodes represent human body keypoints like hips and shoulders (totally J joint nodes within a human), which have 2D coordinate ground truth in training datasets and can be accurately predicted by 2D pose detection networks, such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38]</ref>. Mesh nodes represent the vertices in the body mesh (totally V mesh nodes within a human), our target is to obtain their absolute 3D coordinates.</p><formula xml:id="formula_0">(a) (c) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-Joint Edge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter Edge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint-Mesh Edge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh-Mesh Edge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Node</head><p>Let us dive into each subgraph. To represent the body geometric topology, we connect joint nodes according to the body skeleton structure, and we link mesh nodes based on the body mesh topology. Furthermore, we add edges between these two kinds of nodes according to their distance pre-defined by the SMPL body mesh template <ref type="bibr" target="#b22">[23]</ref>. Each mesh node connects to its two nearest joint nodes. These edges are directional -only from joint nodes to mesh nodes. The reason is that the input of MUG only has 2D coordinates of joints. Therefore, coordinates information should be transmitted strictly from joint nodes to mesh nodes. Formally, we have each subgraph</p><formula xml:id="formula_1">G i = (V i , E i , X i ), i ? {1, ..., K}, where node set V i = V Joint i ? V Mesh i ; edge set E i = E Joint -Joint i ? E Mesh -Mesh i ? E Joint -Mesh i ; and node features X i = X Joint i ?X Mesh i . The node features construction is introduced in Subsection 3.2.</formula><p>To model the spatial correlation across humans, we further integrate all human subgraphs into a multi-layer graph G * = (V * , E * , X * ). Each layer contains a human subgraph</p><formula xml:id="formula_2">G i , we have V * = {V i } K i=1 , X * = {X i } K i=1 .</formula><p>We construct edges E Inter across different subgraphs to represent the inter-human relation. There are two types of E Inter : (1) We set a length threshold ?, any two joint nodes from different humans will be connected to form an edge if these two joints' 2D distance in the image is less than ?. By selecting appropriate ?, pairs of joints in small 2D distances will be connected. The idea is inspired by the first law of geography <ref type="bibr" target="#b40">[41]</ref> that points close in space share similar features. A small 3D distance always leads to a small 2D distance. Joints pairs in small 2D distances are relatively more likely to have interaction (e.g. overlap), thus need more accurate prediction. We connect these joints pairs as edges, inducing the GNN to produce better predictions on these joints and around mesh nodes. (2) Any two humans' root-joint nodes must be connected. The root-joint is usually used to determine the depth of a human from the camera. Relative depth information for each human can be passed through root-to-root edges. More analysis of E Inter can be found in the ablation study <ref type="bibr">(Subsection 4.4)</ref>.</p><p>In summary, we have intra-human edges in each human subgraph (including E Joint -Joint , E Joint -Mesh , E Mesh -Mesh ) and E Inter among the subgraphs to connect individuals. The set of all these edges can be described as</p><formula xml:id="formula_3">E * = E Inter ? {E i } K i=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The node feature construction</head><p>We define the node features as</p><formula xml:id="formula_4">X * = {X i } K i=1 . The i th human's feature X i includes X Joint i ? R J?d for joint nodes and X Mesh i ? R V ?d for mesh nodes, where d is the feature dimension.</formula><p>For joint nodes, we denote the feature for the j th joint of the i th human as X Joint ij . The main information that can be used to construct features is the 2D pose input P 2D i ? R J?2 . Following previous work <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4]</ref>, we first apply standard normalization to P 2D i . Specifically, we subtract the mean of P 2D i and divide by the standard deviation in order to obtain P 2D i . Then P 2D ij is assigned to the j th joint. The normalization <ref type="bibr" target="#b42">[43]</ref> allows for easier convergence of the GCN, but it turns each human's 2D pose into relative coordinates regardless of this human's body size and position. To analyze relative 3D human positions, we need to compare the absolute joint coordinates across humans in the images. Thus the joint coordinates over the entire image range are necessary to pass to joint nodes. In the meantime, to facilitate training, we need to convert different images into a uniform size. We resize the raw image's long edge as 1000 pixels and then center it in the 1000 ? 1000 pixels square. P 2D i will be accordingly transformed into P 2D i . Then P 2D ij is assigned to the j th joint. To fully utilize P 2D i , we also pass whole P 2D i to X Joint ij .</p><p>In addition, we leverage 11 SMPL body mesh models, containing the SMPL template model as well as 10 randomly selected from Human3.6M dataset. Mesh vertices coordinates can be computed by these models. For simplicity, we just denote mesh coordinates from one model as T ? R V ?3 . We can compute the 3D coordinates of joints by multiplying T with the joint regression matrix J ? R J?V , where J is defined in SMPL body mesh model <ref type="bibr" target="#b22">[23]</ref>. Then we pass (J T ) j to X Joint ij . Although these body mesh coordinates are fixed, the graph neural network can learn some rigid geometry information by remembering these examples. Finally, the joint feature can be obtained by concatenating above features,</p><formula xml:id="formula_5">X Joint ij = ( P 2D ij , P 2D i , P 2D ij , (J T ) j ) .<label>(1)</label></formula><p>For mesh nodes, we denote the feature for the v th mesh vertex in i th human as X Mesh iv . Different from joint nodes, we can not know the 2D position information of each mesh vertex by the 2D pose input. We directly use P 2D ij * and P 2D ij * to initialize the mesh features, where j * represents the nearest joint with the v th mesh node in the SMPL body mesh template. Following a similar process with joint nodes, we also feed P 2D i and T v (the v th vertex in T ) to X Mesh iv . Concatenate these features we have,</p><formula xml:id="formula_6">X Mesh iv = ( P 2D ij * , P 2D i , P 2D ij * , T v ) .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network structure</head><p>Then we introduce the model structure, which is illustrated in <ref type="figure">Figure 2</ref>. Considering the two kinds of nodes, we design a dual-branch graph network. Joint nodes with joint features go through one branch, meanwhile mesh nodes with mesh features go through the other. The two branches are connected by a GCN block, namely the connection block. For the branch of joint nodes, they first go through two GCN blocks. Then the output has three destinations. One is passed into the mesh branch via the connection block. Another will step into one GCN block and finally reduce its feature dimension into 1. We pick the value of the root-joint node as the depth measure for its human instance. The last one will reduce the dimension into 3 to output 3D joint coordinates through one GCN block. Although these 3D joint coordinates are not used in 3D mesh reconstruction, they can guide the branch to learn depth representation. The other branch consists of four GCN blocks. After passing one GCN block, the mesh nodes will add their features with the output of the connection block. Then, going through three more GCN blocks and reducing their feature dimension into 3, mesh nodes finally output root-related body mesh coordinates. The GCN blocks are the GCN residual block which is inspired by ResNet Basic blocks <ref type="bibr" target="#b9">[10]</ref>, or just the cascade of three graph convolutional layers. The amount of parameters of the entire network is not large, only 2.3M. The detailed structure and configuration are discussed in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Estimating depth</head><p>For this monocular setting, it is not meaningful to predict an accurate rootcamera depth. Here we aim at providing a comprehensive depth estimation to reflect relative 3D position across humans. Skeletons of different humans usually have similar lengths. Through the pine-hole camera model <ref type="bibr" target="#b5">[6]</ref>, the ratio between the root-camera depth and focal length largely decides the size of the skeletons in the image. Conversely, we attempt to use 2D human pose to infer that ratio. For this, we construct the depth measure as D = D/f , where D is the depth and f is the camera focal length. As mentioned in Subsection 3.2 we resize the raw image to make its long edge width as 1000 pixels, this is equivalent to adjusting the focal length while holding the depth. For the training convenience, we also add a constant coefficient ? to make the the measure in the range of [0,1]. Finally, we get the depth measure,</p><formula xml:id="formula_7">D = DS 1000?f ,<label>(3)</label></formula><p>where S is the long edge width of the raw image and we set ? as 200 in practice.</p><p>In the training process, we utilize the L1 loss to supervise D and adopt a relative depth loss to get more accurate human relative depth,</p><formula xml:id="formula_8">L D = ? D ? D * ? 1 , L rD = 1 N m,n ?( D m ? D n ) ? ( D * m ? D * n )? 1 ,<label>(4)</label></formula><p>where * represents the ground truth and m, n represent different human instances. In training, for the dataset containing root-camera depth and focal length, we can directly compute ground truth D. If the dataset doesn't provide these labels, we assume f as 1500. Then we use SMPLify <ref type="bibr" target="#b31">[32]</ref> to generate pseudo root-camera depth, such that pseudo D can be obtained.</p><p>The assumption of f is sufficient to give the comprehensive depth. Because f is the same for all humans within one image, relative 3D body positions are consistent regardless of the change of f . In inference, we can recover the D with f , S and predicted D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Reconstruct multiple humans</head><p>Besides the depth, the graph network also outputs root-relative mesh vertex coordinates M ? R V ?3 and 3D joint coordinates P 3D ? R J?3 for each human, where M is what our need to recover the body mesh and P 3D play a supporting role in training. Also, we can get the 3D joint coordinates from M by multiply the joint regression matrix J . In the training process, we use the L1 loss function to supervise M , P 3D and J M ,</p><formula xml:id="formula_9">L M = ||M ? M * || 1 , L J = ||P 3D ? P 3D * || 1 , L JM = ||J M ? P 3D * || 1 ,<label>(5)</label></formula><p>where * represents the ground truth. If the dataset doesn't provide 3D labels for joints or mesh coordinates, we use SMPLify <ref type="bibr" target="#b31">[32]</ref> to generate pseudo 3D coordinates ground truth. Following <ref type="bibr" target="#b46">[47]</ref>, we also use the mesh normal vector loss L N and mesh edge vector loss L N to make the mesh result more smooth. These losses and above losses for depth are then summed together to get the total loss,</p><formula xml:id="formula_10">L = L M + ? 1 L J + ? 2 L JM + ? 3 L N + ? 4 L E + ? 5 L D + ? 6 L rD .<label>(6)</label></formula><p>By optimizing these losses, we obtain accurate estimated M and D for each human. Because we can directly get the root-joint 2D image coordinates [X, Y ] from the 2D pose input, the 3D coordinate for the root-joint [x, y, D] can be computed by the camera intrinsic equation,</p><formula xml:id="formula_11">D[X, Y, 1] T = K[x, y, D] T ,<label>(7)</label></formula><p>where K is the camera intrinsic matrix. If we do not have the camera parameters, we construct the intrinsic matrix by setting focal length as 1, 500 and the center point of the image as the principal point. Finally, we can construct multiple humans by calculating the absolute body mesh coordinates for them.</p><formula xml:id="formula_12">M abs = M + [x, y, D] .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted extensive experiments to validate the effectiveness of MUG. We answer the following questions:(1) Can MUG outperform existing multi-human 3D mesh reconstruction approaches on various benchmark datasets? <ref type="formula" target="#formula_6">(2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Human3.6M <ref type="bibr" target="#b10">[11]</ref> is a large-scale single-human 3D pose dataset, where the ground truth 3D pose is obtained through motion capture. Because this dataset has high-quality 3D annotations, we use it for both training and testing. Hu-man3.6M does not provide body mesh ground truth so we adopt the pseudoground-truth provided by <ref type="bibr" target="#b29">[30]</ref>, where SMPLify <ref type="bibr" target="#b31">[32]</ref> is used to generate pseudoground-truth. We follow the evaluation method from <ref type="bibr" target="#b13">[14]</ref>. MuPoTS-3D is an outdoor multi-human 3D pose dataset proposed by <ref type="bibr" target="#b26">[27]</ref>, where ground truth 3D pose is obtained by a multi-view marker-less motixon capture system. We use this dataset to evaluate. We follow the standard evaluation method from <ref type="bibr" target="#b11">[12]</ref>. MuCo-3DHP is a multi-human 3D pose dataset proposed by <ref type="bibr" target="#b26">[27]</ref>. It is composited from the existing MPI-INF-3DHP 3D single-human pose estimation dataset <ref type="bibr" target="#b25">[26]</ref>. We only use this dataset for the training. We adopt the pseudoground-truth provided by <ref type="bibr" target="#b3">[4]</ref>.</p><p>Panoptic <ref type="bibr" target="#b12">[13]</ref> is a multi-human dataset. It provides video clips collected from a panoptic environment. It has ground-truth 3D pose annotation in the multicamera view angle. We follow the evaluation procedure of <ref type="bibr" target="#b49">[50]</ref>. Haggling, Mafia, Ultimatum, and Pizza are four activities used to test, and we just use No. <ref type="bibr" target="#b15">16</ref> and No.30 HD cameras. 3DPW <ref type="bibr" target="#b23">[24]</ref> is a multi-human in-the-wild dataset with full-body mesh annotations. We use its test set for evaluation, following the same protocol as <ref type="bibr" target="#b16">[17]</ref>. COCO <ref type="bibr" target="#b21">[22]</ref> is a large-scale in-the-wild multi-human dataset, providing 2D pose growth truth. Although we don't have 3D pose ground truth, to make the training more diverse, we use COCO to train. We use pseudo human-mesh ground truth provided by <ref type="bibr" target="#b29">[30]</ref>, where SMPLify <ref type="bibr" target="#b31">[32]</ref> is used to generate pseudo-groundtruth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Our implementation is built with PyTorch <ref type="bibr" target="#b30">[31]</ref>. The GCN network uses the DGL library <ref type="bibr" target="#b45">[46]</ref>. In MUG we adopt a coarse body mesh model (431 vertices). According to <ref type="bibr" target="#b33">[34]</ref>, by multiplying the up-sample matrix provided by <ref type="bibr" target="#b33">[34]</ref>, the coarse mesh can be recovered into the full body mesh SMPL model (6890 vertices). In the training process, we use flip as the data augmentation strategy. Also, we add synthetic error to the 2D pose ground truth input in training following <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4]</ref>, which effectively improves the network robustness to wrong poses. For our implementation, we set the batch size to 1 and Adam as the optimizer. We train for 15 epochs in total. The initial learning rate is 0.001, which will be reduced by a factor of 10 after the 10th epoch. In the experiment of evaluating Human3.6, we only use Human3.6 in training. But for other experiments, we jointly train on MuCo-3DHP, COCO, and Human3.6M together. We set the coefficients in equation 6 as ? 1 = 1e ? 3, ? 2 = 1e ? 3, ? 3 = 0.1, ? 4 = 20, ? 5 = 1, ? 6 = 20.</p><p>In inference, we conveniently adopt 2D pose prediction result of Human3.6M provided by <ref type="bibr" target="#b3">[4]</ref>. For other testing datasets (MuPoTS-3D, Panoptic and 3DPW), we first use Mask-RCNN <ref type="bibr" target="#b8">[9]</ref> to detect multi-humans and then use HRNet <ref type="bibr" target="#b37">[38]</ref> (enhanced by Darkpose <ref type="bibr" target="#b51">[52]</ref>) to estimate 2D pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state of the art</head><p>First, we conduct experiments on Human3.6M. Although Human3.6M is a singlehuman dataset and our goal is to solve the multi-human problem, its experiments can still reflect our method's ability to recover body meshes. The result is reported in <ref type="table" target="#tab_1">Table 1</ref>. Compared with previous multi-human mesh methods, the MPJPE drops by 5.5% (51.3mm vs 48.5mm). Part of this improvement comes from (1) the superb generalization ability of 2D pose (2) our carefully designed graph architecture which can model the intra-human relations. Concretely, we also compare our method with two single-human mesh methods GraphCMR <ref type="bibr" target="#b18">[19]</ref> and Pose2Mesh <ref type="bibr" target="#b3">[4]</ref>, because they also apply graph neural network and use mesh coordinates as output. MUG achieves much better performance over GraphCMR and comparable performance with Pose2Mesh (better in MPJPE and worse in PA MPJPE). It is worth noting that our model parameters are significantly less than Pose2Mesh (2.3M vs 8.8M), we can still get a competitive result. The last row gives the performance with the ground truth 2D pose as input, which is the upper bounds of our method on Human3.6M. Then we evaluate our method on different multi-human benchmarks to get comparisons with the SOTA works, containing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b39">40]</ref>. It is worth noting that we only use Human3.6, MuCo-3DHP, and COCO in training for all the multihuman evaluation experiments. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b52">53]</ref> adopt more 2D pose datasets to enhance the performance (e.g., MPII <ref type="bibr" target="#b0">[1]</ref>). ROMP <ref type="bibr" target="#b39">[40]</ref> has two settings-its primary setting utilizes seven datasets (Human36M, COCO, MPII, etc.) in training, and its advanced setting utilizes 11 datasets in training. Therefore, we compare with its primary setting for a fair comparison. Finally, we achieve significantly better performance against these baselines on different benchmarks, demonstrating the superb generalization ability of the 2D pose. Below is the detailed comparison. We examine performance on the Panoptic dataset, a standard benchmark for multi-human 3D human analysis. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison results with <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b39">40]</ref>. Our method has better performance in three activities. Compared with ROMP <ref type="bibr" target="#b39">[40]</ref>, the mean MPJPE of all the activities is decreased by 5.1% (from 134.6mm to 127.8mm). This improvement comes from three places: (1) the good generalization ability of 2D pose (2) our graph neural network's ability to model inter-and intra-human relations (3) well-performing human pose network HRNet <ref type="bibr" target="#b37">[38]</ref> is used to predict 2D pose.</p><p>Also, we compare our method with SOTA 3D multi-human mesh methods on another popular 3D human benchmark MuPoTS-3D dataset in <ref type="table">Table 3</ref>. The evaluation metric is percentage of correct 3D keypoints (3DPCK). Following the evaluation method described by <ref type="bibr" target="#b26">[27]</ref>, MUG outperforms other SOTA methods by 2.44 PCK for all cases and 2.19 PCK for matched cases. Results of two baselines -the combination of OpenPose <ref type="bibr" target="#b2">[3]</ref> and single-human mesh method SMPLify-X <ref type="bibr" target="#b31">[32]</ref> and HMR <ref type="bibr" target="#b13">[14]</ref> are also include in <ref type="table">Table 3</ref>.</p><p>Last, we conduct experiments on the challenging in-the-wild 3DPW test set. It has ground truth mesh coordinates, the error of the mesh vertices (MPVPE) <ref type="figure">Fig. 4</ref>. Illustration of inter-human edges with ? = 0 + , 0, 100, 200, 300 pixels from left to right, where + means no root-to-root edges. Green points represent body joints. Orange and green lines represent root-to-root edges and ?-controlled edges separately. can be reported. We show the result in <ref type="table">Table 4</ref>. Our method significantly outperforms previous multi-human mesh methods on MPJPE and MPVPE, demonstrating that our method can achieve good results on in-the-wild datasets. We also report the result when we use 2D pose ground truth as input to show the upper bound of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>Inter-human edges: Our method aims at multi-human cases. The core component of our proposed graph structure is the inter-human edges E Inter , connecting different human subgraphs. We study how inter-human edges affect the performance. As mentioned in Subsection 3.1, root-joints are connected to form root-to-root edges. Also, joint pairs are connected if the pair's 2D distance in the image is less than ?, where the image is preprocessed (resizing the raw image's long edge as 1000 pixels). A larger ? leads to more dense inter-human edges.</p><p>In this ablation study, we set ? = 0, 100, 200, 300 pixels separately. With ? = 0, E Inter only has root-to-root edges. When we remove the root-to-root edges, the whole graph is just the combination of separate subgraphs, which we denote as ? = 0 + . <ref type="figure">Figure 4.4</ref> gives an example to show the inter-human edges with different ?. <ref type="table" target="#tab_3">Table 5</ref> reports the experiment result. Row two shows the experiments on 3DPW test set with MPVPE as the metric. ? = 0 + gets 113.4mm MPVPE. With more edges, the performance increases. However, when ? is over 200, connections become too dense, which will hurt the performance. Moreover, we follow <ref type="bibr" target="#b27">[28]</ref> to conduct experiments on the MuPoTS-3D dataset to evaluate the ordinal depth relations of all human instance pairs (row three). The number is the percentage of correct depth relations (D%), reflecting the performance of the relative 3D position. If we do not use inter-human edges, the performance is limited, just 78.6%, because the information cannot transmit across humans to analyze the relative 3D positions. When we add the root-to-root edges, the accuracy jumps from 78.6% to 89.6%, demonstrating the effectiveness of root-toroot edges in model inter-human relations. With larger ?, the performance can be better. ? = 200 produces the best result. 2D pose input: Although we add synthetic error <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4]</ref> to the 2D pose input in training, making MUG more robust to wrong 2D poses. Input pose with different accuracy can still cause different performances in inference. This ablation experiment explores how various pose inputs affect MUG. We evaluate on 3DPW test set with MPVPE as the metric. We employ three variants of HRNet to produce pose input -HRNet-32, HRNet-48, and HRNet-48 enhanced by DarkPose. The three variants get better 2D pose predictions one by one. <ref type="table">Table 6</ref> shows that we can get better human mesh reconstruction results with better pose estimation results as input. Then we compare them with the ground truth input. We observe a significant performance gap boost -the ground truth pose input results in the 34.0% MPJPE drop (from 106.2mm to 70.1mm). The performance of MUG still has much room to increase if we can get a more accurate 2D pose input in testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We studied multi-human mesh reconstruction using 2D pose as input. We designed a multi-layer heterogeneous graph to represent the intra-human and interhuman relations. We further proposed a dual-branch graph neural network that leverages the heterogeneous graph to learn human relations and accurately recovers the 3D mesh coordinates. In the future, we will extend our pipeline for other 3D reconstruction problems, such as animal mesh reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Loss functions</head><p>Following <ref type="bibr" target="#b46">[47]</ref>, the equations for the mesh normal vector loss L N and mesh edge vector loss L N are,</p><formula xml:id="formula_13">L N = f {a,b}?f | &lt; M a ? M b ?M a ? M b ? 2 , n * f &gt; | ,<label>(3)</label></formula><formula xml:id="formula_14">L E = f {a,b}?f |?M a ? M b ? 2 ? ?M * a ? M * b ? 2 | ,<label>(4)</label></formula><p>where f and n * f denote a triangle face in the human mesh and a ground truth unit normal vector of f , respectively. M a and M b denote the a th and b th ground truth vertices in f , respectively. * represents the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More qualitative results on MuPoTS</head><p>More examples on the MuPoTS dataset are displayed in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More qualitative results on 3DPW</head><p>More examples on the 3DPW dataset are displayed in <ref type="figure" target="#fig_2">Figure 3</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Previous multi-human mesh networks commonly adopt a detection-style pipeline using images as input. In our pipeline, the proposed Multi-hUman Graph network (MUG) takes the 2D pose as input. The 2D pose can be obtained by 2D human pose estimation networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative results. We show the multi-human mesh reconstruction of MUG on MuPoTS and 3DPW dataset. The second to the forth column are the front, top, side view respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>More qualitative results on 3DPW. The second to the forth column are the front, top, side view respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison for Human3.6M. The evaluation metrics are MPJPE and PA MPJPE (mean per joint position error without and with Procrustes alignment). The unit is mm. * means using ground truth 2D pose as input.</figDesc><table><row><cell>Method</cell><cell cols="3">Multi-human MPJPE PA MPJPE</cell></row><row><cell>GraphCMR [19]</cell><cell>-</cell><cell>88.0</cell><cell>56.8</cell></row><row><cell>Pose2Mesh [4]</cell><cell>-</cell><cell>64.9</cell><cell>46.3</cell></row><row><cell>Jiang [12]</cell><cell>?</cell><cell>-</cell><cell>52.7</cell></row><row><cell>BMP [53]</cell><cell>?</cell><cell>-</cell><cell>51.3</cell></row><row><cell>MUG</cell><cell>?</cell><cell>61.9</cell><cell>48.5</cell></row><row><cell>MUG  *</cell><cell>?</cell><cell>50.3</cell><cell>38.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison for Panoptic. The evaluation metric is MPJPE.</figDesc><table><row><cell></cell><cell cols="5">Method Haggling Mafia Ultim. Pizza Mean</cell></row><row><cell></cell><cell cols="5">Zanfir [50] 140.0 165.9 150.7 156.0 153.4</cell></row><row><cell></cell><cell cols="5">Zanfir [51] 141.1 152.3 145.0 162.5 150.3</cell></row><row><cell></cell><cell>Jiang [12]</cell><cell cols="4">129.6 133.5 153.0 156.7 143.2</cell></row><row><cell></cell><cell>BMP [53]</cell><cell cols="4">120.4 132.7 140.9 147.5 135.4</cell></row><row><cell></cell><cell cols="5">ROMP [40] 111.8 129.0 148.5 149.1 134.6</cell></row><row><cell></cell><cell>MUG</cell><cell cols="4">113.7 120.8 137.5 139.2 127.8</cell></row><row><cell cols="3">Table 3. 3DPCK comparison on</cell><cell cols="4">Table 4. Performance comparison for</cell></row><row><cell cols="3">MuPoTS-3D. The column All and</cell><cell cols="4">3DPW. The evaluation metric is MPJPE,</cell></row><row><cell cols="3">Matched represents the accuracy for</cell><cell cols="4">PA MPJPE and MPVPE. The units are</cell></row><row><cell cols="3">all predictions and the predictions</cell><cell cols="4">mm. MPVPE represents mean per vertex</cell></row><row><cell cols="2">matched with annotations.</cell><cell></cell><cell cols="4">position error.  *  means using ground truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">2D pose as input.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">All Matched</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SMPLify-X [32] 62.84 68.04</cell><cell></cell><cell cols="4">Method MPJPE PA MPJPE MPVPE</cell></row><row><cell>HMR [14]</cell><cell>66.09 70.90</cell><cell></cell><cell cols="2">Jiang [12] 105.3</cell><cell>62.3</cell><cell>122.2</cell></row><row><cell>Jiang [12]</cell><cell>69.12 72.22</cell><cell></cell><cell cols="2">BMP [53] 104.1</cell><cell>63.8</cell><cell>119.3</cell></row><row><cell>BMP [53]</cell><cell>73.83 75.34</cell><cell></cell><cell cols="2">ROMP [40] 91.3</cell><cell>54.9</cell><cell>108.3</cell></row><row><cell cols="2">ROMP [40] 69.90 74.60</cell><cell></cell><cell>MUG</cell><cell>87.0</cell><cell>60.5</cell><cell>106.2</cell></row><row><cell>MUG</cell><cell cols="2">76.27 77.53</cell><cell>MUG  *</cell><cell>60.9</cell><cell>37.4</cell><cell>70.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Ablation for inter-human edges on the 3DPW (MPVPE) and MuPoTS (D%). + means no root-to-root edges.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Table 6. Performance of 3DPW with</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>different 2D pose input. P1-3 represent</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>HRNet-32, HRNet-48, and HRNet-48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>enhanced by DarkPose separately.</cell></row><row><cell>?</cell><cell>0 +</cell><cell>0</cell><cell>100 200 300</cell></row><row><cell cols="4">MPVPE 113.4 109.2 107.0 106.2 115.1</cell><cell>2D Pose P1 P2 P3 GT</cell></row><row><cell>D%</cell><cell cols="3">78.6 89.6 93.2 94.7 90.9</cell><cell>MPVPE 110.3 108.8 106.2 70.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by generous gifts from the Amazon Research Awards program. The project also used computational resources from the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant No. ACI-1548562. The authors appreciate the support and encouragement from Yelin Kim and Adam Fineberg.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. Detailed network structure. The number inside each graph convolution layer represents the feature dimension of the layer output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A The node feature dimension MUG applies the joint set from Human3.6M <ref type="bibr" target="#b10">[11]</ref> for the experiment on Hu-man3.6M, where J = 17. The feature dimension of each joint node is</p><p>ij , (J T ) j in the main text respectively. Following the similar calculation, we can also get the feature dimension of each mesh node is 71. Considering K humans in total and V = 431, the shape of the joint features is K ? 17 ? 71 and the shape of the mesh features is K ? 431 ? 71.</p><p>MUG applies COCO <ref type="bibr" target="#b21">[22]</ref> joint set for the experiments on Panoptic <ref type="bibr" target="#b12">[13]</ref>, MuPoTS-3D <ref type="bibr" target="#b25">[26]</ref> and 3DPW <ref type="bibr" target="#b23">[24]</ref>. Following <ref type="bibr" target="#b3">[4]</ref>, we add root-joint and neck-joint by computing the middle point of left and right hips, and left and right shoulders respectively. We have J = 19 for COCO joint set. The feature dimension of each joint or mesh node is d = 2 + 2 * 19 + 2 + 3 * 11 = 75 ,</p><p>Considering K humans in total and V = 431, the shape of the joint features is K ? 19 ? 75 and the shape of the mesh features is K ? 431 ? 75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed network structure</head><p>The detailed network structure is illustrated in <ref type="figure">Figure 1</ref>. The number inside each graph convolution layer represents the feature dimension of the layer output. We adopt the GCN residual block, which is inspired by <ref type="bibr" target="#b9">[10]</ref>. Our residual block is similar to the basic residual block of <ref type="bibr" target="#b9">[10]</ref>. However, we use Chebyshev spectral graph convolution layers <ref type="bibr" target="#b4">[5]</ref> as the convolutional layers and group normalization as the normalization layers.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="769" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Elements of geometric computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</author>
		<ptr target="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCALCOPIES/FUSIELLO4/tutorial.html" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poseaug: A differentiable pose augmentation framework for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8575" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
		<title level="m">Inductive representation learning on large graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coherent reconstruction of multiple humans from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3334" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review on video-based human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L U</forename><surname>Thuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="131" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3D pose estimation from monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on 3D Vision (3DV)</title>
		<meeting>the International Conference on 3D Vision (3DV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">I2L-MeshNet: Image-to-lixel prediction network for accurate 3D human pose and mesh estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<ptr target="https://pytorch.org" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3D hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to estimate 3D human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating 3d faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="704" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Semantic Web Conference</title>
		<meeting>the European Semantic Web Conference</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Monocular, one-stage, regression of multiple 3d people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11179" to="11188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A computer movie simulating urban growth in the detroit region</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Tobler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic geography</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">sup1</biblScope>
			<biblScope unit="page" from="234" to="240" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bodily expressed emotion understanding research: A multidisciplinary perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Berthouze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Troje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Bodily Expressed Emotion Understanding, in conjunction with the European Computer Vision Conference</title>
		<meeting>the First International Workshop on Bodily Expressed Emotion Understanding, in conjunction with the European Computer Vision Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="733" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalizing monocular 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distilling knowledge from graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7074" to="7083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monocular 3D pose and shape estimation of multiple people in natural scenes-the importance of multiple scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2148" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep network for the integrated 3D sensing of multiple people in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="8410" to="8419" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02467</idno>
		<title level="m">Body meshes as points</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">More qualitative results on MuPoTS. The second to the forth column are the front, top, side view respectively</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
