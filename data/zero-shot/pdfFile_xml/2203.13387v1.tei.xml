<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hassanin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Canberra University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelwahed</forename><surname>Khamiss</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CSIRO</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farid</forename><surname>Boussaid</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Radwan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Canberra University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D human pose estimation can be handled by encoding the geometric dependencies between the body parts and enforcing the kinematic constraints. Recently, Transformer has been adopted to encode the long-range dependencies between the joints in the spatial and temporal domains. While they had shown excellence in long-range dependencies, studies have noted the need for improving the locality of vision Transformers. In this direction, we propose a novel pose estimation Transformer featuring rich representations of body joints critical for capturing subtle changes across frames (i.e., inter-feature representation). Specifically, through two novel interaction modules; Cross-Joint Interaction and Cross-Frame Interaction, the model explicitly encodes the local and global dependencies between the body joints. The proposed architecture achieved state-of-the-art performance on two popular 3D human pose estimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed CrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest counterpart, PoseFormer, using the detected 2D poses and ground-truth settings respectively. 4</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic reconstruction of the 3D human pose from 2D images is a fundamental problem in computer vision. 3D human pose estimation solution provides a geometric representation that is important to many applications including human-computer interaction <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, action understanding <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b13">[14]</ref>, healthcare <ref type="bibr" target="#b0">[1]</ref>, and motion analysis <ref type="bibr" target="#b1">[2]</ref>. The recently developed solutions to this problem can be categorised into two main groups:(1) Two-stage approaches such as <ref type="bibr" target="#b49">[50]</ref> and <ref type="bibr" target="#b3">[4]</ref>, where the input is firstly extracted using 2D human pose estimation (HPE) architectures (for instance, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>), and (2) End-to-end methods <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b33">[34]</ref>, where 3D reconstruction is inferred directly from input images or videos. Owing to the recent advances in the area of 2D pose detectors, the two-stage approaches currently outperform their end-to-end counterparts.</p><p>Despite research spanning decades, 3D human pose estimation remains very challenging. It is an ill-posed problem caused by the ambiguity and high degree of freedom in the input space <ref type="bibr" target="#b37">[38]</ref>. Tackling the problem requires accounting the associated challenges. 2D to 3D pose regression is an under determined problem where many 3D poses  <ref type="bibr" target="#b50">[51]</ref>'s architecture with the proposed cross-feature (joints) and cross-frame interaction modules. might correspond to almost identical 2D projections. In this setting, even the slightest changes in joints positions and appearances can be informative. Given this, the problem calls for two natural requirements for successful regression. First, capturing rich perjoint feature representations to help mitigate the ambiguity and improve the accuracy (R1:cross-feature interaction). A potentially promising direction is leveraging cross-joints features correlations at a detailed level. Second, leveraging information across the Spatio-Temporal steam by attending carefully to features most relevant to the preformed pose (R2:cross-frame interaction). In this work, we draw on the recent advances of Vision Transformer and design an explicit mechanism for meeting those requirements.</p><p>The recent advent of transformers <ref type="bibr" target="#b39">[40]</ref> have progressed many visual recognition tasks. Transformers have been used to encode the long-range relationships between input tokens. As pose estimation is one of the fundamental computer vision problems, it has been approached by many Transformer-based architecture. Yet, one of most notable addition to this line of work is PoseFromer, <ref type="bibr" target="#b50">[51]</ref>. Despite its great success, whose core is ViT <ref type="bibr" target="#b10">[11]</ref>, it inherits some of the ViT limitations pointed out in the literature <ref type="bibr" target="#b24">[25]</ref>, Namely, poor locality. This issue manifests itself in the fact that attention module attends to all tokens. While this design contributed to Transformer excellence in natural language domains, in vision applications it desirable attend to local information. As this limitation can be linked to R2, one can expect an improvement in PoseFormer by addressing it. More superficially, in this paper, we address the following question: is it possible to improve Pose Transformers by improving locality and inter-feature representations?.</p><p>To answer the above question, we propose to integrate locality and rich inter-features interaction (as in <ref type="figure" target="#fig_0">Fig. 1</ref>), while retaining the key advantages of the original PoseFormer <ref type="bibr" target="#b50">[51]</ref> (i.e., capacity to handle large number of tokens and Spatio-Temporal modelling). To this end, we design novel interaction modules to account for the above requirements as follows:</p><p>R1) To capture rich feature that highlights potentially feeble but effective details, we further integrate Bi-linear Pooling <ref type="bibr" target="#b47">[48]</ref> in the locality attention module by modify-ing the cross terms in the attention using outer product. Hence , expanding the attention to all channels (unlike the original inner product that merges information across channels dimension). Bi-linear Pooling was originally motivated by a similar goals of a fine-grained visual classification and has demonstrated success in many applications <ref type="bibr" target="#b46">[47]</ref> from fine-grained recognition to semantic segmentation and video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2)</head><p>There is a growing research that thrives for improving locality of Transformers using various approaches such as local attention <ref type="bibr" target="#b9">[10]</ref> and regional attention maps <ref type="bibr" target="#b4">[5]</ref>. Our work shares the same motivation, albeit using novel methodology. Inspired by nonlocal Neural Networks <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b47">[48]</ref>, we opt for locality attention 5 to leverage the feature representations of the joints across frames. This generalises the vanilla self-attention <ref type="bibr" target="#b42">[43]</ref>, module <ref type="bibr" target="#b40">[41]</ref>, and can be interpreted as seeking a favourable middle ground between the locality-insensitive approaches (vanilla self-attention) and the purely local (stationary convolutional) approaches.</p><p>To summarise, we combine the locality and inter-feature interaction in a transformerbased approach for 3D pose estimation. Our architecture realises the needed requirements and provides two novel cross interaction modules to encode both the local and global dependencies. More specifically, a cross-joint interaction (CJI) module is plugged in the spatial encoder of the Transformer architecture to encode the kinematic constraints between the body parts within a frame. This module (see Sec. 3.2) is composed of depth-wise convolutions followed by group normalization and non-linearity layer (GELU). In addition to the cross-joint interaction module, we also propose a crossframe interaction (CFI) module to handle interactions between the joints across frames. As opposed to PoseFormer <ref type="bibr" target="#b50">[51]</ref>, where the inner-product is used to compute the correlation between frames, the CFI module explicitly learns the correlations between the frames by using the outer-product between feature representations across the frames. This helps in turn model the fine-grained temporal dynamics of the body parts.</p><p>Experiments were performed on Human3.6 <ref type="bibr" target="#b18">[19]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b30">[31]</ref> datasets. Reported results demonstrate the superiority of proposed method over the state-of-theart. Moreover, qualitative comparisons show that our method is efficient in capturing hardly-visible body parts.</p><p>The main contributions of this paper are :</p><p>-A cross-joint interaction module CJI for spatial transformer architectures to encode the kinematic dependencies between body joints while taking into account the local connections of each joint.</p><p>-A cross-frame interaction module CFI for temporal transformer architectures to capture the explicit correlations between body joints across frames.</p><p>-State-of-the-art performance achieved on two popular benchmark datasets; Hu-man3.6 and MPI-INF-3DHP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D Human pose estimation methods are commonly used as the second stage to 2D detection methods. First, the input image is passed to detection frameworks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b7">[8]</ref> to infer 2D poses. Then, 2D poses are lifted to 3D using other methods <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Martinez et al. <ref type="bibr" target="#b29">[30]</ref> used a fully-connected residual network to predict the 3D poses. Fang et al. <ref type="bibr" target="#b11">[12]</ref> lifted to 3D poses using a grammar model for body joints configuration. Several other methods used the temporal information to overcome the occlusion in the input images <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Pavllo et al. <ref type="bibr" target="#b36">[37]</ref> proposed a dilated temporal convolution approach to exploit the temporal information. Cai et al. <ref type="bibr" target="#b3">[4]</ref> used a graph method to choose the center frame and then refine the final estimated 3D pose. Wang et al. <ref type="bibr" target="#b41">[42]</ref> customized graph convolution network (GCN) in a U-shape as they involved motion modeling to learn the 3D poses. In <ref type="bibr" target="#b49">[50]</ref>, a variant of a non-local module is customized to include the semantics of the input images.</p><p>Recently, vision transformers advanced all the visual recognition tasks <ref type="bibr" target="#b39">[40]</ref>. Following <ref type="bibr" target="#b10">[11]</ref>, transformer has been used to lift 2D poses to the corresponding 3D poses. In <ref type="bibr" target="#b26">[27]</ref>, Lin et al. used convolutions and transformer together without temporal information to predict 3D poses. In order to eliminate the redundancy in the sequence with temporal information, Li et al. <ref type="bibr" target="#b23">[24]</ref> proposed a strided transformer network. In <ref type="bibr" target="#b50">[51]</ref>, spatial-temporal transformer is used for 3D HPE tasks. Using transformers in HPE showed significant improvement overall. However, a pre-training on a large dataset is required to learn more representative and effective representations for the sequence HPE data. Our proposed method is different from the previous methods in leveraging the cross-interaction between the joints of the body parts in the spatial and the temporal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>This section presents the proposed architecture to estimate 3D human pose from 2D. Inspired by recently developed transformer approach, namely, Poseformer <ref type="bibr" target="#b50">[51]</ref>, we propose interaction modules inside the spatial and temporal encoders to make the transformer more efficient when lifting the 2D to 3D poses. The 2D input poses can be inferred from any 2D pose detection approach such as <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The poses of the consecutive frames in an input video are concatenated to form the input to the proposed architecture. Suppose, {x 1 , x 2 , ..., x N } denotes the set of the 2D input frames, where x i ? R J?2 is composed of the 2D positions of the body joints for frame i, N is the total number of frames in the input video and J is the number of joints. The output of each frame is the 3D body joints, y ? R J?3 . The proposed architecture incorporates crossinteraction modules into a vanilla spatial and temporal transformer <ref type="bibr" target="#b10">[11]</ref>. Incorporating these modules with the transformer helps to capture both long-range relationships and local interactions between the body joints and across frames in both the spatial and temporal domains, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformers</head><p>Despite the great success achieved by transformers in computer vision tasks <ref type="bibr" target="#b10">[11]</ref>, they focus mostly on global dependencies between frames in the input sequence (as observed by <ref type="bibr" target="#b50">[51]</ref>). Motivated by this, we augment the proposed transformer with modules to capture more joints-related context information within-frame and corss-frames. Below we formulate our problem and review vision Transformer.</p><p>The input corresponds to the 2D poses in subsequent frames {x i ? R J?2 |i = 1, ..., N }. The initial layer of the transformer embeds the high dimensional features of each patch and their positional information. This step is called patch embedding, and is achieved by the following projection operation:</p><formula xml:id="formula_0">Z 0 = [x 1 E; x 2 E; ..., x P E], E ? R (J?2)?D , Z 0 ? R P ?D<label>(1)</label></formula><p>where D is the embedding dimension, P is the number of patches that is equivalent to the number of input body joints in the case of spatial processing and to the number of frames in the case of temporal processing. The output of the patch embedding step, Z 0 , is then passed into the most important step of the transformer, i.e. the self-attention. It implicitly correlates the input patches in the form of attention scores. This step is mainly based on mapping three learnable weight matrices Q, K, V ? R P ?D to attend for the output features.</p><p>The self-attention operation is computed as a scaled dot-product between these matrices as follows:</p><formula xml:id="formula_1">A(Q, K , V ) = Softmax( Q.K T ? P ).V<label>(2)</label></formula><p>This attention operation is applied through a multi-head attention (MHA), which combines various representations with different positions in parallel. The multi-head atten-tion operation is simply achieved by concatenating all heads as follows:</p><formula xml:id="formula_2">MHA = Concat(A j (.)).W, j ? 1, ..., H<label>(3)</label></formula><p>where W is a learnable weight matrix and H is the number of heads. The self-attention module is then combined with other layers such as layer normalisation <ref type="bibr" target="#b43">[44]</ref> and multi-layer perceptron (MLP). The steps in a transformer layer (l), which are following the patch embedding step can be listed, in general, as follows:</p><formula xml:id="formula_3">Z l = MHA(LN (Z l?1 )) + Z l?1 , Z l = MLP (LN (Z l )) + Z l , Z l = LN (Z l ), where l = 1, 2, ..., L<label>(4)</label></formula><p>where LN (.) represents the layer normalization and L is the layer indicator of the transformer. Both the spatial and temporal parts of the transformer consist of identical layers. However, the output feature space for the spatial encoders ? R J?D and for the temporal encoders is in R F ?D , where F is number of frames in the input patch. The final layer is another linear projection step that maps the output space to R J?3 for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Spatial Interaction</head><p>The spatial encoders of the transformer learn the relationships between the body joints within the frame. The input is represented as x ? R J?2 , where each joint is considered as an independent patch, and the output is the feature representation of each joint with respect to the other joints. Inspired by <ref type="bibr" target="#b50">[51]</ref>, the 2D coordinates of each joint are firstly transformed using a linear operation. The output of this step, Z 0 ? R P ?D is then passed forward to compute the self-attention scores. This encodes the dependencies between the different joints. However, these operations disregard the low-score relationships. This is due to the non-local nature of the transformer operations listed in Eq. 4.</p><p>In order to tackle this issue, we propose the Cross-Joints Interaction (CJI) module that we integrate inside the spatial encoders with an aim to achieve two characteristics; 1) getting the transformer to consider the locality nature of the human body parts and their local interactions the same as encoding their non-local interactions (i.e. long-range dependencies), and 2) explicitly encoding the interaction between the joints of the body parts across the channels, which enriches the representation of the joints with low attention scores. Achieving these two characteristics improves the scores of the Multi-Head Attention for the 3D human pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Joints Interaction (CJI) Module</head><p>This module is inserted between the MHA layer and MLP for each block. The CJI module consists of two depth-wise convolutions with kernel size 5, group normalization and non-linearity GELU. Also, the residual connection is added to the output of the module to avoid overfitting. The operations within the CJI module are performed using the sequence of the following operations on outpout of the patch embedding step Z 0 :</p><formula xml:id="formula_4">Z = CONV (GN (GELU (CONV (Z )))) + Z (5)</formula><p>where GELU refers to the non-linear layer in <ref type="bibr" target="#b15">[16]</ref>, CON V is the standard convolution layer with kernel 5 and GN indicates the group normalization used in <ref type="bibr" target="#b43">[44]</ref>. Since the focus of the CJI module is on the cross-interaction between the joints, the output of the MHA part in Eq. 4 has been transposed. That is, it becomes Z 0 ? R D?P . The spatial encoders for a transformer layer l can then be represented by the following list of operations:</p><formula xml:id="formula_5">Z l = MHA(LN (Z l?1 )) + Z l?1 , Z l = CONV (GN (GELU (CONV (Z l )))) + Z l Z l = MLP (LN (Z l )) + Z l Z l = LN (Z l ),<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Interaction</head><p>In contrast to the spatial encoders, which encode the long-range dependencies between the joints within each frame, the temporal counterparts aim at learning rich representations across frames. These encoders are stacked on top of the spatial ones. Their input is firstly flattened from R J?D to R 1?(J?D) leading to R F ?(J?D) for all frames in a patch. Similar to the vanilla transformers <ref type="bibr" target="#b39">[40]</ref>, the temporal positional information are added to the input space. Apart from that, the remaining settings including self-attention modules and MLP blocks are just as in the case of spatial encoders. The input size of the transformer is maintained all over the transformers layers, which is R F ?(J?D) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Frames Interaction (CFI) Module</head><p>The whole attention in temporal encoders is based on the dependencies between the channels (i.e., R f ?(J?D) ), where the explicit interaction between the frames is disregarded. Depending on the scores produced by SoftMax calculation, joints with low scores might be ignored in the process. For example, partially visible or occluded joint won't be properly represented and reflected in the 3D space. In order to resolve this problem, we propose a Cross-Frame Interaction Module, CFI, to explicitly encode the relationships between the same joint across frames using Bi-Linear Pooling operations <ref type="bibr" target="#b47">[48]</ref> instead of the traditional SoftMax. This helps in learning the correlations between the channels explicitly as in <ref type="bibr" target="#b17">[18]</ref> and reflects the the kinematic constraints on the output space.</p><p>Briefly, the Bi-Linear Pooling learns pairwise feature correlations using the outer product. Each element of the correlation matrix C ij = F Z i Z j is a Bi-Linear product of the corresponding embedded features of frames i and j and then it is sum-pooled, where Z i ? R J?D is the input feature of frame i. More precisely, the input is transformed by combining the positional information with the frames where Z ? R F ?(J?D) and then using convolutions we extract K, Q, and V such that:</p><formula xml:id="formula_6">K = ZW k , Q = ZW q , V = ZW v<label>(7)</label></formula><p>The bilinear matrix multiplication is performed between matrices Q, K and V as follows:</p><formula xml:id="formula_7">C = K ? Q ? R F ?F Z = C ? V ? R D?F<label>(8)</label></formula><p>where ? refers to the bilinear pooling operation. Then, the output is added to the input after performing convolution and group normalization. Compared to self-attention modules in <ref type="bibr">Equation 4</ref>, CFI module uses a bilinear pooling to learn pairwise interactions between the same joint across different frames. This highlights the discriminativeness of each frame which leads to rich representation. For example, one frame will focus on the top joints of the body and another on the lower part, while CFI will focus on combining both parts. Finally, CFI is merged with temporal transformer between MHA layer and MLP blocks. The updated sequence of the temporal transformer operations for a layer l can be listed as follows:</p><formula xml:id="formula_8">Z l = MHA(LN (Z l?1 )) + Z l?1 , K = Z l W k , Q = Z l Wq, V = Z l Wv Z l = GN (CONV (((K ? Q) ? V ))) + Z l Z l = MLP (LN (Z l )) + Z l Z l = LN (Z l ),<label>(9)</label></formula><p>The sequence of transformer encoders is combined in a compact form, which enables an end-to-end training. Moreover, following the vanilla transformers [51] on using the three embedding matrices with the input i.e., Q, K, and V allows CJI and CFI to serve as generalised modules, which can be plugged in many other transformer architectures for other various visual recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Regression Head</head><p>The spatial and temporal transformers are stacked together as in <ref type="figure" target="#fig_1">Fig. 2</ref>, where their input is passed to the spatial encoders and then to the temporal ones. The output of the temporal transformer is R F ?(J?D) , which requires to be reduced to R 1?(J?3) . First, 1D convolution is applied as a weighted average for the frames to transform to R 1?(J?D) . Then, a linear layer is used to learn the 3D geometries from the D dimension followed by the normlisation layer. The final output is the estimated 3D position for each joint R 1?(J?3) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss function</head><p>Following the recent work in <ref type="bibr" target="#b36">[37]</ref>, the MPJPE loss function is employed to optimise the parameters of the whole architecture:</p><formula xml:id="formula_9">L = 1 J J k=1 ?g k ? p k ? 2 ,<label>(10)</label></formula><p>where g k represents the ground-truth 3D joint position of joint k and p k is the 3D output of the proposed architecture of the k-th joint. <ref type="table">Table 1</ref>: Comparison between our proposed method and the state-of-the art approaches for 3D human pose estimation. Mean Per Joint Position Error(MPJPE) is used to measure the mean error between the estimated 3D pose and the ground truth 3D pose on Human3.6M under Protocols 1&amp;2 where 2D pose detection is used as input. The top shows results of Protocol 1 (MPJPE), whereas the bottom part shows the results of Protocol 2 (P-MPJPE). f refers to the number of frames used in each method, * denotes that the input 2D pose detection method used is the cascaded pyramid network (CPN), and ? refers to a transformer-based model. (Red: best; Blue: second best) <ref type="bibr">Protocol</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we provide empirical experiments to show the significance of our proposed method. First, we describe the used datasets, the evaluation criteria and protocols. Then, further experiments are conducted along with ablation studies. Finally, we provide the comparisons with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Protocols</head><p>Datasets Our experiments are evaluated on the most popular datasets for HPE tasks, Human3.6 <ref type="bibr" target="#b18">[19]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b30">[31]</ref>. Human3.6 dataset is the most popular dataset and the largest one for HPE. It consists of 3.6 million images in the form of video frames. It includes seven subjects and a total of 15 actions including "walking", "sitting" and "posing". Each video is captured from 4 different views. 3D annotations are provided by an accurate marker-based motion capture. The subjects are split for training and testing as in <ref type="bibr" target="#b36">[37]</ref>, where S1, S5, S6, S7and S8 are used for training and S1 and S11 for testing. One model is used to train all the frames for the various actions. All the videos are recorded in indoor scenes. MPI-INF-3DHP dataset contains 8 actions from 14 different views which result to input GT Poseformer CrossFormer input GT Poseformer CrossFormer <ref type="figure">Fig. 3</ref>: Visual qualitative comparison of the proposed method (CrossFormer), the ground-truth and the state-of-the art approach (PoseFormer) <ref type="bibr" target="#b50">[51]</ref>. The experiments are conducted on Human3.6M test set S11 with the "SittingDown" action. The blue arrows on the ground-truth highlight the locations where our method clearly behaves better.</p><p>in diverse poses. It contains indoor and outdoor complex scenes and thus it is more challenging for HPE tasks. The test set includes 6 various subjects. Without stated, the settings are following <ref type="bibr" target="#b35">[36]</ref>. The scenarios of test set are: studio with a Green Screen (GS), studio without Green Screen (noGS) and outdoor scene (Outdoor). Evaluation Protocols: Human3.6 dataset evaluation for 3D pose estimation performance relative to the 3D ground-truth follow <ref type="bibr" target="#b36">[37]</ref> using the most common metrics, i.e. MPJPE and P-MPJPE. MPJPE (Mean Per Joint Position Error) refers to the average of Euclidean distance in millimeters between the predicted 3D human-body joints and the ground-truth ones. It is also denoted by Protocol 1. For P-MPJPE, it calculates the Euclidean distance between the 3D predicted pose after rigid alignment and the ground-truth. It is referred to Protocol 2 as it is more robust to prediction failure of the joint individuals.</p><p>For MPI-INF-3DHP, Protocol 1 is used, Area Under Curve (AUC), and Percentage of Correct Keypoint (PCK) within the 150mm as defined in <ref type="bibr" target="#b30">[31]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We used Pytorch <ref type="bibr" target="#b34">[35]</ref> to implement our proposed method. Two Tesla A100 GPU 40 GB each were used to run the experiments. Adam optimizer <ref type="bibr" target="#b19">[20]</ref> is chosen to train the model for 100 epochs, decaying with 10%. Another decaying schedule is used with initial learning rate of 0.0001 and an exponential decaying factor of 0.99 after each epoch, whereas the batch size is 512. We follow <ref type="bibr" target="#b36">[37]</ref> for the selection of the 2D pose detector, which is the cascaded pyramid network (CPN) <ref type="bibr" target="#b7">[8]</ref> on Human3.6 dataset. For MPI-INF-3DHP dataset, 2D ground-truth poses are used as in <ref type="bibr" target="#b36">[37]</ref>. Both of the spatial and temporal transformers have 4 layers, and the multi-head attention has 8 heads. The dimension of the features is 32 for spatial transformer and 544 for the temporal one. The receptive fields are 9, 27 and 81. Horizontal flip augmentation is used for the training and testing stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the state-of-the art</head><p>Human3.6 In this part, we compare the proposed method with the state-of-the art methods on Human3.6 dataset. 15 actions have been selected from two subjects, S9 and S11, for the evaluation. In order to guarantee fair comparisons, the input is taken from CPN in the form of 2D keypoints for training and testing. <ref type="table">Table 1</ref> shows the comparison of the SOTA methods with the proposed method (81 frames). Overall, our method achieves the state-of-the art on Human3.6 on all the metrics and it outperforms the state-of-the art (Chen et al. <ref type="bibr" target="#b5">[6]</ref>) with a considerable margin of 0.9%, 1.3% for Protocols 1 and 2, respectively. It is worth noting that the across-joint modules in the spatial and temporal cases are crucial to infer the body-joints dependencies. Comparing the proposed method with PoseFormer (with no pre-training used) shows the significance of the across-joint correlation modules. Our method outperforms with a large margin of 2% the SOTA. In terms of accuracy, we achieve 1% better than the second best accuracy. Additionally, the proposed method achieves the best performance amongst all the compared methods in protocol 2 in <ref type="table">Table 1</ref> (bottom). In some selected difficult poses such as walk together, walk, smoke, where the poses change very quickly, the proposed method showed a significant improvement ranging from 1.1% to 2.5% over the baseline. This highlights the ability of our method to encode the long-range interactions between the body-joints.</p><p>Considering the pre-trained baseline, the proposed method achieves better performance for all the actions. These results show the importance of plugging the across-joints modules in the transformers. Further experiments on Human3.6 using ground-truth 2D poses as input have also been performed. This shows the power of the proposed method where there is no noise in the input as in the previous case. <ref type="table" target="#tab_1">Table 2</ref> shows the comparisons of our method and the baselines. Overall, the proposed method achieved the best performance amongst the baselines. It achieved 28.3% MPJPE, whereas the second best approach achieves 31.3 with gain of 3%. The proposed method outperforms the baselines in all the actions with a considerable improvement range from 2.4% as the minimum difference, and 4.8% for the largest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP</head><p>We further compare the proposed methods (CrossFormer) to previous ones on MPP-INF-3DHP using 9 frames. This is important because it illustrates the ability of the proposed method to train with fewer training samples in an outdoor settings. As <ref type="table" target="#tab_2">Table 3</ref> shows, our method obtains the best performance amongst the compared ones w.r.t. the metrics. Computational Complexity Analysis: <ref type="table" target="#tab_3">Table 4</ref> shows the comparison with different methods of complexity analysis. For the number of parameter analysis, it is relatively bigger than Poseformer in the three settings, it increases slightly. However, this increase is negligible in favor of accuracy gains. Apart from Poseformer, the number of parameters is still competitive to the other methods. It is also noticeable that the increase of frames does not translate in ab increase in the total number of parameters (only by hundreds). Regarding the FLOPs, the proposed method is not the best in comparison to the comparable methods. However, it only exhibits a slight increase over Poseformer. For frames per second (FPS), our method shows a slightly lower number compared to PoseFormer.</p><p>Qualitative Results. In order to show the superiority of the proposed method qualitatively, we compare it with the ground-truth and Poseformer (the state-of-the art approach <ref type="bibr" target="#b50">[51]</ref>). The evaluation is conducted on Human3.6 dataset S11 test set on "Setting-Down" action. From <ref type="figure">Fig. 3</ref>, it is clear that the proposed method is considerably better than Poseformer. We use blue arrows to define the locations where our method behaves better. While our method shows some failures, it is still overall better than Poseformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In order to check the impact of the the proposed method individually, we perform empirical experiments on Human3.6 dataset using protocol 1. Also, one experiment is conducted on the optimal hyper-parameters selection. The impact of cross-joints modules In this ablation study, we investigate the contribution of Cross-Joint Interaction (CJI) on the spatial transformer and on the whole network. We also study the impact of CJI with spatial embedding since it considers inductive bias implicitly and without. Regarding Cross-Frame Interaction (CFI) module, we verify its impact on the network independently with temporal embedding and without. For a fair comparison, we use the optimal parameter settings, including 4 blocks for both transformers. The dimension of the keypoints is unified to 32 for the spatial and 544 for the temporal. <ref type="table">Table 5</ref> shows the results of various settings between CJI and CFI. The results illustrate that using both of the cross-joint modules improve the total performance significantly (from 49.9 to 48.5). <ref type="table">Table 5</ref> (row 4) discusses using CJI module along with both embeddings and without CFI. It achieved better performance than Poseformer (from 49.9 to 49.3). This explains the need of both cross-joint modules to add locality to the transformers. Similarly, using CFI only along with the embeddings achieved better performance compared to PoseFormer. However, it achieved lower performance than CJI. It is clear that using cross-joint modules without the spatial information embeddings shows no difference in terms of performance, which proves our claim of using CJI and CFI modules to add to the locality of the self-attention modules. However, CJI is more independent than CFI to positional information as it shows the same accuracy without the positional information, whereas CFI obtains 0.04% less. Another ablation study (a table is included in the supplementary) is conducted to verify the importance of the spatial and temporal transformers. This shows the various settings of the proposed network and provides comparisons to Poseformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Two interaction modules have been proposed to resolve the issues of using the spatial and temporal transformers for 3D human pose estimation. The first module, cross-joint interaction (CJI), has been presented to resolve the locality issue of the spatial transformers, while the second module, cross-frame interaction (CFI), has been developed to encode the dependencies of the joints across the subsequent frames. Both of two modules have been incorporated into transformer architecture, CrossFormer, and validated on popular 3D pose datasets. The proposed method has achieved new SOTA results</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Conceptual Illustration of CrossFromer Architecture. This augments the PoseFormer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The proposed architecture is composed of two main modules: a spatial transformer along with the Cross-Joint Interaction module (CJI), and a temporal transformer with the proposed Cross-Frame Interaction (CFI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hossain et al. [] ECCV'18 35.7 39.3 44.6 43.0 47.2 54.0 38.3 37.5 51.6 61.3 46.5 41.4 47.3 34.2 39.4 44.1 Cai et al. [4] (f = 7) ICCV'19 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 32.3 39.0 Lin et al. [26] (f = 50) BMVC'19 32.5 35.3 34.3 36.2 37.8 43.0 33.0 32.2 45.7 51.8 38.4 32.8 37.5 25.8 28.9 36.8 Pavllo et al. [37] (f = 243)* CVPR'19 34.1 36.1 34.4 37.2 36.4 42.2 34.4 33.6 45.0 52.5 37.4 33.8 37.8 25.6 27.3 36.5 Liu et al. [29] (f = 243)* CVPR'20 32.3 35.2 33.3 35.8 35.9 41.5 33.2 32.7 44.6 50.9 37.0 32.4 37.0 25.2 27.2 35.6 UGCN [42] (f = 96) ECCV'20 32.9 35.2 35.6 34.4 36.4 42.7 31.2 32.5 45.6 50.2 37.3 32.8 36.3 26.0 23.9 35.5 Chen et al. [6] (f = 81)* TCSVT'21 33.1 35.3 33.4 35.9 36.1 41.7 32.8 33.3 42.6 49.4 37.0 32.7 36.5 25.5 27.9 35.6 PoseFormer (no PT) [51] (f=81) ICCV'21 33.5 35.6 33.5 35.6 36.1 40.4 32.8 32.5 43.5 49.3 35.4 33.2 36.3 25.3 26.6</figDesc><table><row><cell>1</cell><cell cols="16">Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Average</cell></row><row><cell>Debral et al. [9]</cell><cell cols="15">ECCV'18 44.8 50.4 44.7 49.0 52.9 61.4 43.5 45.5 63.1 87.3 51.7 48.5 52.2 37.6 41.9</cell><cell>52.1</cell></row><row><cell>Cai et al. [4] (f = 7)</cell><cell cols="15">ICCV'19 44.6 47.4 45.6 48.8 50.8 59.0 47.2 43.9 57.9 61.9 49.7 46.6 51.3 37.1 39.4</cell><cell>48.8</cell></row><row><cell>Pavllo et al. [37] (f = 243)*</cell><cell cols="15">CVPR'19 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9</cell><cell>46.8</cell></row><row><cell>Lin et al. [26](f = 50)</cell><cell cols="15">BMVC'19 42.5 44.8 42.6 44.2 48.5 57.1 52.6 41.4 56.5 64.5 47.4 43.0 48.1 33.0 35.1</cell><cell>46.6</cell></row><row><cell>Yeh et al. [46]</cell><cell cols="15">NIPS'19 44.8 46.1 43.3 46.4 49.0 55.2 44.6 44.0 58.3 62.7 47.1 43.9 48.6 32.7 33.3</cell><cell>46.7</cell></row><row><cell>Liu et al. [29] (f = 243) *</cell><cell cols="15">CVPR'20 41.8 44.8 41.1 44.9 47.4 54.1 43.4 42.2 56.2 63.6 45.3 43.5 45.3 31.3 32.2</cell><cell>45.1</cell></row><row><cell>SRNet [49] *</cell><cell cols="15">ECCV'20 46.6 47.1 43.9 41.6 45.8 49.6 46.5 40.0 53.4 61.1 46.1 42.6 43.1 31.5 32.6</cell><cell>44.8</cell></row><row><cell>UGCN [42] (f = 96)</cell><cell cols="15">ECCV'20 41.3 43.9 44.0 42.2 48.0 57.1 42.2 43.2 57.3 61.3 47.0 43.5 47.0 32.6 31.8</cell><cell>45.6</cell></row><row><cell>METRO [27] (f = 1)  ?</cell><cell>CVPR'21 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.0</cell></row><row><cell>PoseFormer (no PT) [51] (81)</cell><cell cols="15">ICCV'21 43.0 46.5 41.4 44.1 48.1 53.2 43.7 43.6 54.9 62.3 47.1 44.9 47.7 32.8 33.5</cell><cell>45.7</cell></row><row><cell>Chen et al. [6] (f = 81)*</cell><cell cols="15">TCSVT'21 42.1 43.8 41.0 43.8 46.1 53.5 42.4 43.1 53.9 60.5 45.7 42.1 46.2 32.2 33.8</cell><cell>44.6</cell></row><row><cell>PoseFormer (PT)[51] (81)</cell><cell cols="15">ICCV'21 41.5 44.8 39.8 42.5 46.5 51.6 42.1 42.0 53.3 60.7 45.5 43.3 46.1 31.8 32.2</cell><cell>44.3</cell></row><row><cell>CrossFormer (81)</cell><cell cols="15">40.7 44.1 40.8 41.5 45.8 52.8 41.2 40.8 55.3 61.9 44.9 41.8 44.6 29.2 31.1</cell><cell>43.7</cell></row><row><cell>Protocol 2</cell><cell cols="16">Dir. Disc. Eat. Greet Phone Photo Pose Purch. Sit SitD. Somke Wait WalkD. Walk WalkT. Average</cell></row><row><cell>Pavlakos et al. [36]</cell><cell cols="15">CVPR'18 34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8 42.6 39.6 43.9 32.1 36.5</cell><cell>41.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.3</cell></row><row><cell>PoseFormer [51](f=81)</cell><cell cols="15">ICCV'21 32.5 34.8 32.6 34.6 35.3 39.5 32.1 32.0 42.8 48.5 34.8 32.4 35.3 24.5 26.0</cell><cell>34.6</cell></row><row><cell>CrossFormer (f=81)</cell><cell cols="15">31.4 34.6 32.6 33.7 34.3 39.7 31.6 31.0 44.3 49.3 35.9 31.3 34.4 23.4 25.5</cell><cell>34.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between the estimated 3D pose of the proposed method and the ground truth 3D pose on Human3.6M dataset using the Mean Per Joint Position Error under Protocol 1 (MPJPE). All methods use the ground truth 2D pose as input.DiscEat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg. Martinez et al. [30] ICCV'17 37.7 44.4 40.3 42.1 48.2 54.9 44.4 42.1 54.6 58.0 45.1 46.4 47.6 36.4 40.4 45.5 Lee et al. [22] ECCV'18 32.1 36.6 34.3 37.8 44.5 49.9 40.9 36.2 44.1 45.6 35.3 35.9 30.3 37.6 35.5 38.4 Pavllo et al. [37] CVPR'19 35.2 40.2 32.7 35.7 38.2 45.5 40.6 36.1 48.8 47.3 37.8 39.7 38.7 27.8 29.5 37.8 Cai et al. [4] f = 243 ICCV'19 32.9 38.7 32.9 37.0 37.3 44.8 38.7 36.1 41.0 45.6 36.8 37.7 37.7 29.5 31.6 37.2 Xu et al. [45] CVPR'21 35.8 38.1 31.0 35.3 35.8 43.2 37.3 31.7 38.4 45.5 35.4 36.7 36.8 27.9 30.7 35.8 Liu et al. [29] (f=243) CVPR'20 34.5 37.1 33.6 34.2 32.9 37.1 39.6 35.8 40.7 41.4 33.0 33.8 33.0 26.6 26.9 34.7 Chen et al. ICCV '21 30.0 33.6 29.9 31.0 30.2 33.3 34.8 31.4 37.8 38.6 31.7 31.5 29.0 23.3 23.1 31.3 CrossFormer (f=81) 26.0 30.0 26.8 26.2 28.0 31.0 30.4 29.6 35.4 37.1 28.4 27.3 26.7 20.5 19.9 28.3</figDesc><table><row><cell>(Red:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between the proposed method (CrossFormer) and previous SOTA methods on MPI-INF-3DHP. The metrics of the comparison are the Percentage of Correct Keypoints (PCK) and Area Under the Curve (AUC). The best scores are marked in bold PCK ? AUC ? MPJPE ?</figDesc><table><row><cell>Mehta et al. [32]</cell><cell>3DV'17</cell><cell>75.7 39.3</cell><cell>117.6</cell></row><row><cell>Mehta et al. [33]</cell><cell cols="2">ACM ToG'17 76.6 40.4</cell><cell>124.7</cell></row><row><cell cols="2">Pavllo et al. [37] (f = 81) CVPR'19</cell><cell>86.0 51.9</cell><cell>84.0</cell></row><row><cell cols="2">Lin et al. [26] (f = 25) BMVC'19</cell><cell>83.6 51.4</cell><cell>79.8</cell></row><row><cell>Li et al. [23]</cell><cell>CVPR'20</cell><cell>81.2 46.1</cell><cell>99.7</cell></row><row><cell>Chen et al. [6]</cell><cell>TCSVT'21</cell><cell>87.9 54.0</cell><cell>78.8</cell></row><row><cell cols="2">PoseFormer [51] (f = 9) ICCV'21</cell><cell>88.6 56.4</cell><cell>77.1</cell></row><row><cell>CrossFormer (f=9)</cell><cell></cell><cell>89.1 57.5</cell><cell>76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison between the proposed method and a set of previous methods in terms of the comparison are computational complexity, number of the parameters, MPJPE, and Frames Per Second (FPS). The experiments are conducted on Human3.6M under Protocol 1 with the detected 2D pose as input.</figDesc><table><row><cell></cell><cell cols="5">f Parameters (M) FLOPs (M) MPJPE FPS</cell></row><row><cell cols="2">Hossain and Little [17] -</cell><cell>16.95</cell><cell>33.88</cell><cell>58.3</cell><cell>-</cell></row><row><cell>Chen et al. [6]</cell><cell>27</cell><cell>31.88</cell><cell>61.7</cell><cell cols="2">45.3 410</cell></row><row><cell>Chen et al. [6]</cell><cell>81</cell><cell>45.53</cell><cell>88.9</cell><cell cols="2">44.6 315</cell></row><row><cell>Chen et al. [6]</cell><cell>243</cell><cell>59.18</cell><cell>116</cell><cell cols="2">44.1 264</cell></row><row><cell>PoseFormer [51]</cell><cell>9</cell><cell>9.58</cell><cell>150</cell><cell cols="2">49.9 320</cell></row><row><cell>PoseFormer [51]</cell><cell>27</cell><cell>9.59</cell><cell>452</cell><cell cols="2">47.0 297</cell></row><row><cell>PoseFormer [51]</cell><cell>81</cell><cell>9.60</cell><cell>1358</cell><cell cols="2">44.3 269</cell></row><row><cell>CrossFormer</cell><cell>9</cell><cell>9.93</cell><cell>163</cell><cell cols="2">48.5 284</cell></row><row><cell>CrossFormer</cell><cell>27</cell><cell>9.93</cell><cell>515</cell><cell cols="2">46.5 266</cell></row><row><cell>CrossFormer</cell><cell>81</cell><cell>9.93</cell><cell>1739</cell><cell cols="2">43.8 241</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Codes and models will be publicly available on github.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">originally named "non-local operation"<ref type="bibr" target="#b42">[43]</ref> after the "non-local" mean operation<ref type="bibr" target="#b2">[3]</ref> and to set it apart for the local convolution. Here, we use "locality attention" to signify its role in our architecture and avoid confusion.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for both datasets. In the future, we will test on other visual recognition applications to ensure its generalisation to different visual tasks.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision-based action understanding for assistive healthcare: A short review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anindya</forename><surname>Md Atiqur Rahman Ahad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Das Antar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shahid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-based mouth motion analysis in epilepsy: A 3d perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahmedt-Aristizabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Saquib Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Dionisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1625" to="1629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Regionvit: Regional-to-local attention for vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02689</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anatomyaware 3d human pose estimation with bone-based pose decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="668" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ap-cnn: Weakly supervised attention pyramid convolutional neural network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoguo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2826" to="2836" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual affordance and function understanding: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hassanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Tahtali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning discriminative representations for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hassanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Tahtali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="page">103448</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Real-time vision for humancomputer interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Branislav Kisacanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungoh</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inwoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="119" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploiting temporal contexts with strided transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14304</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trajectory space factorization for deep video-based 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BVMC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rgb-d sensing based human action and interaction analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojie</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghai</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen-Ching</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Asari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5064" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vnect: Realtime 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="752" to="768" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII 16</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Monocular image 3d human pose estimation under self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1888" to="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical adversarial network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nour</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Keating</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Kwang Raymond</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="103619" to="103628" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="764" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16105" to="16114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chirality nets for human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8163" to="8173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast and compact bilinear pooling by shifted random maclaurin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3243" to="3251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Compact generalized non-local network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6511" to="6520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taojiannan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10455</idno>
		<title level="m">Chen Chen, and Zhengming Ding. 3d human pose estimation with spatial and temporal transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
