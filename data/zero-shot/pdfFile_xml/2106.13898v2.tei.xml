<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Closed-form Continuous-time Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-02">2 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
							<email>rhasani@mit.edu.</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology (MIT)</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Science and Technology Austria (IST Austria)</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology (MIT)</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Liebenwein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology (MIT)</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology (MIT)</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tschaikowski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Teschl</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Vienna (Uni Wien)</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology (MIT)</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Closed-form Continuous-time Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-02">2 Mar 2022</date>
						</imprint>
					</monogr>
					<note>These authors contributed equally to the paper * To whom correspondence should be addressed;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tion of an integral appearing in LTCs' dynamics, that has had no known closed-form solution so far. This closed-form solution substantially impacts the design of continuous-time and continuous-depth neural models; for instance, since time appears explicitly in closed-form, the formulation relaxes the need for complex numerical solvers. Consequently, we obtain models that are between one and five orders of magnitude faster in training and inference compared to differential equation-based counterparts. More importantly, in contrast to ODE-based continuous networks, closed-form networks can scale remarkably well compared to other deep learning instances. Lastly, as these models are derived from liquid networks, they show remarkable performance in time series modeling, compared to advanced recurrent models. One Sentence Summary: We find an approximate closed-form solution for the interaction of neurons and synapses and build a strong artificial neural network model out of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Text:</head><p>Continuous neural network architectures built by ordinary differential equations (ODEs)</p><p>(2) opened a new paradigm for obtaining expressive and performant neural models.</p><p>These models transform the depth dimension of static neural networks and the time dimension of recurrent neural networks into a continuous vector field, enabling parameter sharing, adaptive computations, and function approximation for non-uniformly sampled data.</p><p>These continuous-depth (time) models have shown promise in density estimation applications (3-6), as well as modeling sequential and irregularly-sampled data (1,7-9).</p><p>While ODE-based neural networks with careful memory and gradient propaga-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To this end, we compute a tightly-bounded approximation of the solu-tion design <ref type="bibr" target="#b8">(9)</ref> perform competitively with advanced discretized recurrent models on relatively small benchmarks, their training and inference are slow due to the use of advanced numerical DE solvers <ref type="bibr" target="#b9">(10)</ref>. This becomes even more troublesome as the complexity of the data, task and state-space increases (i.e., requiring more precision) <ref type="bibr" target="#b10">(11)</ref>, for instance, in open-world problems such as medical data processing, self-driving cars, financial time-series, and physics simulations.</p><p>The research community has developed solutions for resolving this computational overhead and for facilitating the training of neural ODEs, for instance, by relaxing the stiffness of a flow by state augmentation techniques <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b11">12)</ref>, reformulating the forwardpass as a root-finding problem <ref type="bibr" target="#b12">(13)</ref>, using regularization schemes <ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref>, or improving the inference time of the network <ref type="bibr" target="#b16">(17)</ref>.</p><p>In this paper, we take a step back and propose a fundamental solution: we derive a closed-form continuous-depth model that has the rich modeling capabilities of ODE-based models and does not require any solver to model data (see <ref type="figure" target="#fig_1">Figure 1)</ref>.</p><p>The proposed continuous neural networks yield significantly faster training and inference speeds while being as expressive as their ODE-based counterparts. We provide a derivation for the approximate closed-form solution to a class of continuous neural networks that explicitly models time. We demonstrate how this transformation can be formulated into a novel neural model and scaled to create flexible, highly performant and fast neural architectures on challenging sequential datasets.</p><p>Deriving an Approximate Closed-form Solution for Neural Interactions. Two neurons interact with each other through synapses as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. There are three principal mechanisms for information propagation in natural brains that are abstracted away in the current building blocks of deep learning systems: 1) neural dynamics are typically continuous processes described by differential equations (c.f., dynamics of  <ref type="bibr" target="#b0">(1)</ref>, for which there is no known closed-form expression. Here, we provided an approximate solution for this equation which shows the interaction of nonlinear synapses with a postsynaptic neurons, in closed-form.</p><p>x(t) in <ref type="figure" target="#fig_1">Figure 1)</ref>, 2) synaptic release is much more than scalar weights; it involves a nonlinear transmission of neurotransmitters, the probability of activation of receptors, and the concentration of available neurotransmitters, among other nonlinearities (c.f., <ref type="figure" target="#fig_1">Figure 1)</ref>, and 3) the propagation of information between neurons is induced by feedback and memory apparatuses (c.f. I(t) stimulates x(t) through a nonlinear synapse S(t) which also has a multiplicative difference of potential to the postsynaptic neuron accounting for a negative feedback mechanism). Liquid time-constant (LTC) networks <ref type="bibr" target="#b0">(1)</ref>, which are expressive continuous-depth models obtained by a bilinear approximation (18) of neural ODE formulation (2) are designed based on these mechanisms. Correspondingly, we take their ODE semantics and approximate a closed-form solution for the scalar case of a postsynaptic neuron receiving an input stimuli from a presynaptic source through a nonlinear synapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(t) in</head><p>To this end, we apply the theory of linear ODEs <ref type="bibr" target="#b18">(19)</ref> to analytically solve the dynamics of an LTC differential equation shown in <ref type="figure" target="#fig_1">Figure 1</ref>. We then simplify the so- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Complexity Local Error</p><formula xml:id="formula_0">p-th order solver O(K ? p) O( p+1 ) adaptive-step solver ? O(? p+1 ) Euler hypersolver O(K) O(? 2 ) p-th order hypersolver O(K ? p) O(? p+1 ) CfC (Ours) O(K) not relevant</formula><p>lution to the point where there is one integral left to solve. This integral compartment, t 0 f (I(s))ds in which f is a positive, continuous, monotonically increasing, and bounded nonlinearity, is challenging to solve in closed-form; since it has dependencies on an input signal I(s) that is arbitrarily defined (such as a real-world sensory readouts). To approach this problem, we discretize I(s) into piecewise constant segments and obtain the discrete approximation of the integral in terms of sum of piecewise constant compartments over intervals. This piecewise constant approximation inspired us to introduce an approximate closed-form solution for the integral t 0 f (I(s))ds that is provably tight when the integral appears as the exponent of an exponential decay, which is the case for LTCs. We theoretically justify how this closed-form solution represents LTCs' ODE semantics and is as expressive (see <ref type="figure" target="#fig_1">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit Time Dependency.</head><p>We then dissect the properties of the obtained closedform solution and design a new class of neural network models we call Closed-form Continuous-depth networks (CfC). CfCs have an explicit time dependency in their formulation that does not require an ODE solver to obtain their temporal rollouts. Thus, they maximize the trade-off between accuracy and efficiency of solvers (See <ref type="table" target="#tab_0">Table 1</ref>).</p><p>CfCs perform computations at least one order of magnitude faster training and inference time compared to their ODE-based counterparts, without loss of accuracy. <ref type="table" target="#tab_1">Table 2</ref>: Sequence and time-step prediction complexity. n is the sequence length, k the number of hidden units, and p = order of the ODE-solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Sequence Time-step prediction prediction</p><formula xml:id="formula_1">RNN O(nk) O(k) ODE-RNN O(nkp) O(kp) Transformer O(n 2 k) O(nk) CfC O(nk) O(k)</formula><p>Sequence and Time-step Prediction Efficiency. CfCs perform per-time-step and persequence predictions by establishing a continuous flow similar to ODE-based models.</p><p>However, as they do not require ODE-solvers, their complexity is at least one order of magnitude less than ODE based models. Consider having a performant gated recurrent model <ref type="bibr" target="#b19">(20)</ref> with the abilities to create expressive continuous flows (2) and adaptable dynamics (1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deriving a Closed-form Solution</head><p>In this section, we derive an approximate closed-form solution for liquid time-constant (LTC) networks, an expressive subclass of time-continuous models. We discuss how the scalar closed-form expression derived from a small LTC system can inspire the design of CfC models.</p><p>The hidden state of an LTC network is determined by the solution of the initialvalue problem (IVP) given below <ref type="formula" target="#formula_2">(1)</ref>:</p><formula xml:id="formula_2">dx dt = ?(w ? + f (x, I, ?))x(t) + A f (x, I, ?),<label>(1)</label></formula><p>where x(t) defines the hidden states, I(t) is the input to the system, w ? is a timeconstant parameter vector, A is a bias vector, and f is a neural network parametrized by ?. Theorem 1. Given an LTC system determined by the IVP (1), constructed by one cell, receiving a single dimensional time-series input I with no self connections, the following expression is an approximation of its closed-form solution:</p><formula xml:id="formula_3">x(t) = (x 0 ? A)e ?[w ? + f (I(t),?)]t f (?I(t), ?) + A<label>(2)</label></formula><p>Proof. In the single-dimensional case, the IVP (1) becomes linear in x as follows:</p><formula xml:id="formula_4">d dt x(t) = ? w ? + f (I(t)) ? x(t) + A f (I(t))<label>(3)</label></formula><p>Therefore, we can use the theory of linear ODEs to obtain an integral closed-form solution (19, Section 1.10) consisting of two nested integrals. The inner integral can be eliminated by means of integration by substitution <ref type="bibr" target="#b20">(21)</ref>. With this, the remaining integral expression can be solved in the case of piecewise constant inputs and approximated in the case of general inputs. The three steps of the proof are outlined below.</p><p>Integral closed-form solution of LTC. We consider the ODE semantics of a single neuron that receives some arbitrary continuous input signal I and has a positive, bounded, continuous, and monotonically increasing nonlinearity f :</p><formula xml:id="formula_5">d dt x(t) = ? w ? + f (I(t)) ? x(t) + A ? w ? + f (I(t))</formula><p>Assumption. We assumed a second constant value w ? in the above representation of a single LTC neuron. This is done to introduce symmetry on the structure of the ODE, hence being able to apply the theory of linear ODEs for solving the equation analytically.</p><p>By applying linear ODE systems theory (19, Section 1.10), we obtain:</p><formula xml:id="formula_6">x(t) = e ? t 0 [w ? + f (I(s))]ds ? x(0)+ t 0 e ? t s [w ? + f (I(v))]dv ? A ? (w ? + f (I(s)))ds<label>(4)</label></formula><p>To resolve the double integral in the equation above, we define</p><formula xml:id="formula_7">u(s) := t s [w ? + f (I(v))]dv,</formula><p>and observe that d ds u(s) = ?(w ? + f (I(s))). Hence, integration by substitution allows us to rewrite (4) into:</p><formula xml:id="formula_8">x(t) = e ? t 0 [w ? + f (I(s))]ds ? x(0) ? A u(t) u(0) e ?u du = x(0)e ? t 0 [w ? + f (I(s))]ds + A[e ?u ] u(t) u(0) = x(0)e ? t 0 [w ? + f (I(s))]ds + A 1 ? e ? t 0 [w ? + f (I(s))]ds = (x(0) ? A)e ?w ? t e ? t 0 f (I(s))ds + A<label>(5)</label></formula><p>Analytical LTC solution for piecewise constant inputs. The derivation of a useful closed-form expression of x requires us to solve the integral expression t 0 f (I(s))ds for any t ? 0. Fortunately, the integral t 0 f (I(s))ds enjoys a simple closed-form expression for piecewise constant inputs I. Specifically, assume that we are given a sequence of time points:</p><formula xml:id="formula_9">0 = ? 0 &lt; ? 1 &lt; ? 2 &lt; . . . &lt; ? n?1 &lt; ? n = ?, such that ? 1 , . . . , ? n?1 ? R and I(t) = ? i for all t ? [? i ; ? i+1 ) with 0 ? i ? n ? 1. Then, it holds that t 0 f (I(s))ds = f (? k )(t ? ? k ) + k?1 ? i=0 f (? i )(? i+1 ? ? i ),<label>(6)</label></formula><p>when ? k ? t &lt; ? k+1 for some 0 ? k ? n ? 1 (as usual, one defines ? ?1 i=0 := 0). With this, we have:</p><formula xml:id="formula_10">x(t) = (x(0) ? A)e ?w ? t e ? f (? k )(t?? k )?? k?1 i=0 f (? i )(? i+1 ?? i ) + A,<label>(7)</label></formula><p>when ? k ? t &lt; ? k+1 for some 0 ? k ? n ? 1. While any continuous input can be approximated arbitrarily well by a piecewise constant input (21), a tight approximation may require a large number of discretization points ? 1 , . . . , ? n . We address this next.</p><p>Analytical LTC approximation for general inputs. Inspired by Eq. 6, the next result provides an analytical approximation of x(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1.</head><p>For any Lipschitz continuous, positive, monotonically increasing, and bounded f and continuous input signal I(t), we approximate x(t) in (5) as follows:</p><formula xml:id="formula_11">x(t) = (x(0) ? A)e ? w ? t+ f (I(t))t f (?I(t)) + A (8) Then, |x(t) ?x(t)| ? |x(0) ? A|e ?w ? t for all t ? 0. Writing c = x(0) ? A for convenience,</formula><p>we can obtain the following sharpness results, additionally:</p><formula xml:id="formula_12">1. For any t ? 0, we have sup{ 1 c (x(t) ?x(t)) | I : [0; t] ? R} = e ?w ? t . 2. For any t ? 0, we have inf{ 1 c (x(t) ?x(t)) | I : [0; t] ? R} = e ?w ? t (e ?t ? 1).</formula><p>Above, the supremum and infimum are meant to be taken across all continuous input signals.</p><p>These statements settle the question about the worst-case errors of the approximation. The first statement implies in particular that our bound is sharp.</p><p>The full proof is given in Methods. Lemma 1 demonstrates that the integral solution we  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tightness of the Closed-form Solution in Practice</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Translate a trained LTC network into its closed-form variant</head><p>Inputs: LTC inputs I (N?T) (t), LTC neurons activity x (H?T) (t), and their initial states</p><formula xml:id="formula_13">x (H?1) (0), Synapses adjacency matrix W [(N+H) * (N+H)] Adj LTC's ODE Solver, Solver's step ?t, time-instance vectors of inputs, t (1?T) I(t) time-instance of LTC neurons t x(t) ? time might be sampled irregularly LTC neurons' parameter ? (H?1) LTC network synaptic parameters { ? (N?H) , ? (N?H) , A (N?H) } Outputs: LTC's closed-form approximation of hidden state neurons,x (N?T) (t) x pre (t) = W Adj ? [I 0 . . . I N , x 0 . . . x H ]</formula><p>? all presynaptic signals to nodes for i th neuron in neurons 1 to H do for j in Synapses to i th neuron d?</p><formula xml:id="formula_14">x i += (x 0 ? A ij )e ?t x(t) 1/? i + 1 1+e (?? ij (x pre ij ?? ij )) ) 1 1 + e (? ij (x pre ij ?? ij )) + A ij end for end for returnx(t)</formula><p>We took a trained Neural Circuit Policy (NCP) <ref type="bibr" target="#b21">(22)</ref>, which consists of a perception module and a liquid time-constant (LTC) based network (1) that possess 19 neurons and 253 synapses. The network was trained to autonomously steer a self-driving vehicle. We used recorded real-world test-runs of the vehicle for a lane-keeping task, governed by this network. The records included the inputs, outputs as well as all LTC neurons' activities and parameters. To perform a sanity check whether our proposed closed-form solution for LTC neurons is good enough in practice as well as the theory, we plugged in the parameters of individual neurons and synapses of the differential equations into the closed-form solution (Similar to the representations shown in <ref type="figure" target="#fig_4">Figure 2b and 2c)</ref> and emulated the structure of the ODE-based LTC networks. We then visualized the output neuron's dynamics of the ODE (in blue) and of the closed-form solution (in red). As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, we observed that the behavior of the ODE is captured with a mean-squared error of 0.006 by the closed-form solution. This experiment is an empirical evidence for the tightness results presented in our theory.</p><formula xml:id="formula_15">!" ( ) !# ( ) ( ) = ? " + #" #" ? + $" ( $" ? ) = 0 ? !" % " &amp;! '("! ) * * !" ? + !" + 0 ? #" % " &amp;! '(#! * * #" ? + #" ( ) = ? $ + #$ #$ ? + "$ "$ ? + $$ ( $$ ? ) = ? !# % " &amp; # '("# ) * * !# ? + !# + ? "# % " &amp; # '(!# * * "# ? + "# + ? ## % " &amp; # '(## * * ## ? + ## "# ( ) !" ( ) #" ( ) ( ) !" ( ) !# ( ) #" ( ) "# ( ) ## ( ) ## ( ) #" ( ) !# ( ) "# ( ) ## ( ) a. LTC</formula><p>Hence, the closed-form solution contains the main properties of liquid networks in approximating dynamics. We next show how to design a novel neural network instance inspired by this closed-form solution, that has well-behaved gradient properties and approximation capabilities. integrated into larger representation learning systems. Doing so requires careful attention to potential gradient and expressivity issues that can arise during optimization, which we will outline in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design a Closed-form Continuous-depth Model Inspired by the Solution</head><p>Formally, the hidden states, x(t) (D?1) with D hidden units at each time step t, can be explicitly obtained by:</p><formula xml:id="formula_16">x(t) = B e ?[w ? + f (x,I;?)]t f (?x, ?I; ?) + A,<label>(9)</label></formula><p>where B (D) collapses (x 0 ? A) of Eq. 2 into a parameter vector. A (D) and w the Hadamard (element-wise) product. While the neural network presented in 9 can be proven to be a universal approximator as it is an approximation of an ODE system (1,2), in its current form, it has trainability issues which we point out and resolve shortly:</p><p>Resolving the gradient issues. The exponential term in Eq. 9 derives the system's first part (exponentially fast) to 0 and the entire hidden state to A. This issue becomes more apparent when there are recurrent connections and causes vanishing gradient factors when trained by gradient descent <ref type="bibr" target="#b22">(23)</ref>. To reduce the effect, we replace the exponential decay term with a reversed sigmoidal nonlinearity ?(.). This nonlinearity is approximately 1 at t = 0 and approaches 0 in the limit t ? ?. However, unlike the exponential decay, its transition happens much smoother, yielding a better condition on the loss surface.</p><p>Replacing biases by learnable instances. Next, we consider the bias parameter B to be part of the trainable parameters of the neural network f (?x, ?I; ?) and choose to use a new network instance instead of f (presented in the exponential decay factor). We also replace A with another neural network instance, h(.) to enhance the flexibility of the model. To obtain a more general network architecture, we allow the nonlinearity f (?x, ?I; ?) present in Eq. 9 have both shared (backbone) and independent, (g(.)), network compartments.</p><p>Gating balance. The time-decaying sigmoidal term can play a gating role if we additionally multiply h(.), with (1 ? ?(.)). This way, the time-decaying sigmoid function stands for a gating mechanism that interpolates between the two limits of t ? ?? and t ? ? of the ODE trajectory.</p><p>Backbone. Instead of learning all three neural network instances f , g and h separately, we have them share the first few layers in the form of a backbone that branches out into these three functions. As a result, the backbone allows our model to learn shared representations, thereby speeding up and stabilizing the learning process. More importantly, this architectural prior enables two simultaneous benefits: 1) Through the shared backbone a coupling between time-constant of the system and its state nonlinearity get established that exploits causal representation learning evident in a liquid neural network <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b23">24)</ref>. 2) through separate head network layers, the system has the ability to explore temporal and structural dependencies independently of each other.</p><p>These modifications result in the closed-form continuous-depth (CfC) neural network model:</p><formula xml:id="formula_17">x(t) = ?(? f (x, I; ? f ) t) time-continuous gating g(x, I; ? g )+ (10) 1 ? ?(?[ f (x, I; ? f )] t) time-continuous gating h(x, I; ? h ).</formula><p>The CfC architecture is illustrated in <ref type="figure" target="#fig_6">Figure 4</ref>. The neural network instances could be selected arbitrarily. The time complexity of the algorithm is equivalent to that of discretized recurrent networks <ref type="bibr" target="#b24">(25)</ref>, which is at least one order of magnitude faster than ODE-based networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments with CfCs</head><p>We now assess the performance of CfCs in a series of sequential data processing tasks compared to advanced, recurrent models. We first evaluate how CfCs compare to LTC-based neural circuit policies (NCPs) <ref type="bibr" target="#b21">(22)</ref> in real-world autonomous lane keeping tasks. We then approach solving conventional sequential data modeling tasks (e.g., bitstream prediction, sentiment analysis on text data, medical time-series prediction, and robot kinematics modeling), and compare CfC variants to an extensive set of advanced recurrent neural network baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CfC Network Variants.</head><p>To evaluate how the proposed modifications we applied to the closed-form solution network described by Eq. 9, we test four variants of the CfC architecture: 1) Closed-form solution network (Cf-S) obtained by Eq. 9, 2) CfC without the second gating mechanism (CfC-noGate). This variant does not have the 1 ? ? instance shown in The testing environment consisted of 1km of private test road with unlabeled lanemarkers and we observed that all trained networks were able to successfully complete the lane-keeping task at a constant velocity of 30 km/hr. <ref type="figure" target="#fig_9">Fig. 5</ref> provides an insight into how these networks come with driving decisions. To this end, we computed the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularly and Irregularly-Sampled Bit-Stream XOR</head><p>The bit-stream XOR dataset (9) considers classifying bit-streams implementing an XOR function in time, i.e., each item in the sequence contributes equally to the correct output. The bit-streams are provided in densely sampled and event-based sampled format. The densely sampled version simply represents an incoming bit as an input event.</p><p>The event sampled version transmits only bit-changes to the network, i.e., multiple equal bit is packed into a single input event. Consequently, the densely sampled variant is a regular sequence classification problem, whereas the event-based encoding variant represents an irregularly sampled sequence classification problem.  ing/exploding gradient issues <ref type="bibr" target="#b8">(9)</ref>. The hyperparameter details of this experiment is provided in <ref type="table" target="#tab_0">Table S1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PhysioNet Challenge</head><p>The PhysioNet Challenge 2012 dataset considers the prediction of the mortality of 8000 patients admitted to the intensive care unit (ICU). The features represent time series of medical measurements of the first 48 hours after admission. The data is irregularly sampled in time, and over features, i.e., only a subset of the 37 possible features is given at each time point. We perform the same test-train split and preprocessing as <ref type="bibr" target="#b6">(7)</ref>, and report the area under the curve (AUC) on the test set as metric in <ref type="table" target="#tab_5">Table 5</ref>. We observe that CfCs perform competitively to other baselines while performing 160 times faster training time compared to ODE-RNNs and 220 times compared to continuous latent models. CfCs are also, on average, three times faster than advanced discretized gated recurrent models. The hyperparameter details of this experiment is provided in <ref type="table" target="#tab_1">Table   S2</ref>. Model AUC Score (%) time per epoch (min) ?RNN-Impute <ref type="bibr" target="#b6">(7)</ref> 0.764 ? 0.016 0.5 ?RNN-delta-t <ref type="bibr" target="#b6">(7)</ref> 0.787 ? 0.014 0.5 ?RNN-Decay <ref type="bibr" target="#b6">(7)</ref> 0.807 ? 0.003 0.7 ?GRU-D <ref type="bibr" target="#b35">(36)</ref> 0.818 ? 0.008 0.7 ?Phased-LSTM <ref type="bibr" target="#b36">(37)</ref> 0.836 ? 0.003 0.3 * IP-Nets <ref type="bibr" target="#b30">(31)</ref> 0.819 ? 0.006 1.3 * SeFT <ref type="bibr" target="#b31">(32)</ref> 0.795 ? 0.015 0.5 ?RNN-VAE <ref type="bibr" target="#b6">(7)</ref> 0.515 ? 0.040 2.0 ?ODE-RNN <ref type="bibr" target="#b6">(7)</ref> 0.833 ? 0.009 16.5 ?Latent-ODE-RNN (7) 0.781 ? 0.018 6.7 ?Latent-ODE-ODE <ref type="bibr" target="#b6">(7)</ref> 0.829 ? 0.004 22.0 LTC <ref type="bibr" target="#b0">(1)</ref> 0.6477 ? 0.010 0.5 Cf-S (ours) 0.643 ? 0.018 0.1 CfC-noGate (ours) 0.840 ? 0.003 0.1 CfC (ours) 0.839 ? 0.002 0.1 CfC-mmRNN (ours) 0.834 +-0.006 0.2 Note: The performance of the models marked by ? are reported from <ref type="bibr" target="#b6">(7)</ref> and the ones with * from <ref type="bibr" target="#b43">(44)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis -IMDB</head><p>The IMDB sentiment analysis dataset (47) consists of 25,000 training and 25,000 test sentences. Each sentence corresponds to either positive or negative sentiment. We tokenize the sentences in a word-by-word fashion with a vocabulary consisting of 20,000 most frequently occurring words in the dataset. We map each token to a vector using a trainable word embedding. The word embedding is initialized randomly. No pretraining of the network or the word embedding is performed. <ref type="table">Table 6</ref> represents how CfCs equipped with mixed memory instances outperform advanced RNN benchmarks. The hyperparameter details of this experiment is provided in <ref type="table" target="#tab_2">Table S3</ref>. <ref type="table">Table 6</ref>: Results on the IMDB datasets. The experiment is performed without any pretraining or pretrained word-embeddings. Thus, we excluded advanced attention-based models <ref type="bibr" target="#b43">(44,</ref><ref type="bibr" target="#b44">45)</ref> such as Transformers <ref type="bibr" target="#b45">(46)</ref> and RNN structures that use pretraining. Numbers present mean ? standard deviations, n=5 that on this task, CfCs even outperform Transformers by a considerable 18% margin.</p><p>The hyperparameter details of this experiment is provided in <ref type="table" target="#tab_4">Table S4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scope, Discussions and Conclusions</head><p>We introduced a closed-form continuous-time neural model build from an approxi- as performant modeling of sequential and irregularly-sampled data <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b42">43)</ref>. In this paper, we showed how to relax the need for an ODE-solver to realize an expressive continuous-time neural network model for challenging time-series problems.</p><p>Improving Neural ODEs. ODE-based neural networks are as good as their ODEsolvers. As the complexity or the dimensionality of the modeling task increases, ODEbased networks demand a more advanced solver that significantly impacts their efficiency <ref type="bibr" target="#b16">(17)</ref>, stability <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b59">(60)</ref><ref type="bibr" target="#b60">(61)</ref><ref type="bibr" target="#b61">(62)</ref> and performance <ref type="bibr" target="#b0">(1)</ref>. A large body of research went into improving the computational overhead of these solvers, for example, by designing hypersolvers <ref type="formula" target="#formula_2">(17)</ref>, deploying augmentation methods <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b11">12)</ref>, pruning (6) and by regularizing the continuous flows <ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref><ref type="bibr" target="#b15">(16)</ref>. To enhance the performance of an ODE-based model, especially in time series modeling tasks (63), solutions provided for stabilizing their gradient propagation <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b63">64)</ref>. In this work, we showed that CfCs improve the scalability, efficiency, and performance of continuous-depth neural models.  <ref type="formula" target="#formula_3">(2)</ref>. This is because differential equations guarantee invertibility (i.e., under uniqueness conditions (6), one can run them backwards in time). CfCs only approximate ODEs and therefore they no longer necessarily form a bijection <ref type="bibr" target="#b64">(65)</ref>.</p><p>What are the limitations of CfCs? CfCs might express vanishing gradient problems.</p><p>To avoid this, for tasks that require long-term dependencies, it is better to use them together with mixed memory networks (9) (See CfC-mmRNN). Moreover, we speculate that inferring causality from ODE-based networks might be more straightforward than a closed-form solution <ref type="bibr" target="#b23">(24)</ref>. It would also be beneficial to assess if verifying a continuous neural flow (66) is more tractable by an ODE representation of the system or their closed form.</p><p>In what application scenarios shall we use CfCs? For problems such as language modeling where a significant amount of sequential data and substantial compute resources are available, Transformers (46) are the right choice. In contrast, we use CfCs when: 1) data has limitations and irregularities (e.g., medical data, financial timeseries, robotics (67) and closed loop control and robotics, and multi-agent autonomous systems in supervised and reinforcement learning schemes (68)), 2) training and inference efficiency of a model is important (e.g., embedded applications (69-71)), and 3) when interpretability matters <ref type="bibr" target="#b71">(72)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Supplementary materials</head><p>Materials and Methods.</p><p>Tables S1 to S4.</p><p>where L is the Lipschitz constant of f and the last identity is due to dominated convergence theorem <ref type="bibr" target="#b20">(21)</ref>. To see 2), we first note that the negation of the signal ?I provides us with e ? t 0 f (?I(s))ds ? e ? f (?I(t))t f (I(t)) ?</p><formula xml:id="formula_18">e ?(1??)(t??)???0 ? e ???t (1 ? ?) ? e ?t ? 1,</formula><p>if ?, ? ? 0. The fact that the left-hand side of the last inequality must be at least e ?t ? 1 follows by observing that e ?t ? e ? t 0 f (I (s))ds and e ? f (I (t))t f (?I (t)) ? 1 for any I , I : [0; t] ? R.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>pressive power when they are deployed on computers is bottlenecked by numerical DE solvers. This limitation has significantly slowed down scaling and understanding of numerous natural physical phenomena such as the dynamics of nervous systems. Ideally we would circumvent this bottleneck by solving the given dynamical system in closed-form. This is known to be intractable in general. Here, we show it is possible to closely approximate the interaction between neurons and synapses -the building blocks of natural and artificial neural networks -constructed by liquid time-constant networks (LTCs) (1) efficiently in closed-form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Neural and Synapse Dynamics. A postsynaptic neuron receives stimuli I(t), through a nonlinear conductance-based synapse model. The dynamics of the membrane potential of this postsynaptic neuron is given by the differential equation presented in the middle. This equation is a fundamental building block of liquid time-constant networks (LTCs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>obtained shown in Equation 5 is tightly close to the approximate closed-form solution we proposed in Equation 8. Note that as w ? is positively defined, the derived bound between Equations 5 and 8 ensures an exponentially decaying error as time goes by. Therefore, we have the statement of the theorem. An Instantiation of LTCs and their approximate closed-form expressions.Figure 2 shows a liquid network with two neurons and five synaptic connections. The network receives an input signal I(t). Figure 2 further derives the differential equation expression of the network along with its closed-form approximate solution. In general, it is possible to compile a trained LTC network into its closed-form version. This compilation allows us to speed up inference time of ODE-based networks as the closed-form variant does not require complex ODE solvers to compute outputs. Algorithm 1 provides the instructions on how to transfer a trained LTC network into its closed form variant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>shows an LTC-based network trained for autonomous driving<ref type="bibr" target="#b21">(22)</ref>. The figure further illustrates how close the proposed solution fits the actual dynamics exhibited from a single neuron ODE given the same parametrization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 :</head><label>2</label><figDesc>network with 2 neurons b. LTC differential equations c. Approximate closed-form solution of LTCs Input Legend ( ) potential of neuron i synapse between node i and j time-constant of neuron i /0 synaptic reversal potential for nodes i and j /0 nonlinearity of a synapse between i and j time Instantiation of LTCs in ODE and closed-form representations. a) A sample LTC network with two nodes and five synapses. b) the ODE representation of this two-neuron system. c) the approximate closed-form representation of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Leveraging the scalar closed-form solution expressed by Eq. (2), we can now distill this model into a neural network that can be trained at scale. The solution providing a grounded theoretical basis for solving scalar continuous-time dynamics and it is important to translate this theory into a practical neural network model which can be Tightness of the closed-form solution in practice. We approximate a closed-form solution for LTCs (1) while largely preserving the trajectories of their equivalent ODE systems. We develop our solution into closed-form continuous-depth (CfC) models that are at least 100x faster than neural ODEs at both training and inference on complex time-series prediction tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>parameter vectors, as well, I(t) (m?1) is an m-dimensional input at each time step t, f is a neural network parametrized by ? = {W Closed-form Continuous-depth neural architecture. A baclbone neural network layer delivers the input signals into three head networks g, f and h. f acts as a liquid time-constant for the sigmoidal time-gates of the network. g and h construct the nonlinearieties of the overall CfC network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>How do you deal</head><label></label><figDesc>with time, t? CfCs are continuous-depth models that can set their temporal behavior based on the task-under-test. For time-variant datasets (e.g., irregularly sampled time series, event-based data, and sparse data), t for each incoming sample is set based on its time-stamp or order. For sequential applications where the time of the occurrence of a sample does not matter, t is sampled batch-length-times with equidistant intervals within two hyperparameters a and b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 . 3 )</head><label>43</label><figDesc>Closed-form Continuous-depth model (CfC) expressed by Eq. 10. 4) CfC wrapped inside a mixed-memory architecture (i.e., CfC defines the memory state of an RNN for instance an LSTM). We call this variant CfC-mmRNN.Each of these four proposed variants leverage our proposed solution, and thus are at least one order of magnitude faster than continuous-time ODE models.How well CfCs perform in autonomous driving compared to NCPs and other mod-els?In this experiment, our objective is to evaluate how robustly CfCs learn to perform autonomous navigation as opposed to its ODE-based counterparts LTC networks. The task is to map incoming pixel observations to steering curvature commands. We start off by training neural network architectures that possess a convolutional head stacked with the choice of RNN. The RNN compartment of networks are replaced by LSTM networks, NCPs<ref type="bibr" target="#b21">(22)</ref>, Cf-S, CfC-NoGate, and CfC-mmRNN. We also trained a fully convolutional neural network for the sake of proper comparison.Our training pipeline followed an imitation learning approach with paired pixelcontrol data, from a 30Hz BlackFly PGE-23S3C RGB camera, collected by a human expert driver across a variety of rural driving environments, including times of day, weather conditions, and season of the year. The original 3-hour dataset was further augmented to include off-orientation recovery data using a privileged controller<ref type="bibr" target="#b25">(26)</ref> and a data-driven view synthesizer<ref type="bibr" target="#b26">(27)</ref>. The privileged controller enabled training all networks using guided policy learning<ref type="bibr" target="#b27">(28)</ref>. After training, all networks were trans-ferred on-board our full-scale autonomous vehicle (Lexus RX450H, retrofitted with drive-by-wire capability). The vehicle was consistently started at the center of the lane, initialized with each trained model, and was run to completion at the end of the road. If the model exited the bounds of the lane a human safety driver intervened and restarted the model from the center of the road at the intervention location. All models were tested with and without noise added to the sensory inputs to evaluate robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 :</head><label>5</label><figDesc>Attention Profile of networks. Trained networks receive unseen inputs (first column in each tab) and generate acceleration and steering commands. We use the Visual-Backprop algorithm<ref type="bibr" target="#b28">(29)</ref> to compute the saliency maps of the convolutional part of each network. a) results for networks tested on data collected in summer. b) results for networks tested on data collected in winter. c) results for inputs corrupted by a zero-mean Gaussian noise with variance, ? 2 = 0.35.Furthermore, we include models such as interpolation prediction networks (IP-Net) (31), Set functions for time-series (SeFT) (32), CT-RNNs (33), CT-GRU (34), CT-LSTM (35), GRU-D (36), Phased-LSTM (37), bi-directional RNNs (38). Finally, we benchmarked CfCs against competitive recent RNN architectures with the premise of tackling long-term dependencies, such as Legandre Memory Units (LMU) (39), highorder polynomial projection operators (Hippo) (40), orthogonal recurrent models (ex-pRNNs) (41), mixed memory RNNs such as (ODE-LSTMs) (9), coupled oscillatory RNNs (coRNN) (42), and Lipschitz RNNs (43).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4</head><label>4</label><figDesc>compares the performance of many RNN baselines. Many architectures such as Augmented LSTM, CT-GRU, GRU-D, ODE-LSTM, coRNN, and Lipschitz RNN, and all variants of CfC can successfully solve the task with 100% accuracy when the bit-stream samples are equidistant from each other. However, when the bit-stream samples arrive at non-uniform distances, only architectures that are immune to the vanishing gradient in irregularly sampled data can solve the task. These include GRU-D, ODE-LSTM and CfCs, and CfC-mmRNNs. ODE-based RNNs cannot solve the event-based encoding tasks regardless of their choice of solvers, as they have vanish-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>mate close-form solution of liquid time-constant networks that possesses the strong modeling capabilities of ODE-based networks while being significantly faster, more accurate, and stable. These closed-form continuous-depth models achieve this by explicit time-dependent gating mechanisms and having a liquid time-constant modulated by neural networks. Continuous-Depth Models. Machine learning, control theory and dynamical systems merge at models with continuous-time dynamics (52-56). In a seminal work, Chen et. al. 2018 (2) revived the class of continuous-time neural networks (33, 57), with neural ODEs. These continuous-depth models give rise to vector field representations and a set of functions which were not possible to generate before with discrete neural networks. These capabilities enabled flexible density estimation (3-5, 58, 59), as well</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>is step-size,? is the max step-size and ? &lt;&lt; 0.K is time steps for closed-form continuous depth models (CfCs) which is equivalent to K. Table is reproduced and taken from<ref type="bibr" target="#b16">(17)</ref>.</figDesc><table /><note>Time Complexity of the process to compute K solver's steps.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 CfCs: Flexible Deep Models for Sequential Tasks. CfCs</head><label>2</label><figDesc>compares the time complexity of CfCs to that of standard RNNs, ODE-RNNs and Transformers.</figDesc><table><row><cell>are equipped with novel gat-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Lane-keeping models' parameter count.</head><label>3</label><figDesc>CfC and NCP networks perform lanekeeping in unseen scenarios with a compact representation.</figDesc><table><row><cell>Modes</cell><cell cols="2">Total Parameter Count RNN Parameter Count</cell></row><row><cell></cell><cell>(CNN head + RNN)</cell><cell></cell></row><row><cell>CNN</cell><cell>2,124,901</cell><cell>-</cell></row><row><cell>LSTM</cell><cell>259,733</cell><cell>33089</cell></row><row><cell>NCP</cell><cell>233,139</cell><cell>6495</cell></row><row><cell>Cf-S</cell><cell>227,728</cell><cell>1084</cell></row><row><cell>CfC</cell><cell>230,828</cell><cell>4184</cell></row><row><cell>CfC-NoGate</cell><cell>230,828</cell><cell>4184</cell></row><row><cell>CfC-mmRNN</cell><cell>235,052</cell><cell>8408</cell></row><row><cell cols="3">attention of each network while driving, by using the visual-backprop algorithm (29).</cell></row><row><cell></cell><cell></cell><cell>end-</cell></row><row><cell cols="3">to-end autonomous lane keeping task with around 4k trainable parameters in their</cell></row><row><cell cols="2">recurrent neural network component.</cell><cell></cell></row><row><cell cols="3">In the following, we design sequence data processing pipelines where we exten-</cell></row><row><cell cols="3">sively test CfCs' effectiveness in learning spatiotemporal dynamics, compared to a</cell></row><row><cell cols="2">large range of advanced recurrent models.</cell><cell></cell></row><row><cell cols="3">Baselines. We compare CfCs to a diverse set of advanced algorithms developed for</cell></row><row><cell cols="3">sequence modeling by both discretized and continuous mechanisms. Examples in-</cell></row><row><cell cols="3">clude some variations of classical autoregressive RNNs, such as an RNN with concate-</cell></row><row><cell cols="3">nated ?t (RNN-?t), a recurrent model with moving average on missing values (RNN-</cell></row><row><cell cols="3">impute), RNN Decay (7), long short-term memory (LSTMs) (20), and gated recurrent</cell></row><row><cell cols="3">units (GRUs) (30). We also report results for a variety of encoder-decoder ODE-RNN</cell></row></table><note>We observe that CfCs similar to NCPs demonstrate a consistent attention pattern in each subtask, while maintaining their attention profile under heavy noise depicted in Fig. 5c. Similar to NCPs, CfCs are very parameter efficient. They performed thebased models, such as RNN-VAE, Latent variable models with RNNs, and with ODEs, all from (7).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Bit-stream XOR sequence classification. The performance values for all baseline models are reproduced from<ref type="bibr" target="#b8">(9)</ref>. Numbers present mean ? standard deviations, n=5</figDesc><table><row><cell>Model</cell><cell>Equidistant encoding</cell><cell cols="3">Event-based (irregular) encoding (min) Time Per epoch ODE-based?</cell></row><row><cell>?Augmented LSTM (20)</cell><cell cols="2">100.00% ? 0.00 89.71% ? 3.48</cell><cell>0.62</cell><cell>No</cell></row><row><cell>?CT-GRU (34)</cell><cell cols="2">100.00% ? 0.00 61.36% ? 4.87</cell><cell>0.80</cell><cell>No</cell></row><row><cell>?RNN Decay (7)</cell><cell cols="2">60.28% ? 19.87 75.53% ? 5.28</cell><cell>0.90</cell><cell>No</cell></row><row><cell cols="3">?Bi-directional RNN (38) 100.00% ? 0.00 90.17% ? 0.69</cell><cell>1.82</cell><cell>No</cell></row><row><cell>?GRU-D (36)</cell><cell cols="2">100.00% ? 0.00 97.90% ? 1.71</cell><cell>0.58</cell><cell>No</cell></row><row><cell>?PhasedLSTM (37)</cell><cell>50.99% ? 0.76</cell><cell>80.29% ? 0.99</cell><cell>1.22</cell><cell>No</cell></row><row><cell>?CT-LSTM (35)</cell><cell>97.73% ? 0.08</cell><cell>95.09% ? 0.30</cell><cell>0.86</cell><cell>No</cell></row><row><cell>coRNN (42)</cell><cell cols="2">100.00% ? 0.00 52.89% ? 1.25</cell><cell>0.57</cell><cell>No</cell></row><row><cell>Lipschitz RNN (43)</cell><cell cols="2">100.00% ? 0.00 52.84% ? 3.25</cell><cell>0.63</cell><cell>No</cell></row><row><cell>?ODE-RNN (7)</cell><cell>50.47% ? 0.06</cell><cell>51.21% ? 0.37</cell><cell>4.11</cell><cell>Yes</cell></row><row><cell>?CT-RNN (33)</cell><cell>50.42% ? 0.12</cell><cell>50.79% ? 0.34</cell><cell>4.83</cell><cell>Yes</cell></row><row><cell>?GRU-ODE (7)</cell><cell>50.41% ? 0.40</cell><cell>52.52% ? 0.35</cell><cell>1.55</cell><cell>Yes</cell></row><row><cell>?ODE-LSTM (9)</cell><cell cols="2">100.00% ? 0.00 98.89% ? 0.26</cell><cell>1.18</cell><cell>Yes</cell></row><row><cell>LTC (1)</cell><cell cols="2">100.00% ? 0.00 49.11% ? 0.00</cell><cell>2.67</cell><cell>Yes</cell></row><row><cell>Cf-S (ours)</cell><cell cols="2">100.00% ? 0.00 85.42% ? 2.84</cell><cell>0.36</cell><cell>No</cell></row><row><cell>CfC-noGate (ours)</cell><cell cols="2">100.00% ? 0.00 96.29% ? 1.61</cell><cell>0.78</cell><cell>No</cell></row><row><cell>CfC (ours)</cell><cell cols="2">100.00% ? 0.00 99.42% ? 0.42</cell><cell>0.75</cell><cell>No</cell></row><row><cell>CfC-mmRNN (ours)</cell><cell cols="2">100.00% ? 0.00 99.72% ? 0.08</cell><cell>1.26</cell><cell>No</cell></row><row><cell cols="4">Note: The performance of models marked by  ? are reported from (9).</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>PhysioNet. The experiment is performed without any pretraining or pretrained wordembeddings. Thus, we excluded advanced attention-based models<ref type="bibr" target="#b43">(44,</ref><ref type="bibr" target="#b44">45)</ref> such as Transformers<ref type="bibr" target="#b45">(46)</ref> and RNN structures that use pretraining. Numbers present mean ? standard deviations, n=5</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Per time-step regression. Modeling the physical dynamics of a Walker agent in simulation. Numbers present mean ? standard deviations. n = 5</figDesc><table><row><cell>Model</cell><cell cols="2">Square-error Time per epoch (min)</cell></row><row><cell>?ODE-RNN (7)</cell><cell>1.904 ? 0.061</cell><cell>0.79</cell></row><row><cell>?CT-RNN (33)</cell><cell>1.198 ? 0.004</cell><cell>0.91</cell></row><row><cell cols="2">?Augmented LSTM (20) 1.065 ? 0.006</cell><cell>0.10</cell></row><row><cell>?CT-GRU (34)</cell><cell>1.172 ? 0.011</cell><cell>0.18</cell></row><row><cell>?RNN-Decay (7)</cell><cell>1.406 ? 0.005</cell><cell>0.16</cell></row><row><cell cols="2">?Bi-directional RNN (38) 1.071 ? 0.009</cell><cell>0.39</cell></row><row><cell>?GRU-D (36)</cell><cell>1.090 ? 0.034</cell><cell>0.11</cell></row><row><cell>?PhasedLSTM (37)</cell><cell>1.063 ? 0.010</cell><cell>0.25</cell></row><row><cell>?GRU-ODE (7)</cell><cell>1.051 ? 0.018</cell><cell>0.56</cell></row><row><cell>?CT-LSTM (35)</cell><cell>1.014 ? 0.014</cell><cell>0.31</cell></row><row><cell>?ODE-LSTM (9)</cell><cell>0.883 ? 0.014</cell><cell>0.29</cell></row><row><cell>coRNN (42)</cell><cell>3.241 ? 0.215</cell><cell>0.18</cell></row><row><cell>Lipschitz RNN (43)</cell><cell>1.781 ? 0.013</cell><cell>0.17</cell></row><row><cell>LTC (1)</cell><cell>0.662 ? 0.013</cell><cell>0.78</cell></row><row><cell>Transformer (46)</cell><cell>0.761 ? 0.032</cell><cell>0.8</cell></row><row><cell>Cf-S (ours)</cell><cell>0.948 ? 0.009</cell><cell>0.12</cell></row><row><cell>CfC-noGate (ours)</cell><cell>0.650 ? 0.008</cell><cell>0.21</cell></row><row><cell>CfC (ours)</cell><cell>0.643 ? 0.006</cell><cell>0.08</cell></row><row><cell>CfC-mmRNN (ours)</cell><cell>0.617 ? 0.006</cell><cell>0.34</cell></row></table><note>Note: The performance of the models marked by ? are reported from (9).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Now that we have a closed-form system, where does it make sense to use ODE-based networks?</head><label></label><figDesc>For large-scale time-series prediction tasks, and where closed-loop performance matters (24) CfCs should be the method of choice.This is because, they capture the flexible, continuous-time nature of ODE-based networks while presenting large gains in performance and scalability. On the other hand, implicit ODE-based mod-els can still be significantly beneficial in solving continuously defined physics problems and control tasks. Moreover, for generative modeling, continuous normalizing flows built by ODEs are the suitable choice of model as they ensure invertibility unlike CfCs</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S1 :</head><label>S1</label><figDesc>Bit-Stream XOR experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hyperparameters</cell></row><row><cell>Parameter</cell><cell></cell><cell></cell><cell>Value</cell><cell></cell></row><row><cell></cell><cell>Cf-S</cell><cell>CfC</cell><cell cols="2">CfC-noGate CfC-mmRNN</cell></row><row><cell>clipnorm</cell><cell>5</cell><cell>1</cell><cell>10</cell><cell>10</cell></row><row><cell>optimizer</cell><cell cols="3">Adam RMSProp RMSprop</cell><cell>RMSprop</cell></row><row><cell>batch size</cell><cell>256</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Hidden size</cell><cell>64</cell><cell>192</cell><cell>128</cell><cell>64</cell></row><row><cell>epochs</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>base lr</cell><cell>0.005</cell><cell>0.05</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>decay lr</cell><cell>0.9</cell><cell>0.7</cell><cell>0.95</cell><cell>0.95</cell></row><row><cell cols="2">backbone activation SiLU</cell><cell>ReLU</cell><cell>SiLU</cell><cell>ReLU</cell></row><row><cell>backbone dr</cell><cell>0.0</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell></row><row><cell>forget bias</cell><cell>1.2</cell><cell>1.2</cell><cell>4.7</cell><cell>0.6</cell></row><row><cell>backbone units</cell><cell>64</cell><cell>128</cell><cell>192</cell><cell>128</cell></row><row><cell>backbone layers</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>weight decay</cell><cell cols="2">3e-05 3e-06</cell><cell>5e-06</cell><cell>2e-06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S2 :</head><label>S2</label><figDesc>Physionet experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hyperparameters</cell></row><row><cell>Parameter</cell><cell></cell><cell></cell><cell>Value</cell><cell></cell></row><row><cell></cell><cell>Cf-S</cell><cell>CfC</cell><cell cols="2">CfC-noGate CfC-mmRNN</cell></row><row><cell>epochs</cell><cell>116</cell><cell>57</cell><cell>58</cell><cell>65</cell></row><row><cell>class weight</cell><cell>18.25</cell><cell>11.69</cell><cell>7.73</cell><cell>5.91</cell></row><row><cell>clipnorm</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Hidden size</cell><cell>64</cell><cell>256</cell><cell>64</cell><cell>64</cell></row><row><cell>base lr</cell><cell>0.003</cell><cell>0.002</cell><cell>0.003</cell><cell>0.001</cell></row><row><cell>decay lr</cell><cell>0.72</cell><cell>0.9</cell><cell>0.73</cell><cell>0.9</cell></row><row><cell cols="2">backbone activation Tanh</cell><cell>SiLU</cell><cell>ReLU</cell><cell>LeCun Tanh</cell></row><row><cell>backbone units</cell><cell>64</cell><cell>64</cell><cell>192</cell><cell>64</cell></row><row><cell>backbone dr</cell><cell>0.1</cell><cell>0.2</cell><cell>0.0</cell><cell>0.3</cell></row><row><cell>backbone layers</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>weight decay</cell><cell>5e-05</cell><cell>4e-06</cell><cell>5e-05</cell><cell>4e-06</cell></row><row><cell>optimizer</cell><cell cols="3">AdamW AdamW AdamW</cell><cell>AdamW</cell></row><row><cell>init</cell><cell>0.53</cell><cell>0.50</cell><cell>0.55</cell><cell>0.6</cell></row><row><cell>batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S3 :</head><label>S3</label><figDesc>IMDB experiments.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Hyperparameters</cell></row><row><cell>Parameter</cell><cell></cell><cell></cell><cell>Value</cell><cell></cell></row><row><cell></cell><cell>Cf-S</cell><cell>CfC</cell><cell cols="2">CfC-noGate CfC-mmRNN</cell></row><row><cell>clipnorm</cell><cell>1</cell><cell>10</cell><cell>5</cell><cell>10</cell></row><row><cell>optimizer</cell><cell cols="3">Adam RMSProp RMSprop</cell><cell>RMSprop</cell></row><row><cell>batch size</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell></row><row><cell>Hidden size</cell><cell>320</cell><cell>192</cell><cell>224</cell><cell>64</cell></row><row><cell>embed dim</cell><cell>64</cell><cell>192</cell><cell>192</cell><cell>32</cell></row><row><cell>embed dr</cell><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.3</cell></row><row><cell>epochs</cell><cell>27</cell><cell>47</cell><cell>37</cell><cell>20</cell></row><row><cell>base lr</cell><cell cols="2">0.0005 0.0005</cell><cell>0.0005</cell><cell>0.0005</cell></row><row><cell>decay lr</cell><cell>0.8</cell><cell>0.7</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell cols="2">backbone activation Relu</cell><cell>SiLU</cell><cell>SiLU</cell><cell>LeCun Tanh</cell></row><row><cell>backbone dr</cell><cell>0.0</cell><cell>0.0</cell><cell>0.1</cell><cell>0.0</cell></row><row><cell>backbone units</cell><cell>64</cell><cell>64</cell><cell>128</cell><cell>64</cell></row><row><cell>backbone layers</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell>weight decay</cell><cell cols="2">0.00048 3.6e-05</cell><cell>2.7e-05</cell><cell>0.00029</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S4 :</head><label>S4</label><figDesc></figDesc><table /><note>Walker2D experiments. Hyperparameters</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test accuracy (%) ?HiPPO-LagT <ref type="bibr" target="#b39">(40)</ref> 88.0 ? 0.2 ?HiPPO-LegS <ref type="bibr" target="#b39">(40)</ref> 88.0 ? 0.2 ?LMU <ref type="bibr" target="#b38">(39)</ref> 87.7 ? 0.1 ?LSTM <ref type="bibr" target="#b19">(20)</ref> 87.3 ? 0.4 ?GRU <ref type="bibr" target="#b29">(30)</ref> 86.2 ? n/a * ReLU <ref type="bibr">GRU (48)</ref> 84.8 ? n/a * Skip LSTM <ref type="bibr" target="#b48">(49)</ref> 86.6 ? n/a ?expRNN <ref type="bibr" target="#b40">(41)</ref> 84.3 ? 0.3 ?Vanilla RNN <ref type="bibr" target="#b48">(49)</ref> 67.4 ? 7.7 * coRNN <ref type="bibr" target="#b41">(42)</ref> 86.7 ? 0.3 LTC (1) 61.8 ? 6.1</p><p>Cf-S (ours) 81.7 ? 0.5 CfC-noGate (ours) 87.5 ? 0.1 CfC (ours) 85.9 ? 0.9 CfC-mmRNN (ours) 88.3 ? 0.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note:</head><p>The performance of the models marked by ? are reported from <ref type="bibr" target="#b39">(40)</ref>, and * are reported from <ref type="bibr" target="#b41">(42)</ref>. The n/a standard deviation denotes that the original report of these experiments did not provide the statistics of their analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Physical Dynamics Modeling</head><p>The Walker2D dataset consists of kinematic simulations of the MuJoCo physics engine (50) on the Walker2d-v2 OpenAI gym (51) environment using four different stochastic policies. The objective is to predict the physics state of the next time step.</p><p>The training and testing sequences are provided at irregularly-sampled intervals. We report the squared error on the test set as a metric. As shown in <ref type="table">Table 7</ref>, CfCs outperform the other baselines by a large margin rooting for their strong capability to model irregularly sampled physical dynamics with missing phases. It is worth mentioning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Here, we provide all supplementary materials used in our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and Methods</head><p>In this section, we provide the full proof for Lemma 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 1</head><p>Proof. We start by noting that  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Liquid time-constant networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7657" to="7666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ffjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4541" to="4550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liebenwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Daniela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12718</idno>
		<title level="m">Sparse flows: Pruning continuous-depth models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Latent odes for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03907</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Anode: Unconditionally accurate memoryefficient gradients for neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Biros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10298</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies in irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04418</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High order embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dissecting neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th Conference on Neural Information Processing Systems, NeurIPS 2020 (The Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="690" to="701" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How to train your neural ode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nurbekyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02798</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Massaroli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08063</idno>
		<title level="m">Stable neural flows</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">hey, that&apos;s not an ode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09457</idno>
	</analytic>
	<monogr>
		<title level="m">Faster ode adjoints with 12 lines of code</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hypersolvers: Toward fast continuous-depth models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1273" to="1302" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Perko</surname></persName>
		</author>
		<title level="m">Differential Equations and Dynamical Systems</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Principles of mathematical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<pubPlace>McGraw-Hill New York</pubPlace>
		</imprint>
	</monogr>
	<note>3d ed. edn</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural circuit policies enabling auditable autonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page">91</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technische Universit?t M?nchen</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vorbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08314</idno>
		<title level="m">Causal navigation by continuous-time neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Response characterization for auditing cell dynamics in long shortterm memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vista 2.0: An open, data-driven simulator for multimodal sensing and policy learning for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12083</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning robust control policies for end-to-end autonomous driving from data-driven simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1143" to="1150" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Guided policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualbackprop: Efficient visualization of cnns for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpolation-prediction networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Set functions for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4353" to="4363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kazakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Lindsey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04110</idno>
		<title level="m">Discrete event, continuous time rnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The neural hawkes process: a neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6757" to="6767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Phased lstm: accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3889" to="3897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Legendre memory units: Continuoustime representation in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kaji?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Reproducability Challenge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hippo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07669</idno>
		<title level="m">Recurrent memory with optimal polynomial projections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lezcano-Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mart?nez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3794" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Coupled oscillatory recurrent neural network (co{rnn}): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=F3s69XzWOia" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lipschitz recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=-N7PBXqOUJZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Multi-time attention networks for irregularly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<idno>abs/2102.03902</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gate-variants of gated recurrent unit (gru) neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 60th international midwest symposium on circuits and systems (MWS-CAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1597" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gir?-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06834</idno>
		<title level="m">Skip rnn: Learning to skip state updates in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A comprehensive review of stability analysis of continuous-time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1229" to="1262" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A proposal on machine learning via dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02540</idno>
		<title level="m">The expressive power of neural networks: A view from the width</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Maximum principle based algorithms for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Designing worm-inspired neural networks for interpretable robotic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Absolute stability of global pattern formation and parallel memory storage by competitive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on systems, man, and cybernetics</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="815" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Riemannian continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">; H</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1aa3d9c6ce672447e1e5d0f1b5207e85-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. &amp; Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2503" to="2515" />
		</imprint>
	</monogr>
	<note>Larochelle,</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Der Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09547</idno>
		<title level="m">Stochastic normalizing flows</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Imexnet a forward stable deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Treister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2525" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antisymmetricrnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09689</idno>
		<title level="m">A dynamical system view on recurrent neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gershgorin loss stabilizes the recurrent neural network compartment of an end-to-end robot learning scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5446" to="5452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">c302: a multiscale framework for modelling the nervous system of caenorhabditis elegans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gleeson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page">20170379</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scalable gradients for stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3870" to="3882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On the verification of neural odes with stochastic guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grunbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11525" to="11535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08187</idno>
		<title level="m">Adversarial training is not ready for robot learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Model-based versus model-free deep reinforcement learning for autonomous racing cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brunnbauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04909</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Efficient modeling of complex analog integrated circuits using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 12th Conference on Ph. D. Research in Microelectronics and Electronics (PRIME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A generative neural network model for the quality prediction of work in progress products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ledwoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brintrup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">105683</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Plug-and-play supervisory control using muscle and brain signals for real-time gesture and error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delpreto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1303" to="1322" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Interpretable Recurrent Neural Networks in Continuous-time Control Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hasani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t Wien</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD dissertation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
