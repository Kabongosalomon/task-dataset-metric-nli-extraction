<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Interpretable Sleep Stage Classification Using Cross-Modal Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017">2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jathurshan</forename><surname>Pradeepkumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mithunjha</forename><surname>Anandakumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinith</forename><surname>Kugathasan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhinesh</forename><surname>Suntharalingham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">L</forename><surname>Kappel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjula</forename><forename type="middle">C</forename><surname>De Silva</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chamira</forename><forename type="middle">U S</forename><surname>Edussooriya</surname></persName>
						</author>
						<title level="a" type="main">Towards Interpretable Sleep Stage Classification Using Cross-Modal Transformers</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Generic Colorized Journal</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
							<date type="published" when="2017">2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Automatic Sleep Stage Classification</term>
					<term>Inter- pretable Deep Learning</term>
					<term>Transformers</term>
					<term>Deep Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate sleep stage classification is significant for sleep health assessment. In recent years, several deep learning and machine learning based sleep staging algorithms have been developed and they have achieved performance on par with human annotation. Despite improved performance, a limitation of most deep-learning based algorithms is their Black-box behavior, which which have limited their use in clinical settings. Here, we propose Cross-Modal Transformers, which is a transformer-based method for sleep stage classification. Our models achieve both competitive performance with the state-of-the-art approaches and eliminates the Black-box behavior of deep-learning models by utilizing the interpretability aspect of the attention modules. The proposed cross-modal transformers consist of a novel crossmodal transformer encoder architecture along with a multiscale 1-dimensional convolutional neural network for automatic representation learning. Our sleep stage classifier based on this design was able to achieve sleep stage classification performance on par with or better than the state-of-the-art approaches, along with interpretability, a fourfold reduction in the number of parameters and a reduced training time compared to the current state-of-the-art. Our code is available at https://github. com/Jathurshan0330/Cross-Modal-Transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Accurate sleep stage classification plays a crucial role in sleep medicine and human health. In general, sleep experts use polysomnography (PSG) recordings to diagnose sleep related disorders. PSG mainly comprises of electroencephalogram (EEG), electrooculogram (EOG) and electromyogram (EMG). The PSG recordings are typically segmented into 30 s epochs and manually annotated by sleep experts based on guidelines such as Rechtschaffen and Kales (R&amp;K) <ref type="bibr" target="#b0">[1]</ref> or American Academy of Sleep Medicine (AASM) <ref type="bibr" target="#b1">[2]</ref>. The manual annotation process is tedious, prone to human errors, labour intensive and time consuming. To overcome these drawbacks, multiple studies have proposed automatic sleep stage classification algorithms as the alternative.</p><p>Recent works that employ deep learning based algorithms achieved impressive results in sleep stage classification <ref type="bibr" target="#b2">[3]</ref> than the conventional machine learning algorithms such as support vector machines (SVMs), k-nearest neighbors (KNNs) and random forest classifiers. These include convolutional neural networks (CNNs) <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>, recurrent neural networks (RNNs) <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, deep belief networks (DBNs) <ref type="bibr" target="#b10">[11]</ref>, autoencoders <ref type="bibr" target="#b11">[12]</ref> and hybrid architectures, such as CNN with RNN <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and deep neural networks with RNN <ref type="bibr" target="#b14">[15]</ref>. Furthermore, different learning techniques such as transfer learning <ref type="bibr" target="#b15">[16]</ref>, multi-view learning <ref type="bibr" target="#b16">[17]</ref>, meta learning <ref type="bibr" target="#b17">[18]</ref>, knowledge distillation <ref type="bibr" target="#b18">[19]</ref> and model personalization have improved the performance of automatic sleep staging. With these advances, automatic sleep stage algorithms have been able to achieve a performance on par with manual annotation.</p><p>Despite the improved performance, one major drawback of deep learning based algorithms is their black-box behaviour, which keeps them from being used in a clinical environment. A general concern when it comes to application of artificial intelligence in healthcare and medicine is the underlying mechanism of deep-learning algorithms <ref type="bibr" target="#b19">[20]</ref>. Interpretability of models is a key solution for this problem. An interpretable model has the capability to explain why a decision is made for a certain input.</p><p>Transformers <ref type="bibr" target="#b20">[21]</ref> has become de facto in natural language processing (NLP) tasks. Following the seminal work <ref type="bibr" target="#b20">[21]</ref>, there have been exceptional works on transformers such as BERT <ref type="bibr" target="#b21">[22]</ref>, vision transformers (ViTs) <ref type="bibr" target="#b22">[23]</ref>, which have improved the state-of-the-art in both NLP and computer vision domains. Transformers have introduced interpretability aspects in these domains without compromising high performance. In our proposed method, we employ transformer based architectures on physiological signals, specifically for automatic classification of sleep stages to achieve high prediction performance together with interpretability. To the best of our knowledge, SleepTransformer <ref type="bibr" target="#b23">[24]</ref> is the only work which has explored transformers for sleep stage classification. Transformer encoders used in SleepTransformer <ref type="bibr" target="#b23">[24]</ref> are inspired from the seminal transformer architecture proposed in <ref type="bibr" target="#b20">[21]</ref>, and on top of the transformer encoder they utilize an additional attention layer to achieve a compact representation for each epoch of the PSG signals. We hypothesized that a better compact representation can be learned by employing an additional learnable vector named [CLS] vector similar to BERT <ref type="bibr" target="#b21">[22]</ref> and ViTs <ref type="bibr" target="#b22">[23]</ref>. The vector representation learned by the transformer encoder corresponding to the CLS vector can be utilized as the compact vector representation for each epoch. This learned vector representation aggregates all the sequence information in a PSG epoch. The SleepTransformer <ref type="bibr" target="#b23">[24]</ref> learns only intra and inter epoch relationships in an EEG signal whereas in our method, we explore cross-modal relationships between EEG and EOG signals along with intra-modal and inter epoch relationships, by modifying our transformer encoder architecture.</p><p>In this paper, we propose cross-modal transformers consisting of a novel cross-modal transformer encoder architecture along with a multi-scale one-dimensional convolutional neural network (1D CNN) for automatic representation learning. The major contributions of our work presented in this paper are summarized below:</p><p>? We propose two cross-modal transformers: 1) Epoch cross-modal transformer and 2) Sequence cross-modal transformer to solve the problem of sleep stage classification under two classification schemes which are oneto-one and many-to-many classification. ? Multi-scale 1D-CNNs are leveraged in our method to learn the feature representations from the raw signals of each modality. This enables our model to learn an optimal feature representation by considering both local and global features to achieve better performance compared to the hand-crafted features. ? A novel cross-modal transformer encoder architecture to learn both intra-modal temporal attention, i.e., attention between time steps within a feature representation of a modality, and cross-modal attention, i.e., attention between each modalities. The sequence cross-modal transformer consists of an additional block to learn inter-epoch attention, i.e., attention between adjacent epochs. ? A simple yet an effective method based on attention mechanisms to enable interpretation of the model's predictions. The proposed method is capable of learning and interpreting: 1) intra-modal relationships, 2) cross-modal relationships and 3) inter-epoch relationships. ? Sequence cross-modal transformer achieves state-of-theart performance in G-mean, sensitivity and specificity. In terms of accuracy, Cohen's Kappa coefficient and macro F1-score our method achieved performance on-par with the current state-of-the-art with a fourfold reduction in number of parameters as can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref> . ? Improved performance compared to previous methods, by taking lesser time to train and a much smaller model footprint in terms of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transformers</head><p>Since the introduction by Vaswani et al. <ref type="bibr" target="#b20">[21]</ref>, Transformers and its variants have been successful in NLP and computer vision tasks in terms of performance and powerful representation learning. Currently, Transformers have achieved the state-ofthe-art performance in several NLP tasks. Pre-training have been leveraged in Transformers to achieve better performance in NLP tasks, where BERT <ref type="bibr" target="#b21">[22]</ref> utilized self-supervised pretraining strategies and fine-tuning towards a supervised downstream task, whereas GPT <ref type="bibr" target="#b24">[25]</ref> focused on language modeling in pre-training. Although CNNs have dominated the field of computer vision, there have been several efforts to explore the benefits of Transformers in computer vision domain. Initially, self-attention was introduced along with CNNs for computer vision tasks <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Then the focus was on completely replacing convolutions in the architecture. ViTs <ref type="bibr" target="#b22">[23]</ref> were able to achieve this feat, by directly applying them on images with very few modifications. Furthermore, self-supervised pretraining <ref type="bibr" target="#b27">[28]</ref> was leveraged to improve the performance, which emerged several properties of the ViTs. Likewise, Transformers have been explored in several medical applications <ref type="bibr" target="#b28">[29]</ref> such as medical image segmentation <ref type="bibr" target="#b29">[30]</ref>, detection <ref type="bibr" target="#b30">[31]</ref>, and registration <ref type="bibr" target="#b31">[32]</ref>, whereas SleepTransformer <ref type="bibr" target="#b23">[24]</ref> is the only work in the domain of sleep staging. SleepTransformer was able to achieve a competitive accuracy of 81.4% on leep-EDFexpanded 2018 without any pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning Based Sleep Stage Classification</head><p>Recent work that employ deep-learning based algorithms achieved impressive results in sleep stage classification <ref type="bibr" target="#b2">[3]</ref> than conventional machine learning algorithms. The conventional machine learning algorithms generally use the handcrafted features as the input. Furthermore, this approach may not generalize to a large population due to the heterogeneity of subjects and recording devices <ref type="bibr" target="#b13">[14]</ref>.</p><p>The existing automatic sleep stage algorithms can be classified into two categories based on the input to the network: raw signals and time-frequency maps. A time-frequency map requires prior knowledge of the dataset as well as signal processing, as it heavily relies on the preprocessing steps. In contrast, deep-learning based algorithms are capable of performing automatic feature extraction, thus the drawbacks of handcrafted features are eliminated.</p><p>Several previous works have focused on utilizing the single channel EEG recordings for sleep stage classification. However, EOG is capable of detecting eye movements, which is a fundamental indicator for differentiating rapid eye movement (REM) and non-rapid eye movement (NREM) stages <ref type="bibr" target="#b32">[33]</ref>. Therefore, the information in EOG channels, i.e., multi-modal scheme can be exploited to improve the performance of sleep stage classification <ref type="bibr" target="#b33">[34]</ref>. One-to-One Classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Many-to-Many Classification</head><p>Prediction Prediction Prediction Prediction Prediction Phan et al. <ref type="bibr" target="#b16">[17]</ref>, have used both raw signals and timefrequency representations of the signals as the inputs to a sequence-to-sequence network. The overall accuracy achieved was 84% with only single channel EEG, as well as with both single channel EEG and EOG. XSleepNet is the current state-of-the-art in automatic sleep staging. The inclusion of EOG signal is insignificant in terms of the performance of the XSleepNet model. Supratak et al. <ref type="bibr" target="#b13">[14]</ref> have proposed CNN + LSTM to capture time invariant features and transition characteristics among sleep stages from raw single channel EEG. Mousavi et al. <ref type="bibr" target="#b34">[35]</ref> have proposed SleepEEGNet, composed of CNN to extract features and sequence-to-sequence model to capture long short-term context dependencies between epochs of raw single channel EEG. The sequence-to-sequence model composed of bidirectional RNN and attention mechanisms. Even though RNN and LSTM based approaches yields a competitive performance with state-of-the-art methods, the computational cost of LSTMs and RNNs should be taken into consideration. RNNs have limitations due to their recurrent nature and high complexity, thus difficult to train them in parallel. Eldele et al. <ref type="bibr" target="#b35">[36]</ref> have proposed AttnSleep which consists of a multi-resolution CNN with adaptive feature recalibration and temporal context encoder utilizing multi-head self attention mechanism to capture the temporal dependencies from single channel EEG. AttnSleep achieved accuracy of 82.9%, which is higher than most of the existing algorithms. SeqSleepNet, proposed by Phan et al. <ref type="bibr" target="#b36">[37]</ref>, is a sequenceto-sequence hierarchical RNN classification model trained on multi-channel time-frequency maps as input. SeqSleepNet uses attention based bidirectional RNN for short-term sequence modeling and bidirectional RNN for long-term sequence modeling. SeqSleepNet achieved accuracy of 82.6% with single channel EEG and 83.8% with both single channel EEG and EOG channels demonstrating the significance of including EOG to improve the accuracy. TinySleepNet proposed by Supratak et al. <ref type="bibr" target="#b37">[38]</ref> is an efficient model for sleep stage clas-sification based on raw single channel EEG, which achieved competitive performance with 83.1% accuracy. TinySleepNet is an improved version of DeepSleepNet model with improved efficiency, i.e., less number of parameters, thus reducing the computational resource requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, our proposed cross-modal transformers are presented. First, we formulate the problem definition for oneto-one and many-to-many sleep stage classification tasks. Then we introduce our cross-modal transformers: the epoch cross-modal transformer for one-to-one classification and the sequence cross-modal transformer for many-to-many classification. We further explain the multi-scale 1D CNN based representation learning and cross-modal transformer encoder under the epoch cross-modal transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>We address the problem of classifying the sleep stage of a 30 s epoch of PSG signals which were acquired in the experiment described in section IV-A. Our training set with the size of N , consists of labeled 30 s PSG epochs</p><formula xml:id="formula_0">{x i , y i } N n=1 , where {x i , y i } ? X ? Y .</formula><p>Here X ? R T ?C denotes the input space of recorded PSG signals, where T represents the time steps in an epoch and C ? {EEG (Fpz-Cz), EOG} represents the modalities in the recorded PSG signals. Y ? {WAKE, N1, N2, N3, REM} represents the output space of sleep stages. Our goal is to learn a function f ? : X ? ? Y by minimizing the error E (x,y) (L(f ? (x), y)) on the given training dataset. Here, L denotes the loss function.</p><p>We solve the aforementioned problem using cross-modal transformers under the one-to-one and many-to-many classification schemes illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>. In the one-to-one classification, we consider a single PSG epoch to predict the corresponding sleep stage <ref type="bibr" target="#b7">[8]</ref>. In many-to-many classification, we consider a sequence of PSG epochs and predict their corresponding sleep stages at once <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Epoch Cross-Modal Transformer</head><p>In this subsection, we focus on solving sleep stage classification with the one-to-one scheme using a epoch cross-modal transformer. The proposed epoch cross-modal transformer, shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, consists of two main blocks: 1) multiscale 1D-CNN for automatic representation learning and 2) a novel cross-modal transformer encoder architecture to learn both intra-modal temporal attention and cross-modal attention. The epoch transformer initially learns two separate feature representations from the input EEG and EOG signals in an epoch using 1D-CNN. Here, features are learned and extracted using non-overlapping windows from the signals. Then, the cross-modal transformer encoder learns a representation by considering intra-modal temporal attention and cross-modal attention, which is then fed into a linear layer for classification. The two main blocks of the epoch transformer are further elaborated in proceeding subsections. The key advantage of epoch cross-modal transformer is that it smaller in size and    enables faster training and tuning of the hyper-parameters on a resource constraint environment. The same model can easily be scaled into a sequence cross-modal transformers efficiently. 1) Multi-Scale 1D-CNN for Representation Learning: Inspired by vision transformers using image patches as sequential data <ref type="bibr" target="#b22">[23]</ref>, we employ multi-scale 1D-CNN as shown in <ref type="figure" target="#fig_3">Fig. 3</ref> to learn the feature representation of non-overlapping windows with the size of 0.5 s from the input 30s epoch of raw PSG signals. Here, the 1D signals X c ? R T ?1 will be mapped into a feature space of X c ? R (T /(0.5?fs))?E , where c ? C, f s is the sampling frequency and E is the embedding size. Let T /(0.5 ? f s ) be T for convenience. The features are extracted from non-overlapping windows instead of overlapping windows to improve interpretability, such that the extracted feature vectors can be fed into the cross-modal encoder as sequential data to learn the attentions between all windows.</p><p>We hypothesize that the global and local features in a window of raw signals will contribute towards the classification of the sleep stages. In order to extract both global and local features in a window, we employed multi-scale 1D-CNN, where the raw input signal goes through three parallel paths as shown in <ref type="figure" target="#fig_3">Fig. 3(b)</ref>: 1) One 1D-CNN with kernel size of 50, 2) Two 1D-CNNs with kernel sizes of 25 and 2 respectively, and 3) Three 1D-CNNs with kernel sizes of 5, 5 and 2. Each 1D convolution layer will be followed by a LeakyReLU activation. The features extracted at different scales will then be normalized using batch normalization. Finally, the extracted feature representations will be concatenated along the embedding dimension and will undergo an additional 1D-CNN with kernel size of 1, followed by LeakyReLU activation and batch normalization.</p><p>2) Cross-Modal Transformer Encoder and Classification: We propose a novel cross-modal transformer encoder architecture to learn a powerful feature representation by attending intra-modal temporal information and cross-modal relationships. As illustrated in <ref type="figure" target="#fig_3">Fig. 3(a)</ref>, the cross-modal transformer encoder consists of two main blocks: 1) attention and 2) position wise fully-connected feed-forward network blocks.</p><p>Initially, for each modality C, a learnable CLS c vector ? R (1?E) , similar to the one proposed in BERT <ref type="bibr" target="#b21">[22]</ref>, is randomly initiated and concatenated with the output of the multi-scale 1D-CNN block along the time axis. Similar to the seminal work <ref type="bibr" target="#b20">[21]</ref>, positional encodings are added to the concatenated vector and fed to the intra-modal attention block to learn the relationships between all the time steps in the modality. We extract only the vector representation corresponding to the CLS c vector from the output from each modality, as it aggregates all the information of the intra-modal temporal information <ref type="bibr" target="#b21">[22]</ref>. A new learnable CLS vector named CLS cross is randomly initialized and concatenated along with the CLS c vector representation extracted from each modality. Cross-modal attention block is employed on the aforementioned vector to learn the relationships between the modalities, which get aggregated in the corresponding representation of CLS cross . The vector representation corresponding to the CLS cross is extracted and CLS c representation of each modality is replaced by this vector. Then the concatenated vectors are passed through the feed-forward network. Finally, the vector representations corresponding to CLS cross in each modality are extracted, flattened and passed through a single linear classifier with 5 neurons for classification.</p><p>The operations of the intra-modal attention block, crossmodal attention block and feed-forward network are similar to the seminal work by <ref type="bibr" target="#b20">[21]</ref>. The attention blocks consist of multi-head attention followed by residual addition and layer normalization. The feed-forward network consists of a feedforward layer followed by residual addition and layer normalization. An additional linear layer is sufficient for classification of the sleep stages from the extracted representation, which shows the power of the learned representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sequence Cross-Modal Transformer</head><p>In order to solve sleep stage classification with the many-tomany scheme we employ sequence cross-modal transformer, which is an extension of the epoch cross-modal transformer. The proposed architecture shown in <ref type="figure" target="#fig_4">Fig. 4</ref> consists of multiple epoch level block for each 30s epoch of raw PSG signals in a non-overlapping sequence <ref type="figure" target="#fig_2">(Fig. 2)</ref>. The epoch level block is built using multi-scale 1D-CNN block and intra-modal attention block for each modality and a cross-modal attention block. The corresponding vector representation of CLS cross vector of each epoch block is extracted and concatenated. Interepoch attention block followed by a feed-forward network is employed on the concatenated vector to learn the relationships between the epochs. Finally, we extract and flatten output representation corresponding to each epoch separately and employ a linear classifier to predict their sleep stages. During inference, we let the model run across the sequence of PSG epochs, where multiple predictions for an epoch will be achieved. Then, we calculate the mean probability to predict the correct sleep stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interpretability</head><p>In this subsection, we propose a method to interpret the predictions of cross-modal transformer, which have the potential to help sleep experts and enhance their confident in the system. The main advantage of our architecture is that the attention mechanism can be simply utilized to learn and interpret: 1) intra-modal relationships, 2) cross-modal relationships and 3) inter-epoch relationships.</p><p>We extract the output of the intra-modal attention block and observe the scaled dot-product attention of the representation corresponding to the CLS c token on the representations corresponding to the non-overlapping windows of the raw signal to learn intra-modal relationships. This gives attention weights for each non-overlapping window of the raw signal, which interprets their impact on the prediction. The relationships between modalities is interpreted by the scaled dot-product attention of the representations in the output of the cross-modal attention block corresponding to CLS c of each modality. Finally, we interpret the inter-epoch relationships by the scaled dot-product attention between CLS cross of each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>We used the publicly available sleep-EDF-expanded dataset (Sleep-EDF-78) <ref type="bibr" target="#b38">[39]</ref> from Physionet <ref type="bibr" target="#b39">[40]</ref> to evaluate the proposed architecture for sleep stage classification. We used the sleep cassette (SC) dataset of sleep-EDF 2018 for our experiments, which consists of 153 whole night PSG recordings from 78 healthy individuals. Each recording comprises of two bipolar EEG channels (Fpz-Cz and Pz-Oz), a horizontal EOG signal and a submental chin EMG signal. The EMG data was sampled at 1 Hz whereas all the other signals were sampled at 100 Hz. The PSG recordings were accompanied with hypnograms annotated by sleep technicians based on Rechtschaffen and Kales (R&amp;K) guidelines <ref type="bibr" target="#b0">[1]</ref>. In the hypnogram, each 30 s epoch of the recorded data is assigned to one of the following labels : Stage 0, Stage 1, Stage 2, Stage 3, Stage 4, REM, movement time and '?' <ref type="bibr">(unscored)</ref>. For this study, we converted these annotations to the AASM standards <ref type="bibr" target="#b1">[2]</ref>, by combining Stage 3 and Stage 4 to a single stage N3 while Stage 0, Stage 1 and Stage 2 are relabelled as Wake, N1 and N2, respectively. Also, epochs consisting of annotations 'movement time' and '?' were discarded. Altogether 415, 465 30s epochs of PSG recordings were extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Preprocessing</head><p>Among the extracted epochs, 69% comprised of wake stage. This is due to the existence of long wake stages at the beginning and end of each dataset. Thus, to reduce the redundancy of wake stage, we only considered 30 minutes of wake stages before and after the sleep periods. Hence, in the resultant dataset, number of epochs are reduced to 196, 350.</p><p>For our experiments, we considered EEG from the Fpz-Cz channel and horizontal EOG data. All these data were initially segmented to a sequence of 30s epochs and corresponding labels were assigned based on the provided annotations. The EEG data was bandpass filtered between 0.2 Hz to 40 Hz using a zero-phase finite impulse response filter with a Hamming window. Then, the signals were normalized such that each signal had a zero mean and unit variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Setup</head><p>We defined two experimental setups based on the classification schemes (one-to-one and many-to-many) utilizing our proposed epoch and sequence cross-modal transformers. Both models were trained using the Adam optimizer <ref type="bibr" target="#b41">[42]</ref> with the learning rate (lr), ? 1 and ? 2 set to, 10 ?3 , 0.9 and 0.999, respectively. A weight decay of 0.0001 was applied to avoid overfitting. The batch size was experimentally chosen to 32. We used weighted categorical cross entropy as the loss function (L) for 5-class classification and the weights for the classes were set to {1, 2, 1, 2, 2}, to handle the data imbalance. For the sequence cross-modal transformer, we empirically chose the number of epochs in an input sequence as 5 . 5fold cross validation was used to evaluate the performance of the model, and to tune both hyper-parameters and the model architecture. The model was implemented in the Pytorch environment and trained using a Nvidia Quadro RTX 5000 graphics card with 16 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Metrics</head><p>Accuracy (ACC), Cohen's kappa coefficient (?), macro averaged F1-score (MF1), sensitivity (Sens.), specificity (Spec.) and macro averaged G-mean (MGm) are the metrics used to evaluate our method. Macro averaged F1-score is calculated as the average of F1-scores of all 5 classes. MF1 and MGm are considered because they are suitable metrics to evaluated the performance on imbalanced datasets. Additionally, we report F1-scores for each sleep stages to further evaluate the model's performance. Given the true positives (TP), true negative (TN), false positive (FP) and false negative (FN), MF1, Sens., Spec. and MGm can be calculated as follows,</p><formula xml:id="formula_1">M F 1 = 2 P recision ? Sensitivity P recision + Sensitivity , M Gm = Specif icity ? Sensitivity.</formula><p>where P recision = T P T P + F P ,</p><p>Sens. = T P T P + F N ,</p><formula xml:id="formula_2">Spec. = T N T N + F P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sleep Stage Classification</head><p>We present the overall performance of both epoch and sequence cross-modal transformers in comparison to previous works on the sleep-EDF-expanded 2018 dataset for both classification schemes (one-to-one and many-to-many classification) in <ref type="table" target="#tab_2">Table I and classwise performance in Table II</ref>. Our proposed sequence cross-modal transformer achieves the state-of-the-art performance in terms of sensitivity (79.4), specificity (95.8) and G-mean (87.2). In comparison to those reported in previous work, the sequence cross-modal transformer outperforms the majority of the methods presented in previous work with an overall accuracy (ACC) of 83.7%, Cohen's Kappa (?) of 0.776 and macro-averaged F1-score (MF1) of 78.4. The key factor to be observed is that our sequence cross-modal transformer achieves high or on-par classification performance with a lower number of parameters compared to previous methods. SeqSleepNet <ref type="bibr" target="#b36">[37]</ref> has lower number of parameters compared to ours, but since the method is based on RNNs, it takes a longer time during training and inference.</p><p>Difference in performance between our sequence cross-modal transformer with the current state-of-the-art method XSleepNets <ref type="bibr" target="#b16">[17]</ref> is very small. Compared to XSleepNets, our sequence crossmodal transformer achieves higher performance with less number of PSG epochs in the sequence. We compared the performance with the existing transformer based method SleepTransformer <ref type="bibr" target="#b23">[24]</ref> on sleep-EDF-expanded 2018 dataset without any pretraining on a larger database, where our sequence cross-modal transformer gave superior performance. This shows the modeling capability of our cross-modal transformer on a relatively smaller dataset compared to the SleepTransformer. We strongly believe that deep learning techniques such as transfer learning, meta learning <ref type="bibr" target="#b17">[18]</ref>, knowledge distillation <ref type="bibr" target="#b18">[19]</ref> , large-scale training and self-supervised pretraining could further improve the performance of our method. Our epoch cross-modal transformer was able to achieve good performance by considering only one PSG epoch as input and with significantly less number of parameters compared to majority of the previous works. When considering class-wise performance based on F1-score in <ref type="table" target="#tab_2">Table II</ref>, our sequence cross-modal transformer achieves state-of-theart performance in the prediction of Wake and N1 sleep stages. This enhancement in the performance can be attributed to the capability of our method to learn cross-modal relationships, where EOG along with EEG makes an impact on their predictions. The performance of our sequence cross-modal transformer in predicting N2, N3 and REM stages are on-par with the previous work. <ref type="table" target="#tab_2">Table III</ref> shows the comparison of model size in number of parameters and training time taken for 1000 steps between the proposed cross-modal transformers and the previously reported work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison of Computational Complexity</head><p>Here, our method trains faster compared to current the state-of-theart XSleepNets because their architecture is based on RNNs which requires to process the data sequentially. Unlike RNNs, transformers are capable of processing data in parallel. The parallelism in the transformers enabled our proposed method to train faster compared to current the state-of-the-art XSleepNets and other previous work as given in <ref type="table" target="#tab_2">Table III</ref>. Our epoch transformer achieves performance (ACC : 80.8) closer to the SleepTransformer (ACC : 81.4) model with significantly smaller model footprint (11.5 times smaller) and lower training time. In comparison to XSleepNets, our sequence crossmodal transformer is fourfold smaller in size and faster in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hyper-parameter Tuning</head><p>Ablation study was conducted to identify the most suitable model variation to achieve the optimal performance with minimal model print. The study was conducted on epoch cross-modal transformers, where the embedding dimension (E ? {64, 128, 256}) and the number of neurons in the hidden layer of the -forward networks (D f f ? {256, 512, 1024}) were varied. The epoch cross-modal transformers are used to conduct the study efficiently and, the same parameters were adapted to sequence cross-modal transformers. Tuning the hyper-parameters on epoch cross-modal transformers and then scaling it towards sequence cross-modal transformers is more efficient compared to tuning the hyper-parameters on the level of sequence cross-modal transformers. The results of the study is provided in <ref type="table" target="#tab_2">Table IV</ref>, where the model variation with E = 128 and D f f = 512 achieved the best performance. The same model parameters were used in the sequence cross-modal transformer to achieve good prediction performance with a small model print.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>Here, we systematically study the importance and advantages of using EOG, cross-modal attention and CLS vectors as aggregated representation in our method.    <ref type="figure" target="#fig_5">Fig. 5</ref> and the results are given in <ref type="table" target="#tab_6">Table V</ref>. As anticipated, the model using both EEG and EOG signals had better performance than only EEG, which clearly states that our method is learning from both EEG and EOG signals efficiently. Significant improvement in the prediction of N1 and REM sleep stages can be observed when EOG is added into our method. Additionally, better performance was achieved when the cross-modal relationships were incorporated into our method for sleep stage classification. Inclusion of cross-modal attention significantly improved the prediction of N1, N2, N3 and REM stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EEG (Fpz-Cz)</head><p>2) Importance of using CLS: In our method, utilizing CLS vectors as aggregated representation enabled the reduction of the model size. The size of our initial version of epoch cross-modal transformer without having CLS was ? 2.1M , which is 6.5 times larger than our proposed epoch cross-modal transformer. Addition of CLS vector enabled scaling of the proposed methods in terms of number of modalities and number of epochs in a sequence. Also, the cross-relationships between any number of modalities can be learned easily because we leverage a simple CLS latent vector for each modality which aggregates all the sequence information. Most importantly, the CLS vectors enabled interpretations of the predictions by employing self-attention, which is simple and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Comparison Between Epoch and Sequence Cross-Modal</head><p>Transformers: It can be clearly observed that the sequence crossmodal transformer significantly outperforms epoch cross-modal transformer, because of the inclusion of inter-epoch attention block and predicting sleep stages of multiple epochs simultaneously. Due to smaller model print, epoch cross-modal transformer is suitable towards resource constrained environments, where the algorithm can be implemented on edge devices. Additionally, it also enables efficient training of sequence cross-modal transformer to achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Interpreting the Results</head><p>Along with competitive sleep stage classification performance, the major contribution of cross-modal transformers are their interpretability. As mentioned in section III-D, we leverage the attention mechanisms to interpret the results as intra-modal, cross-modal and inter-epoch relationships illustrated in <ref type="figure">Fig. 6 and Fig. 7</ref>. Here, we interpret the predictions of sequence cross-modal transformer for a sequence of PSG epochs. <ref type="figure">Fig. 6</ref> illustrates the prediction for a sequence of PSG epochs in N3, N3, N3, N2 and wake stages respectively. Inter-epoch attention graphs visualize the relationships between the epochs. Consider epoch <ref type="figure">Fig. 6</ref>. Interpretation results by our sequence cross-modal transformer for a sequence of PSG recordings with N3, N3, N3, N2 and wake stages respectively are visualized. The bar plots in blue represents the inter-epoch attentions scores, bar plots in red represents the cross-modal attention scores and plots with EEG and EOG signals of each epoch visualizes the attention score along the signals. In the signal plots, the darker red areas indicates higher importance and lighter red areas indicates the lesser importance towards sleep stage classification. The visually identified patterns in the signals are highlighted by green circles. 4 in the sequence, where the impact of previous epochs in the prediction is clearly visualized (In a normal sleep cycle, a person can go to the N2 sleep stage from the N3 sleep stage.). Crossmodal attention graphs interprets the impact of each modality on the prediction. Finally, the intra-modal attention maps interprets, which time segments in the PSG signal impacts the predictions. The darker areas in the signals had more impact on the prediction. Our method was able to highlight important patterns in the signals corresponding to specific sleep stages, which shows the reliability of our method. In epoch 2, the ? rhythm is highlighted which is a feature of the N3 sleep stage. Spindles and K-complexes occurs in N2 sleep stages, which is highlighted in epoch 4. During wake stage, muscle artifacts in the EEG and EOG channels increases and eye movements can be observed in EOG channel <ref type="bibr" target="#b1">[2]</ref>. These patterns are clearly highlighted in epoch 5.</p><p>Similarly, <ref type="figure">Fig. 7</ref> shows the interpretations of a sequence of PSG epochs in REM, REM, W, W and N1 stages, respectively. In the cross-modal attention graphs for REM stages and wake stages, we observe that the EOG modality had more impact on prediction. This can be attributed to the eye movements occurring in these stages, which are captured in the EOG signals. The eye movements are clearly highlighted in the intra-modal attention maps of epoch 1 and 5. Additionally, the method was able to highlight low voltage ? rhythm, which occurs during REM stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we present an interpretable, transformer-based deep learning method named Cross-Modal Transformers, for automatic sleep stage classification. Our proposed sequence cross-modal transformer achieved classification performance on-par with the current state-of-the-art method, with 4 times reduction in number of parameters and faster training. In addition to accurate sleep staging, our major contribution is to eliminate the black-box behavior of deep learning by leveraging attention mechanisms to interpret the results. <ref type="figure">Fig. 7</ref>. Interpretation results by our sequence cross-modal transformer similar to <ref type="figure" target="#fig_5">Fig.5</ref>, for a different sequence of PSG recordings with REM, REM, wake, wake and N1 stages, respectively, are visualized.</p><p>We strongly believe that developing interpretable deep learning methods is the most feasible way forward to use artificial intelligence for clinical applications. In future work, the proposed method can be further improved by employing training strategies in deep learning such as transfer learning and self-supervised pre-training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Performance of our proposed cross-modal transformers (in red squares) and other previously reported works (in blue circles) on sleep-EDFexpanded 2018 dataset. Our sequence cross-modal transformer achieves on-par performance with the state-of-the-art, with fourfold reduction in parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>The two classification schemes used in the domain of sleep staging and in our experiments. In one-to-one classification, the sleep stage of an individual PSG epoch is predicted, whereas in many-to-many classification the sleep stages of multiple epochs are predicted simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of the epoch cross-modal transformer consisting of multi-scale 1D-CNN blocks, intra-modal attention blocks, cross-modal attention block and feed forward networks. (a) shows the overall architecture with two signals as input. (b) visualizes multi-scale 1D-CNN blocks, which consists of three pathways to learn both local and global features. (c) and (d) shows the architectures of the attention blocks and feed forward networks. Here CLS EEG , CLS EOG and CLS Cross are the CLS vectors initiated to learn the aggregated representation of intra-modal relationships of EEG and EOG modalities and cross-modal relationship between EEG and EOG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The architecture of sequence cross-modal transformer, which is an extension of epoch cross-modal transformer. The sequence cross-modal transformer consists of multiple epoch level blocks to learn the epoch level representation and an additional block to learn inter epoch relationships.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The experimental epoch level models used to evaluate the importance of EOG signals and cross-modal attention. (a) shows the architecture which only takes EEG as the input and (b) shows a version of the epoch cross-modal transformer without cross-modal attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc>Performance comparison between cross-modal transformer and previous works on sleep-EDF-expanded 2018 dataset.</figDesc><table><row><cell>Method</cell><cell>Channels</cell><cell cols="3">Epochs in a Sequence Parameters ACC Number of</cell><cell>?</cell><cell cols="3">Overall Performance MF1 Sens. Spec.</cell><cell>MGm</cell></row><row><cell>SleepEEGNet [35]</cell><cell>Fpz-Cz</cell><cell>10</cell><cell>? 2.6M</cell><cell>80.0</cell><cell>0.730</cell><cell>73.6</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>DeepSleepNet [41]</cell><cell>Fpz-Cz</cell><cell>25</cell><cell>? 24.7M</cell><cell>77.1</cell><cell>0.69</cell><cell>71.2</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>MultitaskCNN [36]</cell><cell>Fpz-Cz</cell><cell>?</cell><cell>?</cell><cell>79.6</cell><cell>0.72</cell><cell>72.8</cell><cell>?</cell><cell>?</cell><cell>82.5</cell></row><row><cell>AttnSleep [36]</cell><cell>Fpz-Cz</cell><cell>3</cell><cell>?</cell><cell>82.9</cell><cell>0.77</cell><cell>78.1</cell><cell>?</cell><cell>?</cell><cell>85.6</cell></row><row><cell>SleepTransformer [24]</cell><cell>Fpz-Cz</cell><cell>21</cell><cell>? 3.7M</cell><cell>81.4</cell><cell>0.743</cell><cell>74.3</cell><cell>74.5</cell><cell>95.0</cell><cell>84.13</cell></row><row><cell>SeqSleepNet [41]</cell><cell>Fpz-Cz</cell><cell>20</cell><cell>? 0.2M</cell><cell>82.6</cell><cell>0.76</cell><cell>76.4</cell><cell>76.3</cell><cell>95.4</cell><cell>85.32</cell></row><row><cell>FCNN+RNN [17]</cell><cell>Fpz-Cz</cell><cell>20</cell><cell>? 5.6M</cell><cell>82.8</cell><cell>0.761</cell><cell>76.6</cell><cell>75.9</cell><cell>95.4</cell><cell>85.09</cell></row><row><cell>XSleepNet2 [17]</cell><cell>Fpz-Cz</cell><cell>20</cell><cell>? 5.8M</cell><cell>84.0</cell><cell>0.778</cell><cell>77.9</cell><cell>77.5</cell><cell>95.7</cell><cell>86.12</cell></row><row><cell>XSleepNet1 [17]</cell><cell>Fpz-Cz</cell><cell>20</cell><cell>? 5.8M</cell><cell>83.6</cell><cell>0.773</cell><cell>77.8</cell><cell>77.7</cell><cell>95.7</cell><cell>86.23</cell></row><row><cell>NaiveFusion [17]</cell><cell>Fpz-Cz</cell><cell>20</cell><cell>? 5.8M</cell><cell>82.3</cell><cell>0.755</cell><cell>76.2</cell><cell>75.7</cell><cell>95.3</cell><cell>84.94</cell></row><row><cell>TinySleepNet [41]</cell><cell>Fpz-Cz</cell><cell>15</cell><cell>? 1.3M</cell><cell>83.1</cell><cell>0.77</cell><cell>78.1</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>XSleepNet2 [17]</cell><cell>Fpz-Cz &amp; EOG</cell><cell>20</cell><cell>?</cell><cell>84.0</cell><cell>0.778</cell><cell>78.7</cell><cell>77.6</cell><cell>95.7</cell><cell>86.18</cell></row><row><cell>XSleepNet1 [17]</cell><cell>Fpz-Cz &amp; EOG</cell><cell>20</cell><cell>?</cell><cell>84.0</cell><cell>0.777</cell><cell>78.4</cell><cell>77.1</cell><cell>95.6</cell><cell>85.85</cell></row><row><cell>SeqSleepNet [17]</cell><cell>Fpz-Cz &amp; EOG</cell><cell>20</cell><cell>?</cell><cell>83.8</cell><cell>0.776</cell><cell>78.2</cell><cell>77.4</cell><cell>95.6</cell><cell>86.02</cell></row><row><cell>FCNN+RNN [17]</cell><cell>Fpz-Cz &amp; EOG</cell><cell>20</cell><cell>?</cell><cell>82.7</cell><cell>0.759</cell><cell>76.9</cell><cell>75.5</cell><cell>95.3</cell><cell>84.82</cell></row><row><cell>NaiveFusion [17]</cell><cell>Fpz-Cz &amp; EOG</cell><cell>20</cell><cell>?</cell><cell>82.5</cell><cell>0.757</cell><cell>76.9</cell><cell>75.8</cell><cell>95.3</cell><cell>84.99</cell></row><row><cell>Epoch Cross-Modal Transformer</cell><cell>Fpz-Cz &amp; EOG</cell><cell>1</cell><cell>? 0.32M</cell><cell>80.8</cell><cell>0.736</cell><cell>74.7</cell><cell>75.4</cell><cell>95.0</cell><cell>84.6</cell></row><row><cell>Sequence Cross-Modal Transformer</cell><cell>Fpz-Cz &amp; EOG</cell><cell>5</cell><cell>? 1.42M</cell><cell>83.7</cell><cell>0.776</cell><cell>78.4</cell><cell>79.4</cell><cell>95.8</cell><cell>87.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II Class</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Model size and training time comparison between cross-modal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">transformers and previous works.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>No of Parameters</cell><cell>Training time per 1000 steps (s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SleepTransformer [24]</cell><cell>? 3.7M</cell><cell>308</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>XSleepNet2 [17]</cell><cell>? 5.8M</cell><cell>828</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SeqSleepNet [41]</cell><cell>? 0.2M</cell><cell>379</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch Cross-Modal Transformer</cell><cell>? 0.32M</cell><cell>53</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence Cross-Modal Transformer</cell><cell>? 1.42M</cell><cell>174</cell></row><row><cell cols="7">-wise performance comparison between cross-modal transformer</cell></row><row><cell cols="6">and previous works on sleep-EDF-expanded 2018 dataset.</cell><cell></cell></row><row><cell>Method</cell><cell>Chan.</cell><cell>W</cell><cell cols="3">Per-class Performance N1 N2 N3</cell><cell>REM</cell></row><row><cell>SleepEEGNet [35]</cell><cell cols="5">Fpz-Cz 91.7 44.1 82.5 73.5</cell><cell>76.1</cell></row><row><cell>DeepSleepNet [41]</cell><cell cols="5">Fpz-Cz 90.4 46.0 79.1 68.6</cell><cell>71.8</cell></row><row><cell>MultitaskCNN [36]</cell><cell cols="5">Fpz-Cz 90.9 39.7 83.2 76.6</cell><cell>73.5</cell></row><row><cell>AttnSleep [36]</cell><cell cols="4">Fpz-Cz 92.6 47.4 85.5</cell><cell>83.7</cell><cell>81.5</cell></row><row><cell cols="6">SleepTransformer [24] Fpz-Cz 91.7 40.4 84.3 77.9</cell><cell>77.2</cell></row><row><cell>SeqSleepNet [41]</cell><cell cols="3">Fpz-Cz 91.8 42.6</cell><cell>86.5</cell><cell>76.4</cell><cell>84.1</cell></row><row><cell>FCNN+RNN [17]</cell><cell cols="5">Fpz-Cz 92.5 47.3 85.0 79.2</cell><cell>78.9</cell></row><row><cell>XSleepNet2 [17]</cell><cell cols="5">Fpz-Cz 93.3 49.9 86.0 78.7</cell><cell>81.8</cell></row><row><cell>XSleepNet1 [17]</cell><cell cols="5">Fpz-Cz 92.6 50.2 85.9 79.2</cell><cell>81.3</cell></row><row><cell>NaiveFusion [17]</cell><cell cols="5">Fpz-Cz 93.2 49.6 86.2 79.4</cell><cell>82.5</cell></row><row><cell>TinySleepNet [41]</cell><cell cols="5">Fpz-Cz 92.8 51.0 85.3 81.1</cell><cell>80.3</cell></row><row><cell>FCNN + RNN [17]</cell><cell>Fpz-Cz EOG</cell><cell>91.2</cell><cell>45.8</cell><cell>84.7</cell><cell>78.5</cell><cell>84.2</cell></row><row><cell>XSleepNet2 [17]</cell><cell>Fpz-Cz EOG</cell><cell>92.6</cell><cell>50.3</cell><cell>85.5</cell><cell>79.2</cell><cell>85.7</cell></row><row><cell>XSleepNet1 [17]</cell><cell>Fpz-Cz EOG</cell><cell>92.2</cell><cell>49.1</cell><cell>85.6</cell><cell>78.8</cell><cell>86.3</cell></row><row><cell>NaiveFusion [17]</cell><cell>Fpz-Cz EOG</cell><cell>91.0</cell><cell>47.7</cell><cell>84.7</cell><cell>77.7</cell><cell>83.6</cell></row><row><cell>Epoch Cross-Modal Transformer</cell><cell>Fpz-Cz EOG</cell><cell>92.3</cell><cell>45.3</cell><cell>82.9</cell><cell>75.5</cell><cell>77.4</cell></row><row><cell>Sequence Cross-Modal Transformer</cell><cell>Fpz-Cz EOG</cell><cell>93.7</cell><cell>52.4</cell><cell>85.0</cell><cell>76.0</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV</head><label>IV</label><figDesc>Ablation study on different model variations of epoch cross-modal transformer by varying embedding dimension (E) and dimension of the hidden layer in feed forward network (D f f ). In manual annotation of sleep stages, EOG signals plays a major role in identifying Wake, N1 and REM sleep stages. This shows the importance of developing an algorithm to utilize the features of EOG signals along with EEG for sleep staging. We show the importance of efficiently utilizing EOG and learning the cross-relationship between EEG and EOG modalities by conducting a separate study. Here, the epoch cross-modal transformer was utilized, where its model architecture was varied based on the requirements as shown in</figDesc><table><row><cell>Embedding Dim (E)</cell><cell>Dim feed Forward (D f f )</cell><cell>ACC</cell><cell>?</cell></row><row><cell>64</cell><cell></cell><cell cols="2">80.66 0.735</cell></row><row><cell>128</cell><cell>256</cell><cell cols="2">80.47 0.733</cell></row><row><cell>256</cell><cell></cell><cell cols="2">80.63 0.734</cell></row><row><cell>64</cell><cell></cell><cell cols="2">80.39 0.732</cell></row><row><cell>128</cell><cell>512</cell><cell>80.75</cell><cell>0.736</cell></row><row><cell>256</cell><cell></cell><cell cols="2">80.52 0.732</cell></row><row><cell>64</cell><cell></cell><cell cols="2">80.49 0.733</cell></row><row><cell>128</cell><cell>1024</cell><cell cols="2">80.63 0.735</cell></row><row><cell>256</cell><cell></cell><cell cols="2">80.44 0.732</cell></row><row><cell cols="4">1) Importance of EOG and Cross-Modal Attention:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc>The results of the study on the importance of EOG signal and cross-modal relationships for sleep stage classification using epoch cross-modal transformers.</figDesc><table><row><cell>Method</cell><cell>ACC</cell><cell>?</cell><cell>W</cell><cell cols="3">Per-class Performance N1 N2 N3</cell><cell>REM</cell></row><row><cell>Only EEG</cell><cell>78.3</cell><cell>0.704</cell><cell cols="2">91.4 37.7</cell><cell>81.6</cell><cell>75.3</cell><cell>69.3</cell></row><row><cell>EEG + EOG</cell><cell>80.4</cell><cell>0.731</cell><cell cols="2">92.4 44.1</cell><cell>82.4</cell><cell>74.5</cell><cell>76.8</cell></row><row><cell>EEG + EOG + CMA</cell><cell>80.8</cell><cell>0.736</cell><cell cols="2">92.3 45.3</cell><cell>82.9</cell><cell>75.5</cell><cell>77.4</cell></row><row><cell cols="4">*CMA refers to Cross-Modal Attention.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors extend their gratitude towards Dr. Ranga Rodrigo and the National Research Council of Sri Lanka for providing computational resources to conduct our experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A manual for standardized terminology, techniques and scoring system for sleep stages in human subjects. Brain information service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rechtschaffen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
		<respStmt>
			<orgName>Brain Research Institute. US Dept. of Health, Education, and Welfare</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical Specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Iber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ancoli-Israel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>American Academy of Sleep Medicine</publisher>
			<pubPlace>Westchester, Illinois</pubPlace>
		</imprint>
	</monogr>
	<note>1st ed.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated Detection of Sleep Stages Using Deep Learning Techniques: A Systematic Review of the Last Decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci. (Switz.)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">8963</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolution-and Attention-Based Neural Network for Automated Sleep Stage Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Environ. Res. Public Health</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4152</biblScope>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated Multi-Model Deep Neural Network for Sleep Stage Scoring with Unfiltered Clinical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep Breath</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint Classification and Prediction CNN Framework for Automatic Sleep Stage Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1296" />
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Deep Learning Model for Automated Sleep Stages Classification Using PSG Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Baloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Environ. Res. Public Health</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">599</biblScope>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SeqSleep-Net: End-to-End Hierarchical Recurrent Neural Network for Sequenceto-Sequence Automatic Sleep Staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="400" to="410" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic Sleep Stage Classification Using Single-Channel EEG: Learning Sequential Features with Attention-Based Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf</title>
		<meeting>Conf</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1452" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hierarchical neural network for sleep stage classification based on comprehensive feature learning and multi-flow sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1351" to="1366" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?ngkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sleep Stage Classification Using Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1587" to="1597" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Network Analysis of Sleep Stages Enables Efficient Diagnosis of Narcolepsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Stephansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeepSleepNet: A Model for Automatic Sleep Stage Scoring Based on Raw Single-Channel EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixed Neural Network Approach for Temporal Sleep Stage Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="324" to="333" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards more accurate automatic sleep staging via deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1787" to="1798" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Xsleepnet: Multi-view sequential model for automatic sleep staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Metasleeplearner: A pilot study on fast adaptation of bio-signals-based sleep stage classifier to new individual subject using meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Banluesombatkul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ouppaphan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Leelaarporn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lakhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaitusaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaimchariyatam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chuangsuwanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dilokthanakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wilaiprasitporn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1949" to="1963" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transferring structured knowledge in unsupervised domain adaptation of a sleep staging network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What is interpretable? using machine learning to design interpretable decisionsupport systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mastronarde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10799</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting>Adv. Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikkelsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Vos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11043</idno>
		<title level="m">SleepTransformer: Automatic Sleep Staging with Interpretability and Uncertainty Quantification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ocnet: Object context network for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transformers in medical imaging: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shamshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Transformer for polyp detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Vit-v-net: Vision transformer for unsupervised volumetric medical image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06468</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Hierarchical Sequential Neural Network with Feature Fusion for Sleep Staging Based on EOG and RR Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66020</biblScope>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Light-Weight Sleep Monitoring: Electrode Distance Matters More Than Placement for Automatic Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Mikkelsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Rank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Hemmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kidmose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04567</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Afghah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SleepEEGNet: Automated Sleep Stage Scoring with Sequence to Sequence Deep Learning Approach</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">216456</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An Attention-Based Deep Learning Approach for Sleep Stage Classification with Single-Channel EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eldele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="809" to="818" />
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Seqsleepnet: end-to-end hierarchical recurrent neural network for sequence-tosequence automatic sleep staging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ch?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">De</forename><surname>Vos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabilitation Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="410" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tinysleepnet: An efficient deep learning model for sleep stage scoring based on raw single-channel eeg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. IEEE</title>
		<imprint>
			<biblScope unit="page" from="641" to="644" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analysis of a Sleep-Dependent Neuronal Feedback Loop: the Slow-Wave Microcontinuity of the EEG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zwinderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kamphuisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oberye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1185" to="1194" />
			<date type="published" when="2000-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PhysioBank, PhysioToolkit, and PhysioNet : Components of a New Research Resource for Complex Physiologic Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="215" to="220" />
			<date type="published" when="2000-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepsleepnet-lite: A simplified automatic sleep stage scoring model with uncertainty estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fiorillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Faraci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabilitation Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2076" to="2085" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
