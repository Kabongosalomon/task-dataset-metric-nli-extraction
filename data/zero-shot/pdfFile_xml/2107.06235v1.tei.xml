<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Computer Vision and Image Understanding Exploiting Image Translations via Ensemble Self-Supervised Learning for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><forename type="middle">J</forename><surname>Piva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<addrLine>Groene Loper 12</addrLine>
									<postCode>5612AZ</postCode>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gijs</forename><surname>Dubbelman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<addrLine>Groene Loper 12</addrLine>
									<postCode>5612AZ</postCode>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Computer Vision and Image Understanding Exploiting Image Translations via Ensemble Self-Supervised Learning for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce an unsupervised domain adaption (UDA) strategy that combines multiple image translations, ensemble learning and self-supervised learning in one coherent approach. We focus on one of the standard tasks of UDA in which a semantic segmentation model is trained on labeled synthetic data together with unlabeled real-world data, aiming to perform well on the latter. To exploit the advantage of using multiple image translations, we propose an ensemble learning approach, where three classifiers calculate their prediction by taking as input features of different image translations, making each classifier learn independently, with the purpose of combining their outputs by sparse Multinomial Logistic Regression. This regression layer known as meta-learner helps to reduce the bias during pseudo label generation when performing self-supervised learning and improves the generalizability of the model by taking into consideration the contribution of each classifier. We evaluate our method on the standard UDA benchmarks, i.e. adapting GTA V and Synthia to Cityscapes, and achieve state-of-the-art results in the mean intersection over union metric. Extensive ablation experiments are reported to highlight the advantageous properties of our proposed UDA strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, deep learning has shown impressive results in many computer vision tasks. This advancement has largely come as a result of training very deep neural networks on largescale datasets <ref type="bibr" target="#b8">(Deng et al., 2009;</ref><ref type="bibr" target="#b18">Kuznetsova et al., 2020)</ref>. The satisfactory performance of these models is substantially tied to the training data due to dataset bias <ref type="bibr" target="#b31">(Tommasi et al., 2017)</ref>, and these models are unfortunately incapable of generalizing well to unseen data. To circumvent this limited generalization of deep models to unseen data, the goal of Unsupervised Domain Adaptation (UDA) is to improve the model's performance on an a priori known target dataset without requiring that this target dataset is labeled.</p><p>In UDA the model's training data contains both fully labeled samples from a source domain as well as unlabeled data from the target domain. In general, models that are conventionally trained with data from two (source and target) distributions suffer a significant performance drop due to the underlying domain (a) Multi-task tri-training <ref type="bibr" target="#b28">(Ruder and Plank, 2018;</ref><ref type="bibr" target="#b43">Zhang et al., 2018a)</ref> (b) Our approach <ref type="figure">Fig. 1</ref>. To adapt a source annotated set to an unlabeled target set, <ref type="bibr" target="#b28">(Ruder and Plank, 2018;</ref><ref type="bibr" target="#b43">Zhang et al., 2018a)</ref> leverages the images and labels of the source set to train three classifiers C 1 , C 2 , and C 3 in a supervised way, enforcing discrepancy between C 1 and C 2 via a cosine distance loss. Once trained, only the predictions of C 1 and C 2 are used to label the target set, and C 3 is trained separately on this new labeled dataset, obtaining the final inference from C 3 . Our proposed ensemble uses three image translations of the source dataset, and encourages discrepancy across all classifiers by feeding the features of each translation to a different predictor. Hereafter, the meta-learner C m learns to weight the predictions of each classifier for the classes, to create pseudo-labels for the target set. Unlike <ref type="bibr" target="#b43">(Zhang et al., 2018a)</ref>, a round of self-training will consist of training C 1 , C 2 and C 3 on this new labeled target set, and retraining C m to create robust pseudo-labels. Finally, inference is made via the meta-learner C m . arXiv:2107.06235v1 [cs.CV] 13 Jul 2021 gap. For this reason, UDA approaches aim to mitigate this gap by transferring the knowledge learned from the source annotated domain to the unlabeled target domain. Although the nature of the source and target distributions can vary depending on the application and computer vision task, one of the most challenging scenarios involves training a semantic segmentation model using synthetic data as the (labeled) source dataset and real-world data as the (unlabeled) target dataset. Since this synthetic-to-real per-pixel classification scenario involves a domain gap that is very challenging to address, it is used as the standard scenario in practically all recent computer vision research on UDA <ref type="bibr" target="#b20">Luo et al., 2018;</ref><ref type="bibr" target="#b41">Yang and Soatto, 2020;</ref><ref type="bibr" target="#b39">Yang et al., 2020a;</ref><ref type="bibr" target="#b47">Zou et al., 2018)</ref>. To be able to compare our work with these state-of-the-art methods using the standard UDA benchmarks, our work also focuses on performing UDA in the context of synthetic-to-real training of semantic segmentation models. However, we emphasize that the applicability and practical value of UDA are broader than this particular scenario.</p><p>Lately, several UDA methods have shown promising results by using self-supervised learning (SSL), a technique that leverages the model's predictions to label the target domain and retrain the model on this new subset. To determine whether a prediction is a label candidate or not, a criterion needs to be established, and current single encoder-decoder methods adopt a confidence thresholding scheme as the standard procedure <ref type="bibr" target="#b41">Yang and Soatto, 2020;</ref><ref type="bibr" target="#b47">Zou et al., 2018)</ref>. A deficiency of this thresholding approach is that the model can still consider high confident mistaken predictions as pseudo-labels, affecting negatively the retraining process. To address this issue, <ref type="bibr" target="#b28">(Ruder and Plank, 2018)</ref> investigate several ensemble approaches that leverage multiple classifiers. With this adjustment, an extra condition can be added to complement confidence thresholding: if the classifiers agree on the winner class, the prediction can be considered as pseudo-label.</p><p>In the particular ensemble approach Multi-task tri-training proposed in <ref type="bibr" target="#b28">(Ruder and Plank, 2018)</ref>, one encoder is shared across three classifiers and the training is performed in two stages (see <ref type="figure">Fig. 1a</ref>). First, the source annotated set is used to train the encoder along with all three classifiers in a supervised manner, while disagreement between the first two classifiers is enforced with a discrepancy loss, defined as the cosine distance between the weights of the first two classifiers <ref type="bibr" target="#b4">(Bousmalis et al., 2016)</ref>. In the second training stage, these two classifiers will create pseudo-labels for the unlabeled target set using both confidence thresholding and class agreement as labeling criteria, and the third classifier will be trained on this labeled subset of the target set. This process is repeated a certain number of times (known as self-supervision rounds) until convergence.</p><p>Although this general approach of <ref type="bibr" target="#b28">(Ruder and Plank, 2018)</ref> is suitable for UDA, its practical implementation for semantic segmentation <ref type="bibr" target="#b43">(Zhang et al., 2018a)</ref> has shown limited performance. This can be attributed to several pitfalls in the training strategy: 1) the model does not fully exploit all the members of the ensemble for pseudo label generation, since it uses only two out of the three classifiers for this process, and 2) the cosine distance between the weights of two classifiers to encour-age discrepancy implies that the angle between the weights of two classifiers will converge to 90 degrees, but this is not a sufficient condition to ensure useful disagreement between the two classifiers.</p><p>To overcome the aforementioned deficiencies, we research an alternative approach in which all the classifiers participate during pseudo-label generation as well as network retraining, and where the discrepancy is encouraged without a cosine distance loss (see <ref type="figure">Fig. 1b</ref>). We hypothesize that a discrepancy loss is not needed if instead, each classifier learns from a different set of features. Given a source annotated image, we propose in the first training stage to leverage multiple different imageto-image translations, making sure that each classifier along with the encoder learns from a particular translation. As a result, each predictor will focus on a particular translation and therefore we eliminate the need of a specific (cosine) loss for disagreement. In addition, our second contribution is a metalearning layer that ensembles the output of each classifier for the classes, considering a classifier more than the others when it performs better than the rest for a particular class. During the second stage of SSL, the meta-learner is trained on the outputs of the three classifiers to generate robust pseudo-labels on the target set that are used to retrain the entire network. This process is repeated for few iterations until convergence is reached.</p><p>Besides self-supervised learning, practically all state-of-theart UDA methods, of which the most relevant ones are detailed in Section 4.4, exploit a combination of techniques that target different aspects of the UDA problem. The most commonly used techniques are:</p><p>? Image translations: creating alternative representations of either the source, the target, or both domains to reduce the domain gap between source and target and thereby increase the robustness of the model <ref type="bibr" target="#b12">(Gong et al., 2019;</ref><ref type="bibr" target="#b22">Murez et al., 2018;</ref><ref type="bibr" target="#b37">Wu et al., 2018b;</ref><ref type="bibr" target="#b39">Yang et al., 2020a;</ref><ref type="bibr" target="#b41">Yang and Soatto, 2020</ref>). ? Feature alignments: using adversarial training to align the features from both domains without supervision to reduce the domain gap at feature level <ref type="bibr" target="#b16">(Hong et al., 2018;</ref><ref type="bibr" target="#b26">Romijnders et al., 2019;</ref><ref type="bibr" target="#b29">Sankaranarayanan et al., 2017;</ref><ref type="bibr" target="#b32">Tsai et al., 2018;</ref><ref type="bibr" target="#b44">Zhang et al., 2018b)</ref>. ? Model regularization: preventing the model from overfitting to the most dominant classes by regulating the probability distribution on the output space <ref type="bibr" target="#b34">(Vu et al., 2019;</ref><ref type="bibr" target="#b41">Yang and Soatto, 2020)</ref>. Although the focus of this research is to improve selfsupervised learning in the context of UDA through combining ensemble learning with multiple image translations, we also integrate the aforementioned techniques in our approach, to reach state-of-the-art performance. The details of our approach are provided in Section 3. In our experiments, described in Section 4, we take care to differentiate between the performance obtained using the complete set of techniques and the performance obtained as a result of our novel methodology.</p><p>In summary, the main contributions of our work are:</p><p>? A meta-learner that exploits multiple image-to-image translations within the context of Ensemble Learning via weighting each classifier's prediction with a sparse Multi-nomial Logistic Regression. This opens a new line of research where Ensemble Learning <ref type="bibr" target="#b35">(Wolpert, 1992)</ref> and Multitask tri-training <ref type="bibr" target="#b28">(Ruder and Plank, 2018)</ref> meet. ? Our approach achieves state-of-the-art performance for two standard UDA benchmarks: adapting GTA V <ref type="bibr" target="#b25">(Richter et al., 2016)</ref> to Cityscapes <ref type="bibr" target="#b7">(Cordts et al., 2016)</ref> and SYN-THIA <ref type="bibr" target="#b27">(Ros et al., 2016)</ref> to Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Most of the state-of-the-art methods combine different strategies to achieve competitive results. In this section, we focus on related work that, similar to our approach, use strategies such as image translation, self-supervised learning, and/or ensemble methods.</p><p>Image translation methods for UDA have recently been widely used to improve the performance of UDA methods. Since these techniques can be trained without the need of labels, the images of the source dataset can be transformed to a different space where the domain gap between the transformed images and the target dataset is smaller. This is the case for several UDA methods, that choose to transform the source to the target set, either by using a deep neural network <ref type="bibr" target="#b15">(Hoffman et al., 2018;</ref><ref type="bibr" target="#b19">Li et al., 2019;</ref><ref type="bibr" target="#b37">Wu et al., 2018b)</ref> or image processing techniques such as the Fourier transform <ref type="bibr" target="#b41">(Yang and Soatto, 2020)</ref>. Other works have considered mapping to an intermediate space <ref type="bibr" target="#b22">(Murez et al., 2018)</ref>, where the features are domain agnostic. In addition, <ref type="bibr" target="#b39">(Yang et al., 2020a)</ref> has shown that mapping the target dataset to the source is also effective, leading to state-of-the-art results. Alternatively, <ref type="bibr" target="#b12">(Gong et al., 2019)</ref> explores the possibility of generating multiple intermediate representations between the source and the target domain, where each arbitrary representation belongs to a point in a manifold of domains. Regardless of the chosen target space to which the annotated dataset is mapped, to the best of our knowledge, no works have considered using multiple representations in parallel to improve UDA models, as explored in this work.</p><p>Self-supervised Learning in UDA. Many recent UDA methods leverage self-supervised learning as a way of using the model's predictions to learn from the unlabeled target domain. When using the model's outputs, it is needed to establish criteria to filter out spurious predictions and select reliable label candidates. Many methods that use a single encoder-single decoder architecture propose as criteria to use confidence thresholding on the probability maps in the output space <ref type="bibr" target="#b41">Yang and Soatto, 2020;</ref><ref type="bibr" target="#b47">Zou et al., 2018)</ref>, although they still suffer from the propagation of errors due to the inclusion of highly confident but mistaken predictions. Other methods such as <ref type="bibr" target="#b9">(Deng et al., 2019;</ref><ref type="bibr" target="#b38">Xie et al., 2020)</ref> prefer to use a teacherstudent arrangement, in which the teacher network learns first from both domains to consequently transfer this knowledge to the student model via knowledge distillation <ref type="bibr" target="#b23">(Nguyen-Meidine et al., 2021)</ref>. While knowledge distillation has shown promising results, its application to self-supervised learning is limited due to the usage of a single network to generate pseudo-labels, instead of using multiple predictions to agree on the selection of pseudo labels.</p><p>Ensemble methods for UDA propose to increase the number of predictions for a single input by changing the network architecture. For instance, Co-Training (CT) utilizes two classifiers to create different points of view from the same sample to produce pseudo-labels <ref type="bibr" target="#b2">(Blum and Mitchell, 1998;</ref><ref type="bibr" target="#b13">Hady and Schwenker, 2008)</ref>, used later for the unlabeled training data. Recent applications of CT in UDA have demonstrated promising results <ref type="bibr" target="#b20">(Luo et al., 2018)</ref>. Tri-Training (TT) <ref type="bibr" target="#b45">(Zhou and Li, 2005)</ref> can be conceived as an extension of CT, where three members participate in the ensemble, each of these consisting of a feature extractor and a classifier. Since TT is computationally expensive, Multitask tri-training (MTri) <ref type="bibr" target="#b28">(Ruder and Plank, 2018)</ref> was proposed, where a common feature extractor is shared among three classifiers, computing different outputs from the same features. The idea behind MTri is to make the feature extractor learn those features that are invariant across the source and target domain, whilst forcing a discrepancy between the classifiers through a discrepancy operator.</p><p>When sharing a feature extractor in MTri, the discrepancy across classifiers becomes a key factor to generate pseudolabels during SSL <ref type="bibr" target="#b43">(Zhang et al., 2018a)</ref>. While a cosine distance might help to enforce a certain diversity, we hypothesize that feeding constantly the same features to all the classifiers does not optimally allow the encoder to learn domain invariant representations. If we obtain these alternative representations from an image translation model, we can encourage discrepancy by feeding the features of a specific representation to a different classifier, improving simultaneously the generalization capacity of the encoder. These are the principles on which our method is based, bringing together different research lines: image transformations, ensemble learning, and self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem statement</head><p>Given the source dataset S consisting of a set of images X S and corresponding semantic labels Y S (e.g., synthetic data generated by computer graphic simulations) and the unlabeled target dataset T consisting only of the images X T without labels, the goal of UDA is to design and train a neural network for semantic segmentation and to make it perform as close as possible to a model hypothetically trained on X T with ground truth labels Y T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network architecture</head><p>The design of our approach is based on the hypothesis that different image-to-image translations of input images can contribute individually to the generalization process. This brings forward a general design, in which an image translation module T generates different image-to-image translations that are consequently used by the semantic segmentation network during UDA training (see <ref type="figure">Fig. 2</ref>). <ref type="figure">Fig. 2</ref>. Network architecture and losses. Our method combines different transformations coming from an image translation module T by teaching a meta-learner C m to balance the outputs from the classifiers C 1 , C 2 and C 3 with respect to the classes. Each classifier focuses on the features of a single transformation, and the feature extractor E acts as a common features provider. Extra adaption is encouraged through D via adversarial learning to align the features of both source and target distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Image translation module</head><p>Given the high availability of unsupervised image-to-image translation models providing multiple image representations for a single input <ref type="bibr" target="#b11">(Gatys et al., 2016;</ref><ref type="bibr" target="#b17">Huang and Belongie, 2017;</ref><ref type="bibr" target="#b22">Murez et al., 2018;</ref><ref type="bibr" target="#b46">Zhu et al., 2017)</ref>, we assume that we can use any of these models for T , and therefore we focus our contribution in the ensemble as well as a training strategy. Regardless of the chosen method, T is trained accordingly before starting the first training stage using both sets of images in an unsupervised way {X S , X T }, to obtain the transformations X T 1 S , X T 2 S and X T 3 S . Implementation details on the used image translation network can be found in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Semantic segmentation network</head><p>Our proposed semantic segmentation network consists of a shared feature extractor E along with three classifiers C 1 , C 2 and C 3 . To integrate the transformations X T 1 S , X T 2 S and X T 3 S from T as well as the source labels Y S , our ensemble approach has a two-stage training process.</p><p>In the first training stage, the classifiers C 1 , C 2 and C 3 accumulate knowledge along with the encoder E by computing their predictions from the features E(X T 1 S ), E(X T 2 S ) and E(X T 3 S ) respectively, learning each one from a different image translation while sharing the same set of label maps Y S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Ensemble layer</head><p>After the encoder and classifiers are trained, we need to train the meta-learner to ensemble the predictions of the classifiers. This is done by freezing the weights of the semantic segmentation network while feeding the features of the translated images E(X T 1 S ), E(X T 2 S ) and E(X T 3 S ) into the classifiers C 1 , C 2 and C 3 respectively. With these predictions along with the ground truth labels Y S , the meta-learner C m learns to weigh each classifier's prediction for the classes. The resulting layer C m condenses rich information as it is capable of balancing each classifier's output to create a single prediction. This is needed to create reliable pseudo-labels for the unlabeled target domain.</p><p>The second training stage consists in self-supervised learning, where the predictions of the classifiers on the target images C 1 (E(X T )), C 2 (E(X T )) and C 3 (E(X T )) are used as input for C m , from which the first set of pseudo-labels? <ref type="formula">(0)</ref> T are obtained using confidence thresholding as in <ref type="bibr" target="#b41">Yang and Soatto, 2020)</ref>. But our confidence is based on the output of three classifiers combined in an ensemble instead of one classifier, making the pseudo-labels more reliable. By using the pair of target images and pseudo-labels {X T ,? (0) T }, the entire semantic segmentation network is retrained, concluding the first training round. For the consecutive rounds of SSL, the meta-learner is retrained using the classifiers' prediction on the target images along with the pseudo-labels of the previous round. After that, the outputs of C m are used to create new pseudo-labels for the target images to finally retrain the semantic segmentation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Feature alignment module</head><p>During both training stages, the discriminator D is responsible for performing feature alignments with adversarial training between E(X T ) and E(X T c S ) where X T c S is determined by the image translation module in use. Implementation details on this can be found in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training objectives 3.3.1. First training stage</head><p>Given the source annotated set of images X S with label maps Y S , and the alternative sets of image translations X T 1 S , X T 2 S and X T 3 S , the semantic segmentation outputs from each classifier C k , are computed from the features E(X T k S ) of each transformation and used to train the encoder as well as the classifiers during the first training stage:</p><formula xml:id="formula_0">L stage1 = L seg (C k (E(X T k S )), Y S ) + ? adv L adv (X T c S , X T ) + ? ent L ent (C k (E(X T )))<label>(1)</label></formula><p>?k ? {1, 2, 3}, where the supervised semantic segmentation loss for the classifier C k is defined by:</p><formula xml:id="formula_1">L seg (C k (E(X T k S )), Y S ) = ? Y S , log C k (E(X T k S ))<label>(2)</label></formula><p>The parameter ? adv denotes the hyperparameter that controls the relative importance of the adversarial loss. This adversarial component ensures convergence between the features from the transformed images X T c S and the target images X T and is defined as:</p><formula xml:id="formula_2">L adv (X T c S , X T ) = E[log(D(X T c S ))] + E[log(1 ? D(X T ))] (3)</formula><p>where E represents the expected value operator.</p><p>Considering that entropy minimization has recently shown to improve SSL by means of model regularization <ref type="bibr" target="#b34">(Vu et al., 2019;</ref><ref type="bibr" target="#b41">Yang and Soatto, 2020)</ref>, ? ent represents a scalar that adjusts the weight of the entropy minimization loss L ent . Given the unlabeled target set of images X T , the classifiers C k will first compute their predictions C k (E(X T )) to regularize their Shannon Entropy <ref type="bibr" target="#b30">(Shannon, 1948)</ref> with the function:</p><p>L ent (C k (E(X T ))) = ??(? C k (E(X T )), log(C k (E(X T ))) ) (4) ?k ? {1, 2, 3}, where ? = ?1 log(C) , and ?(x) = (x 2 + 0.001 2 ) ? is the Charbonnier penality function proposed in <ref type="bibr" target="#b41">(Yang and Soatto, 2020)</ref>, that penalizes high entropy predictions more than the low entropy ones when ? &gt; 0.5, preventing the model from overfitting on the most predominant classes whilst assisting those less present in the dataset.</p><p>To finish the first training stage, the meta-learner C m is trained to ensemble the outputs of the classifiers C 1 , C 2 and C 3 with respect to the transformations of the source dataset. This is done by freezing the semantic segmentation network to obtain the sparse Multinomial Logistic Regression weight vectors w 1 , w 2 and w 3 by minimizing the cross-entropy loss:</p><formula xml:id="formula_3">arg min w 1 ,w 2 ,w 3 L seg (C m (X T 1 S , X T 2 S , X T 3 S ), Y S ),<label>(5)</label></formula><p>where the output of the meta-learner is computed for every pixel (h, w) as follows:</p><formula xml:id="formula_4">C m (X T 1 S , X T 2 S , X T 3 S ) (h,w) = w 1 C 1 (E(X T 1 S )) (h,w) +w 2 C 2 (E(X T 2 S )) (h,w) +w 3 C 3 (E(X T 3 S )) (h,w) .<label>(6)</label></formula><p>The dimension of the weight vectors w k is the same as the total number of classes and denotes element-wise multiplication. Because the output of C m for a given pixel and class depends only on the three classifier outputs for that specific pixel and class, we refer to it as a sparse version of the standard Multinomial Logistic Regression <ref type="bibr" target="#b1">(Bishop, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Second training stage</head><p>The second stage consists mainly in self-supervised learning, a process in which the meta-learner C m generates pseudo-label? Y T for the target images X T to retrain the semantic segmentation network. Since C m is already trained, we feed the features of the target images E(X T ) in all the classifiers in Eq. (6) and apply global confidence thresholding on the probability maps <ref type="bibr" target="#b41">Yang and Soatto, 2020)</ref> to obtain the initial pseudo-labels? <ref type="formula">(0)</ref> T . Hereafter, we start the self-supervised learning rounds i = {1, 2, ...}, each one consisting of three steps. First, the semantic segmentation network is retrained through the loss:</p><formula xml:id="formula_5">L stage2 = L seg (C k (E(X T k S )), Y S ) + ? adv L adv (X T c S , X T ) + ? ent L ent (C k (E(X T ))) + L seg (C k (E(X T )),? (i?1) T )<label>(7)</label></formula><p>?k ? {1, 2, 3}. Second, the meta-learner C m is retrained on the predictions of the three updated classifiers on the target images along with the pseudo-labels? (i?1) T : arg min</p><formula xml:id="formula_6">w 1 ,w 2 ,w 3 L seg (C m (X T , X T , X T ),? (i?1) T )<label>(8)</label></formula><p>And finally, with the updated weight vectors w k , the metalearner is able to generate new pseudo-labels for the target im-ages? <ref type="bibr">(i)</ref> T to be used for the next rounds. The number of SSL rounds will be dictated by C m , specifically until the performance gap between C m and the three classifiers C k is no longer significant. The entire training procedure is summarized in Algorithm 1.</p><p>Algorithm 1: Training process of our method Input : (X S , Y S ), (X T , Y T = ?) Output: E, C 1 , C 2 , C 3 and C m obtain X T 1 S , X T 2 S and X T 3 S from T // stage 1 train E, C 1 , C 2 , C 3 and D with Eq. (1) train C m with Eq. (5) using (X S , Y S ) generate? <ref type="bibr">(0)</ref> T from C m using X T // stage 2 for i ? 1 to number of rounds do train E, C 1 , C 2 , C 3 and D with Eq. <ref type="formula" target="#formula_5">(7)</ref> retrain C m with Eq. (8) using (X T ,? (i?1) T ) generate? <ref type="bibr">(i)</ref> T from C m using X T end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Since the goal in UDA involves adapting a source annotated domain to an unlabeled target domain, we use the challenging synthetic-to-real UDA benchmarks for semantic segmentation to prove our main hypotheses as well as compare our approach with state-of-the-art methods. In this set-up, models are trained jointly with fully-annotated synthetic data as well as unlabeled real-world data, both considered source and target domain respectively. The models are then validated on an unseen portion of the target domain, measuring their mean intersection-overunion (mIoU) <ref type="bibr" target="#b10">(Everingham et al., 2014)</ref>.</p><p>Considering that our approach aims to improve on the discrepancy loss proposed in <ref type="bibr" target="#b43">(Zhang et al., 2018a)</ref> by training each individual classifier with a different set of features and using a meta-learning layer is to ensemble the classifiers' outputs, the first experiment focuses on analyzing the difference between Multi-task tri-training (MTri) <ref type="bibr" target="#b43">(Zhang et al., 2018a)</ref> and our method (see <ref type="figure">Fig. 1</ref>) during the first training stage. This comparison is complemented with a (vanilla) single encoderdecoder (SED) network architecture, often used in the literature for UDA <ref type="bibr" target="#b41">Yang and Soatto, 2020;</ref><ref type="bibr" target="#b39">Yang et al., 2020a;</ref><ref type="bibr" target="#b47">Zou et al., 2018)</ref>. The goal of this experiment is to study how the meta-learner can balance the outputs of the classifiers with respect to the classes, and how this compares to the aforementioned existing UDA architectures. Additionally, we also show the distribution of the weights for the meta-learner over all the classes and analyze the influence of entropy minimization.</p><p>The second experiment compares our proposed UDA approach with current state-of-the-art methods, putting our model into context with approaches that rely on a combination of image translation, feature matching, entropy minimization and SSL.</p><p>Finally, although most UDA methods focus only on analyzing their approach on the standard benchmarks of adapting synthetic-to-real domains, they tend to neglect the generalization capacity of the resulting model. For this reason, we designed a third experiment to study how well our model can <ref type="table">Table 1</ref>. Adaptation from GTA V ? ?Cityscapes, analyzing different architectures as well as the impact of entropy minimization for the first training stage. We show IoU for each class and total mean IoU. Apart from indicating the best IoU in bold, we make an intra comparison between C 1 , C 2 and C 3 against C m when using entropy minimization (underlined with red), and without the entropy loss (underlined with blue). Note that although entropy minimization helps to close the gap between SED and our method, there is still a remarkable difference specially considering the performance of C m . <ref type="bibr">GTA</ref>  generalize to a completely unseen dataset that was not used during the UDA training process. The protocol for this experiment consists of first adapting GTA V <ref type="bibr" target="#b25">(Richter et al., 2016)</ref> to Cityscapes <ref type="bibr" target="#b7">(Cordts et al., 2016)</ref>, and then evaluating the resulting model on WildDash <ref type="bibr" target="#b42">(Zendel et al., 2018)</ref>. To compare with other state-of-the-art methods, we select those whose code is publicly available and provide an evaluation script, and proceed to 1) reproduce their result on the proposed synthetic-toreal benchmark and 2) evaluate the model on WildDash.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Target dataset. Cityscapes <ref type="bibr" target="#b7">(Cordts et al., 2016)</ref> is a largescale and real urban scene semantic segmentation dataset that provides 5000 finely annotated images split into three sets: train (2975), validation (500) and test (1525). These sets are pixelwise labeled, with a resolution of 1024 ? 2048 pixels. The number of classes is 34 but only 19 are officially considered in the evaluation protocol.</p><p>Source datasets. GTA V <ref type="bibr" target="#b25">(Richter et al., 2016)</ref> is a synthetic dataset that contains 24966 labeled frames taken from a realistic open-world computer game called Grand Theft Auto V (GTA V). The resolution of the images is 1052 ? 1914 pixels and most of the frames are vehicle-egocentric. All the classes are compatible with the 19 official classes of Cityscapes. SYNTHIA <ref type="bibr" target="#b27">(Ros et al., 2016)</ref> is a synthetic dataset consisting of driving scenes rendered from a virtual city. We use the SYNTHIA-RAND-CITYSCAPES subset as source set, which contains 9400 1280 ? 760 images for training and 16 common classes with Cityscapes, and we evaluate the resulting model on these 16 classes.</p><p>Unseen dataset. WildDash <ref type="bibr" target="#b42">(Zendel et al., 2018</ref>) is a realworld dataset containing 4256 finely annotated images in a pixel-wise manner, created with the purpose of testing the robustness of models under different driving scenarios (e.g. rain, road coverage, darkness, overexposure). These images have a resolution of 1920 ? 1080 pixels and the labels are fully compatible with Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Image translation module and discriminator. We have chosen CycleGAN <ref type="bibr" target="#b46">(Zhu et al., 2017)</ref> as our image translation model T , as it is a commonly used approach that can be trained efficiently. Its architecture provides two image translations: the translation from the source to the target domain X t S and the reconstruction back to the source set X r S . These two translations are combined with the original source images to obtain three different representation X T 1 S = X S , X T 2 S = X r S and X T 3 S = X t S . Regarding the discriminator D, feature alignment between targetlike and target images is frequently done when using Cycle-GAN's as image translation module <ref type="bibr" target="#b15">Hoffman et al., 2018)</ref>, and therefore X T c S turns into X T 3 S in Eq. (3). We note that potentially better performing image translations approaches exist but in our work we opt for the commonly used approach CycleGAN.</p><p>Training protocols for MTri and SED. We have respected the protocol for MTri as reported in <ref type="bibr" target="#b43">(Zhang et al., 2018a)</ref>, i.e. minimizing a cross-entropy loss for semantic segmentation combined with a cosine distance loss for the discrepancy between C 1 and C 2 . As for the single encoder-decoder (SED) approach, all our losses were implemented using one encoder and one classifier, while using all three available transformations. In essence, the SED approach is similar to our approach but does not use the ensemble approach with the three classifiers.</p><p>Hardware and network architecture. In our experiments, we have implemented our method using Tensorflow <ref type="bibr" target="#b0">(Abadi et al., 2016</ref>) and trained our model using a single NVIDIA TITAN RTX with 24 GB memory. Regarding the segmentation network, we have chosen ResNet101 <ref type="bibr" target="#b14">(He et al., 2016)</ref> pretrained on ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009</ref>) as feature extractor for E. When it comes to the decoders C k , k ? {1, 2, 3}, DeepLab-v2 <ref type="bibr" target="#b6">(Chen et al., 2018)</ref> framework was used. For the network D we adopt a similar structure than <ref type="bibr" target="#b24">(Radford et al., 2015)</ref>, which consists of 5 convolution layers with kernel size of 4 ? 4, stride  <ref type="table">Table 4</ref>. Number of classes where each classifier outperforms the others on GTA V ? ? Cityscapes. Although there is a clear dominance of C 3 before starting SSL, this trend tends to wear off as the self supervision process unfolds.</p><p>classifier stage 1 SSL: round 1 SSL: round 2 C 1 5 6 4 C 2 4 7 7 C 3 10 6 8  <ref type="bibr" target="#b21">(Maas et al., 2013)</ref> as activation function with a slope of 0.2, except the last layer that has no activation. Throughout the training process, we use SGD <ref type="bibr" target="#b3">(Bottou, 2010)</ref> as optimizer with momentum of 0.9, encoder and decoders follow a poly learning rate policy, where the initial learning rate is set to 2.5e ?4 . The discriminator is also optimized with SGD but with a fixed learning rate of 1e ?5 . As for the entropy loss, we chose ? ent = 0.005 and ? = 2.0 for all experiments. During the first stage the network is trained for 150k iterations. Then we perform SSL until convergence is reached on each round. We use a crop size of 512 ? 1024 during training, and we evaluate on full resolution 1024 ? 2048 images from Cityscapes validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Our approach vs MTri vs single encoder-decoder (SED)</head><p>We can infer from Tab. 1 that our method outperforms both SED and MTri, and the difference is even more noticeable when comparing against the meta-learner. C m performs better than the individual classifiers over 14 classes if we make the comparison with entropy minimization and over 11 classes without entropy minimization. It is also important to mention that there is a considerable correlation between the weights of the metalearner that are depicted in <ref type="figure" target="#fig_0">Fig. 3 and the</ref>  meta-learner. If we analyze the classes in Tab. 1 where C m outperforms the three classifiers (for instance: fence, traffic light, rider, bus, bike), we can see also a pattern in <ref type="figure" target="#fig_0">Fig. 3</ref> where the meta-learner tends to amplify the contribution of the two best performing classifiers whilst penalizing the one with the lowest mIoU. For some other classes such as traffic sign and truck, it prefers to perform a mixture of weak classifiers, resulting in a final prediction that outperforms the strongest member of the ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Our approach vs state-of-the-art methods</head><p>The quantitative results for the adaption GTA V ? ? Cityscapes can be seen in Tab. 2. When comparing it with stateof-the-art methods, we can see that our approach outperforms PCEDA <ref type="bibr" target="#b40">(Yang et al., 2020b)</ref>, FDA-MBT <ref type="bibr" target="#b41">(Yang and Soatto, 2020)</ref> and BDL , three recent methods that use a combination of strategies. Qualitative results in <ref type="figure">Fig. 5</ref> shows Image GT C m <ref type="figure">Fig. 5</ref>. Qualitative comparison from GTA V to Cityscapes. The meta-learner rebalances the predictions from C 1 , C 2 and C 3 to achieve a smoother output over all the classes, where less predominant classes such as t. sign and pole have more presence on the pixel space of C m . satisfactory results on the output space, leading to consistently clean predictions.</p><p>As for SYNTHIA ? ? Cityscapes, the mIoU values are shown in Tab. 3. We achieved competitive results over all 16 classes with respect to state-of-the-art methods such as PCEDA <ref type="bibr" target="#b40">(Yang et al., 2020b)</ref> and AdaptPatch <ref type="bibr" target="#b33">(Tsai et al., 2019)</ref>, dominating on some difficult classes such as pole, traffic light and traffic sign.</p><p>If we analyze the improvements during SSL, we see that the meta-learner consistently scores better than the individual three classifiers, although its gain diminishes with each round of SSL (see <ref type="figure">Fig. 4</ref>). This can be attributed to the fact that all three classifiers are being optimized with the same images, and thus the model is losing the capability to keep diversity among the predictors. The results from Tab. 2 and Tab. 4 also show that the dominance of the members of the ensemble can alternate since C 3 is the best predictor after the first round (R1) and C 2 takes over after the second one (R2). This suggests that some predictors can learn more than others, even if they share the same input images, showing that all of them are equally important. This can be better appreciated in Tab. 4 where C 2 stands out after R1 and remains close to C 3 after R2 when analyzing the mIoU per class during SSL.</p><p>Using the image translation module T during the second stage by transforming the target set into the closest transformation possible to each classifier, i.e., transforming the target images to the source domain for the first two classifiers while keeping them unaltered for the third one, leads to slightly worse performance (see Tab. 5). This can be attributed to the fact that, since SSL aims to close the gap for the target distribution, it is needed to keep the inputs as similar as possible to those that the algorithm would receive during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Generalization capacity of our approach</head><p>The results of the proposed generalization test in Tab. 6 shows different UDA methods along with their arrangement for the semantic segmentation network and the corresponding mIoU performance on WildDash, after adapting GTA V to Cityscapes. ADVENT <ref type="bibr" target="#b34">(Vu et al., 2019)</ref> is a UDA approach that does not leverage any image translation strategy, while BDL  and FDA-MBT <ref type="bibr" target="#b41">(Yang and Soatto, 2020</ref>) make use of one and three image translations respectively. If we consider the amount of encoders and classifiers, we can notice that using a single encoder-decoder gives a quite limiting generalization performance, although BDL outperforms ADVENT. This slightly better performance of BDL can be attributed to the us-age of one image translation (transforming the source to the target with CycleGAN) to increase the robustness of the model.</p><p>FDA-MBT uses three image representations, mapping the source annotated images to the target using three different parameters for the image translation module, and performing UDA by training each encoder-decoder segmentation model with a specific representation. The reported performance of 31.07 is the result of averaging the three trained models, and although it is close to ours, our semantic segmentation network takes up significantly fewer parameters to train (one encoder and three classifiers). This makes our approach attractive as it is a good trade-off between its number of parameters and performance. More importantly, this experiment shows the advantageous effect of using multiple image translations, as done in FDA-MBT and our approach, on the generalizability of the trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we used challenging synthetic-to-real semantic segmentation UDA benchmarks to verify our main hypotheses that self-supervised learning for UDA can be improved by: 1) using multiple image translations instead of a discrepancy loss to encourage diversity of classifiers when generating the pseudo-labels, and 2) adding a meta-learner that utilizes the classifiers to improve the quality and robustness of the obtained pseudo-labels.</p><p>We have shown empirically in Section 4.3 and Section 4.4 that the proposed approach, which combines the benefits of image translations, self-supervised learning, and ensemble learning, improves the model's accuracy for two standard UDA benchmarks when adapting synthetic to real data and also shows satisfactory generalization capacity as discussed in Section 4.5.</p><p>We can conclude from the results that increasing the input variability via different image translations induces the network to learn better domain agnostic representations in the feature extractor while keeping specificity on each classifier. Using ensemble learning to integrate the outputs of the classifiers is also beneficial as it allows the model to generate high-quality pseudo-labels and thereby improve the self-supervised learning process.</p><p>We should remember that although we focused on the standard synthetic-to-real UDA benchmarks, it is also possible to extend this work to real-to-real applications where both source and target domains are real-world datasets. While this can represent a more realistic application of UDA, we consider that in this work we made a step in improving the generalization capability of deep learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>weights after optimizing Eq. (6) for the adaption GTA V ? ? Cityscapes. Before starting SSL, C 1 and C 3 are the most dominant predictors on the output space of C m .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>35.69 78.91 33.2 19.95 34.86 25.25 3.28 80.77 34.37 71.85 60.06 18.46 84.6 22.66 21.41 1.09 23.29 21.82 39.43 SED (w/ ent) 84.89 34.30 82.64 31.24 18.91 36.78 32.18 15.20 82.33 31.89 72.78 63.42 13.43 83.36 24.20 25.15 0.06 30.96 30.08 41.78 MTri (Zhang et al., 2018a) (C1) (w/ ent) 64.88 19.33 61.55 12.76 20.81 30.61 42.13 14.69 75.2 12.17 60.8 64.45 29.6 82.1 25.61 32.41 5.29 32.92 27.09 37.6 MTri (Zhang et al., 2018a) (C2) (w/ ent) 62.1 19.64 59.0 15.18 20.87 30.43 41.99 14.55 75.41 12.2 60.67 64.35 29.47 82.18 25.74 32.46 5.34 33.04 27.04 37.46 MTri (Zhang et al., 2018a) (C3) (w/ ent) 58.11 19.18 55.65 16.78 21.13 30.39 41.91 13.93 75.84 12.14 58.99 64.1 29.0 82.95 25.99 32.51 5.61 33.75 27.46 37.13 .64 80.30 27.33 19.37 35.95 33.10 27.49 83.84 30.29 81.53 61.98 26.54 81.50 21.48 20.18 0.03 29.05 40.57 43.10 Ours (C3) (w/ ent) 87.22 36.57 81.26 28.65 17.82 35.55 32.58 29.11 83.46 30.39 77.06 62.48 28.78 81.49 22.75 22.85 0.06 29.85 35.14 43.32 Ours (Cm) (w/ ent) 85.29 35.57 81.69 29.93 20.24 35.53 36.63 35.94 83.24 28.1 81.75 63.75 29.18 81.8 23.44 24.58 4.67 31.0 47.26 45.24</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">V ? ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Experiment</cell><cell>ro a d</cell><cell>s id e .</cell><cell>b u il .</cell><cell>w a ll</cell><cell>fe n c e</cell><cell>p o le</cell><cell>li g h t</cell><cell>s ig n</cell><cell>v e g e t.</cell><cell>te rr .</cell><cell>s k y</cell><cell>p e rs o n</cell><cell>ri d e r</cell><cell>c a r</cell><cell>tr u c k</cell><cell>b u s</cell><cell>tr a in</cell><cell>m o to r</cell><cell>b ik e</cell><cell>mIoU</cell></row><row><cell cols="7">SED (w/o ent) 77.72 Ours (C1) (w/o ent) 82.9 34.06 74.9 25.74 15.76 33.8</cell><cell cols="13">33.6 17.09 84.94 34.37 74.21 60.81 14.65 84.73 23.86 26.31 0.64 22.14 32.0</cell><cell>40.87</cell></row><row><cell>Ours (C2) (w/o ent)</cell><cell cols="19">78.24 31.48 71.71 26.37 19.18 36.22 32.49 25.61 85.1 31.41 84.28 60.28 18.06 84.79 26.16 29.64 0.28 23.29 32.9</cell><cell>41.97</cell></row><row><cell>Ours (C3) (w/o ent)</cell><cell cols="20">81.91 30.15 77.22 26.38 15.0 34.63 31.53 27.42 83.75 35.18 81.37 61.71 17.32 85.07 26.5 29.86 0.2 21.36 33.43 42.10</cell></row><row><cell>Ours (Cm) (w/o ent)</cell><cell cols="20">82.78 35.74 75.81 26.83 19.89 34.96 34.47 25.60 85.23 35.35 79.75 62.02 14.82 84.68 25.30 32.05 0.03 27.48 41.23 43.39</cell></row><row><cell>Ours (C1) (w/ ent)</cell><cell cols="11">83.24 33.4 78.04 27.48 18.37 33.26 35.34 22.34 83.87 27.47 82.2</cell><cell cols="9">62.7 28.26 80.76 20.49 15.41 0.22 27.12 37.78 41.99</cell></row><row><cell>Ours (C2) (w/ ent)</cell><cell cols="2">84.67 33</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Adapting from GTA V to Cityscapes. S1 and S2 indicate the training stage, while R1 and R2 denote the first and second round of SSL, respectively. .32 84.69 67.23 29.77 85.78 32.73 29.9 20.12 35.55 57.05 51.66Table 3. Adapting from SYNTHIA to Cityscapes. Total mIoU values with * are reported only on 13 subclasses (excluding wall, fence and pole). Our method achieves the best performance over all the 16 classes.</figDesc><table><row><cell>GTA V ? ? Cityscapes</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Influence of T on GTA V ? ? Cityscapes during first round of SSL. Using T during SSL by feeding a transformation to its corresponding classifier provokes a slight performance drop in the mIoU.</figDesc><table><row><cell cols="4">classifier stage 1 SSL without T SSL with T</cell></row><row><cell>C 1</cell><cell>41.99</cell><cell>48.87</cell><cell>47.9</cell></row><row><cell>C 2</cell><cell>43.10</cell><cell>48.91</cell><cell>47.8</cell></row><row><cell>C 3</cell><cell>43.32</cell><cell>48.94</cell><cell>48.0</cell></row><row><cell cols="4">2 and channel numbers {4, 8, 16, 32, 1}. Each layer uses Leaky-</cell></row><row><cell>ReLU</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>performance of theFig. 4. Effect of SSL for C m . Retraining the meta-learner after each round of SSL makes sure that it keeps outperforming the other classifiers with a decreasing margin.</figDesc><table><row><cell>Method</cell><cell cols="3"># encoders # classifiers mIoU</cell></row><row><cell>ADVENT (Vu et al., 2019)</cell><cell>1</cell><cell>1</cell><cell>25.9</cell></row><row><cell>BDL (Li et al., 2019)</cell><cell>1</cell><cell>1</cell><cell>26.57</cell></row><row><cell>FDA-MBT (Yang and Soatto, 2020)</cell><cell>3</cell><cell>3</cell><cell>31.07</cell></row><row><cell>Ours</cell><cell>1</cell><cell>3</cell><cell>31.2</cell></row><row><cell cols="4">Table 6. Generalization test on WildDash after adapting GTA V ? ?</cell></row><row><cell>Cityscapes.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<title level="m">USENIX (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>Tensorflow: A system for large-scale machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Combining labeled and unlabeled data with cotraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
		<respStmt>
			<orgName>COLT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
		<respStmt>
			<orgName>COMPSTAT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">All about structure: Adapting structural information across domains for boosting semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1900" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TPAMI</publisher>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cluster alignment with a teacher for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV, IEEE</publisher>
			<biblScope unit="page" from="9943" to="9952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Co-training by committee: A new semisupervised learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F A</forename><surname>Hady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ICDM</publisher>
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>ICML</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1994" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial network for structured domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<title level="m">The open images dataset V4. IJCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1956" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6929" to="6938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge distillation methods for efficient unsupervised adaptation across multiple domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Nguyen-Meidine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blais-Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A domain agnostic normalization layer for unsupervised adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
			<biblScope unit="page" from="1866" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Strong baselines for neural semi-supervised learning under domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACL</publisher>
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation with GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1711.06969</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Domain Adaptation in Computer Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Pattern Recognition</title>
		<editor>Csurka, G.</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="55" />
		</imprint>
	</monogr>
	<note>A deeper look at dataset bias</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Direction-aware neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">DCAN: dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Label-driven reconstruction for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="480" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Phase consistent ecological domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9008" to="9017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">FDA: fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4084" to="4094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Wilddash -creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Dom?nguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="407" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A fully convolutional tri-branch network (fctn) for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICASSP</publisher>
			<biblScope unit="page" from="3001" to="3005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Tri-training: exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1529" to="1541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
