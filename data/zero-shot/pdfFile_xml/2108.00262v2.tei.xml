<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
							<email>uttaranb@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Childs</surname></persName>
							<email>ehchilds@terpmail.umd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rewkowski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
							<email>dmanocha@umd.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland College Park</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We synthesize 3D pose sequences of co-speech upper-body gestures with appropriate affective expressions. We extract the affective cues from the speech, the sentiments from the corresponding text transcripts, the individual speaker styles, and the joint-based affective expressions from the seed poses (shown on the left). We train a generative adversarial network to synthesize gestures aligned with the speech by leveraging the affective information in both the generation and the discrimination phases. We show two such affective gestures on the right, with the affects furious and appalled denoted in italics.</p><p>Abstract-We present a generative adversarial network to synthesize 3D pose sequences of co-speech upper-body gestures with appropriate affective expressions. Our network consists of two components: a generator to synthesize gestures from a joint embedding space of features encoded from the input speech and the seed poses, and a discriminator to distinguish between the synthesized pose sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral coefficients and the text transcript computed from the input speech in separate encoders in our generator to learn the desired sentiments and the associated affective cues. We design an affective encoder using multi-scale spatial-temporal graph convolutions to transform 3D pose sequences into latent, pose-based affective features. We use our affective encoder in both our generator, where it learns affective features from the seed poses to guide the gesture synthesis, and our discriminator, where it enforces the synthesized gestures to contain the appropriate affective expressions. We perform extensive evaluations on two benchmark datasets for gesture synthesis from the speech, the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the best baselines, we improve the mean absolute joint error by 10-33%, the mean acceleration difference by 8-58%, and the Fr?chet Gesture Distance by 21-34%. We also conduct a user study and observe that compared to the best current baselines, around 15.28% of participants indicated our synthesized gestures appear more plausible, and around 16.32% of participants felt the gestures had more appropriate affective expressions aligned with the speech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Co-speech gestures are bodily expressions associated with a person's speech <ref type="bibr" target="#b0">[1]</ref>. They help underline the subject matter and the context of the speech, particularly in the form of beat, deictic, iconic, or metaphoric expressions <ref type="bibr" target="#b1">[2]</ref>. Beat gestures are rhythmic movements following the speech, and deictic gestures point to an entity. Iconic gestures describe physical concepts, e.g., spreading and contracting the arms to denote "large" and "small", and metaphoric gestures describe abstract concepts, e.g., putting a hand to the heart to denote "love". Synthesizing co-speech gestures is an important task in creating socially engaging characters and virtual agents. These are useful in a variety of multimedia application such as online learning <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, interviewing and counseling <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, robot assistants <ref type="bibr" target="#b0">[1]</ref>, character de-signs and game development <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and visualizing stories and scripts <ref type="bibr" target="#b9">[10]</ref>.</p><p>In our work, we focus on synthesizing the upper-body gestures associated with speech. We consider the joints at the root, spine, head, and the two arms as part of the upper body, which are the joints most commonly used in co-speech gestures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Current state-of-the-art methods for cospeech upper-body gesture synthesis are based on an end-toend learning approach <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>. These methods train deep neural networks using gestures (available as videos or motion-captured datasets), raw speech waveforms and the corresponding text transcripts, and individual speaker styles. While these methods can generate different beat, deictic, iconic, and metaphoric co-speech gestures and adapt to speaker-specific styles, they do not have any mechanism to reliably incorporate affective expressions in the gestures.</p><p>Affective expressions are the modulations in gestures resulting from the emotions experienced by the speakers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Even for a given speaker, the style of gesture expressions can change depending on the emotional context, and human observers are keenly alert to these changes <ref type="bibr" target="#b13">[14]</ref>. The combined understanding of the content of the speech and the speaker's gesture-based affective expressions are crucial to human-human interactions <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Therefore, it is essential to incorporate affective expressions in cospeech gestures of animated characters and virtual agents to improve their plausibility in human-machine interactions.</p><p>In human-human interactions, we can break the gesturebased affective expressions down into a set of biomechanical features known as affective features, such as body postures, head positions, and arm motions <ref type="bibr" target="#b12">[13]</ref>. Each affective expression is a combination of one or more affective features, e.g., rapid arm swings and head jerks are often used as expressions of anger or excitement <ref type="bibr" target="#b13">[14]</ref>. A multitude of macroscopic and microscopic factors influence the affective features in a given context, including the social setting and the speaker's idiosyncrasies, making an exhaustive enumeration of affective features tedious and challenging <ref type="bibr" target="#b16">[17]</ref>. Nevertheless, it is essential to learn these affective features to understand and synthesize the desired affective expressions.</p><p>Moreover, co-speech affective gesture synthesis also requires aligning the gestures with the affective cues obtained from the speech. To this end, prior methods have either learned to map the raw speech waveforms to gestures via latent embeddings <ref type="bibr" target="#b10">[11]</ref> or utilized the log-Mel spectrograms to obtain a richer understanding of the affective cues, including the prosody and the intonations in the speech <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, these are high-dimensional representations of speech that require significant computation overhead to be downscaled into convenient latent embedding spaces. Main Contributions. We present an end-to-end learning approach for generating 3D pose sequences of co-speech gestures with appropriate affective expressions while maintaining the speakers' individual styles and following a short sequence of seed poses. <ref type="bibr" target="#b0">1</ref> We leverage the Mel-frequency cepstral coefficients (MFCCs) from the speeches obtained by performing DCT on the log-Mel spectrograms. MFCCs are highly compressible representations containing sufficient information for speaker identification and also encode affective cues such as prosody and intonation for speech-based emotion recognition. We use separate encoders to encode the MFCCs from the raw speeches, the text transcripts obtained from the speeches, the speakers' styles, and the seed poses. We use available text-and speaker-encoders proposed by Yoon et al. <ref type="bibr" target="#b10">[11]</ref> to learn latent features from the text transcript and a latent style embedding space using a variational encoding of the speaker styles. We propose an encoder for the MFCCs that captures the affective cues in the speech. We also develop an "affective encoder" that transforms the 3D pose sequences to latent affective features using multiscale spatial-temporal graph convolutions (STGCNs). We design our multi-scale STGCNs to expand attention from the local joints to the macroscopic body parts in a bottom-up manner. We use our affective encoder both in the generator to learn affective features from the seed poses to guide the gesture synthesis and in our discriminator to differentiate between the real and the synthesized gestures based on the affective expressions. To the best of our knowledge, we are the first to learn affective features directly from the gesture data to synthesize gestures with affective expressions. Our main contributions include:</p><p>? Synthesizing co-speech affective gestures. We synthesize 3D pose sequences of gestures with appropriate affective expressions given a speaker's speech, maintaining the speakers' individual styles of gesticulation and following a short sequence of seed poses. ? Affective encoder for learning latent affective features. Our affective encoder leverages the localized joint movements and the macroscopic body movements in the 3D pose sequences to learn latent affective features that are used for synthesizing the future poses from the seed poses and adversarially guiding the synthesis as per affective expressions. ? MFCC encoder for leveraging the affective cues from the speech. Our MFCC encoder takes in lowdimensional MFCCs containing information on the affective cues from the speech, including prosody and intonations, and transforms them into latent embeddings for affective gesture synthesis. We evaluate the quantitative performance of our network on two benchmark datasets, the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref> and the GENEA Challenge 2020 Dataset <ref type="bibr" target="#b18">[19]</ref>. We observe an improvement of 10-33% on the mean absolute joint error, 8-58% on the mean acceleration difference, and 21-34% on the Fr?chet Gesture Distance (FGD) <ref type="bibr" target="#b10">[11]</ref> for our network compared to the current state-of-the-art baselines. We also conduct a user study to evaluate the plausibility of our synthesized gestures and the consistency between the affective expressions in the gestures and the speech. Around 15.28% participants indicated that our synthesized gestures are more plausible than the best current baseline of Yoon et al. <ref type="bibr" target="#b10">[11]</ref>, and around 16.32% participants felt the gestures had more appropriate affective expressions aligned with the speech compared to the same baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly summarize related prior work on how humans perceive affective body expressions and how these studies were leveraged to synthesize emotionally expressive characters. We also summarize works on synthesizing body motions, especially those aligned with a speech, a text transcript, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Perceiving Affective Body Expressions</head><p>Affect is traditionally expressed in psychology in terms of its valence, arousal, and dominance (VAD) <ref type="bibr" target="#b19">[20]</ref>. Valence measures the level of pleasantness (e.g., happy vs. sad), arousal measures how animated the person is (e.g., angry vs. bored), and dominance measures the level of control over the affect (e.g., admiration vs. fear). Studies in both psychology and affective computing indicate the existence of biomechanical affective features that provide cues to a person's perceived affect to human observers <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. These affective features can be observed at different scales: they can be localized joint movements such as rapid arm swings and head jerks, indicating excitement or anger, as well as macroscopic body movements such as the upper body being expanded, indicating pride or confidence, or collapsed, indicating shame or nervousness. Subsequently, there has been work on detecting perceived emotions by leveraging known affective features either as input to a neural network <ref type="bibr" target="#b23">[24]</ref> or to constrain the embedding space <ref type="bibr" target="#b16">[17]</ref>. In contrast, we design our neural network to explicitly attend to the body movements at these multiple scales to learn latent affective features directly from the input gesture samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Synthesizing Affective Body Expressions</head><p>There has been substantial work on synthesizing affective expressions for embodied conversation agents <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and other social virtual agents to interact via facial expressions <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> or gaits <ref type="bibr" target="#b28">[29]</ref>. Furthermore, the synthesis of affective facial expressions has been aligned with a character's speech using data-driven techniques <ref type="bibr" target="#b29">[30]</ref>. While synthesizing speech-aligned affective facial expressions has been relatively well-studied, aligning the speech with affective body expressions has been more challenging. Some of the widely used approaches are rule-based systems such as that of DeVault et al. <ref type="bibr" target="#b6">[7]</ref>, which has a virtual human counselor expressing appropriate affective hand and body gestures following known mappings between the emotional states and the stored animations. Recent methods utilize gait datasets annotated with categorical emotions such as happy, sad, and angry to generate emotive gaits <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Other techniques have extended to the VAD space of affect, where body gestures are generated given the text transcripts of speech and the corresponding intended emotion as a point in the VAD space <ref type="bibr" target="#b32">[33]</ref>. Our approach is based on designing an end-to-end system that can synthesize body expressions by automatically understanding the affective content in the input speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Synthesizing Gestures</head><p>There is a rich body of work gesture synthesis using rulebased systems, as surveyed comprehensively by Wagner et al. <ref type="bibr" target="#b34">[34]</ref>. However, scalability to novel scenarios remains a challenge for rule-based systems on account of manually designing new rules. Instead, we focus on a summary of the recent data-driven approaches of automated gesture synthesis in novel scenarios <ref type="bibr" target="#b35">[35]</ref>, which are in line with our learning-based approach. Existing techniques have utilized hidden Markov models <ref type="bibr" target="#b36">[36]</ref>, recurrent neural network variants <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b0">[1]</ref>, and autoencoders <ref type="bibr" target="#b39">[38]</ref> to learn robust latent features that encode the input speech, available as either an audio or a text transcript, and can be used to decode the output gestures. Other approaches have opted to learn stochastic generation processes using tools such as invertible sub-transformations <ref type="bibr" target="#b40">[39]</ref> to map between the speech and the gesture spaces. To improve the realism of the generated speech-driven gestures, more recent works incorporate the speech semantics into the training process <ref type="bibr" target="#b8">[9]</ref>, and even combined the synthesized gestures with rule-based head nods and hand waves for embodied conversation agents <ref type="bibr" target="#b41">[40]</ref>.</p><p>Our approach is complementary to these approaches in that we learn mappings from the text transcripts of speech to gestures. It eliminates the noise in speech signals and helps us focus only on the relevant content and context. Learning from the text also enables us to focus on a broader range of gestures, including iconic, deictic, and metaphoric gestures <ref type="bibr" target="#b1">[2]</ref>. Our work is most closely related to that of Yoon et al. <ref type="bibr" target="#b0">[1]</ref>. They learn upper body gestures as PCAbased, low-dimensional pose features, corresponding to text transcripts from a dataset of TED-talk videos, then map these 3D gestures to an NAO robot. They have also followed up this work by generating upper-body gestures aligned with the three modalities of speech, text transcripts, and person identity <ref type="bibr" target="#b10">[11]</ref>. On the other hand, we learn to map text transcripts to 3D pose sequences corresponding to semanticaware, full-body gestures of more human-like virtual agents using an end-to-end trainable transformer network and blend in emotional expressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Incorporating Speaker Styles</head><p>Co-speech gesture generation is intrinsically related to stylized gesture generation. There has been considerable progress on stylized generation of head motions <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b43">[42]</ref>, facial motions <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b27">[28]</ref> as well as locomotions <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b46">[45]</ref>, <ref type="bibr" target="#b47">[46]</ref>. At the same time, many techniques have been proposed to generate appropriately styled body motions from textual descriptions of the actions <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>. Other approaches have developed separate gesture generation networks for individual speakers to adapt to their individual styles <ref type="bibr" target="#b11">[12]</ref>, together with adversarial losses to improve the fidelity of the <ref type="figure">Figure 2</ref>: Our network consists of a generator (pale-green box) and a discriminator (pale-blue box). Our generator takes in the MFCC from the speech, the text transcript, the speaker ID, and a sequence of 3D seed poses. We use four encoders: the MFCC encoder (Sec. 3.1.1), the text encoder (Sec. 3.1.2), the speaker encoder (Sec. 3.1.3), and the affective encoder (Sec. 3.1.4). We feed the concatenation of these latent features into our Bi-GRU followed by a set of FC layers to synthesize the gestures aligned with the speech. Our discriminator learns to discriminate between the real and the synthesized gestures based on the latent affective features from the affective encoder, constraining the generator to synthesize appropriate affective expressions.</p><p>generation <ref type="bibr" target="#b50">[49]</ref>. Recently, Yoon et al. <ref type="bibr" target="#b10">[11]</ref> proposed a unified architecture that considers the speech, its text transcript, and the speaker identity to generate co-speech gestures with continuously varying speaker styles. We extend such speaker-aware gesture synthesis to further incorporate the appropriate affective body expressions that align with the affective content in the speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to generate 3D pose sequences of co-speech upper-body gestures with appropriate affective expressions and speaker styles, given the raw speech waveform, the speaker identity, and a short sequence of seed poses. We consider affective expressions to be specific sequences of joint movements, generally as a combination of the affective features <ref type="bibr" target="#b32">[33]</ref>. We learn these affective expressions both at the localized joint neighborhoods and the macroscopic body movements, and use them to condition the training of a generative adversarial network. We show our overall network architecture in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synthesizing Co-Speech Gestures</head><p>Our generative network takes in the raw speech waveform as a 1D array, the corresponding text transcript as a sequence of words, the speaker identity as a unique number, and the seed poses as a 3D pose sequence. Similar to Yoon et al. <ref type="bibr" target="#b10">[11]</ref>, we encode the speech waveform, the text transcript, and the speaker identities using separate encoders. However, unlike Yoon et al., we convert the speech waveform to Mel-Frequency Cepstral Coefficients (MFCCs) to guide the encoding process based on the affective cues from speech. We also propose an affective encoder to encode the pose-based affective expressions into latent features for both gesture generation and discrimination. In the generation process, we combine the latent embeddings learned from the four encoders, speech, text, speaker, and affective, into a joint embedding for learning the upper-body gestures.</p><p>3.1.1. MFCC Encoder. MFCCs are known to encode signal frequencies consistent with how humans perceive sound, and are therefore particularly useful for tasks such as speech recognition <ref type="bibr" target="#b51">[50]</ref>, speaker identification <ref type="bibr" target="#b52">[51]</ref> and speechbased emotion recognition <ref type="bibr" target="#b53">[52]</ref>. In our case, we design our MFCC encoder to embed the speech-based affective cues such as prosody and intonations captured by the MFCCs and incorporate them in gesture synthesis. Given a raw waveform as a 1D array, we transform it to its top 14 MFCCs. These include the log-energy spectrum and 13 coefficients containing sufficient information on the speaker's pitch, intonation, prosody, and other relevant parameters <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b44">[43]</ref>. We also append the first-and the second-order discrete forward differences of the 13 coefficients, obtaining a total of 37 values. Using a window size W on a input waveform of length L, we obtain individual MFCCs of shape L/W , leading to a combined feature tensor f m ? R 37? L/W . We pass these features through a series of 1D temporal convolutions, followed by a single fully-connected (FC) layer, to obtain a latent feature sequencef m ? R Dm?T of sequence length T equal to 3D pose sequence length of the seed poses, a?</p><formula xml:id="formula_0">f m = Conv ? FC mf cc (f m ; W mf cc ) ,<label>(1)</label></formula><p>where D m is the dimension of the latent features, Conv ? FC mf cc denotes the series of 1D convolutions followed by the FC layer, and W mf cc its set of trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Text Encoder.</head><p>Given the text transcript corresponding to the speech, we first pad the transcript with padding tokens following the approach of Yoon et al., to ensure that the text transcript has the same sequence length T as the seed poses. We then use the pre-trained FastText <ref type="bibr" target="#b55">[54]</ref> word embedding model to transform the word sequence into 300-dimensional features, leading to a feature tensor f x ? R 300?T . We use FastText for its memory efficiency and its usefulness in sentiment analysis <ref type="bibr" target="#b56">[55]</ref>, which is important in understanding text-based affect. We pass the FastText features through a series of temporal 1D convolutions to obtain a latent feature sequencef x ? R Dx?T , a?</p><formula xml:id="formula_1">f x = Conv text (f x ; W text ) ,<label>(2)</label></formula><p>where D x is the dimension of the latent features, Conv text denotes the series of 1D convolutions with trainable parameters W text . </p><formula xml:id="formula_2">? s = FC ? (f s ; w ? ) (3) log ? s = FC ? (f s ; w ? ) ,<label>(4)</label></formula><p>where D s is the dimension of the latent distribution space, FC ? and FC ? denote the two sets of FC layers, and W ? and W ? denote the corresponding sets of trainable parameters. Intuitively, this latent distribution space consists of all the available speakers, plus speakers that can be "constructed" by linear combinations of those speakers in the latent space.</p><p>As a result, we can pick a random point from the latent space to use in the synthesis, resulting in some variability in the synthesized gestures even when the speech remains the same. We term this variability as having "speaker-aware" styles. Given the parameters ? s and ? s of latent distribution space, we use the re-parametrization trick <ref type="bibr" target="#b57">[56]</ref> to generate a random speaker-aware style samplef s ? R Ds and repeat it for all the T time steps of the input pose sequence.</p><p>3.1.4. Affective Encoder. We propose an encoding mechanism that transforms the pose-based affective expressions into a latent embedding. Since gestures typically consist of movements in the trunk, arms, and head, we only consider ten joints corresponding to these parts of the body: root, spine, neck, head, left and right shoulders, left and right elbows, and left and right wrists. We consider a directed graph for the pose, where the joints are the vertices, and the edges are directed from the root towards the extremities. We assume the edge lengths are known for each input and train our encoder only on the directions of the edges. We consider nine unit-vector sequences U = [u 1 ; . . . ; u 9 ], each of sequence length T , to denote the edge directions at the corresponding T time steps of the input pose sequence. We employ a hierarchical encoding strategy using spatial-temporal graph convolutions (STGCNs) <ref type="bibr" target="#b58">[57]</ref>. STGCNs are adapted to leverage localized dependencies in generalized graph-structured data, and are therefore suitable for our pose graph sequences. We use two levels of hierarchy, the first at the level of individual bones and the second at the level of the three body parts, the trunk and the two arms. At the first level, our unweighted adjacency matrix</p><formula xml:id="formula_3">A 1 ? {0, 1}</formula><p>9?9?T captures the temporal counterparts of each edge at the four nearest time steps (past two and future two), and spatially adjacent edges with a maximum hop of two, i.e., we consider two edges to be spatially adjacent if they either share a vertex or are connected to the two ends of a third edge. This size of the adjacent neighborhood sufficiently groups the edges influenced by typical affective expressions such as arm swings, head jerks, and upper-body collapse. Consequently, the convolution filters can learn a latent feature sequencef a1 ? R Da 1 ?9?T from the edges based on the variations in the affective expressions, obtained asf</p><formula xml:id="formula_4">a1 = STGCN 1 (U, A 1 ; W a1 ) ,<label>(5)</label></formula><p>where D a1 is the dimension of the per-edge latent features, STGCN 1 denotes the first-level STGCN with trainable parameters W a1 . At the second level, the three body parts, the trunk and the two arms, capture the macroscopic body movements such as raising or crossing the arms, and bending or straightening the trunk. In the second-level adjacency matrix A 2 ? {0, 1} 3?3?T , we assume both the arms to be adjacent to the torso but not to each other, since the movements on one arm need not influence the other. We again consider the temporal counterparts of each body part in the four nearest time steps in the temporal adjacency. We reshape the latent featuresf a1 to 3D a1 ? 3 ? T , to collect the per-edge features corresponding to the three body parts in the feature dimension. Our second-level STGCN then operates on these reshaped features to produce the secondlevel latent featuresf a2 ? R Da 2 ?3?T a?</p><formula xml:id="formula_5">f a2 = STGCN 2 f a1 , A 2 ; W a2 ,<label>(6)</label></formula><p>where D a2 is the dimension of the per-edge latent features, STGCN 2 denotes the second-level STGCN with trainable parameters W a2 . We then apply a series of 1D convolutions on the reshaped second-level featuresf a2 ? R 3Da 2 ?T to obtain the latent affective feature sequencef a ? R Da?T , a?</p><formula xml:id="formula_6">f a = Conv af f f a2 ; W a ,<label>(7)</label></formula><p>where D a is the dimension of the latent affective features, and Conv af f denotes the series of 1D convolutions with trainable parameters W a .</p><p>3.1.5. Gesture Generator. Given the latent feature sequencesf m ,f x ,f s , andf a , we concatenate them, pass them through a bidirectional gated recurrent unit (Bi-GRU), and sum the bidirectional outputs to obtain the predicted edge embeddings sequence? e ? R De?T , as out f rw , out bkw = GRU e f m ;f x ;f s ;f a ; W e , (8)</p><formula xml:id="formula_7">u e = out f rw + out bkw ,<label>(9)</label></formula><p>where D e is the dimension of the predicted edge embeddings, GRU e denotes the bidirectional GRU with the corresponding set of trainable parameters W e , and out f rw and out bkw respectively denote the outputs of the forward and the backward channels of the GRU. As in Yoon et al., we then transform the predicted edge embeddings to predicted edge vector sequences? = [? 1 ; . . . ;? 9 ], each of sequence length T , using a set of FC layers a? U = FC gen (? e ; W gen ) ,</p><p>where FC gen denotes the set of FC layers with the trainable parameters W gen . Thus, our generator is designed to take in a sequence of seed poses of length T and predicts a pose sequence of gestures for the next T time steps. Finally, we scale each predicted edge vector? i to have the corresponding bone length b i , i = 1, . . . , <ref type="bibr">9.</ref> We add it to the 3D position pos s(i) of the source joint s (i) of that edge vector to obtain 3D position pos d(i) of the destination joint </p><formula xml:id="formula_9">pos d(i) = pos s(i) + b i ?? i ? i .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discriminating Gestures</head><p>Our discriminator takes in a gesture of sequence length T and computes its latent affective feature sequencef a ? R Da?T using our affective encoder (Sec. 3.1.4). We pass this feature sequence through another bidirectional GRU, and sum the bidirectional outputs to obtain the discriminator embeddings sequenced ? R h?T , as</p><formula xml:id="formula_10">out f rw , out bkw = GRU disc f a ; W GRU,disc ,<label>(12)</label></formula><formula xml:id="formula_11">d = out f rw + out bkw ,<label>(13)</label></formula><p>where D d is the dimension of the predicted discriminator embeddings, GRU disc denotes the bidirectional GRU with trainable parameters W GRU d isc , and out f rw and out bkw respectively denote the outputs of the forward and the backward channels of the GRU. We then transform the discriminator embeddings to a probability vector c ? [0, 1] using a set of FC layers as</p><formula xml:id="formula_12">c = FC disc d ; W F Cdisc ,<label>(14)</label></formula><p>where FC disc denotes the set of FC layers with trainable parameters W F Cdisc , and c is such that c ? 0.5 implies the discriminator predicts the input gesture to be real, and generated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset and Training</head><p>We train our network on the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref>, which consists of videos of English-language speakers at TED Talks.  We use loss functions L G and L D identical to Yoon et al. <ref type="bibr" target="#b10">[11]</ref> to train our generator and discriminator respectively:</p><formula xml:id="formula_13">L G = ? Hub L Hub + ? gen L gen + ? stl L stl + ? KLD L KLD ,<label>(15)</label></formula><formula xml:id="formula_14">L D = ?E [log (Disc (U ))] ? E log 1 ? Disc ? ,<label>(16)</label></formula><p>where Disc denotes the discriminator network (Sec. 3.2), ? * are the weights of the corresponding loss terms with the same values as in Yoon et al. <ref type="bibr" target="#b10">[11]</ref>, and the individual loss terms of the generator are: ? Huber loss <ref type="bibr" target="#b59">[58]</ref> between the ground truth and predicted edge vectors, ? generative adversarial loss on the output of the discriminator,</p><formula xml:id="formula_15">L gen = ?E log Disc ? ,<label>(17)</label></formula><p>? diversity regularization between the synthesized gestures and other gestures in the dataset to ensure that the styles of different speakers appear visually different, ? Kullback-Leibler (KL) divergence between the latent distribution space of the styles defined by ? s and ? s , and the normal distribution N (0, I). <ref type="table" target="#tab_1">Table 1</ref> lists the latent dimensions we use for training our network. We use the Adam optimizer <ref type="bibr" target="#b60">[59]</ref> with ? 1 = 0.5, ? 2 = 0.999, batch size of 512, and learning rate of 5E ?4 for the generator and 1E ?4 for the discriminator with no warmup epochs (i.e., ? gen &gt; 0 starting from the first epoch). We train our network for 300 epochs, which took close to <ref type="bibr" target="#b46">45</ref> hours on an NVIDIA GeForce GTX 1080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We describe the objective evaluation of our method compared to current baseline methods. We highlight the benefit of our proposed components via ablation studies on the objective evaluation metrics. We also show the qualitative performance of our method on selected samples from the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref> and the perceived quality of our synthesized gestures through a user study. <ref type="figure">Figure 3</ref>: Qualitative results on the gestures synthesized by our method for two sample speech excerpts from the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref>. The italicized words very excited and bored indicate the primary affect in the corresponding speeches. We compare with the corresponding gestures of the original speakers, the output of GTC <ref type="bibr" target="#b10">[11]</ref>, and that of the two ablated versions of our network (Sec. 5.3). See Sec. 5.4 for a detailed discussion of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline Methods</head><p>We compare our method with the baseline methods on two benchmark datasets, the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref>, and the GENEA Challenge 2020 Dataset <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>On the TED Gesture Dataset, we compare with the methods of Seq2Seq <ref type="bibr" target="#b0">[1]</ref>, Speech to Gestures with Individual Styles (S2G-IS) <ref type="bibr" target="#b11">[12]</ref>, Joint Embedding Model (JEM) <ref type="bibr" target="#b61">[60]</ref>, and Gestures from Trimodal Context (GTC) <ref type="bibr" target="#b10">[11]</ref>. Seq2Seq and JEM generate gestures based only on the text transcript of the speech, whereas S2G-IS uses only the speech to generate the gestures. GTC uses the speech, the corresponding text transcript, and the speaker styles to generate gestures. Seq2Seq follows an encoder-decoder architecture, where the authors transform the text to latent features and predict gestures based on both the latent features and a short gesture history. The authors of S2G-IS employ a generative adversarial network that generates gestures from a latent space obtained from the input log-Mel spectrograms. JEM maps both the text and the target gesture into a common latent embedding space and uses a decoder to reconstruct the gestures from the embedding space. The authors train the model to learn to align the text-based and the gesture-based embeddings for the same input and decode gestures from only the text-based embeddings. For Seq2Seq, S2G-IS, and JEM, we follow the training routine and the hyperparameters used by Yoon et al. <ref type="bibr" target="#b10">[11]</ref>. For GTC, we directly use the pretrained model provided by Yoon et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>The GENEA Challenge 2020 Dataset is the publicly available version of the Trinity Gesture Dataset <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. It consists of the speech and the full-body motion capture of a male actor talking unrestrained about various topics over multiple recording sessions. The full dataset is about 242 minutes long, of which 221 minutes are used as training data, and the remaining 21 minutes are kept for testing. We do not fine-tune our network on this dataset and evaluate our network on the test partition. Since we consider only upper-body gestures, we consider the ten relevant upperbody joints at the root, the spine, the head, and the two arms for evaluating our performance. On this dataset, we compare with the method of Gesticulator <ref type="bibr" target="#b8">[9]</ref>, which leverages the acoustics and the semantics of the speech to generate semantically consistent beat, deictic, metaphoric, and iconic gestures. For a fair comparison, we use the pre-trained model provided by the authors and compare the performance on the same ten joints that we use for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Objective Evaluation</head><p>While evaluation metrics for gesture synthesis are not standardized, we evaluate on the commonly used metrics of mean absolute joint error (MAJE), mean acceleration difference (MAD), and the Fr?chet Gesture Distance (FGD) proposed by Yoon et al. <ref type="bibr" target="#b10">[11]</ref>. MAJE measures the mean of the absolute differences between the ground truth and the predicted joint positions over all the time steps, joints, and samples. MAD measures the mean 2 -norm error between the ground truth and predicted joint accelerations over all the time steps, joints, and samples. FGD measures the difference between the distributions of the latent features of the ground truth and the predicted gestures. The latent features are computed from an autoencoder network trained on the well-known Human 3.6M dataset <ref type="bibr" target="#b62">[61]</ref> of human motions using the ten joints in the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref>. MAJE indicates how closely the predicted joint positions follow the ground truth joint positions. MAD indicates how closely the ground truth and predicted joint movements match. Since affective expressions are based on joint movements, a lower MAD is especially desirable for our stated aim of generating gestures with appropriate affective expressions. FGD is shown to align well with the perceived plausibility of the synthesized gestures to human users <ref type="bibr" target="#b10">[11]</ref>; therefore, a lower FGD is equally desirable to gauge the quality of our synthesized gestures. <ref type="table" target="#tab_2">Table 2</ref> summarizes the performance of all the methods on all these evaluation metrics. Our method consistently has the lowest MAJE, MAD, and FGD on both the benchmark datasets. On the TED Gesture Dataset, we observe improvements of 10.29%, 8.44%, and 21.16% on MAJE, MAD, and FGD, respectively, over the best current baseline of GTC. On the GENEA Challenge 2020 Dataset, we observe improvements of 33.34%, 58.84%, and 34.41% on MAJE, MAD, and FGD, respectively, over the baseline of Gesticulator. We note that the absolute FGD values are significantly higher on the GENEA Challenge 2020 Dataset than on the TED Gesture Dataset. We hypothesize that this is because the gestures in the GENEA Challenge 2020 Dataset are more abstract and unscripted compared to the well-defined actions in the Human 3.6M Dataset or polished speeches in the TED Gesture Dataset. As a result, the pre-trained latent embeddings used for FGD are not as good at reconstructing the joints movements in the GENEA Challenge 2020 Dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Studies</head><p>We perform ablation studies on our two proposed components: the MFCC encoder (Sec. 3.1.1) and the affective encoder (Sec. 3.1.4). In one study, we replace only our MFCC encoder with an encoder for the raw audio waveform, identical to that of GTC <ref type="bibr" target="#b10">[11]</ref>, and train the resultant network. In the other study, we remove the affective encoder from our network. Our generator takes in the raw seed poses instead of the latent affective features. Our discriminator uses a convolution filter identical to GTC <ref type="bibr" target="#b10">[11]</ref> to transform the input gestures to latent features for the bidirectional GRU.</p><p>Without the MFCC encoder, our generator cannot take the speech-based affective cues into account. It results in a degradation of the synthesis, leading to higher MAJE, MAD, and FGD. However, without the affective encoder, our network is severely limited in understanding both the affective expressions in the seed poses and the affective expressions of the synthesized poses. It results in more severe performance degradation on all the evaluation metrics, and the synthesized gestures appear less diverse and plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative Results</head><p>We show qualitative results on two sample speech excerpts from the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref> in <ref type="figure">Fig. 3</ref>. It has five rows of gestures: those of the original speakers', those  synthesized by GTC <ref type="bibr" target="#b10">[11]</ref> ( the current state-of-the-art), those by the two ablated versions of our network: one without our MFCC encoder (Sec. 3.1.1) and the other without our affective encoder (Sec. 3.1.4), and those by our proposed network with all the encoders. We observe a diversity of speaker styles in the synthesized gestures compared to the original speaker, which results from using a variational embedding of speaker styles using the speaker encoder (Sec. 3.1.3). GTC, however, cannot generate affective expressions except for a few words with strong intonations in the speech, such as "excited" (second row, left column). Without our MFCC encoder, our network can still match the speech content but cannot align the gestures with the affective cues from the speech. For example, it can match the words "I was, I believe" with a deictic gesture pointing to the speaker himself (third row, right column) but cannot generate any expressions for "bored". Without our affective encoder, we observe only slight body movements but no appreciable affective expressions in the synthesized gestures. With all our encoders in place, we observe appropriate affective expressions that align well with the speech. For example, we observe rapid arm movements when saying "excited" (fifth row, left column) and dropping of the arms and shoulders when saying "bored" (fifth row, right column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">User Study</head><p>We conducted a user study to evaluate the perceptual quality of our synthesized gestures in terms of how plausible they appear and how well-aligned are their affective expressions with the corresponding speeches. 24 participants took part in our study, of which 20 were male, and 4 were female. 10 participants were between 18 and 24 years of age, 13 were between 25 and 34, and one was above 35. Each participant observed gestures corresponding to the same 12 speech excerpts, each taken from a different TED Talk in the TED Gesture Dataset <ref type="bibr" target="#b0">[1]</ref>. For each speech excerpt, the participants observed three different types of gestures: that of the original speaker as a 3D pose sequence (provided in the dataset), those synthesized by GTC <ref type="bibr" target="#b10">[11]</ref>, the current state-of-the-art, and those synthesized by our network. The order of the gestures was unknown to and randomized for each participant. We then asked the participants to answer two questions. The first question was how plausible the gestures appeared on a five-point Likert scale ranging from "very unnatural" (1) to "look like they could be from a real person" <ref type="bibr" target="#b4">(5)</ref>. The second question was how well the gestures synchronized with the corresponding speeches on a five-point Likert scale, ranging from "no or arbitrary gestures" (1) to "well-synchronized with the speech, and are appropriately emotionally expressive" <ref type="bibr" target="#b4">(5)</ref>. Intuitively, our Likert-scale points for both questions reflect the participants' individual assessments of quality, with 1 being the worst, 3 being average, and 5 being the best. The entire study took around 20 minutes on average for each participant.</p><p>We summarize the participants' responses in <ref type="figure" target="#fig_2">Fig. 4</ref>. When adjudging the plausibility of the gestures <ref type="figure" target="#fig_2">(Fig. 4a</ref>), we observe that 15.28% more participants marked our synthesized gestures either 4 or 5 compared to the gestures synthesized by GTC <ref type="bibr" target="#b10">[11]</ref>. Further, 3.82% more participants marked our synthesized gestures 4 or 5 than the original speakers' gestures, indicating that the participants found our synthesized gestures to have visual quality comparable to that of the original data. When adjudging the synchronization of the movements and the affective expressions of different types of gestures with speech ( <ref type="figure" target="#fig_2">Fig. 4b)</ref>, we observed that 16.32% more participants marked our synchronization quality either 4 or 5 compared to that of GTC <ref type="bibr" target="#b10">[11]</ref>. Also, 4.86% more participants marked out synchronization quality 4 or 5 than that of the original speakers, indicating that the participants perceived our synthesized gestures to be as wellsynchronized and expressive as the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion, Limitations and Future Work</head><p>We have presented an end-to-end learning approach to generate 3D pose sequences of co-speech gestures with appropriate affective expressions. Our contributions include an MFCC encoder to guide the gesture synthesis based on the speech-based affective cues such as prosody and intonation, and an affective encoder to learn joint-based affective features from the gesture data. Using these encoders in a generative adversarial learning framework, we have synthesized affective gestures that advance the state-of-the-art on co-speech gesture synthesis on multiple evaluation metrics. Our synthesized gestures also appeared more plausible and well-synced with the corresponding speeches to participants in a user study.</p><p>Our work has some limitations. First, we do not build a mechanism to control the affective expressions; we compute them automatically from the input modalities. We plan to investigate the interface between affective expressions from the speech and the gestures, especially when expressing contradictory cues such as sarcasm and irony. Second, we plan to use a finer representation of poses in the future since affective expressions in the gestures are often associated with subtle movements not captured by our current representation. Lastly, our network uses only upper-body gestures with a fixed root. We plan to expand to affect-aware gestures of the whole body, incorporating locomotion in the global coordinate space. We also plan to align the body gestures with the corresponding facial expressions, leading to fully emotive characters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>It provides 3D pose sequences of the upperbody gestures of the speakers, their speech audio, and the associated text transcripts. Each data sample has a sequence length of T = 34 time steps at a rate of 15 fps. There are 200,038 training samples in total, constituting around 80% of the dataset. The evaluation set consists of 26,903 samples or around 10% of the dataset. The test set consists of 26,245 samples, making up the remaining 10% of the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Plausibility of the different types of gestures.(b) Synchronization of the movements and the affective expressions of the different types of gestures with the speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Mean fraction of participant responses on each point of the Likert scales across the 12 speech excerpts from the TED Gesture Dataset<ref type="bibr" target="#b0">[1]</ref> and the corresponding gestures in our user study. See Sec. 5.5 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Following Yoon et al.<ref type="bibr" target="#b10">[11]</ref>, we use two sets of FC layers to learn an embedding space capturing the mean ? s ? R Ds and the variance ? s ? R Ds?Ds + of the latent distribution of the speaker styles as</figDesc><table /><note>3.1.3. Speaker Encoder. For the speaker IDs, we use one- hot vectors f s ? {0, 1}S , assuming S is the number of available speakers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 :</head><label>1</label><figDesc>Hyperparameters (HPs) for our network. We chose all the values via empirical search. Latent feature from the MFCC encoder 32 D x Latent feature from the text encoder 32 D s Latent distribution space of speaker styles 16 D a1 Per-edge latent features after STGCN 1 in the affective encoder 16 D a2 Per-edge latent features after STGCN 2 in the affective encoder 16 D a Latent affective features from the affective encoder 16 D e Predicted edge embeddings from the GRU in the generator 150 D d Predicted embeddings from the GRU in the discriminator 150 d (i) of the same edge vector, as</figDesc><table><row><cell>HP Description</cell><cell>Value</cell></row><row><cell>D m</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Evaluation of our method with baselines and ablated versions of our method on two benchmark dataset, using the objective metrics of mean absolute joint error (MAJE), mean acceleration difference (MAD), and the Fr?chet Gesture Distance (FGD). Bold indicates best.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="3">MAJE (mm) MAD (mm/s 2 ) FGD</cell></row><row><cell></cell><cell>Seq2Seq [1]</cell><cell>45.62</cell><cell>6.33</cell><cell>6.62</cell></row><row><cell>TED Gesture [1]</cell><cell>S2G-IS [12] JEM [60] GTC [11]</cell><cell>45.11 48.56 27.30</cell><cell>7.22 4.31 3.20</cell><cell>6.73 5.88 4.49</cell></row><row><cell></cell><cell>Ours w/o MFCC Enc.</cell><cell>27.84</cell><cell>3.02</cell><cell>4.21</cell></row><row><cell></cell><cell>Ours w/o Aff. Enc.</cell><cell>25.38</cell><cell>3.51</cell><cell>4.84</cell></row><row><cell></cell><cell>Ours</cell><cell>24.49</cell><cell>2.93</cell><cell>3.54</cell></row><row><cell>GENEA</cell><cell>Gesticulator [9]</cell><cell>82.41</cell><cell>3.62</cell><cell>31.04</cell></row><row><cell>Challenge 2020 [19]</cell><cell>Ours w/o MFCC Enc. Ours w/o Aff. Enc.</cell><cell>105.71 92.90</cell><cell>1.57 2.81</cell><cell>23.03 24.28</cell></row><row><cell></cell><cell>Ours</cell><cell>54.93</cell><cell>1.49</cell><cell>20.36</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work has been supported in part by ARO Grants W911NF1910069 and W911NF1910315, and Intel.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-R</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://github.com/youngwoo-yoon/Co-SpeechGesture" />
	</analytic>
	<monogr>
		<title level="m">Proc. of The International Conference in Robotics and Automation (ICRA)</title>
		<meeting>of The International Conference in Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hand and mind: What gestures reveal about thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>University of Chicago press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Social robots and virtual agents as lecturers for video instruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kizilcec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ju</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0747563215002885" />
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1222" to="1230" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtual classmates: Embodying historical learners&apos; messages as learning companions in a vr classroom through comment mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Live: The human role in learning in immersive virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Simeone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Daiber</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357251.3357590</idno>
		<ptr target="https://doi.org/10.1145/3357251.3357590" />
	</analytic>
	<monogr>
		<title level="m">Symposium on Spatial User Interaction, ser. SUI &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A job interview simulation: Social cue-based interaction with a virtual character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gebhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Porayska-Pomsta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Andr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 International Conference on Social Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simsensei kiosk: A virtual human interviewer for healthcare decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Benn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gainer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hartholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lhommet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems</title>
		<meeting>the 2014 international conference on Autonomous agents and multi-agent systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A virtual agent toolkit for serious games developers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mascarenhas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guimar?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Star</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kommeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computational Intelligence and Games (CIG)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gesticulator: A framework for semantically-aware speech-driven gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Waveren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alexandersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<idno type="DOI">10.1145/3382507.3418815</idno>
		<ptr target="https://doi.org/10.1145/3382507.3418815" />
	</analytic>
	<monogr>
		<title level="m">ser. ICMI &apos;20</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="242" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Storyprint: An interactive visualization of stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schriber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Muniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3301275.3302302</idno>
		<ptr target="https://doi.org/10.1145/3301275.3302302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces, ser. IUI &apos;19</title>
		<meeting>the 24th International Conference on Intelligent User Interfaces, ser. IUI &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="303" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech gesture generation from the trimodal context of text, audio, and speaker identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning individual styles of conversational gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Affective body expression perception and recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Body movements for affective expression: A survey of automatic recognition and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gorbet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>K?hnlenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuli?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonverbal communication: Science and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hwang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Sage Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Horgan</surname></persName>
		</author>
		<title level="m">Nonverbal communication in human interaction. Cengage Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roncal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iva: Investigating the use of recurrent motion modelling for speech gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
		<ptr target="https://trinityspeechgesture.scss.tcd.ie" />
	</analytic>
	<monogr>
		<title level="m">IVA &apos;18 Proceedings of the 18th International Conference on Intelligent Virtual Agents</title>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolfert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397481.3450692</idno>
		<ptr target="https://doi.org/10.1145/3397481.3450692" />
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Intelligent User Interfaces, ser. IUI &apos;21</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An approach to environmental psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What do we express without knowing? emotion in gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, ser. AAMAS &apos;19. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems, ser. AAMAS &apos;19. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="702" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning unseen emotions from gestures via semantically-conditioned zero-shot perception with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08906</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Brvo: Predicting pedestrian trajectories using velocityspace reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilkie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="217" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Step: Spatial temporal graph convolutional networks for emotion perception from gaits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, ser. AAAI&apos;20</title>
		<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence, ser. AAAI&apos;20</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1342" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computational models of emotion, personality, and social relationships for interactions in games: (extended abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blanchfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flintham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent Systems, ser. AAMAS &apos;16. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems</title>
		<meeting>the 2016 International Conference on Autonomous Agents and Multiagent Systems, ser. AAMAS &apos;16. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1343" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An emotionally aware embodied conversational agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Geraci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, ser. AAMAS &apos;18. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and MultiAgent Systems, ser. AAMAS &apos;18. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2250" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint end-to-end learning of pose and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073658</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073658" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A perceptual study on the manipulation of facial features for trait portrayal in virtual agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3267851.3267891</idno>
		<ptr target="https://doi.org/10.1145/3267851.3267891" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Intelligent Virtual Agents, ser. IVA &apos;18</title>
		<meeting>the 18th International Conference on Intelligent Virtual Agents, ser. IVA &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eva: Generating emotional behavior of virtual agents using expressive features of gait and gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Applied Perception</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated generation of emotive virtual humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Virtual Agents</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="490" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Identifying emotions from walking using affective and deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Randhavane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kapsaskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11884</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating emotive gaits for virtual agents using affect-based autoregression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rewkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rewkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Conference on Virtual Reality and 3D User Interfaces</title>
		<imprint>
			<publisher>IEEE VR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gesture and speech in interaction: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Malisz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kopp</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0167639313001295" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Predicting co-verbal gestures: A deep and temporal modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Virtual Agents</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="152" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gesture controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="DOI">10.1145/1833349.1778861</idno>
		<ptr target="https://doi.org/10.1145/1833349.1778861" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2010 Papers, ser. SIGGRAPH &apos;10</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluation of speech-to-gesture generation using bi-directional lstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sakuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Intelligent Virtual Agents, ser. IVA &apos;18</title>
		<meeting>the 18th International Conference on Intelligent Virtual Agents, ser. IVA &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3267851.3267878</idno>
		<ptr target="https://doi.org/10.1145/3267851.3267878" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analyzing input and output representations for speech-driven gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308532.3329472</idno>
		<ptr target="https://doi.org/10.1145/3308532.3329472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, ser. IVA &apos;19</title>
		<meeting>the 19th ACM International Conference on Intelligent Virtual Agents, ser. IVA &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Style-controllable speech-driven gesture synthesis using normalising flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alexanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13946</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13946" />
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="487" to="496" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Speech-driven animation with meaningful behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0167639318300013" />
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Predicting head pose from speech with a conditional variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laycock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Novel realizations of speech-driven head movements with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6169" to="6173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Synthesizing obama: Learning lip sync from audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073640</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073640" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897824.2925975</idno>
		<ptr target="https://doi.org/10.1145/2897824.2925975" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Phase-functioned neural networks for character control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural state machine for character-scene interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">209</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, ser. AAAI&apos;18</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, ser. AAAI&apos;18</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="7065" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Text guided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-objective adversarial gesture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359566.3360053</idno>
		<ptr target="https://doi.org/10.1145/3359566.3360053" />
	</analytic>
	<monogr>
		<title level="m">Motion, Interaction and Games, ser. MIG &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generalized mel frequency cepstral coefficients for large-vocabulary speaker-independent continuous-speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vergin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;shaughnessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="525" to="532" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Combining evidence from residual phase and mfcc features for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S R</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yegnanarayana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="55" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Emotion recognition in spontaneous speech using gmms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Laskowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth international conference on spoken language processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sentiment analysis using convolutional neural network with fasttext embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nedjah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Macedo Mourelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Latin American Conference on Computational Intelligence</title>
		<meeting><address><addrLine>LA-CCI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">An introduction to variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/12328" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A robust version of the probability ratio test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="1753" to="1758" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Language2pose: Natural language grounded pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
