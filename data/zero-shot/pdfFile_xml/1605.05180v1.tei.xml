<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bugra</forename><surname>Tekin</surname></persName>
							<email>bugra.tekin@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">CVLab EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
							<email>isinsu.katircioglu@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">CVLab EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
							<email>mathieu.salzmann@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">CVLab EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<email>lepetit@icg.tugraz.at</email>
							<affiliation key="aff1">
								<orgName type="institution">CVARLab TU Graz</orgName>
								<address>
									<settlement>Graz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<email>pascal.fua@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">CVLab EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>TEKIN ET AL.: STRUCTURED PREDICTION OF 3D HUMAN POSE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most recent approaches to monocular 3D pose estimation rely on Deep Learning. They either train a Convolutional Neural Network to directly regress from image to 3D pose, which ignores the dependencies between human joints, or model these dependencies via a max-margin structured learning framework, which involves a high computational cost at inference time.</p><p>In this paper, we introduce a Deep Learning regression architecture for structured prediction of 3D human pose from monocular images that relies on an overcomplete auto-encoder to learn a high-dimensional latent pose representation and account for joint dependencies. We demonstrate that our approach outperforms state-of-the-art ones both in terms of structure preservation and prediction accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose can now be estimated reliably by training algorithms to exploit depth data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27]</ref> or video sequences <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref>. However, estimating such a 3D pose from single ordinary images remains challenging because of the many ambiguities inherent to monocular 3D reconstruction, including occlusions, complex backgrounds, and, more generally, the loss of depth information resulting from the projection from 3D to 2D.</p><p>These ambiguities can be mitigated by exploiting the structure of the human pose, that is, the dependencies between the different body joint locations. This has been done by explicitly enforcing physical constraints at test time <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref> and by data-driven priors over the pose space <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. Recently, dependencies have been modeled within a Deep Learning framework using a max-margin formalism <ref type="bibr" target="#b19">[20]</ref>, which resulted in state-of-the-art prediction accuracy. While effective, these methods suffer from the fact that they require solving a computationally expensive optimization problem to estimate the 3D pose.  <ref type="figure">Figure 1</ref>: Our architecture for the structured prediction of the 3D human pose. (a) An auto-encoder whose hidden layers have a larger dimension than both its input and output layers is pretrained. In practice we use either this one or more sophisticated versions that are described in more detail in Section 3.1 (b) A CNN is mapped into the latent representation learned by the auto-encoder. (c) the latent representation is mapped back to the original pose space using the decoder.</p><p>By contrast, regression-based methods, such as <ref type="bibr" target="#b18">[19]</ref>, directly and efficiently predict the 3D pose given the input image. While this often comes at the cost of ignoring the underlying structure, several methods have been proposed to account for it <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, this was achieved by making use of Kernel Dependency Estimation (KDE) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>, which maps both input and output to high-dimensional Hilbert spaces and learns a mapping between these spaces. Because this approach relies on handcrafted features and does not exploit the power of Deep Learning, it somewhat under-performs more recent CNN-based techniques <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In this paper, we demonstrate that we can account for the human pose structure within a deep learning framework by first training an overcomplete auto-encoder that projects body joint positions to a high dimensional space represented by its middle layer, as depicted by <ref type="figure">Fig. 1(a)</ref>. We then learn a CNN-based mapping from the input image to this highdimensional pose representation as shown in <ref type="figure">Fig. 1(b)</ref>. This is inspired by KDE in that it can be understood as replacing kernels by the auto-encoder layers to predict the pose parameters in a high dimensional space that encodes complex dependencies between different body parts. As a result, it enforces implicit constraints on the human pose, preserves the human body statistics, and improves prediction accuracy, as will be demonstrated by our experiments. Finally, as illustrated in <ref type="figure">Fig. 1(c)</ref>, we connect the decoding layers of the auto-encoder to this network, and fine-tune the whole model for pose estimation.</p><p>In short, our contribution is to show that combining traditional CNNs for supervised learning with auto-encoders for structured learning preserves the power of CNNs while also accounting for dependencies, resulting in increased performance. In the remainder of the paper, we first briefly discuss earlier approaches. We then present our structured prediction approach in more detail and finally demonstrate that it outperforms state-of-the-art methods on the Human3.6m dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Following recent trends in Computer Vision, human pose estimation is now usually formulated within a Deep Learning framework. The switch away from earlier representations started with 2D pose estimation by learning a regressor from an input image to either directly the pose vectors <ref type="bibr" target="#b31">[32]</ref> or the heatmaps encoding 2D joint locations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31]</ref>. Recently, this trend has extended to 3D pose estimation <ref type="bibr" target="#b18">[19]</ref>, where the problem is typically formulated in terms of continuous 3D joint locations, since discretizing the 3D space is more challenging than in the case of 2D.</p><p>Another important difference between 2D and 3D pose estimation comes from the additional ambiguities in the latter one due to the fact that the input only shows a projection of the output. To overcome these ambiguities, recent algorithms have attempted to encode the dependencies between the different joints within Deep Learning approaches, thus effectively achieving structured prediction. In particular, <ref type="bibr" target="#b9">[10]</ref> uses auto-encoders to learn a shared representation for 2D silhouettes and 3D poses. This approach, however, relies on accurate foreground masks and exploits handcrafted features, which mitigate the benefits of Deep Learning. In the context of hand pose estimation, <ref type="bibr" target="#b20">[21]</ref> introduces a bottleneck, lowdimensional layer that aims at accounting for joint dependencies. This layer, however, is obtained directly via PCA, which limits the kind of dependencies it can model.</p><p>To the best of our knowledge, the work of <ref type="bibr" target="#b19">[20]</ref> constitutes the most effective approach to encoding dependencies within a Deep Learning framework for 3D human pose estimation. This approach extends the structured SVM model to the Deep Learning setting by learning a similarity score between feature embeddings of the input image and the 3D pose. This process, however, comes at a high computational cost at test time, since, given an input image, the algorithm needs to search for the highest-scoring pose. Furthermore, the final results are obtained by averaging over multiple high-scoring ground-truth training poses, which might not generalize well to unseen data since the prediction can thus only be in the convex hull of the ground-truth training poses. By contrast, we draw inspiration from the KDE-based approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, that map both image and 3D pose to high-dimensional Hilbert spaces and learn a mapping between these spaces. Here, however, we show how to do this in a Deep Learning context with CNNs and auto-encoders. The benefits are twofold: We can leverage the power of learned features that have proven more effective than handcrafted ones such as HOG <ref type="bibr" target="#b0">[1]</ref>, and our framework relies on a direct and efficient regression between the two spaces, thus avoiding the computational burden of the state-of-the-art approach of <ref type="bibr" target="#b19">[20]</ref>.</p><p>Using auto-encoders for unsupervised feature learning has proven effective in several recognition tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>. In particular, denoising auto-encoders <ref type="bibr" target="#b33">[34]</ref> that aim at reconstructing the perfect data from a corrupted version of it have demonstrated good generalization ability. Similarly, contractive auto-encoders have been shown to produce intermediate representations that are robust to small variations of the input data <ref type="bibr" target="#b23">[24]</ref>. All these methods, however, rely on auto-encoders to learn features for recognition tasks. By contrast, here, we exploit auto-encoders to model the output structure for regression purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this work, we aim at directly regressing from an input image x to a 3D human pose. As in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>, we represent the human pose in terms of the 3D locations y ? R 3J of J body joints relative to a root joint. An alternative would have been to predict the joint angles and limb lengths, however this is a less homogeneous representation and is therefore rarely used for regression.</p><p>As discussed above, a straightforward approach to creating a regressor is to train a conventional CNN such as the one used in <ref type="bibr" target="#b18">[19]</ref>. However, this fails to encode dependencies between joint locations. In <ref type="bibr" target="#b19">[20]</ref>, this limitation was overcome by introducing a substantially more complex, deep architecture for maximum-margin structured learning. Here, we encode dependencies in a simpler, more efficient, and ultimately more accurate way by learning a mapping between the output of a conventional CNN and a latent representation obtained using an overcomplete auto-encoder, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The auto-encoder is pre-trained on human poses and comprises a hidden layer of higher dimension than its input and output. In effect, this hidden layer and the CNN-based representation of the image play the same role as the kernel embeddings in KDE-based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>, thus allowing us to account for structure within a direct regression framework. Once the mapping between these two high-dimensional embeddings is learned, we further fine-tune the whole network for the final pose estimation task, as depicted at the bottom of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In the remainder of this section, we describe the different stages of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using Auto-Encoders to Learn Structured Latent Representations</head><p>We encode the dependencies between human joints by learning a mapping of 3D human pose to a high-dimensional latent space. To this end, we use a denoising auto-encoder that can have one or more hidden layers. Following standard practice <ref type="bibr" target="#b34">[35]</ref>, given a training set of pose vectors {y i }, we add isotropic Gaussian noise to create noisy versions {? i } of these vectors. We then train our auto-encoder to take as input a noisy? i and return a denoised y i as output. The corresponding reconstruction function f ae (?) must satisf?</p><formula xml:id="formula_0">y = f ae (y, ? ae ) ,<label>(1)</label></formula><p>where? is the reconstruction and ? ae = (W enc, j , b enc, j ,W dec, j , b dec, j ) L j=1 contains the model parameters, that is, the weights and biases for L encoding and decoding layers. We take the middle layer to be our latent pose representation and denote it by h L . We use ReLU as the activation function of the encoding layer. This favors a sparse hidden representation <ref type="bibr" target="#b7">[8]</ref>, which has been shown to be effective at modeling a wide range of human poses <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. A linear activation function is used at the decoding layer of the auto-encoder to reproject to both negative and positive joint coordinates. To keep the number of parameters small and reduce overfitting, we use tied weights for the encoder and the decoder, that is, W dec, j = W T enc, j . To learn the parameters ? ae , we rely on the square loss between the reconstruction,?, and the original input, y, over the N training examples. To increase robustness to small pose changes, we regularize the cost function by adding the squared Frobenius norm of the Jacobian of the hidden mapping g(?), that is,</p><formula xml:id="formula_1">J(y) = ? g ? y (y) where g(?)</formula><p>is the encoding function that maps the input? to the middle hidden layer, h L . Training can thus be expressed as finding</p><formula xml:id="formula_2">? * ae = argmin ? ae N ? i ||y i ? f (y i , ? ae )|| 2 2 + ? J(y i ) 2 F ,<label>(2)</label></formula><p>where ? is the regularization weight. Unlike when using KDE, we do not need to solve a complex pre-image problem to go from the latent pose representation to the pose itself. This mapping, which corresponds to the decoding part of our auto-encoder, is learned directly from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Regression in Latent Space</head><p>Once the auto-encoder is trained, we aim to learn a mapping between the input image and the latent representation of the human pose. To this end, and as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, we make use of a CNN to regress the image to a high-dimensional representation, which is itself mapped to the latent pose representation. More specifically, let ? cnn be the parameters of the CNN, including the mapping to the latent pose representation. Given an input image x, we consider the square loss function between the representation predicted by the CNN, f cnn (x, ? cnn ), and the one that was previously learned by the auto-encoder, h L . Given our N training samples, learning amounts to finding</p><formula xml:id="formula_3">? * cnn = argmin ? cnn N ? i || f cnn (x i , ? cnn ) ? h L,i || 2 2 .<label>(3)</label></formula><p>In practice, as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, we rely on a standard CNN architecture similar to the one of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref>. It comprises three convolutional layers-C1, C2 and C3-each followed by a pooling layer-P1, P2, and P3. In our implementation, the input volume is a three channel image of size 128 ? 128. P3 is directly connected to a cascade of fully-connected layers-FC1, FC2 and FC3-that produces a 4096-dimensional image representation, which is then mapped linearly to the latent pose embedding. Except for this last linear layer, each layer uses a ReLU activation function.</p><p>As in <ref type="bibr" target="#b18">[19]</ref>, prior to training our CNN, we first initialize the convolutional layers using a network trained for the detection of body joints in 2D. We then replace the fully-connected layers of the detection network with those of the regressor to further train for the pose estimation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-Tuning the Whole Network</head><p>Finally, as shown in <ref type="figure" target="#fig_1">Fig. 2(c)</ref>, we append the decoding layers of the auto-encoder to the CNN discussed above, which reprojects the latent pose estimates to the original pose space. We then fine-tune the resulting complete network for the task of human pose estimation. We take the cost function to be the squared difference between the predicted and ground-truth 3D poses, which yields the optimization problem</p><formula xml:id="formula_4">? * f t = argmin ? f t N ? i || f f t (x i , ? f t ) ? y i || 2 2 ,<label>(4)</label></formula><p>where ? f t are the complete set of model parameters and f f t is the mapping function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section, we first describe the large-scale dataset we tested our approach on. We then give implementation details and describe the evaluation protocol. Finally, we compare our results against those of the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate our method on the Human3.6m dataset <ref type="bibr" target="#b13">[14]</ref>, which comprises 3.6 million image frames with their corresponding 2D and 3D poses. The subjects perform complex motion scenarios based on typical human activities such as discussion, eating, greeting and walking. The videos were captured from 4 different camera viewpoints. Following the standard procedure of <ref type="bibr" target="#b18">[19]</ref>, we collect the input images by extracting a square region around the subject using the bounding box present in the dataset and resize it to 128 ? 128. The output pose is a vector of 17 3D joint coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We trained our auto-encoder using a greedy layer-wise training scheme followed by finetuning as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35]</ref>. We set the regularization weight of Eq. 2 to ? = 0.1. We experimented with single-layer auto-encoders, as well as with 2-layer ones. The size of the layers were set to 2000 and 300-300 for the 1-layer and 2-layer cases, respectively. We corrupted the input pose with zero-mean Gaussian noise with standard deviation of 40 for 1-layer and 40-20 for 2-layer auto-encoders. In all cases, we used the ADAM optimization procedure <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 0.001 and a batch size of 128. The number and individual sizes of the layers of our CNNs are given in <ref type="figure" target="#fig_1">Fig. 2</ref>. The filter sizes for the convolutional layers are consecutively 9?9, 5?5 and 5?5. Each convolutional layer is followed by a 2 ? 2 max-pooling layer. The activation function is the ReLU in all the layers except for the last one that uses linear activation. As for the auto-encoders, we used ADAM <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 0.001 and a batch size of 128. To prevent overfitting, we applied dropout with a probability of 0.5 after each fully-connected layer and augment the data by randomly cropping 112 ? 112 patches from the 128 ? 128 input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Protocol</head><p>For a fair comparison, we used the same data partition protocol as in earlier work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> for training and test splits. The data from 5 subjects (S1,S5,S6,S7,S8) was used for training and   <ref type="table">Table 1</ref>: Average Euclidean distance in mm between the ground-truth 3D joint locations and those predicted by competing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref> and ours.</p><p>the data from 2 different subjects (S9,S11) was used for testing. We evaluate the accuracy of 3D human pose estimation in terms of average Euclidean distance between the predicted and ground-truth 3D joint positions as in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The accuracy numbers are reported in milimeters for all actions on which the authors of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> provided results. Training and testing were carried out monocularly in all camera views for each separate action. <ref type="figure" target="#fig_2">Fig. 3</ref> depicts selected pose estimation results on Human3.6m. In <ref type="table">Table 1</ref>, we report our results on this dataset along with those of three state-of-the-art approaches: KDE regression from HOG features to 3D poses <ref type="bibr" target="#b13">[14]</ref>, jointly training a body part detector and a 3D pose regressor network <ref type="bibr" target="#b18">[19]</ref>, and using a maximum-margin formalism within a Deep Learning framework for structured prediction <ref type="bibr" target="#b19">[20]</ref>. For the latter, the estimation is taken to be either the highest-scoring pose or the average of the 500 highest-scoring training poses. Our method consistently outperforms all the baselines. The results reported in <ref type="table">Table 1</ref> were obtained using a two layer auto-encoder. However, as discussed in Section 3.1 our formalism applies to auto-encoders of any depth. Therefore, in <ref type="table">Table 2</ref> (a), we report results obtained using a single layer one as well as by turning off the final fine-tuning of Section 3.3. For completeness, we also report results obtained by using a CNN similar to the one of <ref type="figure" target="#fig_1">Fig. 2(b)</ref> to regress directly to a 51-dimensional 3D pose vector without using an auto-encoder at all. We will refer to it as CNN-Direct. We found that both kinds of auto-encoders perform similarly and better than CNN-Direct, especially for actions  <ref type="table">Table 2</ref>: Average Euclidean distance in mm between the ground-truth 3D joint locations and those computed (a) using either no auto-encoder at all (CNN) or 1-layer and 2-layer encoders (OURS), with or without fine-tuning (FT), (b) by replacing the auto-encoder by either an additional fully-connected layer (CNN-ExtraFC) or a PCA layer (CNN-PCA). The bracketed numbers denote the various dimensions of the additional layer we tested. Our approach again yields the most accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>such as Taking Photo and Walking Dog that involve interactions with the environment and are thus physically more constrained. This confirms that the power of our method comes from auto-encoding. Furthermore, as expected, fine-tuning consistently improves the results. During fine-tuning, our complete network has more fully-connected layers than CNN-Direct. One could therefore argue that the additional layers are the reason why our approach outperforms it. To disprove this, we evaluated the baseline, CNN-ExtraFC, in which we simply add one more fully-connected layer. We also evaluated another baseline, CNN-PCA, in which we replace our auto-encoder latent representation by a PCA-based one. In <ref type="table">Table 2</ref>(b), we show that our approach significantly outperforms these two baselines on the Taking Photo action. This suggests that our overcomplete auto-encoder yields a representation that is more discriminative than other latent ones. Among the different PCA configurations, the one with 40 dimensions performs the best. However, training an auto-encoder with 40 dimensions outperforms it.</p><p>Following <ref type="bibr" target="#b11">[12]</ref>, we show in <ref type="figure">Fig. 4</ref> the differences between the ground-truth limb ratios and the limb ratios obtained from predictions based on KDE, CNN-Direct and our approach. These results evidence that our predictions better preserve these limb ratios, and thus better model the dependencies between joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Parameter Choices</head><p>In <ref type="table">Table 3</ref>, we compare the results of different auto-encoder configurations in terms of number of layers and channels per layer on the Greeting action. Similarly to what we did in <ref type="table">Table 2</ref>(b), the bracketed numbers denote the dimension of the auto-encoder's hidden layers. We obtained the best result for 1 layer with 2000 channels or 2 layers with 300-300 channels. These values are those we used for all the experiments described above. They were chosen for a single action and used unchanged for all others, thus demonstrating the versatility of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced a novel Deep Learning regression architecture for structured prediction of 3D human pose from a monocular image. We have shown that our approach to combining auto-encoders with CNNs accounts for the dependencies between the human body parts efficiently and yields better prediction accuracy than state-of-the-art methods. Since our framework is generic, in future work, we intend to apply it to other structured prediction problems, such as deformable surface reconstruction.   <ref type="figure">Figure 4</ref>: Matrix of differences between estimated log of limb length ratios and those computed from ground-truth poses. The rows and columns correspond to individual limbs. For each cell, the ratios are computed by dividing the limb length in the horizontal axis by the one in the vertical axis as in <ref type="bibr" target="#b11">[12]</ref> for (a) KDE <ref type="bibr" target="#b13">[14]</ref>, (b) CNN-Direct as in <ref type="table">Table 2</ref>, and (c,d) our method without and with fine-tuning. An ideal result would be one in which all cells are blue, meaning the limb length ratios are perfectly preserved. Best viewed in color. (e) Sum of the log of limb length ratio errors for different parts of the human body. All methods perform well on the lower body. However, ours outperforms the others on the upper body and when considering all ratios in the full body. <ref type="table">Table 3</ref>: Average Euclidean distance in mm between the ground-truth 3D joint locations and the ones predicted by our approach trained using auto-encoders in various configurations, with different number of layers and number of channels per layer as indicated by the bracketed numbers. This validation was performed on the Greeting action and the optimal values used for all other actions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>c 2016 .</head><label>2016</label><figDesc>The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. * indicates equal contribution. arXiv:1605.05180v1 [cs.CV] 17 May 2016</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Auto-encoder training (b) Regression in latent space (c) Fine-tuning Our approach. (a) We train a stacked denoising auto-encoder that learns and enforces implicit constraints about human body in its latent middle layer h L . (b) Our CNN architecture maps the image to the latent representation h L learned by the auto-encoder. (c) We stack the decoding layers of the auto-encoder on top of the CNN for reprojection from the latent space to the original pose space and fine-tune the entire network by updating the parameters of all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>3D poses for the Walking, Eating, Taking Photo, Greeting, Discussion and Walking Dog actions of the Human3.6m database. In each case, the first skeleton depicts the groundtruth pose and the second one the pose we recover. Best viewed in color.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D Human Pose from Silhouettes by Relevance Vector Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-Conditioned Joint Angle Limits for 3D Human Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3D Pose Estimation and Tracking by Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Twin Gaussian Processes for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A General Regression Technique for Learning Transductions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Optimization and Filtering for Human Motion Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient Regression of General-Activity Human Poses from Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Multimodal Deep Autoencoder for Human Pose Recovery. TIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human Shape and Pose Tracking Using Keyframes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent Structured Models for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6m: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Human Pose Estimation Features with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-bias autoencoders and the benefits of co-adapting features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3D Human Pose Estimation from Monocular Images with Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hands Deep in Deep Learning for Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Flowing ConvNets for Human Pose Estimation in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reconstructing 3D Human Pose from 2D Image Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contractive Auto-encoders: Explicit invariance during feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Combining Discriminative and Generative Methods for 3D Deformable Surface and Articulated Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Real-Time Human Pose Recognition in Parts from a Single Depth Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic Tracking of 3D Human Figures Using 2D Image Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kinematic Jump Processes for Monocular 3D Human Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Direct Prediction of 3D Body Poses from Motion Compensated Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeppose: Human Pose Estimation via Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Priors for People Tracking from Small Training Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kernel Dependency Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
