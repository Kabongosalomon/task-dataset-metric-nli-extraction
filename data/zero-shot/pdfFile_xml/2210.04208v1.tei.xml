<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Let Images Give You More: Point Cloud Cross-Modal Training for Shape Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
							<email>xuyan1@link.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heshen</forename><surname>Zhan</surname></persName>
							<email>heshenzhan@link.</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">USV</orgName>
								<orgName type="institution" key="instit2">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Pengcheng Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<email>lizhen@cuhk.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fnii</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuhk-Shenzhen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sse, Cuhk-Shenzhen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sds, Cuhk-Shenzhen</surname></persName>
						</author>
						<title level="a" type="main">Let Images Give You More: Point Cloud Cross-Modal Training for Shape Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although recent point cloud analysis achieves impressive progress, the paradigm of representation learning from a single modality gradually meets its bottleneck. In this work, we take a step towards more discriminative 3D point cloud representation by fully taking advantages of images which inherently contain richer appearance information, e.g., texture, color, and shade. Specifically, this paper introduces a simple but effective point cloud cross-modality training (PointCMT) strategy, which utilizes view-images, i.e., rendered or projected 2D images of the 3D object, to boost point cloud analysis. In practice, to effectively acquire auxiliary knowledge from view images, we develop a teacher-student framework and formulate the crossmodal learning as a knowledge distillation problem. PointCMT eliminates the distribution discrepancy between different modalities through novel feature and classifier enhancement criteria and avoids potential negative transfer effectively. Note that PointCMT effectively improves the point-only representation without architecture modification. Sufficient experiments verify significant gains on various datasets using appealing backbones, i.e., equipped with PointCMT, PointNet++ and PointMLP achieve state-of-the-art performance on two benchmarks, i.e., 94.4% and 86.7% accuracy on ModelNet40 and ScanObjectNN, respectively. Code will be made available at https://github.com/ZhanHeshen/PointCMT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the fundamental 3D representation, point clouds have attracted increasing attention for various applications, e.g., self-driving <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, robotics perception <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>, etc. Generally, a point cloud consists of sparse and unordered points in the 3D space, which is significantly different from a 2D image with a dense and regular pixel array. Prior studies treat the understanding of 2D images and 3D point clouds as two separate problems, and both have their own merits and drawbacks. Concretely, rich color and fine-grained texture are easily obtained in 2D images, but they are ambiguous in depth and shape sensing. Previous works extract features on images through convolution neural networks (CNN). In contrast, point clouds are superior in providing spatial and geometric information but only preserve sparse and textureless features. Several prior studies process features on unstructured point clouds through local aggregation operators <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref>. It is natural to raise a question: Could we use the rich information hidden in 2D images to boost 3D point cloud shape analysis?</p><p>To address the above issue, one straightforward way is to leverage the benefits of both images and point clouds, i.e., fusing information from two complementary representations with task-specific design <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>. However, utilizing additional image representation requires designing Moreover, the exploiting extra-images is usually computation-intensive and paired-images are usually unavailable during inference. Thus, multi-modal learning meets its bottleneck in many aspects.</p><p>This paper tries to ease the barrier of cross-modal learning between images and point clouds. Inspired by knowledge distillation (KD) that achieves knowledge transfer from a teacher model to a student one, we formulate the cross-modal learning as a KD problem, conducting alignment between sample representations learned by images and point clouds. However, previous KD approaches usually assume that the training data used by the teacher and student are from the same distribution <ref type="bibr" target="#b16">[17]</ref>. Thus, since sparse and disordered point clouds represent visual information different from images, naive feature alignment between two representations appeals to cause limited gains or negative transfer for the cross-modal scenario. To this end, we design a novel framework for cross-modal KD and propose the point cloud cross-modal training strategy, i.e., PointCMT in <ref type="figure" target="#fig_0">Figure 1</ref> (a), which distills features derived from images into the point cloud representation. Specifically, multiple view images for each 3D object can be generated through either rendering the CAD model or conducting perspective projection on the point cloud from different viewpoints. These free auxiliary images are fed into the image network to obtain the global representations for the object. Besides, feature and classifier enhancements are conducted between the point cloud and image features, in which the newly proposed criteria effectively avoid negative transfer between different modalities, i.e., directly applying <ref type="bibr" target="#b16">[17]</ref> hampers the performance on ModelNet40. After training, the model gains higher performance, only taking point clouds as input without architecture modification.</p><p>Compared with multi-modal approaches, our solution has the following preferable properties: 1) Generality: PointCMT can be integrated with arbitrary point cloud analysis models without structural modification. 2) Effectively: It significantly boosts the performance upon several baseline approaches, e.g., PointNet++ <ref type="bibr" target="#b33">[34]</ref> achieves state-of-the-art 94.4% from 93.4% overall accuracy on ModelNet40, as shown in <ref type="figure" target="#fig_0">Figure 1 (b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>3D Shape Recognition Based on Point Clouds. These stream methods directly process raw point clouds as input (also called point-based methods). They are pioneered by PointNet <ref type="bibr" target="#b32">[33]</ref>, which approximates a permutation-invariant set function using a per-point Multi-Layer Perceptron (MLP) followed by a max-pooling layer. Later on, point-based methods aim at designing local aggregation operators for local feature extraction. Specifically, they generally sample multiple sub-points from the original point cloud, and then aggregate neighboring features of each sub-point through local aggregation operators, in which point-wise MLPs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31]</ref>, adaptive weight <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref> and pseudo grid based methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b19">20]</ref> are proposed. More recently, there are some attempts to utilize non-local operator <ref type="bibr" target="#b53">[54]</ref>, or Transformer <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b13">14]</ref> to mine the long distance dependency. This paper also follows the paradigm of point-based methods to conduct point cloud shape analysis.</p><p>3D Shape Recognition Based on Images. Since point clouds are irregular and unordered, some works consider projecting the 3D shapes into multiple images from different viewpoints (also called view-based methods) and then leverage the well-developed 2D CNNs to process 3D data. One seminal work of multi-view learning is MVCNN <ref type="bibr" target="#b37">[38]</ref>. It extracts per-view features with a shared CNN in parallel, then aggregates via a view-level max-pooling layer. Most follow-up works propose more effective modules to aggregate the view-level features. For instance, some of them enhance the aggregated feature by considering similarity among views <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b55">56]</ref> while others focus on the viewpoint relation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b21">22]</ref>. The above methods usually utilize ad-hoc rendered images for each 3D shape, including shade and texture for the surface mesh. Therefore, they generally achieve higher performance than point-based methods using sparse point clouds as input. Recently, <ref type="bibr" target="#b11">[12]</ref> proposes a simple but effective method (SimpleView) through directly projecting sparse point clouds onto image planes, achieving comparable performance with point-based methods. Inspired by view-based methods, this paper takes advantage of extracted image features from the view-based method, which are utilized as prior knowledge to boost point cloud shape analysis.</p><p>Knowledge Distillation. Knowledge distillation (KD) aims at compressing a large network (teacher) to a compact and tiny one (student), and boost the performance of the student at the same time. The concept was first shown by Hinton et al. <ref type="bibr" target="#b16">[17]</ref>, which trains a student by using the softened logits of a teacher as targets. Over the past few years, several subsequent approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b60">61]</ref> use different criteria to align the sample representations between the teacher and student. However, almost all the existing works assume that the training data used by the teacher and student networks are from the same distribution. Our experiment illustrates that new biases and negative transfer will be introduced in the distillation process if cross-modal data from different distribution (e.g., features extracted from unordered point cloud and regular grid image) is utilized directly on previous KDs.</p><p>Cross-Modal Knowledge Transfer. Cross-modal knowledge transfer in computer vision is a relatively emerging field that aims to utilize additional modalities at the training stage and enhance the model's performance on the target modality at the inference. Recently, there are 3D-to-2D knowledge transfer approaches, which adopt geometric aware 3D features from point clouds to enhance the performance of 2D tasks through a contrastive manner <ref type="bibr" target="#b17">[18]</ref> or feature alignment <ref type="bibr" target="#b29">[30]</ref>. Later on, approaches attempt to transfer priors in images to enhance 3D point cloud-related tasks, and some are designed for specific tasks. Concretely, <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26]</ref> propose the images-to-point contrastive pre-training, <ref type="bibr" target="#b49">[50]</ref> inflates 2D convolution kernels to the 3D ones and <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53]</ref> independently apply cross-modal training for visual grounding, captioning and semantic segmentation. Inspired but different from the above, we are the first to conduct image-to-point knowledge distillation for point cloud analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodlogy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Let P ? R N ?3 and y ? R 1 be the point cloud and ground-truth label of the 3D object. Its corresponding view-image counterparts can be denoted as I ? R V ?H?W ?3 , where N , V and (H, W ) are the number of points, number of view-images and image size, respectively. View images can be gained by rendering the 3D CAD model <ref type="bibr" target="#b37">[38]</ref> or perspective projecting the raw point cloud <ref type="bibr" target="#b11">[12]</ref>.</p><p>We denote T and S as the image and point cloud analysis networks, respectively, and regard them as the teacher and student in traditional knowledge distillation (KD). For these networks, we split each of them into two parts: (i) Encoders (i.e., feature extractors Enc img (?) and Enc pts (?)), the output of  which at the last layer are global feature representations F img ? R D and F pts ? R D . (ii) Classifiers, which project the feature representation F img and F pts into class logits through Cls img (F img ) and Cls pts (F pts ), where Cls(?) denotes the classifier.</p><formula xml:id="formula_0">LCE LFeature LEMD I P F img F pts F img F img F pts P pts P img</formula><p>Since we formulate the cross-modal learning as a KD problem, its goal in the learning process is to distill the prior knowledge from the image into point cloud features, obtaining an image-enhanced ideal feature F KD . During the knowledge distillation, we parameterize the teacher and student networks with ? T and ? S , and denote the knowledge of the teacher network as K T . From the Bayesian perspective, a neural network can be viewed as a probability model, e.g., P (y|P, ? S ) for point cloud analysis model as an example: given an input point cloud P, the network assigns an output probability with the parameters ? S . Therefore, if we want the student network guided by image knowledge on the input sample, our goal can be further reformulated as maximizing the probability P(F KD |P, y; ? S , K T ), which can also be used to measure the ability of student network extracting the feature with cross-modal information. To find the lower bound of the above probability, we define the discrepancy g between theoretically discriminative features F img * and F pts *</p><formula xml:id="formula_1">as g = P(F img * |I, y; ? T ) ? P(F pts * |P, y; ? S , K T ),<label>(1)</label></formula><p>where F img * and F pts * are ideal features in specific modalities. In Lemma 1, we give the lower bound of cross-modal learning as a KD problem. The proof is provided in the supplementary material.</p><p>Lemma 1: By the definition above, P(F KD |P, y; ? S , K T ) is bounded below by P(F KD |I, y;</p><formula xml:id="formula_2">? T ) + ? ? g, where ? is ? = P(F img * |I, y; ? T ) ? P(F KD |I, y; ? T ).<label>(2)</label></formula><p>In Lemma 1, ? measures the compatibility of knowledge distillation of the image networks, and can be viewed as a constant when the architectures are determined. P(F KD |I, y; ? T ) is also a constant when the parameters ? T and the architecture of the point cloud analysis model are fixed, e.g., using a pre-trained image network. Therefore, the Lemma ensures that during the knowledge distillation, one can maximize P(F KD |P, y; ? S , K T ) through minimizing g, which gives a theoretical guarantee for the KD problem.</p><p>If we adopt previous KD studies in cross-modal scenario, they minimize g through making student directly approximate the teacher's features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref> or predict logits <ref type="bibr" target="#b16">[17]</ref>. However, in a common KD problem, the teacher and student are generally trained on the same dataset with an identical distribution <ref type="bibr" target="#b16">[17]</ref>. Moreover, the teacher generally achieves better performance than the student. In contrast, the image and point cloud analysis models tend to learn different feature representations and logits distribution which are generally complementary. Direct alignment may cause a limited gain or even negative transfer. Moreover, previous KD approaches treat encoders and classifiers as a whole architecture since the teacher and student networks generally have the same components. However, the point cloud convolution in encoder is significantly different from the 2D CNN, but they have the same classifier design. </p><formula xml:id="formula_3">(F img )} M i=1 supervised by ground-truth labels {yi} M i=1 .</formula><p>Stage II: Training cross-modal point generator: As shown in <ref type="figure" target="#fig_1">Figure 2</ref>  </p><formula xml:id="formula_4">(b), the generator takes features {F img i } M i=1 as input, reconstructs point cloud {P img i } M i=1</formula><formula xml:id="formula_5">(F pts )} M i=1 and {Cls pts (F img )} M i=1 .</formula><p>To solve the cross-modal KD problem, we propose PointCMT, which effectively solves the crossmodal learning problem. The workflow of PointCMT is demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref> (a) and can be summarized as Algorithm 1. Specifically, there are three stages exist in PointCMT. In Stage I (Section 3.2), we train the image encoder and classifier using view-images and ground-truth labels.</p><p>In Stage II (Section 3.3), we train the cross-modal point generator (CMPG) through image features, and we independently align features and logits of two modalities in stage III (Section 3.4), where the encoder and classifier of point cloud analysis network are both enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Image Priors</head><p>For each 3D object, we use multiple view-images (i.e., rendered color images or projected images from raw point cloud) as additional data, and the whole process can be described as:</p><formula xml:id="formula_6">F img = A{CNN(I v )} V v=1 .<label>(3)</label></formula><p>Inspired by view-based learning approaches <ref type="bibr" target="#b37">[38]</ref>, V view-images from I flow into a shared-weights image feature extractor CNN(?) to obtain a series of vectors. By aggregating the view-level vectors via an aggregation function A{?}, we obtain a global feature representation F img from all images, which integrates shape information from multiple views. Finally, an image classifier maps the above global feature to gain a prediction logits through Cls img (F img ), which is supervised by the ground truth label y through cross-entropy loss L CE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-Modal Point Generator</head><p>A cross-modal point generator (CMPG) can be seen as a nonlinear transformation R D ? R N ?3 that maps the global feature representation F img acquired from images into the Euclidean space. Thus, it can avoid potential negative transfer effectively by directly aligning cross-modal features from different distributions. In order to better learn image priors in the point cloud analysis network, we pre-train the CMPG through the image feature F img ? R D , and fix it in the distillation stage. <ref type="figure" target="#fig_1">Figure 2 (b)</ref> illustrates the pre-training stage of CMPG. It takes the image feature as input and reconstruct a point cloudP img ? R N ?3 , which is supervised by the original point cloud P through Earth Mover's distance (EMD) <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_7">L EMD (P,P img ) = min ? p?P ||p ? ?(p)||,<label>(4)</label></formula><p>where |P| = |P img | and ? : P ?P img is a bijection, i.e., for each point p ? P, ?(?) finds a sole point correspondence inP img . After pre-training, CMPG reconstructs a point cloud through an image-relative feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Image Priors Assisted Training</head><p>During the training stage, three objectiveness should be optimized:</p><p>Classification Loss. In our PointCMT, arbitrary point cloud analysis models can be assembled. Generally, it should be designed through a point encoder and a classifier, where the point encoder takes a point cloud as input, generates the point cloud feature representation F pts and feeds it into the classifier Cls pts (?) to obtain a class logits. Finally, the cross-entropy loss L CE is used as criteria.</p><p>Feature Enhancement Loss. Unlike previous KD methods that directly align features of teacher and student, we first transform the cross-modal features into Euclidean space. As shown in <ref type="figure" target="#fig_1">Figure 2 (c)</ref>, the pre-trained CMPG independently transform F img and F pts , obtaining two point cloudsP img andP pts , respectively. After that, EMD loss is conducted on the two-point clouds as an objectiveness:</p><formula xml:id="formula_8">L Feature = L EMD (P pts ,P img ) = min ? p?P pts ||p ? ?(p)||,<label>(5)</label></formula><p>where |P pts | = |P img | and ? :P pts ?P img is a bijection. Compared with traditional L 2 loss, the EMD distance is natural for solving an assignment problem for permutation-invariant point sets. For all but a zero-measure subset of point set pairs, the optimal bijection ? is unique and invariant under the infinitesimal movement of the points. Thus, EMD is differentiable almost everywhere.</p><p>Classifier Enhancement Loss. In addition to supervising the point feature extractor through the above Feature Enhancement, we further propose constraints conducted on the point classifier (as <ref type="figure" target="#fig_1">Figure 2 (a)</ref>). Specifically, the image feature generated by the teacher network is fed into the point classifier, in which the gradient only back-propagates to the point classifier. Besides, classifier enhancement is proposed to enable the point classifier to handle the image feature during the distillation. It aligns outputs logits of the point classifier by using image and point features as inputs. This constraint is modified based on the distillation loss in Hinton et al. <ref type="bibr" target="#b16">[17]</ref> as in Equation <ref type="formula" target="#formula_9">(6)</ref>, but in this case, two sets of logits come from the same classifier.</p><formula xml:id="formula_9">L Hinton = D KL (Cls img (F img )||Cls pts (F pts )),<label>(6)</label></formula><p>where D KL (?||?) is KL divergence. In contrast, our proposed classifier enhancement is more suitable for the cross-modal scenario where a great discrepancy exist between image and point cloud features. Concretely, the loss for classifier enhancement can be written as L Classifier = D KL (Cls pts (F img )||Cls pts (F pts )).</p><p>Final Loss. The final loss is a weighted sum of the above three losses L = L CE + ?L Feature + ?L Classifier , where ? = 30 and ? = 0.3 are the weights to adjust the ratios of each loss, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shape Classification on ModelNet40</head><p>Raw Point Cloud Rendered from CAD Projection  We firstly evaluate our PointCMT on synthetic dataset ModelNet40 <ref type="bibr" target="#b47">[48]</ref>, which is a large-scale 3D CAD model dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Description and Processing.</head><p>ModelNet40 is composed of 9,843 train models and 2,468 test models in 40 classes. For the input of the 3D network, we use point clouds provided by the official dataset, which is the same as PointNet <ref type="bibr" target="#b32">[33]</ref>. For the input of the image network, we use 20 rendered view images from CAD models utilized in RotationNet <ref type="bibr" target="#b21">[22]</ref>. These images have a resolution of 224 ? 224. Since they consider both the mesh surfaces and illumination, they can provide more information to the 3D network. Selected samples of point clouds and corresponding multi-view images are shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Implementation. For image network, we use ResNet-18 <ref type="bibr" target="#b15">[16]</ref> pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> as the feature extractor. Following MVCNN <ref type="bibr" target="#b37">[38]</ref>, we obtain the global shape feature by applying viewwise max-pooling to the view-level features. Finally, a fully-connected layer is used to output the classification logits. During the training process of the above network, we use SGD as our optimizer with a learning rate of 0.01. The batch size is set to 128 for 50 epochs. After that, we fix the image network and train the CMPG with Adam and 32 batch size for 50 epochs. In practice, CMPG consists of three-layer MLP. For the point cloud analysis models, DGCNN <ref type="bibr" target="#b42">[43]</ref> and RS-CNN <ref type="bibr" target="#b26">[27]</ref> are independently trained with the training strategies provided in their official codes. PointNet++ <ref type="bibr" target="#b33">[34]</ref> is trained with strategy of RS-CNN <ref type="bibr" target="#b26">[27]</ref> as <ref type="bibr" target="#b11">[12]</ref> for a better performance.</p><p>Comparison with State-of-the-arts. The classification results on ModelNet40 are shown in <ref type="table" target="#tab_2">Table 1</ref>, where the overall accuracy (OA) and class mean accuracy (mAcc) are compared. The upper part of the table illustrates the results of current state-of-the-art methods, in which we use PointNet++ <ref type="bibr" target="#b33">[34]</ref>, RS-CNN <ref type="bibr" target="#b26">[27]</ref> and DGCNN <ref type="bibr" target="#b42">[43]</ref> as our baselines. We do not use PointMLP <ref type="bibr" target="#b30">[31]</ref> as our baseline since it cannot robustly reproduce the highest results on ModelNet40, where the issue is mentioned in their open-sourced codes. For models trained from scratch, PointMLP <ref type="bibr" target="#b30">[31]</ref> achieves the highest accuracy. As shown in the lower part of the table, after training with PointCMT, the performance of all baselines is greatly boosted, i.e., 1.0% improvement upon PointNet++ and 0.9% for RS-CNN and 0.6% for DGCNN. We also compare our method to several open-sourced methods and report the parameters and testing speed. As shown in the last two columns of the table, though PointMLP gains 0.1 higher overall accuracy, its network consists of about 7.7? parameters and only achieves 46% speed of PointNet++. In contrast, PointCMT performs well on light-weighted models, which shows its great potential for real-time applications, e.g., scene parsing in autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Shape Classification on ScanObjectNN</head><p>Though ModelNet40 is the widely used benchmark for point cloud analysis, it may not meet the realistic requirement due to its synthetic nature. To this end, we also conduct experiments on the ScanObjectNN benchmark <ref type="bibr" target="#b40">[41]</ref>, which is a real-world dataset.</p><p>Dataset Description and Processing. ScanObjectNN collects 2,902 objects from real-world indoor scenes ScanNet <ref type="bibr" target="#b6">[7]</ref> and SceneNN <ref type="bibr" target="#b18">[19]</ref>, categorizing into 15 categories. Several variants are provided in the dataset, where the most challenging one is PB_T50_RS, i.e., introducing perturbation objects (11,416 and 2,882 data for training and test) via random translation, shift, rotation and scaling. Due to background, noise, and occlusions, this benchmark poses significant challenges to existing point cloud analysis methods. Furthermore, since the PB_T50_RS dataset only preserves the spatial coordinates (XYZ) for each object while the other information, such as RGB, is discarded, we also compared the original 2,902 objects (OBJ_ONLY), which includes additional RGB information. On both above datasets, we only use depth images through conducting perspective projection on raw point cloud as additional inputs, as shown in the last column in <ref type="figure" target="#fig_3">Figure 3</ref>. Section 4.6 will discuss more results using projection with additional color information.</p><p>Implementation. All view-images in ScanObjectNN are gained by the projection of raw point clouds, and we follow the structure of <ref type="bibr" target="#b11">[12]</ref> only generate six images for PB_T50_RS and OBJ_ONLY. We train the image network from scratch with batch size 32 and SGD optimizer for longer epochs of 1,000. The training strategy of CMPG is the same as ModelNet40. For both point cloud models trained from scratch and with PointCMT, we use SGD optimizer for 1,000 epochs with batch size 32.</p><p>Comparison with State-of-the-arts. The results are shown in <ref type="table" target="#tab_5">Table 2</ref>, where PointNet++ <ref type="bibr" target="#b33">[34]</ref> and current state-of-the-art PointMLP <ref type="bibr" target="#b30">[31]</ref> are chosen as our baselines. PointCMT significantly improves the performance on both class mean accuracy (mAcc) and the overall accuracy (OA), even on state-of-the-art methods. Specifically, although background, noise, and occlusions exist on PB_T50_RS dataset, PointCMT still improves the overall accuracy of PointNet++ by 3.9%. Moreover, PointCMT also achieves state-of-the-art results on OBJ_ONLY dataset. Note that there is no auxiliary information provided in images, and all view images are generated through the perspective projection of points coordinates. Nevertheless, PointCMT still dramatically increases the mAcc of PointMLP from 89.4% to 92.0% (+2.6%). We evaluate our approach under limited data scenarios in <ref type="table" target="#tab_6">Table 3</ref>. Here, we only sample a small amount of training data in each category on ModelNet40, and evaluate the entire testing data. Our PointCMT shows an even more significant gap when using a small subset of the training data, again compared to Point-Net++ which trained from scratch. Especially when facing only 2% and 10% of the training data, we achieve about 1.9% and 2.8% improvements, respectively. This result illustrates that PointCMT provides more vital guidance for point cloud models in the low data regime. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Efficient Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>The ablation results on three datasets are summarized in <ref type="table" target="#tab_7">Table 4</ref>, in which we use PointNet++ as our baselines. We first test the effectiveness of feature enhancement (FE) and classifier enhancement (CE) in PointCMT. The results demonstrate that only using FE already significantly boosts the performance on both datasets, i.e., increases the overall accuracy by 0.4% and 3.1% on ModelNet40 and ScanObjetNN. Only using classifier enhancement (CE) improves the accuracy by around 0.6% and 2.9%. Finally, it is surprising that when we use both FE and CE during the training phase, it achieves the best result of 94.4% and 83.3%, respectively. In this section, we compare results with different view-image generation strategies. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, multiple view-image types can be applied in our framework, and we compare the results in <ref type="table">Table 6</ref>. As illustrated in the table, images rendered from the CAD model improve more compared with only using projection since the former provides additional shade and texture information. In contrast, we find out that using additional colors in the OBJ_ONLY dataset cannot boost performance. The reason is that OBJ_ONLY dataset only contains 2,902 objects, and image networks are easier to overfit when using the color information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with Knowledge Distillation Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a point cloud cross-modal training strategy named PointCMT. By exploiting some sophisticated architecture and reasonable criteria function, our PointCMT can boost the performance significantly for point cloud analysis methods on several benchmarks, outperforming previous methods by a large margin. We believe that our work can be applied to a broader range of other scenarios in the future, such as 3D semantic segmentation and object detection. Meanwhile, our method provides an alternative solution to the comprehension of 3D scenes with severe texture details missing. It can improve performance through image priors and knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Theoretical Proof</head><p>We provide the detailed theoretical proof for the lemmas proposed in the main manuscript. Define the discrepancy g between the discriminative image and point cloud features as</p><formula xml:id="formula_11">g = P(F img * |I, y; ? T ) ? P(F pts * |P, y; ? S , K T ),<label>(8)</label></formula><p>Lemma 1: By the definition above, P(F KD |P, y; ? S , K T ) is bounded below by P(F KD |I, y;</p><formula xml:id="formula_12">? T ) + ? ? g, where ? is ? = P(F img * |I, y; ? T ) ? P(F KD |I, y; ? T ).<label>(9)</label></formula><p>Proof of Lemma 1:</p><formula xml:id="formula_13">P(F KD |P, y; ? S , K T ) =P(F KD |P, y; ? S , K T ) ? P(F pts * |P, y; ? S , K T ) +P(F pts * |P, y; ? S , K T ) ? P(F img * |I, y; ? T ) +P(F img * |I, y; ? T ) ? P(F KD |I, y; ? T ) + P(F KD |I, y; ? T ).<label>(10)</label></formula><p>For a successful distillation, the term P(F KD |P, y; ? S , K T ) ? P(F pts * |P, y; ? S , K T ) should be greater than or equal to 0, i.e., the distillated model outperforms the original point cloud network. By the definition, we have P(F KD |P, y; ? S , K T ) ? P(F KD |I, y; ? T ) + ? ? g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Comparison with Different Normalization</head><p>As we mentioned in the manuscript, the discrepancy between two modalities make the KD problem challenging, which is the initial motivation of our PointCMT. In this section, we analyze different strategies that are used to eliminate the discrepancy. Specifically, we design two normalization:</p><p>Normalize-I: We assume the features from image and point cloud networks follow two Gaussian distributions. We normalize the mean and standard deviation of the features from image network and make it closer to the features from point cloud network. For every batch of paired image and point cloud data, denote N as batch size, let {(F pts i , F img i )} N i=1 be feature pairs from point cloud and image networks. Let mean(F pts ), std(F pts ) be the mean and standard deviation of {F pts i } N i=1 , and mean(F img ) and std(F img ) be the mean and standard deviation of {F img i } N i=1 . We normalize image features through:</p><formula xml:id="formula_14">F img i = ((F img i ? mean(F img ))/std(F img )) * std(F pts ) + mean(F pts ).<label>(11)</label></formula><p>Normalize-II: We regard features as vectors in a feature space, and normalize their norms into the same scale. Let norm(F pts ) and norm(F img ) be the mean of</p><formula xml:id="formula_15">{||F pts i || 2 } n i=1 and {||F img i || 2 } n i=1</formula><p>, where || ? || 2 denote 2-norm, we normalize image features through: During the experiment, we train PointNet++ with above normalization through KD loss:</p><formula xml:id="formula_16">F img i = (F img i /norm(F img )) * norm(F pts ).<label>(12)</label></formula><formula xml:id="formula_17">L KD = M SE(F pts ? Norm(F img )),<label>(13)</label></formula><p>where Norm(?) denotes the normalization operations mentioned above. As shown in <ref type="table" target="#tab_9">Table 7</ref>, exploiting normalization strategies can slightly improve the performance on ModelNet40 and eliminate the negative transfer. Nevertheless, our PointCMT still achieves superior performance upon such naive normalization. In <ref type="table" target="#tab_10">Table 8</ref>, we illustrate the results on ModelNet40 without pre-trained image feature extractor. As shown in the table, when we train image feature extractor from scratch, PointCMT still gains 94.2% overall accuracy, i.e., with only 0.2% performance drop. Moreover, the results on ScanObjectNN are obtained without pre-trained image feature extractor, since the view-images used on this dataset are from perspective projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Effect of Pre-trained Image Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Performance of Image Networks</head><p>In this section, we demonstrate the performance of image networks pre-trained in the Stage I. As shown in <ref type="table" target="#tab_11">Table 9</ref>, through exploiting rendered view-image from CAD models, the image network can achieve very high overall accuracy of 97.0% on ModelNet40, boosting the PointNet++ via PointCMT by a 1% performance gain. Only using projection in the image network can only obtain 93.8% on ModelNet40. Nevertheless, it still increases the performance of PointNet++ by 0.6%. Moreover, we find out that in the cross-modal KD scenario, the performance of the teacher will not always better than the student. Still, PointCMT effectively learn the complementary information from the teacher, and improve the performance of point cloud analysis approaches. For the ScanObjectNN dataset, using additional color information makes image networks overfit on the OBJ_ONLY sub-set, and thus hampers the performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Training Speed</head><p>In this section, we demonstrate the training speed of our PointCMT. As shown in the <ref type="table" target="#tab_2">Table 10</ref>, the additional training stage of I (image encoder and image classifier) and II (CMPG) actually introduce little extra cost in the entire training phrase since the small epoch numbers for stage I and few parameters of CMPG. For the speed evaluation of the Stage III, since the image network has been trained, we fixed pre-trained network and generate objects' features offline, which can be directly exploited in the Stage II and III without repeatedly forwarding the image network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Part Segmentation</head><p>To illustrate the superiority of our PointCMT, we set two experiments for part segmentation on ShapeNetPart dataset, as shown in <ref type="table" target="#tab_2">Table 11</ref>. (a) PointNet++ with pre-trained encoder (trained from scratch on ModelNet40); (b) PointNet++ with pre-trained encoder (trained with PointCMT on ModelNet40). As shown in <ref type="table">Table,</ref> utilizing pre-trained encoder trained with PointCMT effectively improve the performance, especially for the more challenging metric of Class avg IoU. Here, Instance avg IoU and Class avg IoU denote the IoU averaged by all instances and each class, respectively. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Our proposed general Cross-Modal Training (PointCMT) strategy. It introduces priors from images into point cloud shape analysis models only in the training stage without any baseline model modification. (b) Classification accuracy (%) on ModelNet40 with or without training with our proposed PointCMT strategy. Noticeable improvements can be observed. a multi-modal network, which takes the extra image inputs in both training and inference phases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) The architecture of PointCMT. Multiple view images are gained through rendering the 3D CAD model or perspective projecting the raw point cloud, and the pre-trained image network distills the knowledge to the point cloud analysis network via two matching processes. The first is feature enhancement, which aligns features through a pre-trained cross-modal point generator through the process in (b). The second one is classifier enhancement, which aligns the output distribution of the point classifier by taking cross-modal features as inputs. (b) Training process of cross-modal point generator (CMPG). (c) Illustration of feature enhancement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Different view-image generation strategies used in our experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Process in point cloud cross-modal training (PointCMT) Data: The point clouds {Pi} M i=1 , corresponding ground-truth labels {yi} M i=1 and view-images set {Ii} M i=1 , where M is a number of training data samples. Stage I: Training image encoder and image classifier: Taking view-images set {Ii} M i=1 as input, image encoder produce the image features {F img i } M i=1 , which are then fed into image classifier and obtain prediction logits {Cls img</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and supervised by point clouds {Pi} M i=1 . Stage III: Image priors assisted training: The point encoder takes point clouds {Pi} M i=1 as input and generate point features {F pts i } M i=1 . The point classifier gains prediction logits {Cls pts (F pts )} M Feature Enhancement, and Classifier Enhancement enhance the point classifier via matching {Cls pts</figDesc><table><row><cell cols="2">i=1</cell></row><row><cell>through taking features {F pts i } M i=1 . The feature enhancement align cross-modal features {F img i {F pts i } M i=1 through</cell><cell>} M i=1 and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Classification results on ModelNet40 dataset. With only 1k points, PointNet++ trained with PointCMT achieves state-of-the-art results on both class mean accuracy (mAcc) and overall accuracy (OA) metrics. Here, 'pnt' and 'nor' denote points and normal vectors, respectively. The speed (samples/second) tested on one Tesla V100 GPU and four cores AMD EPYC 7351@2.60GHz CPUs, where ? denotes the results from the original paper. * For PointNet++, we train it with protocol of RS-CNN<ref type="bibr" target="#b26">[27]</ref> as mentioned in<ref type="bibr" target="#b11">[12]</ref>. The best and second best are marked in bold and underline.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell>#Points</cell><cell>mAcc(%)</cell><cell>OA(%)</cell><cell cols="2">Speed Param.</cell></row><row><cell>PointNet [33]</cell><cell>pnt</cell><cell>1k</cell><cell>86.0</cell><cell>89.2</cell><cell>-</cell><cell>3.47M</cell></row><row><cell>PointNet++ [34]</cell><cell>pnt, nor</cell><cell>5k</cell><cell>-</cell><cell>91.9</cell><cell>-</cell><cell>1.47M</cell></row><row><cell>PointCNN [25]</cell><cell>pnt</cell><cell>1k</cell><cell>88.0</cell><cell>92.5</cell><cell>-</cell><cell>-</cell></row><row><cell>PointConv [47]</cell><cell>pnt, nor</cell><cell>1k</cell><cell>-</cell><cell>92.5</cell><cell>80  ?</cell><cell>18.6M</cell></row><row><cell>KPConv [39]</cell><cell>pnt</cell><cell>7k</cell><cell>-</cell><cell>92.9</cell><cell>10  ?</cell><cell>15.2M</cell></row><row><cell>PointASNL [54]</cell><cell>pnt, nor</cell><cell>1k</cell><cell>-</cell><cell>93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>PosPool [29]</cell><cell>pnt</cell><cell>5k</cell><cell>-</cell><cell>93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>Point Transformer [60]</cell><cell>pnt</cell><cell>1k</cell><cell>90.6</cell><cell>93.7</cell><cell>-</cell><cell>-</cell></row><row><cell>GBNet [36]</cell><cell>pnt</cell><cell>1k</cell><cell>91.0</cell><cell>93.8</cell><cell>112  ?</cell><cell>8.4M</cell></row><row><cell>GDANet [51]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>93.8</cell><cell>14  ?</cell><cell>0.9M</cell></row><row><cell>SimpleView [12]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>93.9</cell><cell>2208</cell><cell>1.64M</cell></row><row><cell>CurveNet [49]</cell><cell>pnt</cell><cell>1k</cell><cell>-</cell><cell>94.2</cell><cell>15  ?</cell><cell>2.0M</cell></row><row><cell>PointMLP [31]</cell><cell>pnt</cell><cell>1k</cell><cell>91.4</cell><cell>94.5</cell><cell>139</cell><cell>12.6M</cell></row><row><cell>DGCNN [43] (baseline)</cell><cell>pnt</cell><cell>1k</cell><cell>90.2</cell><cell>92.9</cell><cell>518</cell><cell>1.68M</cell></row><row><cell>RS-CNN [27] (baseline)</cell><cell>pnt</cell><cell>1k</cell><cell>89.3</cell><cell>92.9</cell><cell>2174</cell><cell>1.17M</cell></row><row><cell>PointNet++ [34] (baseline)</cell><cell>pnt</cell><cell>1k</cell><cell>90.1</cell><cell>93.4*</cell><cell>300</cell><cell>1.62M</cell></row><row><cell>DGCNN w/ PointCMT</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">90.8 (+0.6) 93.5 (+0.6)</cell><cell>518</cell><cell>1.68M</cell></row><row><cell>RS-CNN w/ PointCMT</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">90.1 (+0.8) 93.8 (+0.9)</cell><cell>2174</cell><cell>1.17M</cell></row><row><cell>PointNet++ w/ PointCMT</cell><cell>pnt</cell><cell>1k</cell><cell cols="2">91.2 (+1.1) 94.4 (+1.0)</cell><cell>300</cell><cell>1.62M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Classification on ScanObjectNN. We examine all methods on original objects (OBJ_ONLY) and more challenging variant (PB_T50_RS). We train PointNet++ and PointMLP with protocol of<ref type="bibr" target="#b11">[12]</ref> for a fair comparison. The best and second best are marked in bold and underline. We train and test for four runs and report mean ? std results.</figDesc><table><row><cell></cell><cell cols="2">OBJ_ONLY</cell><cell cols="2">PB_T50_RS</cell></row><row><cell>Method</cell><cell>mAcc(%)</cell><cell>OA(%)</cell><cell>mAcc(%)</cell><cell>OA(%)</cell></row><row><cell>3DmFV [3]</cell><cell>-</cell><cell>73.8</cell><cell>58.1</cell><cell>63.0</cell></row><row><cell>PointNet [33]</cell><cell>-</cell><cell>79.2</cell><cell>63.4</cell><cell>68.2</cell></row><row><cell>SpiderCNN [52]</cell><cell>-</cell><cell>79.5</cell><cell>69.8</cell><cell>73.7</cell></row><row><cell>PointNet++ [34]</cell><cell>-</cell><cell>84.3</cell><cell>75.4</cell><cell>77.9</cell></row><row><cell>DGCNN [43]</cell><cell>-</cell><cell>86.2</cell><cell>73.6</cell><cell>78.1</cell></row><row><cell>PointCNN [25]</cell><cell>-</cell><cell>85.5</cell><cell>75.1</cell><cell>78.5</cell></row><row><cell>DRNet [35]</cell><cell>-</cell><cell>-</cell><cell>78.0</cell><cell>80.3</cell></row><row><cell>GBNet [36]</cell><cell>-</cell><cell>-</cell><cell>77.8</cell><cell>80.5</cell></row><row><cell>SimpleView [12]</cell><cell>86.2</cell><cell>89.0</cell><cell>-</cell><cell>80.8</cell></row><row><cell>PRANet [4]</cell><cell>-</cell><cell>-</cell><cell>79.1</cell><cell>82.1</cell></row><row><cell>MVTN [15]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.8</cell></row><row><cell>PointNet++ [34] (baseline)</cell><cell>85.4?0.2</cell><cell>87.4?0.1</cell><cell>75.5?0.3</cell><cell>79.2?0.2</cell></row><row><cell>PointMLP [31] (baseline)</cell><cell>89.1?0.3</cell><cell>92.2?0.3</cell><cell>83.9?0.5</cell><cell>85.4?0.3</cell></row><row><cell>PointNet++ w/ PointCMT</cell><cell cols="4">89.0?0.3 (+3.7) 91.6?0.2 (+4.3) 79.9?0.3 (+4.4) 83.1?0.2 (+3.9)</cell></row><row><cell>PointMLP w/ PointCMT</cell><cell cols="4">91.8?0.2 (+2.6) 93.2?0.3 (+1.0) 84.4?0.4 (+0.4) 86.4?0.3 (+1.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Data efficient learning on ModelNet40. We</cell></row><row><cell cols="3">train PointNet++ [34] with a small amount of training</cell></row><row><cell cols="2">data and train with PointCMT.</cell><cell></cell></row><row><cell cols="3">Data percentage Train from scratch w/ PointCMT</cell></row><row><cell>2%</cell><cell>73.3</cell><cell>75.2 (+1.9)</cell></row><row><cell>5%</cell><cell>82.1</cell><cell>83.5 (+1.4)</cell></row><row><cell>10%</cell><cell>85.1</cell><cell>87.9 (+2.8)</cell></row><row><cell>20%</cell><cell>88.4</cell><cell>89.3 (+0.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on ModelNet40 and ScanObjetNN datasets. Overall accuracy (%) as metrics.</figDesc><table><row><cell>Model</cell><cell cols="3">FE CE ModelNet40 OBJ_ONLY PB_T50_RS</cell></row><row><cell></cell><cell>93.4</cell><cell>87.5</cell><cell>79.4</cell></row><row><cell>PointNet++</cell><cell>93.8 (+0.4) 94.0 (+0.6)</cell><cell>89.2 (+1.7) 91.3 (+3.8)</cell><cell>82.5 (+3.1) 82.3 (+2.9)</cell></row><row><cell></cell><cell>94.4 (+1.0)</cell><cell>91.8 (+4.3)</cell><cell>83.3 (+3.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>To further verify the effectiveness of our</cell><cell cols="3">: Comparison with knowledge distillation meth-</cell></row><row><cell>proposed method compared with typi-</cell><cell cols="3">ods. We compare overall accuracy (OA,%) gained by</cell></row><row><cell>cal teach-student architecture and other</cell><cell cols="3">PointNet++ on ModelNet40 and ScanObjectNN.</cell></row><row><cell>knowledge distillation manners, we com-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>pare PointCMT with typical approaches of knowledge transfer in Table 5. Among all the methods, Hinton et al. [17] is</cell><cell>Method Baseline Hinton et al. [17]</cell><cell cols="2">ModelNet40 PB_T50_RS 93.4 79.4 93.1 (-0.3) 81.8 (+2.4)</cell></row><row><cell>a pioneer study for knowledge distilla-</cell><cell>Huang et al. [21]</cell><cell>93.6 (+0.2)</cell><cell>82.0 (+2.6)</cell></row><row><cell>tion, while Huang et al. [21] and Yang</cell><cell>Yang et al. [55]</cell><cell>93.9 (+0.5)</cell><cell>81.1 (+1.7)</cell></row><row><cell>et al. [55] are recent works. As shown</cell><cell>PointCMT (ours)</cell><cell>94.4 (+1.0)</cell><cell>83.3 (+3.9)</cell></row><row><cell>in the table, directly aligning features</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">as [17] between two modalities will cause a negative transfer on ModelNet40. This phenomenon</cell></row><row><cell cols="4">does not appear on ScanObjectNN, since view images projected via point clouds may have a smaller</cell></row><row><cell cols="4">gap than rendered images of CAD models. Nevertheless, other KD techniques only achieve marginal</cell></row><row><cell>improvement compared with PointCMT.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4.6 Different View-image Generation Table 6: Results through different view-image genera-</cell></row><row><cell></cell><cell cols="3">tion. We compare overall accuracy (OA,%) on Model-</cell></row><row><cell></cell><cell cols="3">Net40 and ScanObjectNN OBJ_ONLY datasets.</cell></row><row><cell></cell><cell>View-images</cell><cell cols="2">ModelNet40 OBJ_ONLY</cell></row><row><cell></cell><cell>Rendered from CAD</cell><cell>94.4</cell><cell>-</cell></row><row><cell></cell><cell>Projection</cell><cell>94.0</cell><cell>91.8</cell></row><row><cell></cell><cell>Projection w/ color</cell><cell>-</cell><cell>90.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">Comparison with different</cell></row><row><cell cols="2">normalization on ModelNet40.</cell></row><row><cell>Method</cell><cell>ModelNet40</cell></row><row><cell>Baseline</cell><cell>93.4</cell></row><row><cell>Hinton et al. [17]</cell><cell>93.1 (-0.3)</cell></row><row><cell>w/ Normalize-I</cell><cell>93.4 (+0.0)</cell></row><row><cell>w/ Normalize-II</cell><cell>93.5 (+0.1)</cell></row><row><cell>PointCMT (ours)</cell><cell>94.4 (+1.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Results of using a image network without pre-training on ModelNet40.</figDesc><table><row><cell>Method</cell><cell>ModelNet40</cell></row><row><cell>Baseline</cell><cell>93.4</cell></row><row><cell>PointCMT w/o pre-train</cell><cell>94.2 (+0.8)</cell></row><row><cell>PointCMT</cell><cell>94.4 (+1.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Results of image networks on ModelNet40 and ScanObjectNN datasets.</figDesc><table><row><cell>Image Networks</cell><cell cols="3">ModelNet40 OBJ_ONLY PB_T50_RS</cell></row><row><cell>Rendered from CAD</cell><cell>97.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Gains on PointNet++</cell><cell>(+1.0)</cell><cell>-</cell><cell>-</cell></row><row><cell>Projection</cell><cell>93.8</cell><cell>89.0</cell><cell>80.8</cell></row><row><cell>Gains on PointNet++</cell><cell>(+0.6)</cell><cell>(+4.3)</cell><cell>(+3.9)</cell></row><row><cell>Projection w/ color</cell><cell>-</cell><cell>87.5</cell><cell>-</cell></row><row><cell>Gains on PointNet++</cell><cell>-</cell><cell>(+3.2)</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>The cost of each stage with the form of time for per sample (ms) and total epochs (h).</figDesc><table><row><cell cols="3">Stage I (Image Network) Stage II (CMPG) Stage III (PointNet++)</cell></row><row><cell>27.35ms / 4.36h</cell><cell>2.30ms / 0.46h</cell><cell>10.64s / 36.38h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Results on ShapeNetPart with metrics of instance average IoU and class average IoU..</figDesc><table><row><cell>Method</cell><cell cols="2">Inctance avg IoU Class avg IoU</cell></row><row><cell>Pre-trained PointNet++ w/o PointCMT</cell><cell>85.3</cell><cell>82.0</cell></row><row><cell>Pre-trained PointNet++ w/ PointCMT</cell><cell>85.6 (+0.3)</cell><cell>82.6 (+0.6)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pra-net: Point relation-aware network for 3d point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4436" to="4448" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comprehensive knowledge distillation with causal intervention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rgb and lidar fusion based 3d semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>El Madawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazem</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><forename type="middle">El</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Yogamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognition</title>
		<meeting>IEEE Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revisiting point cloud shape classification with a simple and effective baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3809" to="3820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mvtn: Multi-view transformation network for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Hamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11225</idno>
		<title level="m">Pri3d: Can 3d priors help 2d representation learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scenenn: A scene meshes dataset with annotations</title>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<editor>Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation: An inheritance and exploration framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3579" to="3588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshifumi</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fuseseg: Lidar point cloud segmentation fusing multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Krispel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Simipu: Simple 2d image and 3d point cloud unsupervised pre-training for spatial-aware visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
	<note>Bin Fan, Shiming Xiang, and Chunhong Pan</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yueh</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Tang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04687</idno>
		<title level="m">Learning from 2d: Pixel-to-point knowledge transfer for 3d pretraining</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="326" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">3d-to-2d distillation for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking network design and local geometry in point cloud: A simple residual MLP framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Haoxi Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sensor fusion for joint 3d object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Charland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vallespi-Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidasj</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dense-resolution network for point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3813" to="3822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric back-projection network for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1588" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4604" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Walk in the cloud: Learning curves for point clouds shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="915" to="924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04180</idno>
		<title level="m">Image2point: 3d point-cloud understanding with pretrained 2d convnets</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning geometry-disentangled representation for complementary understanding of 3d object point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mutian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10921</idno>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">2dpass: 2d priors assisted semaantic segmentation on lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiantao</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoda</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Knowledge distillation via softmax regression representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning relationships for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7505" to="7514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sat: 2d semantics assisted training for 3d visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia, MM &apos;18</title>
		<meeting>ACM Int. Conf. Multimedia, MM &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">X-trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16259" to="16268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Rethinking soft labels for knowledge distillation: A bias-variance tradeoff perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
