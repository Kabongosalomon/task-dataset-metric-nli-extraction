<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale Pre-training for Person Re-identification with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Cloud AI</orgName>
								<address>
									<postCode>4 IDEA</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Cloud AI</orgName>
								<address>
									<postCode>4 IDEA</postCode>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@idea.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale Pre-training for Person Re-identification with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper aims to address the problem of pre-training for person re-identification (Re-ID) with noisy labels. To setup the pre-training task, we apply a simple online multiobject tracking system on raw videos of an existing unlabeled Re-ID dataset "LUPerson" and build the Noisy Labeled variant called "LUPerson-NL". Since theses ID labels automatically derived from tracklets inevitably contain noises, we develop a large-scale Pre-training framework utilizing Noisy Labels (PNL), which consists of three learning modules: supervised Re-ID learning, prototypebased contrastive learning, and label-guided contrastive learning. In principle, joint learning of these three modules not only clusters similar examples to one prototype, but also rectifies noisy labels based on the prototype assignment. We demonstrate that learning directly from raw videos is a promising alternative for pre-training, which utilizes spatial and temporal correlations as weak supervision. This simple pre-training task provides a scalable way to learn SOTA Re-ID representations from scratch on "LUPerson-NL" without bells and whistles. For example, by applying on the same supervised Re-ID method MGN, our pre-trained model improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%, 2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or few-shot setting, the performance gain is even more significant, suggesting a better transferability of the learned representation. Code is available at https: //github.com/DengpanFu/LUPerson-NL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A large high-quality labeled dataset for person reidentification (Re-ID) is labor intensive and costly to create. Existing fully labeled datasets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b60">61]</ref>   <ref type="figure">Figure 1</ref>. Comparing person Re-ID performances of three pretrained models on two methods (IDE <ref type="bibr" target="#b58">[59]</ref> and MGN <ref type="bibr" target="#b50">[51]</ref>). Results are reported on Market1501 and DukeMTC, with different scales under the small-scale setting. IN.sup. refers to the model supervised pre-trained on ImageNet, LUP.unsup. is the model unsupervised pre-trained on LUPserson, and LUPnl.pnl. is the model pre-trained on our LUPerson-NL dataset using our proposed PNL. comes a crucial approach to achieve good Re-ID performance. However, due to the lack of large-scale Re-ID dataset, most previous methods simply use the models pretrained on the crowd-labeled ImageNet dataset, resulting in a limited improvement because of the big domain gap between generic images in ImageNet and person-focused images desired by the Re-ID task. To mitigate this problem, the recent work <ref type="bibr" target="#b11">[12]</ref> has demonstrated that unsupervised pre-training on a web-scale unlabeled Re-ID image dataset "LUPerson" (sub-sampled from massive streeview videos) surpasses that of pre-training on ImageNet. In this paper, our hypothesis is that scalable ReID pretraining methods that learn directly from raw videos can generate better representations. To verify it, we propose the noisy labels guided person Re-ID pre-training, which leverages the spatial and temporal correlations in videos as weak supervision. This supervision is nearly cost-free, and can be achieved by the tracklets of a person over time derived from any multi-object tracking algorithm, such as <ref type="bibr" target="#b55">[56]</ref>. In particular, we track each person in consecutive video frames, and automatically assign the tracked persons in the same tracklet to the same Re-ID label and vice versa. Enabled by the large amounts of raw videos in LUPerson <ref type="bibr" target="#b11">[12]</ref>, publicly available data of this form on the internet, we create a new variant named "LUPerson-NL" with derived pseudo Re-ID labels from tracklets for pre-training with noisy labels. This variant totally consists of 10M person images from 21K scenes with noisy labels of about 430K identities.</p><p>We demonstrate that contrastive pre-training of Re-ID is an effective method of learning from this weak supervision at large scale. This new Pre-training framework utilizing Noisy Labels (PNL) composes three learning modules: (1) a simple supervised learning module directly learns from Re-ID labels through classification; (2) a prototype-based contrastive learning module helps cluster instances to the prototype which is dynamically updated by moving averaging the centroids of instance features, and progressively rectify the noisy labels based on the prototype assignment. and (3) a label-guided contrastive learning module utilizes the rectified labels subsequently as the guidance. In contrast to the vanilla momentum contrastive learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> that treats only features from the same instance as positive samples, our label-guided contrastive learning uses the rectified labels to distinguish positive and negative samples accordingly, leading to a better performance. In principle, joint learning of these three modules make the consistency between the prototype assignment from instances and the high confident (rectified) labels, as possible as it can.</p><p>The experiments show that our PNL model achieves remarkable improvements on various person Re-ID benchmarks. <ref type="figure">Figure 1</ref> indicates that the performance gain from our pre-trained models is consistent on different scales of training data. For example, upon the strong MGN <ref type="bibr" target="#b50">[51]</ref> baseline, our pre-trained model improves the mAP by 4.4%, 4.9% on Market1501 and DukeMTMC over the Im-ageNet supervised one, and 0.9%, 2.2% over the unsupervised pre-training baseline <ref type="bibr" target="#b11">[12]</ref>. Moreover, the gains are even larger under the small-scale and few-shot settings, where the labeled Re-ID data are extremely limited. To the best of our knowledge, we are the first to show that largescale noisy label guided pre-training can significantly benefit person Re-ID task.</p><p>Our key contributions can be summarized as follows:</p><p>? We propose noisy label guided pre-training for person Re-ID, which incorporates supervised learning, prototypebased contrastive learning, label-guided contrastive learn-ing and noisy label rectification to a unified framework. ? We construct a large-scale noisy labeled person Re-ID dataset "LUPerson-NL" as a new variant of "LUPerson". It is by far the largest noisy labeled person Re-ID dataset without any human labeling effort. ? Our models pre-trained on LUPerson-NL push the stateof-the-art results on various public benchmarks to a new limit without bells and whistles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised Person Re-ID. Most studies of person Re-ID employ supervised learning. Some <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b54">55]</ref> introduce a hard triplet loss on the global feature, ensuring a closer feature distance for the same identity, while some <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref> impose classification loss to learn a global feature from the whole image. There are also some other works that learn part-based local features with separate classification losses. For example, Suh et al. <ref type="bibr" target="#b45">[46]</ref> presented part-aligned bi-linear representations and Sun et al. <ref type="bibr" target="#b47">[48]</ref> represented features as horizontal strips. Recent approaches investigate learning invariant features concerning views <ref type="bibr" target="#b33">[34]</ref>, resolutions <ref type="bibr" target="#b30">[31]</ref>, poses <ref type="bibr" target="#b31">[32]</ref>, domains <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, or exploiting groupwise losses <ref type="bibr" target="#b35">[36]</ref> or temporal information <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref> to improve performance. The more advantageous results on public benchmarks are achieved by MGN <ref type="bibr" target="#b50">[51]</ref>, which learns both global and local features with multiple losses. In <ref type="bibr" target="#b39">[40]</ref>, Qian et al further demonstrated the potential of generating cross-view images for person re-indentification conditioned on normalized poses. In this paper, we focus on model pretraining, and our pre-trained models can be applied to these representative methods and boost their performance.</p><p>Unsupervised Person Re-ID. To alleviate the lack of precise annotations, some works resort to unsupervised training on unlabeled datasets. For example, MMCL <ref type="bibr" target="#b48">[49]</ref> formulates unsupervised person Re-ID as a multi-label classification to progressively seek true labels. BUC <ref type="bibr" target="#b32">[33]</ref> jointly optimizes the network and the sample relationship with a bottom-up hierarchical clustering. MMT <ref type="bibr" target="#b13">[14]</ref> collaboratively trains two networks to refine both hard and soft pseudo labels. SpCL <ref type="bibr" target="#b14">[15]</ref> designs a hybrid memory to unify the representations for clustering and instance-wise contrastive learning. Both MMT <ref type="bibr" target="#b13">[14]</ref> and SpCL <ref type="bibr" target="#b14">[15]</ref> rely on explicit clustering of features from the whole training set, making them quite inefficient on large datasets like MSMT17. Since the appearance ambiguity is difficult to address without supervision, these unsupervised methods have limited performance. One alternative to address this issue is introducing model pre-training on large scale data. Inspired by the success of self-supervised representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b52">53]</ref>, Fu et al. <ref type="bibr" target="#b11">[12]</ref> proposed a large scale unlabeled Re-ID dataset, LUPerson, and illustrated the effectiveness of its unsupervised pre-trained models. In this work, we further try to make use of noisy labels from video tracklets to improve the pre-training quality through large-scale weakly-supervised pre-training.</p><p>Weakly Supervised Person Re-ID. Several approaches also employ weak supervision in person Re-ID training. Instead of requiring bounding boxes within each frame, Meng et al. <ref type="bibr" target="#b37">[38]</ref> rely on precise video-level labels, which reduces annotation cost but still need manual efforts to label videos. On the contrary, we resort to noisy labels that can be automatically generated from tracklets on a much larger scale. Some <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">50]</ref> also leverage tracklets to supervise the training of Re-ID tasks. But unlike these approaches, we are proposing a large-scale pre-training strategy for person Re-ID, by both building a new very largescale dataset and devising a new pre-training framework: the new dataset, LUPerson-NL, is even larger than LU-Person <ref type="bibr" target="#b11">[12]</ref> and has large amount of noisy Re-ID labels;</p><p>The new framework, PNL, combines supervised learning, label-guided contrastive learning and prototype based contrastive learning to exploit the knowledge under large-scale noise labels. Most importantly, our pre-trained models have demonstrated remarkable performance and generalization ability, helping achieve state-of-the-art results superior to all existing methods on public person Re-ID benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LUPerson-NL: LUPerson With Noisy Labels</head><p>Supervised models based on deep networks are always data-hungry, but the labeled data they rely on are expensive to acquire. It is a tremendous issue for person Re-ID task, since the human labelers need to check across multiple views to ensure the correctness of Re-ID labels. The data shortage is partially alleviated by a recently published dataset, LUPerson <ref type="bibr" target="#b11">[12]</ref>, a dataset of unlabeled person images with a significantly larger scale than previous person Re-ID datasets. Unsupervised pre-trained models <ref type="bibr" target="#b11">[12]</ref> on LUPerson have demonstrated remarkable effectiveness without utilizing additional manual annotations, which arouses our curiosity: can we further improve the performance of pre-training directly by utilizing temporal correlation as weak supervision? To verify this, we build a new variant of LUPerson on top of the raw videos from LUPerson and assign label to each person image with automatically generated tracklet. We name it LUPerson-NL with NL standing for Noisy Labels. It consists of 10M images with about 430K identities collected from 21K scenes. To our best knowledge, this is the largest person Re-ID dataset constructed without human labelling by far. Our LUPerson-NL will be released for scientific research only, while any usage for other purpose is forbidden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Constructing LUPerson-NL</head><p>We utilize the off-the-shelf tracking algorithm <ref type="bibr" target="#b55">[56]</ref>    raw videos of <ref type="bibr" target="#b11">[12]</ref>. We assign each tracklet with a unique class label. The detection is not perfect: e.g. the bounding boxes may only cover partial bodies without heads or upper parts. Human pose estimation <ref type="bibr" target="#b46">[47]</ref> is thus appended that helps filter out imperfect boxes by predicting landmarks. We track every person in the video frame by frame. In order to guarantee both the sufficiency and diversity, we adopt the following strategy: i) We first remove the person identities that appear in too few frames, i.e. no more than 200; ii) Within the tracklet of each identity, we then perform sampling with a rate of one image per 20 frames to reduce the number of duplicated images. Thus we can make sure that there would be at least 10 images associating to each identity. Through this filtering procedure, we have collected 10, 683, 716 images of 433, 997 identities in total. They belong to 21, 697 videos which are less than the videos that <ref type="bibr" target="#b11">[12]</ref> uses, due to our extra filtering strategy for more reliable identity labels. Thus, LUPerson-NL is very different from LUPerson, as it adopts very different sampling and post-processing strategies, not to mention the noisy labels driven from the spatial-temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Properties of LUPerson-NL</head><p>LUPerson-NL is advantageous in following aspects: Large amount of images and identities. We detail the statistics of existing popular person Re-ID datasets in <ref type="table" target="#tab_0">Table   Datasets   #images  #scene  #persons  labeled environment camera view  detector  crop size  VIPeR [16</ref> 1. As we can see, the proposed LUPerson-NL, with over 10M images and 433K noisy labeled identities, is the second largest among the listed. Indeed, SYSU30K has more images, but it extracts images from only 1K TV program videos frame by frame, making it less competitive in variability and less compatible in practice, the pre-training performance comparison can be found at supplementary materials. Besides, LUPerson-NL was constructed without human labeling effort, making it more suitable to scale-up. Balanced distribution of identities. We illustrate the cumulative percentage of identities with respect to the number of their corresponding person images as a curve in <ref type="figure" target="#fig_2">Figure 2</ref>. A point (X, Y ) on the curve represents that there are in total Y % identities in LUPerson-NL, that each of them has less than X images. It can be observed that: i) about 75% of all the identities in LUPerson-NL have a person image number within <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>; ii) the percentage of identities that have more than 50 person images each, occupy only a very small portion of about 6.4% <ref type="bibr">(27, 767/433, 997)</ref> in LUPerson-NL. These observations all show that our LUPerson-NL is well balanced in terms of identity distribution, making it a suitable dataset for person Re-ID tasks.</p><p>In spite of our dedicatedly designed tracking and filtering strategies as proposed in Sec 3.1, the identity labels we obtained can never be very accurate due to the technical upper bounds of current tracking methods. <ref type="figure" target="#fig_3">Figure 3</ref> visualizes the two noise types in LUPerson-NL that are caused by different labeling errors, which are Noise-I, where the same person is split into different tracklets and is mistaken as different persons; and Noise-II, that different persons are recognized as the same person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PNL: Pre-training with Noisy Labels for Person Re-ID</head><p>Based on the new LUPerson-NL dataset with large scale noisy labels, we devise a novel Pretraining framework with Noisy Labels for person Re-ID, namely PNL.</p><p>Denote all the data samples from LUPerson-NL as <ref type="figure">Figure 4</ref>. The overview of our PNL framework. It comprises a supervised classification module, a prototype based contrastive learning module, and a label-guided contrastive learning module.</p><formula xml:id="formula_0">fc ! " # ! # ! " # $ ! &amp; ! !! , !" , ? , !# $ !! , $ !" , ? , $ !# &amp; ! prototypes queue ! " , # , ? , $ Label Guided Contrastive Loss Classification Loss ! Label Update &amp; ! Prototype Based Contrastive Loss ! update prototypes enqueue momentum update w. gradient w/o. gradient !</formula><formula xml:id="formula_1">{(x i , y i )} n i=1</formula><p>, with n being the size of the dataset, x i a person image and y i ? {1, . . . , K} its associated identity label. Here K represents the number of all identities that are recorded in LUPerson-NL.</p><p>Inspired by recent methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>, our PNL framework adopts Siamese networks that have been fully investigated for contrastive representation learning. As shown by <ref type="figure">Figure 4</ref>, given an input person image x i , we first perform two randomly selected augmentations (T, T ? ), producing two augmented images (x i ,x ? i ). We feed one of them,x i , into an encoder E q to get a query feature q i ; while the other one,x ? i , is fed into another encoder E k to get a key feature k i . Following <ref type="bibr" target="#b18">[19]</ref>, we design E k to be a momentum version of E q , i.e. the two encoders E k and E q share the same network structure, but with different weights. The weights in E k are exponential moving averages of the weights in E q . During training, weights of E k are refreshed through a momentum update from E q . And the detailed algorithm can be found at supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Supervised Classification</head><p>Since the raw labels {y i } n i=1 in LUPerson-NL contain lots of noises as illustrated in previous section, they have to be rectified during training. Let? i be the rectified label of image x i . As long as? i is given, it would be intuitive that we train classification based on the corrected label? i . In particular, we would append a classifier to transform the feature from E q into probabilities p i ? R K with K being the number of classes. Then we impose a classification loss</p><formula xml:id="formula_2">L i ce = ? log(p i [? i ]).<label>(1)</label></formula><p>However, the acquisition of? i is not straight-forward. We resort to prototypes, the moving averaged centroids of features from training instances, to accomplish this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Label Rectification with Prototypes</head><p>As depicted by <ref type="figure">Figure 4</ref>, we maintain prototypes as a dictionary of feature vectors {c 1 , c 2 , . . . , c K }, where K is the number of identities, c k ? R d is a prototype representing a class-wise feature centroid. In each training step, we would first evaluate the similarity score s k i between the query feature q i and each of the current prototypes c k by</p><formula xml:id="formula_3">s k i = exp(q i ? c k /? ) K k=1 exp(q i ? c k /? ) .<label>(2)</label></formula><p>Let p i be the classification probability given by the classifier with weights updated in the previous step. The rectified label? i for this step is then generated by combining both the prototype scores s i = {s k i } K k=1 and the classification probability p i as</p><formula xml:id="formula_4">l i = 1 2 (p i + s i ), y i = arg max j l j i if max j l j i &gt; T , y i otherwise.<label>(3)</label></formula><p>Here we compute a soft pseudo label l i and convert it to a hard one? i based on a threshold T . If the highest score in l i is larger than T , the corresponding class would be selected as? i , otherwise the original raw label y i would be kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Prototype Based Contrastive Learning</head><p>The newly rectified label? i can then be used to supervise the cross-entropy loss L i ce for classification as formulated by Equation 1. Besides, it also helps train prototypes c k in return. In specific, we propose a prototype based contrastive loss L i pro to constrain that the feature of each sample should be closer to the prototype it belongs to. We formulate the loss as</p><formula xml:id="formula_5">L i pro = ?log exp(q i ? c? i /? ) K j=1 exp(q i ? c j /? ) ,<label>(4)</label></formula><p>with q i being the query feature from E q , ? being a hyperparameter representing temperature.</p><p>All the prototypes are maintained as a dictionary, with step-wise updates following a momentum mechanism as</p><formula xml:id="formula_6">c? i = mc? i + (1 ? m)q i .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Label-Guided Contrastive Learning</head><p>Instance-wise contrastive learning proved to be very effective in self-supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. It learns instance-level feature discrimination by encouraging similarity among features from the same instance, while promoting dissimilarity between features from different instances. The instance-wise contrastive loss is given by</p><formula xml:id="formula_7">L i ic = ?log exp(q i ? k + i /? ) exp(q i ? k + i /? ) + M j=1 exp(q i ? k ? j /? ) ,<label>(6)</label></formula><p>with q i being the query feature of current instance i. k + i (= k i ) is the positive key feature generated from the momentum encoder E k . It is marked positive since it shares the same instance with q i . k ? * ? R d , on the contrary, are the rest features stored in a queue that represent negative samples. The queue has a size of M . At the end of each training step, the queue would be updated by en-queuing the new key feature and de-queuing the oldest one.</p><p>Such instance-level contrastive learning is far from perfect, as it neglects the relationships among different instances. For example, even though two instances depict the same person, it would still strengthen the gap between their features. Instead, we propose a label guided contrastive learning module, making use of the rectified labels? i to ensure more reasonable grouping of contrastive pairs.</p><p>We redesign the queue to additionally record labels? i . Represented by Q = [(k jt ,? jt )] M t=1 , our new queue accepts not only a key feature k i but also its rectified label? i during update. These newly recorded labels help better distinguish positive and negative pairs. Let P(i) be the new set of positive features and N (i) the new set of negative features: features in P(i) share the same rectified label with the current instance i while features in N (i) do not. Our label guided contrastive loss can be given by</p><formula xml:id="formula_8">L i lgc = ?1 |P(i)| log k + ?P(i) exp qi?k + ? k + ?P(i) exp qi?k + ? + k ? ?N (i) exp qi?k ? ? ,<label>(7)</label></formula><p>with</p><formula xml:id="formula_9">P(i) = {k jt |? jt =? i , ?(k jt ,? jt ) ? Q} ? {k i }, N (i) = {k jt |? jt ? =? i , ?(k jt ,? jt ) ? Q},<label>(8)</label></formula><p>where k i and? i are the key feature and the rectified label of the current instance i. Finally we combine all the components above to pretrain models on LUPerson-NL with the following loss</p><formula xml:id="formula_10">L i = L i ce + ? pro L i pro + ? lgc L i lgc .<label>(9)</label></formula><p>We set ? pro = ? lgc = 1 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation</head><p>Hyper-parameter settings. We set the hyper-parameters ? = 0.1 and T = 0.8. The momentum m for updating both the momentum encoder E k and the prototypes is set to 0.999. More hyper-parameters exploration and training details can be found at supplementary materials. Dataset and protocol. We conduct extensive experiments on four popular person Re-ID datasets: CUHK03, Market, DukeMTMC and MSMT17. We adopt their official settings, except CUHK03 where its labeled counterpart with new protocols proposed in <ref type="bibr" target="#b61">[62]</ref> is used. We follow the standard evaluation metrics: the mean Average Precision (mAP) and the Cumulated Matching Characteristics top-1 (cmc1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Improving Supervised Re-ID</head><p>To evaluate our pre-trained model based on LUPerson-NL with respect to supervised person Re-ID tasks. we conduct experiments using three representative supervised Re-ID baselines with different pre-training models. These baseline methods include two simpler approaches driven only by the triplet loss (Trip <ref type="bibr" target="#b20">[21]</ref>) or the classification loss (IDE <ref type="bibr" target="#b58">[59]</ref>), as well as a stronger and more complex method MGN <ref type="bibr" target="#b50">[51]</ref> that use both triplet and classification losses.</p><p>We report results in <ref type="table" target="#tab_2">Table 2</ref>, where the abbreviations {"IN", "LUP", "LUPnl"} represent ImageNet <ref type="bibr" target="#b42">[43]</ref>, LU-Person <ref type="bibr" target="#b11">[12]</ref> and our LUPerson-NL respectively; while the {"sup.", "unsup.", "pnl."} stand for the {"supervised", "unsupervised", and "pretrain with noisy label"} pre-training methods. e.g. the "LUPnl pnl." in the bottom rows of <ref type="table" target="#tab_2">Table 2</ref> all refer to our model, which is pre-trained on our LUPerson-NL dataset using our PNL framework.</p><p>From <ref type="table" target="#tab_2">Table 2</ref> we can see, for all of the three baseline methods, our pre-trained model improves their performances greatly on the four popular person Re-ID datasets. Specifically, the improvements are at least 5.7%, 0.9%, 1.2% and 2.3% in terms of mAP on CUHK03, Market1501, DukeMTMC and MSMT17 respectively.</p><p>Note that even though the performance of the baseline MGN on Market1501 has been extremely high, our model still brings considerable improvement over it. The other way around, our pre-trained models obtain more significant improvements on relatively weak methods (Trip and IDE), unveiling that model initialization plays a critical part in person Re-ID training.</p><p>Our noisy label guided pre-training models are also significantly advantageous over the previous "LUPerson unsup" models, which emphasizes the superiority of our PNL framework and our LUPerson-NL dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Improving Unsupervised Re-ID Methods</head><p>Our pre-trained model can also benefit unsupervised person Re-ID methods. Based on the state-of-the-art unsupervised method SpCL <ref type="bibr" target="#b14">[15]</ref>, we explore different pre-training models utilizing two settings proposed by SpCL: the pure unsupervised learning (USL) and the unsupervised domain adaptation (UDA). Results in <ref type="table">Table 4</ref> illustrate that our pretrained model outperforms the others in all UDA tasks, as well as the USL task on DukeMTMC dataset. In the USL task on Market1501, we achieve the second best scores slightly lower than the LUPerson model <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison on Small-scale and Few-shot</head><p>Following the same protocols proposed by <ref type="bibr" target="#b11">[12]</ref>, we conduct experiments under two small data settings: the smallscale setting and the few-shot setting. The small-scale setting restricts the percentage of usable identities, while the few-shot setting restricts the percentage of usable person images each identity has. Under both settings, we vary the usable data percentages of three popular datasets from 10% ? 100%. We compare different pre-trained models under these settings with MGN as the baseline method. The results shown in <ref type="table" target="#tab_3">Table 3</ref> verify the consistent improvements brought by our model on all the datasets under both settings.</p><p>Besides, the results in <ref type="table" target="#tab_3">Table 3</ref> show that the gains of our pre-trained models are even larger under a more limited amount of labeled data. For example, under the "smallscale" setting, our model outperforms "LUPerson unsup" by 7.8%, 7.1% and 2.7% on Market1501, DukeMTMC and MSMT17 respectively with 10% identities. The improvements rise to 15.6%, 16.4% and 6.5% under the "few-shot" setting with 10% person images.</p><p>Most importantly, our pre-trained "LUPnl pnl" model helps achieve advantageous results with a mAP of 72.4 and a cmc1 of 88.8, using only 10% labeled data from the Mar-ket1501 training set. The task is really challenging, considering that the training set composes only 1, 170 images belonging to 75 identities; while evaluations are performed on a much larger testing set with 19, 281 images belonging to 750 identities. We consider these results extremely appealing as they demonstrate the strong potential of our pre-trained models in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with other pre-training methods</head><p>We compare our proposed PNL with some other popular pre-training methods in <ref type="table">Table 5</ref>. LUP <ref type="bibr" target="#b11">[12]</ref> is a varient pre-train Trip <ref type="bibr" target="#b20">[21]</ref> IDE <ref type="bibr">[</ref>    <ref type="table">Table 6</ref>. Ablating components of PNL on MSMT with data percentages 20%, 40% and 100% under the small scale setting. ce: supervised classification; ic: instance-wise contrastive learning; pro: prototypes for both prototype-based contrastive learning and label rectification; lgc: label-guided contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study</head><p>We also investigate the effectiveness of each designed component in PNL through ablation experiments. Results shown by <ref type="table">Table 6</ref> illustrate the efficacy of our proposed components. We have the following observations: i) Training with an instance-wise contrastive loss L i ic (row 2) without using any labels leads to even better performance than training with a classification loss L i ce (row 1) that utilizes the labels from LUPerson-NL, implying that the noisy labels in LUPerson-NL would misguide representation learning if directly adopted as supervision. ii) Jointly training with both losses L i ce and L i ic (row 3) improves over using only one loss (row 1, row 2), suggesting that learning instance-wise discriminative representations complements label supervision. iii) The prototypes which contribute to both prototype-based contrastive learning and the label correction, are very important under various settings, as verified by comparing row 1 with row 4; row 3 with row 6; and row 5 with row 7. iv) Our label-guided contrastive learning component consistently outperforms the vanilla instancewise contrastive learning under various settings, as verified by comparing row 3 with row 5, and row 6 with row 7. v) Combining all our components (supervised classification, prototypes and label-guided contrastive learning) together leads to the best performance as shown by row 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Label Correction</head><p>Our PNL can indeed correct noisy labels. We demonstrate two typical examples in <ref type="figure">Figure 5</ref> visualizing the label corrections with respect to the two kinds of noises. As we can see, in <ref type="figure">Figure 5a</ref> the same person are marked as three different persons in LUPerson-NL due to Noise-I in labels. After our PNL pre-training, these three tracklets are merged together since their trained features are very close, as verified by the right-hand similarity matrix. In training, these mis-labeled person identities are all correctly re-grouped into two identities, which can also be reflected by the right-hand similarity matrix. we also ablate the label correction module in <ref type="table">Table 7</ref> with different settings, and observe it can improve the performance. It also validates the importance of combining label rectification with label-guided contrastive learning together, where more accurate positive/negative pairs can be leveraged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Comparison with State-of-the-Art Methods</head><p>We compare our results with current state-of-the-art methods on four public benchmarks. We don't apply any post-processing techniques such as IIA <ref type="bibr" target="#b12">[13]</ref> and RR <ref type="bibr" target="#b61">[62]</ref>. To ensure fairness, we adopt ResNet50 as our backbone and does not compare with methods that rely on stronger backbones (results with stronger backbones e.g. ResNet101 can be found at supplementary materials). Results in <ref type="table">Table 8</ref> verify the remarkable advantage brought by our pre-trained models. Without bells and whistles, we achieve state-ofthe-art performance on all four benchmarks, outperforming the second with clear margins. We show best scores in bold and the second scores underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we demonstrate that large-scale Re-ID representation can be directly learned from massive raw videos by leveraging the spatial and temporal information. We not only build a large-scale noisy labeled person Re-ID dataset LUPerson-NL based on tracklets of raw videos from LUPerson without using manual annotations, but also design a novel weakly supervised pretraining framework PNL comprising different learning modules including supervised learning, prototypes-based learning, label-guided contrastive learning and label rectification. Equipped with our pre-trained models, we push existing benchmark results to a new limit, which outperforms unsupervised pre-trained models and ImageNet supervised pre-trained models by a large margin.</p><p>In this material, we will 1) show demo cases for scene changing for specific person in LUPerson-NL, 2) share the training details for PNL, 3) provide the detailed algorithm table for PNL, 4) compare models pre-trained on SYSU30K and LUPerson-NL 5) analyze the hyper parameters of our PNL, 6) demonstrate results of our method using a stronger backbones, 7) explore the impact of pre-training image scale, and 8) list more detailed results for our "small-scale" and "few-shot" experiments. All these experiments are under MGN settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Scene changing in LUPerson-NL</head><p>Our LUPerson-NL are driven from street view videos, <ref type="figure" target="#fig_5">Figure 6</ref> shows a demo person in our LUPerson-NL. As we can see, our LUPerson-NL is able to cover multiple scenes, since the videos we use have lots of moving cameras and moving persons, and only traklets with ? 200 frames are selected. It is approximate close to the real person Re-ID scenario. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training details</head><p>During training, all images are resized to 256 ? 128 and pass through the same augmentations verified in <ref type="bibr" target="#b11">[12]</ref>, which are Random Resized Crop, Horizontal Flip, Normalization, Random Gaussian Blur, Random Gray Scale and Random Erasing.</p><p>Specifically, the images are normalized with mean and std of [0.3452, 0.3070, 0.3114], [0.2633, 0.2500, 0.2480], which are calculated from all images in LUPerson-NL. We train our model on 8 ? V 100 GPUs for 90 epochs with a batch size of 1, 536. The initial learning rate is set to 0.4 with a step-wise decay by 0.1 for every 40 epochs. The optimizer is SGD with a momentum of 0.9 and a weight decay of 0.0001. We set the hyper-parameters ? = 0.1 and T = 0.8. The momentum m for updating both the momentum encoder E k and the prototypes is set to 0.999. We design a large queue with a size of 65, 536 to increase the occurrence of positive samples for the label guided contrastive learning. The label correction according to Equation 3 starts from the 10-th epoch. The deployment of the label guided contrastive loss L i lgc starts from the 15-th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithm for PNL</head><p>Algorithm 1 shows the procedure of training PNL, we train our framework for 90 epochs, and apply label correction from 10 epochs. Once the rectification begins, we keep rectifying for every iteration.</p><p>Algorithm 1: PNL algorithm.</p><p>Input: total epochs N , correction start epochs N s , prototype feature vectors {? c 1 , ? c 2 , . . . , ? c K }, where K is the number of identities, temperature ? , threshold T , momentum m, encoder network E q (?), classifier h(?), momentum encoder E k (?), loss weight ? pro and ? lgc .</p><formula xml:id="formula_11">for epoch = 1 : N do {(? x i , y i )} b i=1 sampled from data loader. for i ? {1, ..., b} d? ? x i = aug 1 (? x i ),? x ? i = aug 2 (? x i ) ? q i = E q (? x i ), ? k i = E k (? x ? i ) ? p i = h(? q i ) ? s i = {s k i } K k=1 , s k i = exp( ? qi?? c k /? ) K k=1 exp(? qi?? c k /? ) ? l i = (? p i + ? s i )/2 if epoch &gt; N s and max k l j i &gt; T then y i = arg max j ? l j i else? i = ? y i L i ce = ? log(? p i [? i ]) L i pro = ?log exp(? qi?? c? i /? ) K j=1 exp(? qi?? cj /? ) L i lgc = ?1 |P(i)| log X exp ? q i ? ? k + ? ? k + ?P(i) exp ? q i ? ? k + ? + ? k ? ?N (i) exp ? q i ? ? k ? ? ? c? i = m? c? i + (1 ? m)? q i L i = L i ce + ? pro L i pro + ? lgc L i lgc update networks E q , h to minimize L i update networks E q , h to minimize L i update momentum encoder E k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Compare with SYSU30K</head><p>As show in <ref type="table">Table 9</ref>, we compare the pre-trained models between LUPerson-NL and SYSU30K. We pre-train PNL on both LUPerson-NL and SYSU30K for 40 epochs with same experiment settings for a fair comparison. The performance of LUPerson-NL pre-training is much better than SYSU30K pre-training, showing the superiority of our LUPerson-NL, and also suggesting that a large number of images with limited diversity does not bring more representative representation, but large-scale images with diversity does. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Hyper Parameter Analysis</head><p>In our PNL, there are two key hyper-parameters: temperature factor ? and the threshold for correction T . Here, we provide the analysis of these two parameters. <ref type="table" target="#tab_0">Table 10</ref> shows the performance comparison with different ? values with a fixed label correction threshold T = 0.8. As we can see, the setting ? = 0.1 achieves the best results on both DukeMTMC and MSMT17, while ? = 0.07 achieves the second best. When we use a larger ? , the performance drops rapidly. It may be because Re-ID is a more fine-grained task, larger ? will cause smaller interclass variations and make positive samples too close to negative samples. In all the experiments, we set ? = 0.1. <ref type="table" target="#tab_0">Table 11</ref> shows the results with different label correction threshold values T . As we can see, the performance is relatively stable to different T values varying in a large range of 0.6 ? 0.8. However, if T is too small or too large, the performance drops rapidly. For the former case, labels are easier to be modified, which may cause wrong rectifications, while for the latter case, the label noises become harder to be corrected, which also has consistently negative effects on the performance. In all the experiments, we set T = 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Temperature Factor ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Threshold T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results for stronger backbones</head><p>We train our PNL using two stronger backbones ResNet101 and ResNet152, and report the results in Table 12. As we can see, the stronger ResNet bring more superior performances. These results also outperform the  <ref type="table">Table 8</ref> of our main submission. Most importantly, we are the FIRST to obtain a mAP score on MSMT17 that is larger than 70 without any post-processing for convolutional network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Pre-training data scales</head><p>We study the impact of pre-training data scale. Specifically, we involve various percentages (10%,30%,100% pseudo based) of LUPerson-NL into pre-training and then evaluate the finetuing performance on the target datasets. As shown in <ref type="table" target="#tab_0">Table 13</ref>, the learned representation is much stronger with the increase of the pre-training data scale, indicating the necessity of building large-scale dataset, and our LUPerson-NL is very important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. More results for small-scale and few-shot</head><p>To complement <ref type="table" target="#tab_3">Table 3</ref> in the main text, we provide more detailed results under "small scale" and "few shot" settings in <ref type="table" target="#tab_0">Table 14</ref>. As we can see, our weakly pre-trained models are consistently better than other pre-trained models. Our advantage is much larger with less training data, suggesting the potential practical value of our pre-trained models for real-world person ReID applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>for person Re-ID are all of limited scale and diversity compared to other vision tasks. Therefore, model pre-training be-* Corresponding author. (a) Market1501 with MGN (b) Market1501 with IDE (c) DukeMTMC with MGN (d) DukeMTMC with IDE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1 to detect persons and extract person tracklets from the same 1 FairMOT: https://github.com/ifzhang/FairMOT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Identity distribution of LUPerson-NL. A curve point (X, Y ) indicates Y % of identities each has less than X images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>(a) Correctly labeled identities (b) Noise-I (c) Noise-II Besides the correctly labeled identities as shown by (a), there are two types of labeling errors in LUPerson-NL. Noise-I: same person labeled as different identities, e.g. D, E and F shown in (b). Noise-II: different persons labeled as the same identity, e.g. G shown in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Table 7 .</head><label>57</label><figDesc>Figure 5b different persons are labeled as the same identity in LUPerson-NL due to Noise-II in labels. After PNL Visualizing the label correction functionality of our PNL framework with respect to the two noise types from LUPerson-NL. Person images in the same rectangle indicate that they are recognized as the same identity. The right-hand similarity matrices are calculated based on the image features all learned using our PNL framework with label correction. Ablating the label correction. lc: "label correction".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Scene changing for a specific person in LUPerson-NL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>]</cell><cell>1,264</cell><cell>2</cell><cell>632</cell><cell>yes</cell><cell>-</cell><cell>fixed</cell><cell>hand</cell><cell>128 ? 48</cell></row><row><cell>GRID [35]</cell><cell>1,275</cell><cell>8</cell><cell>1,025</cell><cell>yes</cell><cell>subway</cell><cell>fixed</cell><cell>hand</cell><cell>vary</cell></row><row><cell>CUHK03 [30]</cell><cell>14, 096</cell><cell>2</cell><cell>1, 467</cell><cell>yes</cell><cell>campus</cell><cell>fixed</cell><cell>DPM [11]+hand</cell><cell>vary</cell></row><row><cell>Market [58]</cell><cell>32, 668</cell><cell>6</cell><cell>1, 501</cell><cell>yes</cell><cell>campus</cell><cell>fixed</cell><cell cols="2">DPM [11]+hand 128 ? 64</cell></row><row><cell>Airport [25]</cell><cell>39, 902</cell><cell>6</cell><cell>9, 651</cell><cell>yes</cell><cell>airport</cell><cell>fixed</cell><cell>ACF [10]</cell><cell>128 ? 64</cell></row><row><cell>DukeMTMC [61]</cell><cell>36, 411</cell><cell>8</cell><cell>1, 852</cell><cell>yes</cell><cell>campus</cell><cell>fixed</cell><cell>Hand</cell><cell>vary</cell></row><row><cell>MSMT17 [52]</cell><cell>126, 441</cell><cell>15</cell><cell>4, 101</cell><cell>yes</cell><cell>campus</cell><cell>fixed</cell><cell>FasterRCNN [42]</cell><cell>vary</cell></row><row><cell>SYSU30K [50]</cell><cell>29,606,918</cell><cell>1,000</cell><cell>30,508</cell><cell cols="2">weakly TV program</cell><cell>dynamic</cell><cell>YOLOv2</cell><cell>vary</cell></row><row><cell>LUPerson [12]</cell><cell cols="2">4, 180, 243 46, 260</cell><cell>&gt; 200k</cell><cell>no</cell><cell>vary</cell><cell>dynamic</cell><cell>YOLOv5</cell><cell>vary</cell></row><row><cell>LUPerson-NL</cell><cell cols="4">10, 683, 716 21, 697 ? 433, 997 noisy</cell><cell>vary</cell><cell>dynamic</cell><cell>FairMOT [56]</cell><cell>vary</cell></row></table><note>. Comparing statistics among existing popular Re-ID datasets. LUPerson-NL is by far the largest Re-ID dataset with better diversity without human labeling effort. SYSU30K is partly annotated by human annotator.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparing three supervised Re-ID baselines using different pre-trained models. "IN sup."/"IN unsup." indicates model that is supervisely/unsupervisely pre-trained on ImageNet; "LUP unsup." is the model unsupervisely pre-trained on LUPerson; "LUPnl pnl." refers to the model that pre-trained on LUPerson-NL using our PNL framework. All results are shown in mAP/cmc1. /66.9 65.<ref type="bibr" target="#b7">8</ref>/80.2 72.5/84.4 76.3/86.9 78.5/88.7 32.4/48.0 65.3/80.2 73.7/85.1 77.7/87.8 79.4/89.0 LUP unsup. 53.5/72.0 69.4/81.9 75.6/86.7 78.9/88.2 81.1/90.0 35.8/50.2 72.3/83.8 77.7/87.4 80.8/89.2 82.0/90.6 LUPnl pnl. 60.6/75.8 74.5/86.3 78.8/88.3 81.6/89.5 83.3/91.2 52.2/64.1 77.7/87.9 81.1/89.6 83.2/91.1 84.1/91.3 LUP unsup. 25.5/51.1 44.6/71.4 53.0/77.7 59.5/81.8 63.7/85.0 17.0/36.0 49.0/73.6 57.4/80.5 62.9/83.5 65.0/85.1 LUPnl pnl. 28.2/51.1 47.7/71.2 55.5/77.2 61.6/81.8 66.1/84.8 24.5/42.7 53.2/74.4 62.2/81.0 65.8/83.8 67.4/85.3</figDesc><table><row><cell>pre-train</cell><cell>10%</cell><cell>30%</cell><cell>small-scale 50%</cell><cell>70%</cell><cell>90%</cell><cell>10%</cell><cell>30%</cell><cell>few-shot 50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell>IN sup.</cell><cell cols="10">53.1/76.9 75.2/90.8 81.5/93.5 84.8/94.5 86.9/95.2 21.1/41.8 68.1/87.6 80.2/92.8 84.2/94.0 86.7/94.6</cell></row><row><cell>IN unsup.</cell><cell cols="10">58.4/81.7 76.6/91.9 82.0/94.1 85.4/94.5 87.4/95.5 18.6/36.1 69.3/87.8 78.3/90.9 84.4/94.1 87.1/95.2</cell></row><row><cell cols="11">LUP unsup. 64.6/85.5 81.9/93.7 85.8/94.9 88.8/95.9 90.5/96.4 26.4/47.5 78.3/92.1 84.2/93.9 88.4/95.5 90.4/96.3</cell></row><row><cell cols="11">LUPnl pnl. 72.4/88.8 85.2/94.2 88.3/95.5 90.1/96.2 91.3/96.4 42.0/61.6 83.7/94.0 88.1/95.2 90.5/96.3 91.6/96.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Market1501</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell>10%</cell><cell>30%</cell><cell>small-scale 50%</cell><cell>70%</cell><cell>90%</cell><cell>10%</cell><cell>30%</cell><cell>few-shot 50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell>IN sup.</cell><cell cols="10">45.1/65.3 64.7/80.2 71.8/84.6 75.5/86.8 78.0/88.3 31.5/47.1 65.4/79.8 73.9/85.7 77.2/87.8 79.1/88.8</cell></row><row><cell>IN unsup.</cell><cell cols="5">48.1(b) DukeMTMC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>pre-train</cell><cell>10%</cell><cell>30%</cell><cell>small-scale 50%</cell><cell>70%</cell><cell>90%</cell><cell>10%</cell><cell>30%</cell><cell>few-shot 50%</cell><cell>70%</cell><cell>90%</cell></row><row><cell>IN sup.</cell><cell cols="10">23.2/50.2 41.9/70.8 50.3/76.9 56.9/81.2 61.9/84.2 14.7/34.1 44.5/71.1 56.2/79.5 60.9/82.8 63.4/84.5</cell></row><row><cell>IN unsup.</cell><cell cols="10">22.6/48.8 40.4/68.7 49.0/75.0 55.7/79.9 60.9/83.0 13.2/29.2 41.4/67.1 53.3/77.6 59.1/81.5 62.4/83.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) MSMT17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparing pre-trained models on three labeled Re-ID datasets, under the small-scale setting and the few-shot setting, with different usable data percentages. "LUPnl pnl." is our model pre-trained on LUPerson-NL using PNL. Results are shown in mAP/cmc1.</figDesc><table><row><cell>pre-train</cell><cell>M</cell><cell>USL</cell><cell>D</cell><cell>UDA D ? M M ? D</cell><cell>method MSMT17</cell><cell>SupCont [26] LUP [12] PNL(ours) 66.5/84.7 65.3/84.0 68.0/86.0</cell></row><row><cell cols="5">IN sup. IN unsup. LUP unsup. 76.2/90.2 67.1/81.6 79.2/91.7 69.1/83.2 72.4/87.8 64.9/80.3 76.4/90.1 67.9/82.3 72.9/88.6 62.6/78.8 77.1/90.6 66.3/81.6</cell><cell cols="2">Table 5. Performance comparison for different pre-training meth-ods on LUPerson-NL dataset.</cell></row><row><cell>LUPnl pnl.</cell><cell cols="4">75.6/89.3 68.1/82.0 80.7/92.2 72.2/84.9</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 4. Performances of different pre-trained models on the un-</cell><cell></cell><cell></cell></row><row><cell cols="5">supervised Re-ID method SpCL [15] under two unsupervised task</cell><cell></cell><cell></cell></row><row><cell cols="5">settings: the pure unsupervised learning (USL) and the unsuper-</cell><cell></cell><cell></cell></row><row><cell cols="5">vised domain adaptation (UDA). Here M and D refer to the Mar-ket1501 dataset and the DukeMTMC dataset respectively.</cell><cell cols="2">of MoCoV2 for person Re-ID based on unsupervised con-strastive learning, while SupCont [26] considers both su-pervised and constrastive learning. Our PNL outperforms all these rep-resentative pre-training methods, indicating the superiority of our proposed method.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9 .Table 10 .</head><label>910</label><figDesc>Comparison of applying our PNL on both LUPerson-NL and SYSU30K. Performances under different ? values on DukeMTMC and MSMT17 with data percentages 40% and 100% under the small scale setting. The threshold is set as T = 0.8. The best scores are in bold and the second ones are underlined.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>LUPerson-NL</cell><cell>SYSU30K</cell></row><row><cell cols="2">MSMT17</cell><cell>66.1/84.6</cell><cell>55.2/76.7</cell></row><row><cell>?</cell><cell cols="2">DukeMTMC 40% 100%</cell><cell>MSMT17 40% 100%</cell></row><row><cell cols="4">0.05 76.1/87.2 83.5/91.4 49.8/73.3 65.4/83.8 0.07 76.4/87.5 83.6/91.4 50.7/74.1 67.2/85.3 77.0/87.3 84.3/92.0 51.9/74.9 68.0/86.0 0.1 0.2 75.8/86.8 83.4/91.0 50.1/73.7 66.4/85.0 0.3 74.7/86.2 82.7/90.6 48.6/72.0 65.5/83.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>76.1/86.5 83.3/91.0 51.1/74.6 67.5/85.2 0.6 77.1/87.7 84.1/91.6 52.3/75.5 68.1/85.7 0.7 77.0/87.5 84.0/91.8 51.9/75.6 68.2/85.8 0.8 77.0/87.3 84.3/92.0 51.9/75.0 68.0/86.0 0.9 75.7/86.4 83.0/90.8 50.9/74.3 67.2/85.0Table 11. Performances under different T values on DukeMTMC and MSMT17 with data percentages 40% and 100% under the small scale setting. The temperature factor is set as ? = 0.1. The best scores are in bold and the second ones are underlined.</figDesc><table><row><cell>T</cell><cell cols="2">DukeMTMC 40% 100%</cell><cell cols="2">MSMT17 40% 100%</cell></row><row><cell cols="5">0.5 Arch CUHK03 Market1501 DukeMTMC MSMT17</cell></row><row><cell cols="3">R50 80.4/80.9 91.9/96.6 R101 80.5/81.2 92.5/96.9 R152 80.6/81.2 92.7/96.8</cell><cell cols="2">84.3/92.0 68.0/86.0 85.5/92.8 70.8/87.1 85.6/92.4 71.6/87.5</cell></row><row><cell cols="5">Table 12. Results with different ResNet backbones. R50, R101</cell></row><row><cell cols="5">and R152 stand for ResNet50, ResNet101 and ResNet152 respec-</cell></row><row><cell>tively.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Scale</cell><cell></cell><cell>10%</cell><cell>30%</cell><cell>100%</cell></row><row><cell cols="2">MSMT17</cell><cell>57.4/79.2</cell><cell>62.2/82.1</cell><cell>68.0/86.0</cell></row></table><note>Table 13. Comparison for different pre-training data scale scores reported in</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is partially supported by the National Natural Science Foundation of China (NSFC, 61836011).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">10%  20%  30%  40%  50%  60%  70%  80%  90%  100%  #id  75  150  225  300  375  450  525  600  675  751  #images  1,170  2,643  3,</ref> <ref type="table">Table 14</ref><p>. Performance for small-scale and few-shot setting with MGN method for Market1501, DukeMTMC and MSMT17. "IN sup." and "IN unsup." refer to supervised and unsupervised pre-trained model on ImageNet, "LUP unsup." refers to unsupervised pre-trained model on LUPerson, "LUPws wsp." refers to our model pre-trained on LUPerson-WS using WSP. The first number is mAP and second is cmc1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Weakly supervised tracklet person re-identification by deep featurewise mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14333</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person reidentification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3691" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving person re-identification with iterative impression aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="9559" to="9571" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal knowledge propagation for imageto-video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9647" to="9656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Guided saliency feature learning for person re-identification in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="357" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sbsgan: Suppression of inter-domain background shift for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9527" to="9536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantics-aligned representation learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="11173" to="11180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Angels Rates-Borras, Octavia Camps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Srikrishna Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard J Radke</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09653</idno>
	</analytic>
	<monogr>
		<title level="m">A comprehensive evaluation and benchmark for person reidentification: Features, metrics, and datasets</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global-local temporal representations for video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3958" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mopro: Webly supervised learning with momentum prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised tracklet person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1770" to="1782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recover and identify: A generative dual model for cross-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8090" to="8099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ci-Siang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Bo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">View confusion feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6639" to="6648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Person re-identification by manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Chen Change Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3567" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral feature transformation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4976" to="4985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyang</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingke</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="760" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Relation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11839" to="11847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Posenormalized image generation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="650" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Auto-reid: Searching for a part-aware convnet for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijie</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3750" to="3759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Es-net: Erasing salient parts to learn more in re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Person re-identification with deep similarity-guided graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10981" to="10990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Weakly supervised person re-id: Differentiable graphical learning and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Second-order non-local attention networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Bryan Ning Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poellabauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3760" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">In defense of the triplet loss again: Learning robust person re-identification with fast approximated triplet loss and label distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="354" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairmot</surname></persName>
		</author>
		<title level="m">On the fairness of detection and re-identification in multiple object tracking. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Densely semantically aligned person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1367" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A discriminatively learned cnn embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Identity-guided human semantic parsing for person re-identification. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
