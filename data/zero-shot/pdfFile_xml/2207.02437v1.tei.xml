<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complementary Bi-directional Feature Compression for Indoor 360?Semantic Segmentation with Self-distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zishuo</forename><surname>Zheng</surname></persName>
							<email>zszheng@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Lin</surname></persName>
							<email>cylin@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Nie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Complementary Bi-directional Feature Compression for Indoor 360?Semantic Segmentation with Self-distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>panoramic images</term>
					<term>semantic segmentation</term>
					<term>self-distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, horizontal representation-based panoramic semantic segmentation approaches outperform projection-based solutions, because the distortions can be effectively removed by compressing the spherical data in the vertical direction. However, these methods ignore the distortion distribution prior and are limited to unbalanced receptive fields, e.g., the receptive fields are sufficient in the vertical direction and insufficient in the horizontal direction. Differently, a vertical representation compressed in another direction can offer implicit distortion prior and enlarge horizontal receptive fields. In this paper, we combine the two different representations and propose a novel 360?semantic segmentation solution from a complementary perspective. Our network comprises three modules: a feature extraction module, a bi-directional compression module, and an ensemble decoding module. First, we extract multi-scale features from a panorama. Then, a bi-directional compression module is designed to compress features into two complementary low-dimensional representations, which provide content perception and distortion prior. Furthermore, to facilitate the fusion of bi-directional features, we design a unique self distillation strategy in the ensemble decoding module to enhance the interaction of different features and further improve the performance. Experimental results show that our approach outperforms the state-of-the-art solutions with at least 10% improvement on quantitative evaluations while displaying the best performance on visual appearance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Panoramic images captured by omnidirectional cameras can provide a wide fieldof-view (FoV), making it more practical in many crucial scene perception tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b38">[39]</ref>. As a fundamental topic in computer vision, semantic segmentation aims to assign each pixel in the image a category label and is critical for arXiv:2207.02437v1 <ref type="bibr">[</ref>  various applications such as pose estimation <ref type="bibr" target="#b24">[25]</ref>, autonomous vehicles <ref type="bibr" target="#b28">[29]</ref>, augmented reality <ref type="bibr" target="#b1">[2]</ref>. Directly applying normal FoV semantic segmentation methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b40">[41]</ref> to 360?images is not satisfactory due to the significant distortions in panoramas (usually produced from equirectangular projection-ERP) and large mismatch of FoV between panoramas and normal Fov images. To overcome the above limitations, some researchers propose to adopting different projection formats (e.g., cubemap projection and icosahedron groups) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b37">[38]</ref> or spherical convolutions <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref> to decrease the negative effects of panoramic distortions. However, these methods sacrifice either accuracy or efficiency and fail to perceive precise panoramic structures.</p><p>Most recently, inspired by the geometric nature of gravity-aligned panoramas, some horizontal representation-driven methods <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref> are proposed to address the above problems. They squeeze the ERP image into 1D vector along the vertical direction, making it to be more content-focused, as shown in <ref type="figure" target="#fig_0">Fig.1a</ref> (top). Such a manner can be regarded as a process of shrinking spherical data towards the equator. Thus each element in the representation shares the same distortion magnitude and removes the negative effects of panoramic distortions effectively. However, due to the fixed compression direction, it is inherently limited to local receptive fields which lack receptive capability in the horizontal direction ( <ref type="figure" target="#fig_0">Fig.1b)</ref>. Additionally, if there is no extra guidance during the decoding stage, it will lead to the lack of distortion distribution information in panoramic segmentation results, resulting in unsatisfactory performance.</p><p>Motivated by the horizontal features, we observe that compressing spherical data in the horizontal or vertical direction yields different data representations. Essentially, extracting the vertical feature along the horizontal direction is to contract panoramic images towards a certain meridian. Considering the data at the same latitude shares the same magnitude of distortion, this operation gathers data belonging to the same magnitude. Despite this representation may blur the image content, it makes feature distortions more prominent which can provide implicit distortion distribution prior. Compared with existing works that devote to eliminate the effects of distortions, we introduce implicit distortion information to guide the panoramic segmentation. Furthermore, the vertical representation also enhances the receptive capability in the horizontal direction.</p><p>In this paper, we present a novel neural network for 360?semantic segmentation consisting of three parts: a feature extraction module, a bi-directional compression module, and an ensemble decoding module. To be more specific, we first extract multi-scale features from an ERP image. Then our bi-directional compression module encodes the features into two complementary 1D flattened sequences. To achieve it, we design a Mix-MLP layer to yield a useful representation before we contract the dimensions. Subsequently, we propose a pyramid pooling compression (PPC) layer to perceive both distortion and content information by aggregating different sub-regions with different receptive fields. During the ensemble decoding process, we adopt A-Conv <ref type="bibr" target="#b25">[26]</ref> to stretch dimensions and rebuild two different 2D features. Finally, the different features are fused for complementarity to predict the segmentation results.</p><p>However, considering the difference between two representations, the feature domains diverge severely, making them difficult to integrate harmoniously. Here, we address it by designing a unique self distillation strategy <ref type="bibr" target="#b39">[40]</ref>. Specifically, We divide self distillation into three parallel ones: horizontal-driven branch (HDB), vertical-driven branch (VDB), and ensembled branch (EB), of which EB is the fusion result of two representations. HDB and VDB are regarded as student models, while EB is the teacher model. The knowledge in the fusion portion will be shared with the separate portions. Finally, the retuned features in student models will feedback to the teacher model, which enhances the interaction of different features and further improves the performance.</p><p>With abundant experiments, we verify that the proposed solution can significantly outperform state-of-the-art algorithms in panoramic semantic segmentation and achieve at least 10% improvement on the quantitative evaluation. Besides, the ablation studies also reveal the effectiveness of our bi-directional representations and specially designed self distillation. In summary, our principle contributions are summarized as follows:</p><p>-To enlarge the limited horizontal receptive fields and offer implicit distortion prior, we combine horizontal and vertical representations to establish a novel 360?semantic segmentation network from a complementary perspective.</p><p>-To facilitate the fusion of bi-directional representations, we design a unique self distillation strategy to enhance the interaction of different feature and further improve the performance.</p><p>-Experiments demonstrate that our method significantly outperforms the current state-of-the-art approaches at least 10% improvement on all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semantic Segmentation of Omnidirectional Images</head><p>With the progress of 360?camera devices, a wide scene context can be captured in single imagery and thus we can focus on the 360?semantic perception. Early approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b33">[34]</ref> were based on the synthesized panoramic dataset or manually labeled samples to build semantic segmentation systems. Motivated by the style transfer <ref type="bibr" target="#b41">[42]</ref> and data distillation <ref type="bibr" target="#b26">[27]</ref>, Yang et al. proposed a framework <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> for re-using the models trained on perspective images by dividing the ERP into multiple restricted FoV sections for predictions. Although quite accurate, their strategy relies on labeled perspective image datasets with similar categories and scenes. However, recent works find their solutions directly on the real-world datasets <ref type="bibr" target="#b0">[1]</ref>. Tateno et al. <ref type="bibr" target="#b31">[32]</ref> presented spherical convolution filters to make the network aware of the distortion from ERP images. Compared to solutions that operate directly on ERP, <ref type="bibr" target="#b37">[38]</ref> projected spherical signals into subdivided icosahedron mesh to mitigate distortion as well as improve prediction accuracy. But these methods need to redesign some convolution operations and the performance drops dramatically at the sub-patches boundaries. Eder et al. <ref type="bibr" target="#b8">[9]</ref> introduced tangent images, a novel representation that renders the image onto narrow FoV images tangent to a subdivided icosahedron. While solving the above problems, it suffers from complex preprocess and low inference speed. Sun et al. <ref type="bibr" target="#b30">[31]</ref> used compressing method to encode latent features and use discrete cosine transform (DCT) to finish holistic scene modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Horizontal Representation</head><p>Unlike the most existing methods that use pure 2D features to perform prediction, exploiting 1D horizontal representation can make the network learn the underlying geometric correlated knowledge. Su et al. <ref type="bibr" target="#b29">[30]</ref> utilized different kernel sizes of the standard convolution to overcome the distortions. Particularly, the weight can only be shared along the horizontal. With the assumption that the horizontal dimension contains rich contextual information, Yang et al. <ref type="bibr" target="#b36">[37]</ref> proposed a horizontal-driven attention method to capture omni-range priors in 360?images. Particularly, Sun et al. <ref type="bibr" target="#b30">[31]</ref> used 1D horizontal representation to design HorizonNet for the task of estimating room layout. This trend prompts a variety of works on scene understanding. For instance, Pintore et al. <ref type="bibr" target="#b25">[26]</ref> proposed SliceNet and adopt Long Short-Term Memory (LSTM) to model the longrange dependencies for 360?depth estimation. However, these methods do not consider the latitudinal distortion property and horizontal respective capability, thus leading the accuracy degradation. Our solution solves this issue by integrating horizontal and vertical representations simultaneously which we believe is the optimal manner to eliminate the influence of distortions and preserve details. Moreover, adding extra tensors will not increase the model complexity and computational cost greatly, because our complexity changes from O (W ) to O (W + H). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b12">[13]</ref> is one of the most popular compression approaches. It is inspired by knowledge transfer from teachers to students. And it has shown its superiority in other domains such as data argumentation <ref type="bibr" target="#b2">[3]</ref>, adversarial attack <ref type="bibr" target="#b23">[24]</ref>, and model transfer <ref type="bibr" target="#b10">[11]</ref>. However, it requires substantial efforts and experiments to build teacher models, and we will spend many datasets and long training time to refine student models. To overcome the setbacks of traditional distillation, Zhang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a novel training technique named self distillation, which means student and teacher models come from the same networks. Therefore, to facilitate the fusion of bi-directional representations, we redesign this technique to adapt to our framework. Specifically, we regard two 1D representations as student models, while their fusion results as teacher models, and introduce three loss functions for optimization. The well-designed self distillation can enhance the interaction of different feature and further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we describe the details of the proposed method for 360?semantic segmentation. We first show the overview of our framework. Then, the bi-directional compression module that reduces the dimensions of feature maps along horizontal and vertical directions is discussed. Finally, to foster the fusion of bi-directional representations and narrow the feature domain gap, we design a special self distillation strategy to adapt our network structure in the ensemble decoding module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Overview</head><p>The framework is depicted in <ref type="figure">Fig. 2</ref>. In our feature extraction module, M e , an ERP format 360?image with the size of H ? W will be passed into a deep convolutional neural network, such as ResNet <ref type="bibr" target="#b11">[12]</ref>, to progressively decrease the panorama resolution and produce hierarchical feature maps at {1/4, 1/8, 1/16, 1/32} of the original image resolution. Then we adopt feature pyramid network <ref type="bibr" target="#b17">[18]</ref> to form multi-scale features, denoted as {F hi?wi i } i=1,2,3,4 . In the next step, these feature maps are fed into the bi-directional compression module M c in parallel which contains a lightweight Mix-MLP layer to yield useful representations and a PPC layer to contract the dimensions in the horizontal and vertical directions. In particular, we use different pooling operations for different feature maps to aggregate local and global context information. We detail this module in Sec.3.2. Then we concatenate multi-level 1D tensors in a single sequence and obtain two representations: S eqh and S eqv .</p><p>During the decoding period, an ensemble decoding module M d is employed to reconstruct 2D dense feature maps in Sec.3.3. Finally, the fused features are fed into the segmentation head to predict the final segmentation results. Besides, for most padding operations, we use circular padding for the left-right boundaries of the feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bi-directional Compression Module</head><p>To obtain the bi-directional spherical representations in a more effective and efficient way, we introduce a bi-directional compression module. This module compresses features into two complementary low-dimensional representations which provide content perception and distortion perception separately.</p><p>Mix-MLP Layer. Considering the unique structure of ERP, we argue that location information is necessary for our 360?semantic segmentation. However, due to the consistent resolution requirements during training and testing, the positional encoding <ref type="bibr" target="#b7">[8]</ref> suffers from the inflexible extension problem. To enable our network the capability of size-free positional encoding, we present a lightweight Mix-MLP layer. Inspired by <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b32">[33]</ref>, our Mix-MLP mixes a 3 ? 3 convolution with zero padding and two MLP into a unified framework to introduce implicit location information. It can be formulated as:</p><formula xml:id="formula_0">F out = Linear (? (DW Conv (Linear (F in )))) + F in<label>(1)</label></formula><p>where F in is the feature maps from the backbone, and ? is activation function, we use GELU in our experiments. The number of channels in Linear is four times as input. Note that we exploit depth-wise convolutions (DWConv) for improving efficiency and reducing the number of parameters. As a result, our backboneextracted features with location information are useful for bi-directional feature compression.</p><p>Pyramid Pooling Compression. To squeeze the height (h) and width (w), the most straightforward operation is to conduct two Conv2D layers with the kernel sizes of h ? 1 and 1 ? w. However, although the receptive field of ResNet  <ref type="figure">Fig. 3</ref>. Illustration of pyramid pooling compression (PPC). Given a feature map F1 with the height of h1, to generate horizontal representation, we first use global average pooling to harvest different sub-region representations, then a Conv2D layer is applied to compress the height to 1, followed by concatenation and convolutions layers to form the final 1D representation, which carries both local and global context information.</p><p>Note that the sizes of sub-regions only vary in the vertical direction.</p><p>is already enough for the works that utilize the 2D feature, it is shown that it is still small for our method that only uses the 1D representations.</p><p>Having observed that the global average pooling is a helpful method as the global contextual prior <ref type="bibr" target="#b19">[20]</ref>, we design an efficient compression method to overcome the above problems. Moreover, it is more reasonable to introduce a more powerful representation using sub-regions with different sizes instead of the same size <ref type="bibr" target="#b40">[41]</ref>. Hence, our PPC layer fuses feature under several different pyramid scales, <ref type="figure">Fig.3</ref> gives an example. Note that the number of pyramid levels and size of each level can be modified manually. They are related to the size of the feature maps that are fed into the pooling layer. Therefore, combined with our hierarchical structure, the number of pyramid levels decreases with the increase of the network stage. Concretely, given the feature maps  <ref type="figure">Fig.2</ref>) should be added to align them. For example, we only upsample the feature maps along the horizontal direction to align the horizontal feature. By compressing the features in different directions, our model can implicitly perceive content information and distortion distribution in a panorama from two perspectives.</p><formula xml:id="formula_1">{F hi?wi i } i=1,2,3,4 , the pooling size is { hi 2 j ? w i } i=1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ensemble Decoding Module</head><p>To produce per-pixel predictions from 1D representations, Sun et al. <ref type="bibr" target="#b30">[31]</ref> exploited interpolation operations and inverse discrete cosine transform (IDCT), leading to reduction of the model learnability. Different from the strategy of reshaping the size directly, we utilize n A-Conv layers <ref type="bibr" target="#b25">[26]</ref>, each of which includes an upsampling layer, a Conv2D layer, a BN layer, and a PReLU, to progressively stretch dimensions. Note that we replace the PReLU with ReLU for our segmentation modalities, and the final resolution is H 4 ? W 4 . In this way, we can obtain two distinct reconstructed tensors, denoted as D h and D v .  <ref type="figure">Fig. 4</ref>. Illustration of our self distillation strategy. During the network's training process, we principally divide our self distillation structure into three sections according to their sources. The De is the summation of D h , Dv, and F1. In the test stage, these structures in rectangle boxes which only be introduced in training processes will be removed.</p><p>Self Distillation. Despite we can directly ensemble these features to capture complementary semantic information in horizontal and vertical directions, the direction-dominated characteristics are still underutilized. Furthermore, due to bi-directional features representing different panoramic characteristics, the feature domain gap will hinder the fusion process.</p><p>To facilitate the fusion of different features, we do not design a more complex fusion network that can significantly enlarge the model size. On the contrary, we design a unique self distillation strategy in the decoding stage to narrow the domain gap between different features. As illustrated in <ref type="figure">Fig. 4</ref>, our self distillation structure can be divided into three parts: HDB, VDB, and EB. The EB comprises HDB, VDB, and the backbone feature F h1?w1 1 . During the training process, HDB and VDB are regarded as student models while EB is the teacher model. The students can learn beneficial knowledge from the teacher, and the teacher can obtain good feedback from the students. In this manner, both students and teacher can benefit from each other. We set several convolution layers (bottleneck) and a SegHead as the segmentation part, and both students and teacher share the same network structure. The bottleneck contains three Conv2D layers with kernel sizes of 1 ? 1, 3 ? 3, and 1 ? 1. The SegHead comprises two upsampling layers to reach the original resolution, and two Conv2D layers to predict the segmentation mask at a H ? W ? N cls resolution, where N cls is the number of categories. Due to page limitations, we exhibit a complete example in the supplementary material.</p><p>After predicting the segmentation map of a 360?image, it is straightforward to adopt the segmentation labels to supervise the HDB and VDB, which can produce better D h and D v . Nevertheless, if we only use this supervision, the knowledge will not interact between the students and teacher. To this end, we introduce two extra supervisions (supervisions from intermediate features and final softmax outputs of the teacher model) to encourage the student models to learn from the teacher model. In brief, we use cross-entropy loss, kullback-Leibler(KL) divergence loss, and L2 loss as the optimization functions in our self distillation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Objective Function</head><p>Our objective function comprises three kinds of loss as the objective functions for optimizing predictions.</p><p>Cross-Entropy Loss. The first supervision is the cross-entropy loss. Almost all CNNs for this task exploit cross-entropy loss. It is computed with the ground truth (GT) from the training samples and the predictions of the softmax layer. We deploy it not only to the teacher's branch but also to two student branches. Through cross-entropy loss, the knowledge hidden in the training set is introduced directly from GT to all the branches. It can be written as:</p><formula xml:id="formula_2">L ce = ??g log(p i )? (2)</formula><p>where g, p i denote the GT and predicted values from softmax layer's output, respectively; i ? {1, ..., N }, where N denotes the number of SegHeads in the training period, and N = 3 in our experiments (refer to Fig4). Moreover, classwise weighted <ref type="bibr" target="#b37">[38]</ref> is utilized to balance different classes examples.</p><p>KL Divergence Loss. The second supervision is the KL divergence loss. We use KL divergence to measure the difference between two distributions. It can be obtained through the computation of softmax outputs between students and teachers. Under the teacher's guidance, the distributions of students' SegHeads can approximate the teacher's, which indicates the supervision from distillation. It can be obtained by:</p><formula xml:id="formula_3">L kl = ?p N log( p N p i )? (3)</formula><p>L2 Loss. The last supervision is L 2 loss which works by decreasing the distance between feature maps in the student branches and the teacher branch. In this way, the knowledge in feature maps is distilled to each student's bottleneck layer.</p><formula xml:id="formula_4">L l2 = ?f i ? f N ? 2 2<label>(4)</label></formula><p>Note that the last two losses for the teacher model are zero, which means the supervision in the teacher model only comes from GT. Most importantly, we denote it as base loss L b in our network without distillation. For student models, we collect all supervision to obtain self distillation loss L s . Meanwhile, to make the fusion process more interactive, we adopt three hyper-parameters to balance them.</p><formula xml:id="formula_5">L total = L b + L s , = i=N L ce + N ?1 i=1 (? * L ce + ? * L kl + ? * L l2 ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of our model in this section by carrying out comprehensive experiments on a real-world dataset. In the following subsections, we first introduce the dataset and implementation details, then report quantitative and qualitative results compared with the state-of-the-art approaches. Finally, we perform a series of ablation experiments for the proposed components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We evaluate our method on the Stanford 2D-3D-S dataset <ref type="bibr" target="#b0">[1]</ref>, which consists of 1413 real-world equirectangular RGB-D images over 13 categories. The dataset contains six large-scale indoor areas and provides semantic labels with the ERP format as GT annotations. Furthermore, the panoramas have a resolution of 2048 ? 4096 and contain black void regions at the top and bottom. Following the prior works, we report averaged quantitative results from the 3-fold cross-validation splits. We adopt Mean IoU (mIoU, mean of class-wise intersection over union) and Mean Acc (mAcc, mean of class-wise accuracy) as evaluation metrics for the task of 360?semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We conduct our experiments on three resolutions: 64 ? 128, 256 ? 512, and 512 ? 1024. We train our learning model using Adam <ref type="bibr" target="#b15">[16]</ref> optimizer on a GTX 3090 GPU, and the batch sizes are set to 16, 8, and 4. For the low-resolution (the first two) inputs, we use the residual UNet-style architecture as backbone <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b37">[38]</ref> and replace the specialized kernels with planar one. For the highresolution (the last one) inputs, we adopt ResNet-101 pre-trained on COCO <ref type="bibr" target="#b18">[19]</ref> as backbone <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b30">[31]</ref> to capture the larger receptive field. Inspired by <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, we employ the poly learning rate policy where the base learning rate is multiplied by (1 ? iter max iter ) power with power = 0.9. The learning rate is set to 1 ? 10 ?3 with max iter = 300 for low-resolution and 1 ? 10 ?4 with max iter = 60 for high resolution. To prevent overfitting, we adopt a simple augmentation strategy of randomly cutting a patch of the input image and padding this region with a black mask, where the sizes of the hole are chosen from the set {20 ? 40, 80 ? 160, 160 ? 320}. In our objective function, we set ? = 0.7, ? = 0.3, and ? = 0.003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Analysis</head><p>Compared with State-of-the-arts In this subsection, we compare our method with the state-of-the-art methods on 360?semantic segmentation in both quantitative and qualitative evaluations, for which the numerical results or segmentation map on the same dataset is available.</p><p>Quantitative Evaluation: <ref type="table">Table.</ref>1 shows the quantitative comparison results with the current state-of-the-art methods on different input resolutions. It is evident that our approach substantially outperforms the compared approaches in all metrics. From these evaluations on the lowest resolution, we can conclude that: (i) Compared with the spherical CNNs methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> [15] which aim to directly learn distortion-aware representation from the sphere, our approach avoids complex convolutional design on the transfer between planar and sphere, showing more promising generality and flexibility.</p><p>(ii) Compared with the distortion-tolerate approaches <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b37">[38]</ref>, which project 360?images into icosahedron format, our approach only need ERP as input and omit the process of transformation. For example, our approach outperforms .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head><p>HoHoNet Ours GT RGB HoHoNet Ours GT HexRUNet <ref type="bibr" target="#b37">[38]</ref> which equip a specially non-rectangular kernel by a significant margin, with approximately 9% improvement on mIoU and 4% improvement on mAcc.</p><p>(iii) As benefits of introducing vertical representation which provides guidance of distortion distribution and enlarges receptive fields in horizontal direction during the learning stage, our approach achieves a 17% improvement on both mIoU and mAcc on the lowest resolution compared with the Sun et al. <ref type="bibr" target="#b30">[31]</ref> that only use horizontal representation.</p><p>To further demonstrate the generality of our method, we conduct experiments on other resolutions. It can be observed that our network has achieved satisfactory results on the 256 ? 512 resolution with at least 24% improvements on mIoU and 23% on mAcc. Unfortunately, due to the limitation of our device, we failed to train our network on higher resolutions as <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b30">[31]</ref>. However, based on the fact that the segmentation performance is positively correlated with the resolution size, we train our network with ResNet-101 as the backbone on a lower 512 ? 1024 resolution and still achieve competitive performance.</p><p>Qualitative Evaluation: <ref type="figure" target="#fig_4">Fig.5</ref> shows qualitative results on Stanford2D3DS dataset, compared to HoHoNet <ref type="bibr" target="#b30">[31]</ref>. From the figure, we can observe that our approach performs well on all indoor scenes, while the horizontal representation method shows inferior segmentation results, especially in regions with distortions or regions with complex contextual information. With the complementary relationship between two representations, our method has a larger receptive field and sufficient distortion information. For example, the class with a strong distribution along horizontal direction while weaker along the vertical direction (see <ref type="figure" target="#fig_4">Fig.5</ref> (left) first two rows) has an inferior performance in HoHoNet. Because these pixels occupy a small proportion in each column, they will be omitted when compressing the height dimension and are difficult to recover. In contrast, our vertical representation perceives this distribution in another dimension and supplement it to the decoding module. In general, our approach achieves a better performance from local details (receptive fields) to global distribution (distortion shape), which benefits from the designed modules. More qualitative results are included in the supplementary material. Ablation Studies To validate the effectiveness of different components in our approach, we conduct ablation studies and as illustrated in <ref type="table">Table.</ref>2 and <ref type="table">Table.</ref>3. Note that all the experiment results are evaluated on the lowest resolution input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of bi-directional representations:</head><p>We remove the self distillation to explore the effectiveness of combining two representations (horizontal (H), vertical (V)). Concretely, we implement our model without other key schemes (Mix-MLP (M), PPC (P)) and provide the quantitative results of variants equipped with different representations. From <ref type="table">Table.</ref>2 (first three rows), we can observe that the joint representation performs better than only using one directional representation, which indicates that our network gains information from two complementary perspectives to facilitate the accuracy. In addition, we can observe that the H performs well than the V, which proves that the vertical representation contains implicit distortion prior and blurs the content.</p><p>Effectiveness of components in M c : Subsequently, we gradually add the removed components to show the different segmentation performances. Note that we utilize a Conv2D layer to compress dimensions without M. As seen from <ref type="table">Table.</ref>2 (last three rows), the mIoU is improved from 42.76 to 44.71 with a percentage gain of 4.6%, and the mAcc is boosted from 55.00 to 57.03 with the percentage gain of 3.7%. It also can be found that our network with useful position information derived from M achieves pleasing results. For the compression strategy, P provides large receptive fields and sufficient contextual information, making our model gains further improvements and outperforms a single Conv2D layer with 3.1% on mIoU and 2.1% on mAcc. Finally, the completed framework achieves the best results proving the effectiveness of our proposed components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of self distillation:</head><p>Since the different representations have a severe feature domain gap, it is difficult to integrate them harmoniously. Thus the self distillation plays the role of facilitating the fusion of bi-directional representations in our method. Furthermore, different from other knowledge distillation methods that pre-training a large teacher model, we exploit self distillation by directly dividing our network into student models (VDB and HDB ) and teacher model (EB ). To validate the effects of this strategy, we experiment with removing  all supervision for student models, which means the knowledge from the teacher and dataset are obstructed. The quantitative results are shown in <ref type="table">Table.</ref>3, we report detailed semantic segmentation results on three folds. From <ref type="table">Table.</ref>3, we can conclude that via self distillation, all branches gain a significant improvement, which indicates that the well-designed training technique can foster the interaction of bi-directional representations and notably improve the segmentation performance. We also present the qualitative comparison results in <ref type="figure" target="#fig_5">Fig.6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, a novel panoramic semantic segmentation network is presented from a complementary perspective by combining horizontal and vertical representations, which is capable of expanding the limited horizontal receptive fields and offering implicit distortion prior. To integrate complementary bi-directional representations, we design a unique self distillation strategy to enhance the interaction of different representations and make the predicted segmentation map more accurate. To our knowledge, this is the first work in panoramic visual perception that uses bi-directional feature compression to achieve the complementary. As the benefit of the proposed complementary representation, our approach significantly outperforms current state-of-the-art solutions in prediction accuracy on the real-world indoor dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The motivation of the proposed 360?semantic segmentation approach: (a) The horizontal representation in each channel shares the same distortion magnitude while the vertical can perceive the distortion distribution. (b) The horizontal representation is limited by local receptive fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 hFig. 2 .</head><label>12</label><figDesc>The architecture of the proposed network. This network consists of a feature extraction module Me, a bi-directional compression module Mc and a ensemble decoding module M d . Note that the input of four parts in Mc have different resolutions, we draw them at the same size for a convenience exhibition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative evaluations of the segmentation results on 64 ? 128 (left) and 256 ? 512 (right) resolutions. Black rectangles are used to highlight difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Visual ablation comparison on our 360?semantic segmentation approach. (a) panoramic image. (b) ground truth. (c) VDB w/o self distillation. (d) VDB w/ self distillation. (e) HDB w/o self distillation. (f) HDB w/ self distillation. (g) EB w/o self distillation. (h) EB w/ self distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>cs.CV] 6 Jul 2022</figDesc><table><row><cell cols="3">Horizontal feature-driven method</cell><cell>Limited horizontal receptive fields</cell></row><row><cell></cell><cell cols="2">Shared distortion magnitude ...</cell></row><row><cell></cell><cell>...</cell><cell></cell></row><row><cell>Vertical compression</cell><cell cols="2">Horizontal representation</cell><cell>Segmentation map</cell></row><row><cell>Gravity-aligned</cell><cell></cell><cell></cell></row><row><cell>ERP</cell><cell></cell><cell>...</cell></row><row><cell></cell><cell>...</cell><cell>Different distortion</cell></row><row><cell></cell><cell></cell><cell>magnitude</cell></row><row><cell>Horizontal compression</cell><cell cols="2">Vertical representation</cell><cell>Ground truth</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2,3,4;j=0,...,4?i for horizontal features, and {h i ? wi 2 j } i=1,2,3,4;j=0,...,5?i for vertical features, respectively. Besides, because the feature maps in different stages have different sizes, extra upsampling layers (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation on Stanford2D3D dataset. Note that the results are averaged over the 3-folds. Reasons for different high resolutions, refer to Sec.4.3.</figDesc><table><row><cell>H ? W</cell><cell>Input</cell><cell>Method</cell><cell cols="3">Pub. &amp; Year mIoU ? mAcc ?</cell></row><row><cell cols="2">Low-resolution input</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">RGB-D Gauge Net [7]</cell><cell>ICML'19</cell><cell>39.4</cell><cell>55.9</cell></row><row><cell></cell><cell cols="2">RGB-D UGSCNN [15]</cell><cell>ICLR'19</cell><cell>38.3</cell><cell>54.7</cell></row><row><cell></cell><cell cols="2">RGB-D HexRUNet [38]</cell><cell>ICCV'19</cell><cell>43.3</cell><cell>58.6</cell></row><row><cell>64 ? 128</cell><cell cols="3">RGB-D SWSCNN [10] NeruIPS'20</cell><cell>43.4</cell><cell>58.7</cell></row><row><cell></cell><cell cols="2">RGB-D TangentImg [9]</cell><cell>CVPR'20</cell><cell>37.5</cell><cell>50.2</cell></row><row><cell></cell><cell cols="2">RGB-D HoHoNet [31]</cell><cell>CVPR'21</cell><cell>40.8</cell><cell>52.1</cell></row><row><cell></cell><cell>RGB-D</cell><cell>Ours</cell><cell>-</cell><cell>47.2</cell><cell>61.2</cell></row><row><cell></cell><cell cols="2">RGB-D TangentImg [9]</cell><cell>CVPR'20</cell><cell>41.8</cell><cell>54.9</cell></row><row><cell>256 ? 512</cell><cell cols="2">RGB-D HoHoNet [31]</cell><cell>CVPR'21</cell><cell>43.3</cell><cell>53.9</cell></row><row><cell></cell><cell>RGB-D</cell><cell>Ours</cell><cell>-</cell><cell>53.8</cell><cell>66.5</cell></row><row><cell cols="2">High-resolution input</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">2048 ? 4096 RGB TangentImg [9]</cell><cell>CVPR'20</cell><cell>45.6</cell><cell>65.2</cell></row><row><cell cols="2">1024 ? 2048 RGB</cell><cell>HoHoNet [31]</cell><cell>CVPR'21</cell><cell>52.0</cell><cell>65.0</cell></row><row><cell>512 ? 1024</cell><cell>RGB</cell><cell>Ours</cell><cell>-</cell><cell>52.2</cell><cell>65.6</cell></row><row><cell cols="3">2048 ? 4096 RGB-D TangentImg [9]</cell><cell>CVPR'20</cell><cell>51.9</cell><cell>69.1</cell></row><row><cell cols="3">1024 ? 2048 RGB-D HoHoNet [31]</cell><cell>CVPR'21</cell><cell>56.3</cell><cell>68.9</cell></row><row><cell cols="2">512 ? 1024 RGB-D</cell><cell>Ours</cell><cell>-</cell><cell>56.7</cell><cell>70.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study with the key components on our 360?semantic segmentation approach. Experiment resolution: 64 ? 128.</figDesc><table><row><cell>H</cell><cell>V</cell><cell>M</cell><cell>P</cell><cell>mIoU</cell><cell>mAcc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>41.50</cell><cell>53.27</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>40.63</cell><cell>52.65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>42.76</cell><cell>55.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>43.35</cell><cell>55.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>44.71</cell><cell>57.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study with the self distillation strategy. Experiment resolution: 64 ? 128. Fold w/o self distillation w/ self distillation VDB HDB EB VDB HDB EB mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc 1 38.68 50.16 44.91 56.11 47.08 58.05 45.12 57.29 49.34 61.06 50.48 61.93 2 33.48 48.72 36.19 51.68 38.23 52.37 37.22 53.90 40.41 57.00 40.87 57.83 3 41.19 53.71 46.59 59.39 48.82 60.68 46.30 61.12 49.20 63.57 50.35 63.84</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell><cell>(e)</cell><cell>(f)</cell><cell>(g)</cell><cell>(h)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of augmented reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Azuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: teleoperators &amp; virtual environments</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="385" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02641</idno>
		<title level="m">Label refinery: Improving imagenet classification through label progression</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large scale joint semantic re-localisation and scene understanding via globally unique instance coordinate regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10239</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<title level="m">Conditional positional encodings for vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gauge equivariant convolutional networks and the icosahedral cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kicanaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tangent images for mitigating spherical distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12426" to="12434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spin-weighted spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8614" to="8625" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spherical cnns on unstructured grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eliminating the blind spot: Adapting 3d object detection and monocular depth estimation to 360 panoramic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>De La Garanderie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="789" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stage-wise salient object detection in 360 omnidirectional image via object-level semantical saliency ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3535" to="3545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE symposium on security and privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4561" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11536" to="11545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data distillation: Towards omni-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4119" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distortion-tolerant monocular depth estimation on omnidirectional images using dual-cubemap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparative study of real-time semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdel-Razek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="587" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hohonet: 360 indoor holistic understanding with latent horizontal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="707" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic segmentation of panoramic images using a synthetic dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11169</biblScope>
		</imprint>
	</monogr>
	<note>p. 111690B. International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pass: Panoramic annular semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4171" to="4185" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ds-pass: Detailsensitive panoramic annular semantic segmentation through swaftnet for surrounding sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Capturing omni-range context for omnidirectional segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1376" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Orientation-aware semantic segmentation on icosahedron spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3533" to="3541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deeppanocontext: Panoramic 3d scene understanding with holistic scene context graph and relation-based optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12632" to="12641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
