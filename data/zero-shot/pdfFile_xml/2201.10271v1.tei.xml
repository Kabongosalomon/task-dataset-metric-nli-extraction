<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONVOLUTIONAL XFORMERS FOR VISION PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
							<email>asethi@iitb.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CONVOLUTIONAL XFORMERS FOR VISION PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Transformer ? Image classification ? Linear Attention ? Deep Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision transformers (ViTs) have found only limited practical use in processing images, in spite of their state-of-the-art accuracy on certain benchmarks. The reason for their limited use include their need for larger training datasets and more computational resources compared to convolutional neural networks (CNNs), owing to the quadratic complexity of their self-attention mechanism. We propose a linear attention-convolution hybrid architecture -Convolutional X-formers for Vision (CXV) -to overcome these limitations. We replace the quadratic attention with linear attention mechanisms, such as Performer, Nystr?mformer, and Linear Transformer, to reduce its GPU usage. Inductive prior for image data is provided by convolutional sub-layers, thereby eliminating the need for class token and positional embeddings used by the ViTs. We also propose a new training method where we use two different optimizers during different phases of training and show that it improves the top-1 image classification accuracy across different architectures. CXV outperforms other architectures, token mixers (e.g., ConvMixer, FNet and MLP Mixer), transformer models (e.g., ViT, CCT, CvT and hybrid Xformers), and ResNets for image classification in scenarios with limited data and GPU resources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Even though transformers <ref type="bibr" target="#b0">Vaswani et al. [2017]</ref>, <ref type="bibr" target="#b1">Devlin et al. [2019]</ref> have become the state-of-the-art and at par with humans for several natural language processing (NLP) tasks, their applications in vision has been severely limited by their quadratic complexity with respect to sequence length. Even low resolution images, when unrolled, become long 1D sequences of tens of thousands of pixels, and impose a large computational and memory burden on a GPU. A transformer, being a general architecture without an inductive prior, also requires a large number of training images for giving good generalization compared to convolutional models. It also needs extra architectural changes, including the addition of positional embeddings, to gather the positional information of various image pixels. This demand for large amount of data and GPU resources is not suitable for resource-constrained scenarios where data and GPU capabilities are limited, such as green or edge computing <ref type="bibr" target="#b2">Khan et al. [2021]</ref>.</p><p>On the other hand, CNNs have the inductive priors, such as translational equivariance due to convolutional weight sharing and partial scale invariance due to pooling, to handle 2D images which enables them to learn from smaller datasets with less computational expenditure. But, they fail to capture long range dependencies compared to transformers and require deeper networks with several layers to increase their receptive fields. Combining the efficiency and inductive priors of CNNs with the long range information capturing ability of attention can create better architectures that are suitable for computer vision applications.</p><p>We propose a novel hybrid architecture, Convolutional Xformers for Vision (CXV) 1 that uses both attention and convolutions for image classification. We use linear attention mechanisms in conjunction with convolutions in each layer to capture both long range dependencies and utilise the inductive priors of convolutions. This combination uses less data and GPU. The convolutions provides the prior which eliminates the need to unroll the image into sequence of pixels and patches and adding positional embeddings to keep the 2D positional information. The careful placement of convolutions and attention along with the layer normalizations enable our model to capture both spatial information and inter pixel relationships better. The resolution and size of the image is maintained throughout the layers. In other words, our model does not unroll the image before feeding to network nor does it need positional embeddings to capture location information.</p><p>Our architecture does not try to reduce the input sequence length using a hierarchical inverted pyramid architecture as done in other hybrid models, such as Convolutional vision Transformer (CvT) <ref type="bibr" target="#b3">Wu et al. [2021]</ref> and LeViT <ref type="bibr" target="#b4">Graham et al. [2021]</ref>, nor does it use sequence-pooling as is done in Compact Convolutional transformers (CCT) <ref type="bibr" target="#b5">Hassani et al. [2021]</ref>. Instead we figured out an optimal arrangement of existing architectural elements, such as linear attention mechanisms, convolutions, layer normalizations, and residual connections to develop an efficient architecture than can give better generalization at reduced computational costs.</p><p>Additionally, we found that adding pre-norm to attention and feed-forward layers was harming the performance. Replacing it with a single normalization in each layer helped. We also introduce Dual Optimizer Training (DualOpT), a novel training strategy using two different optimizers at two different phases of training. Application of DualOpT in our experiments resulted in faster convergence and higher accuracy compared to using a single optimizer. Our extensive ablation studies provide several new insights about designing novel transformers for images. We compare our model with most of the established architectures for image classification, including convolutional models <ref type="bibr">(ResNet-18 and ResNet-34 He et al. [2015]</ref>), token mixing models (FNet Lee- <ref type="bibr" target="#b7">Thorp et al. [2021]</ref>, <ref type="bibr">MLP-Mixer Tolstikhin et al. [2021], WaveMix P and</ref><ref type="bibr">Sethi [2022]</ref> and ConvMixer <ref type="bibr" target="#b10">Anonymous [2022]</ref>), and vision transformers <ref type="bibr">(ViT Dosovitskiy et al. [2021]</ref>, Hybrid Vision Nystr?mformer (ViN), Hybrid Vision Performer (ViP) <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref>, Hybrid Vision Linear Transformer (ViLT), Compact Convolutional Transformer (CCT) <ref type="bibr" target="#b5">Hassani et al. [2021]</ref> and Convolutional vision Transformer (CvT) <ref type="bibr" target="#b3">Wu et al. [2021]</ref>). Our experiments show that CXV outperforms all of them in low resource settings for the same number of parameters. When we compare the GPU usage and accuracy, we find that our CXV provides improved accuracy with less data without consuming as much GPU memory and computations compared to the other models. We created a new hybrid Xformer architecture, Hybrid Vision Linear Transformer (Hybrid ViLT) using Linear Transformer <ref type="bibr">Katharopoulos et al. [2020]</ref> as the linear attention mechanism. Hybrid ViLT performs on par with other hybrid xformers (hybrid ViN and hybrid ViP) but consumes only half of the GPU compared to them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>ViT  overcame the quadratic complexity of self-attention by breaking the image into patches of size 16x16, which significantly reduced the sequence length upon unrolling. It also used positional embedding to capture the spatial information of patches and added a learnable class embedding to capture the global image representation for classification. But, ViT still suffered from requirement for a large amount of data to provide good generalization. Additionally, ViT needed GPUs with large memories to fit state-of-the-art models, which also prevented its widespread use. <ref type="bibr">CCT Hassani et al. [2021]</ref>, <ref type="bibr">LeViT Graham et al. [2021]</ref> and CvT <ref type="bibr" target="#b3">Wu et al. [2021]</ref> used convolutions to provide inductive priors to the vision transformers, which was shown to improve their performance. LeViT replaced patch-wise projections with convolutional embeddings and used 2D relative positional biases instead of initial absolute positional bias used in ViT. It also down-sampled the image in stages using an extra non-linearity in attention and also replaced layer-norm with batch-norm. CvT used a hierarchy of transformers containing a new convolutional token embedding, and a convolutional transformer block. CCT eliminated the class token and positional embeddings and used a sequence pooling strategy and convolutions for better performance. CCT also showed that transformer models can be trained to achieve state-of-the-art generalization using less data. Hybrid Xformers <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref> replaced the quadratic attention with linear attention mechanisms such as Performer <ref type="bibr" target="#b13">Choromanski et al. [2021]</ref> and Nystr?mformer <ref type="bibr" target="#b14">Xiong et al. [2021]</ref>, and also used convolutional layers to generate embeddings. They showed good performance with significant reduction in GPU usage. The Hybrid Vision Linformer (ViL) was shown to perform poorly in their experiments on image classification. Therefore, we used the hybrid xformer architecture to create Hybrid Vision Linear Transformer (Hybrid ViLT), where we used Linear Transformer as a linear attention module.</p><p>Search for alternative architectures that mix information between tokens like self-attention, but require fewer computations and GPU memories, and require less data to generalize has led to the development of many token-mixer models. FNet Lee- <ref type="bibr" target="#b7">Thorp et al. [2021]</ref> replaces the self-attention layer in the transformer with the Fourier transform and has shown to perform well compared to self-attention models, such as <ref type="bibr">BERT Devlin et al. [2019]</ref>. We use FNet for image classification similar to experimentation in <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref>. MLP-mixer <ref type="bibr" target="#b8">Tolstikhin et al. [2021]</ref> replaces self-attention using two multi-layer perceptrons (MLPs) which are applied independently, first to image patches and then across patches. ConvMixer <ref type="bibr" target="#b10">Anonymous [2022]</ref> works similar to MLP-Mixer but uses standard convolutions instead of MLPs for mixing of information in spatial and channel dimensions. We compare the performance and GPU usage of our CXV models with these different architectures and show that CXV performs better in low-data low-GPU settings.</p><p>A jump in accuracy was obtained by convolutional and recurrent networks while training when optimizer was switched from Adam to Stochastic gradient descent (SWATS) for various machine learning tasks Keskar and Socher <ref type="bibr">[2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CXV Model Architecture</head><p>The overall architecture of Convolutional Xformer for Vision (CXV) is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Before describing the overall architecture in detail, we explain the motivation behind and the design of some of its key components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear Attention</head><p>It has been shown that replacing quadratic attention in ViT with linear attention mechanisms <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref>, such as Nystr?mformer <ref type="bibr" target="#b14">Xiong et al. [2021]</ref> and Performer <ref type="bibr" target="#b13">Choromanski et al. [2021]</ref> can reduce their GPU consumption considerably while improving their performance in low data image classification. The use of Xformers will enable us to handle longer image pixel sequences without incurring much computational costs. We use Nystr?mformer, Performer, and Linear Transformer Katharopoulos et al. <ref type="bibr">[2020]</ref> in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Embedding</head><p>It has been shown that using convolutional layers to generate the pixel embeddings is better <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref>, <ref type="bibr" target="#b4">Graham et al. [2021]</ref> than using the linear layers as used in ViT  as the convolutional layers provide the inductive prior suitable for images to the transformer network significantly reducing the need for more training data. We also use convolutional layers to generate the embeddings which will be fed to the subsequent layers of the network. In case of high resolution images, kernel size and stride can be adjusted in these convolutional layers so that the sequence length of image pixel remains within the range as fixed by the available GPU size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Convolutional Sub-layer</head><p>With Convolutional vision Transformer (CvT) architecture <ref type="bibr" target="#b3">Wu et al. [2021]</ref>, it was shown that adding a convolutional sublayer in the transformer layer improves performance and robustness, and it also maintains a high degree of computational and memory efficiency. But, CvT is a multi-stage hierarchical architecture where convolutional token embedding performed an overlapping convolution operation with stride on a 2D-reshaped token map. It also progressively decreased the sequence length while simultaneously increasing the dimension of token features across stages. They also used one convolutional sub-layer per stage (three overall) instead of adding one per layer.</p><p>We use a convolutional sub-layer in our transformer layers to provide the spatial information and inductive priors to the linear attention sub-layer that follows. The use of convolutional sub-layer enables the architecture to maintain the 2D image shape throughout the network, except within the linear attention sub-layer where it is unrolled as a sequence of image tokens. The presence of convolutional sub-layer also eliminates the need for additional positional embeddings which are otherwise needed in other ViT architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Layer Normalization</head><p>Even though the original transformer <ref type="bibr" target="#b0">Vaswani et al. [2017]</ref> used post-norm, where layer normalization occurs after both attention and feed-forward sub-layers in order to reduce the variance of the inputs to the following sub-layer, Vision Transformers Dosovitskiy et al. <ref type="bibr">[2021]</ref> have been using pre-norm residual units, where layer normalization occurs before attention and feed-forward sub-layers. Post-norm transformers were observed to have larger magnitude gradient in later layers compared to initial layers <ref type="bibr">Xiong et al. [2020]</ref>. Pre-norm has been shown to make back-propagation more efficient in training deep transformer models and often yielded improved performance.</p><p>It has been shown that in shallow transformer architectures (? 6 layers), pre-norm tends to degrade the performance Nguyen and Salazar <ref type="bibr">[2019]</ref>. LeViT architecture removes the pre-norm completely from the attention and feed-forward sub-layers for faster inference and only uses a batch normalization in the convolutional layers <ref type="bibr" target="#b4">Graham et al. [2021]</ref>. <ref type="bibr" target="#b17">Shleifer et al. [2021]</ref> showed that while pre-norm improves stability over post norm, it creates the opposite issue on gradients; gradients at earlier layers tend to be larger than gradients at later layers.</p><p>In our CXV architecture, we remove pre-norm used in ViT and replace it with a single layer normalization placed between the convolutional block and self-attention in the each layer. The presence of one layer normalization per layer instead of two is enough for shallow networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Overall Architecture of CXV</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the input image is fed to the convolutional layers, which generates the embeddings of an appropriate dimension. We use two convolutional layers to increase the channel dimension from three to the required embedding dimension. The kernel and strides are chosen to ensure that 32?32 sized 2D feature maps are generated and the number of feature maps matches the embedding dimension.</p><p>The output from the convolutional embedding layer is then send to the convolutional xformer layers. In each layer, the input is passed through a convolutional sub-layer of kernel size 3x3, with stride and padding of 1. Then its output is normalized in the channel dimension in the normalization sub-layer. The output after normalization is given to the linear attention sub-layer where it is processed by Performer, Nystr?mformer or Linear Transformer. The output from attention sub-layer is finally passed through an multi-layer perceptron (MLP) before it is send to the next layer. Residual connections <ref type="bibr" target="#b6">He et al. [2015]</ref> are used with both the linear attention and MLP sub-layers.</p><p>The output from the final layer is passed through a pooling layer, where it is average pooled and fed to an MLP head, which gives the output class. The resolution and size of input remains the same throughout the CXV layers. The 2D structure of the image is also maintained everywhere except within the linear attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">DualOpT: Dual-Optimizer Training</head><p>We implemented a new training paradigm, Dual-Optimizer training (DualOpT) where two different optimizers where used to train the model at different phases of training. During the initial epochs of training, we used AdamW optimizer which showed faster convergence to final validation accuracy. Once the top-1 accuracy converged to a fixed value which remained unchanged for 20 epochs, we changed the optimizer to SGD. This change showed a sudden improvement in accuracy within a few epochs. The number of epochs needed in the training phase of the second optimizer is much shorter than the first optimizer for reaching the final top-1 accuracy. The accuracy obtained was much higher than the value obtained by using either of the two optimizers alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Our experiments were conducted using multiple data sets, multiple comparative neural architectures, and variations of our own architecture for ablation studies. For the model nomenclature, we use the name of the model followed by the number of layers and number of heads as Model-layer/heads, e.g., the CCT model with 6 layers and 8 heads is CCT-6/8. For models that does not have multiple heads, we omit the heads part in the model name, e.g., FNet having 8 layers is FNet-8. For CXV model names, we replace the X (Xformer) with the linear attention mechanism used in the model, e.g., CPV is Convolutional Performer for Vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Other Models Compared</head><p>Except ConvMixer, MLP-Mixer, and WaveMix, all other models used in our experiments have 128-dimensional embeddings. ConvMixer and MLP-Mixer used 256-dimensional embeddings and WaveMix used 64 dimension embeddings. A dropout of 0.5 was used across all models. We used 64 benchmark points in Nystr?mformer and a local window size of 256 with ReLU non-linearity as the kernel function in Performer.</p><p>For the DualOpT training of the proposed CXV models, AdamW optimizer with ? = 0.001 (learning rate), ? 1 = 0.9 and ? 2 = 0.999 were used for computing running averages of gradient and its square, = 10 ?8 , and 0.01 as weight decay coefficient was used in the initial phase of training and SGD (stochastic gradient descent) with learning rate of 0.001 and momentum = 0.9 was used for the final phase.</p><p>We used automatic mixed precision in PyTorch to make the training faster and consume less GPU memory. Experiments were done with 16 GB Tesla P100-PCIe and Tesla T4 GPUs available in Kaggle and Google Colab. Number of model parameters, MACs, top-1 % accuracy and GPU usage for a batch size of 32 are reported. All reported top-1 % accuracy values are best out of three runs based on accepted reporting protocols <ref type="bibr" target="#b5">Hassani et al. [2021]</ref>.</p><p>All models use MLP dimension that was double the size of its input embedding dimension. Only WaveMix architecture used 32 as MLP dimension. The hybrid xformers used three convolutional layers with 32, 64, and 128 kernels respectively of size 3?3 with stride of one with padding to generate the embeddings. We used learnable position embedding and 3?3 convolutional and pooling kernels having stride equal to 1 and padding in CCT. For CvT, we used two attention modules in the first and second stages and one in the third stage with 3?3 convolutions in each stage without down-sampling.</p><p>While training with Tiny ImageNet dataset, the stride of initial convolution layer was adjusted to create model of same size as those used for CIFAR-10 and CIFAR-100. In ViT, we used a patch size of 2?2 for training Tiny ImageNet. All models were trained on the three datasets without using any data augmentations or warm up. RandAugment Cubuk et al. <ref type="bibr">[2019]</ref> was used only for the CXV later to see the improvements from augmentations (see ablation studies).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Classification Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p># Params GPU (GB) MACs CIFAR-10 CIFAR-100 Tiny ImageNet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Models</head><p>ResNet  <ref type="table" target="#tab_0">Table 1</ref> shows the top-1 accuracy of all the models trained on the each of the three datasets. The results shows that CXV models generalize better while also consuming lesser GPU memory compared to all the other models for the three datasets. CXV models require orders of magnitude fewer parameters compared to ResNets and yet they outperform ResNets in image classification by 6% in CIFAR-10 and 16% in Tiny ImageNet datasets. <ref type="figure" target="#fig_2">Fig. 2</ref> shows that token-mixing models (green triangles) consume very less GPU memory compared to transformer models (blue circles) and they still perform comparable to self-attention. WaveMix and ConvMixer outperformed conventional transformer architectures by significant margin. ConvMixer performed better than CXVs in CIFAR-100 dataset but it consumed almost double the GPU RAM consumed by CLTV (Convolutional Linear Transformer for Vision). WaveMix was the most GPU efficient model among all models tested. Both FNet and MLP-Mixer performed poorly compared to other models even though they consumed very little GPU. Even though MLP-Mixer had similar number of parameters compared to ResNet-34, it consumed twice the GPU memory and under-performed compared to it, which shows, once again, that CNNs are better at handling images than MLPs.</p><p>Among the transformer models, we observe that hybrid ViX models consume the least amount of GPU RAM while giving comparable performance to CCT and CvT. CXV models (red squares) perform ? 10% better in CIFAR-10 while consuming two orders of magnitude less GPU RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Studies</head><p>CXV models were tested extensively using variety of architectural modifications to build the optimal architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet-18</head><p>ResNet  <ref type="table">Table 2</ref>: Variation of accuracy with number of initial convolutional embedding layers in CNV for classification of CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architectural Variations</head><p>We compare the performance of CXV with other efficient transformers, such as hybrid vision nystromformer and hybrid vision performer that were shown to perform well, but hybrid vision linformer performed poorly in experiments. So we replaced hybrid vision linformer with another linear attention mechanism, the linear transformer. We used the hybrid xformer architecture used by <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref> and used Linear Transformer Katharopoulos et al.</p><p>[2020] as the attention mechanism. Linear transformer express self-attention as a linear dot-product of kernel feature maps and uses the associative property of matrix products to reduce complexity. To suit the image data, Q, K and V matrices were created using convolutional operations 3 This allowed us to pass the input in 2D form to the attention sub-layer eliminating the need for the extra class token and positional embeddings. The architecture of Hybrid ViLT (Vision Linear Transformer) is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Hybrid ViLT model consumed less than half the GPU used by other hybrid ViX models and performed almost comparable to them in all three datasets, as shown in <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Reducing the resolution of the input image by removing padding from initial convolution embedding layers showed around 17% reduction in GPU usage but only a reduction in accuracy by less than 2%. <ref type="table">Table 2</ref> shows that accuracy first increases and then starts to decrease as we increase the number of convolutional layers for creating the embeddings.</p><p>We experimented with using GeLU activation in between the initial convolutional embedding layers and observed a decrease in accuracy by about 2 percentage points. When we used GeLU after the convolution sub-layer in each layer, it was also shown to decrease the final accuracy of the model. A similar decrease in accuracy was observed when we used 2D max-pooling as used in CCT model.</p><p>We also experimented with decreasing the convolutional sub-layers by using only one convolutional sub-layer for every two layers and observed a degradation in performance. So we concluded that placement of a convolutional sub-layer is essential for improvement in performance.</p><p>We also tried different combinations of residual connections by using residual connection around the convolutional sub-layer and around both the convolutional and layer normalization sub-layers. These extra residual connection did not lead to increase in accuracy. This may be due to the shallowness (? 6 layers) of the networks and these connections might become essential while using deeper networks with more layers.</p><p>Since the network processes the images mostly in 2D and unrolling happens only within the attention sub-layers, it was observed that the accuracy did not improve by addition of positional embeddings (both 1D learnable  and rotary <ref type="bibr" target="#b20">Su et al. [2021]</ref>). This might be due to the presence of the convolutional sub-layers which possess the 2D inductive priors needed to handle 2D images and they provide the information to the attention sub-layers eliminating the need for extra positional information.</p><p>Replacing layer-normalization with batch-normalization was shown to reduce the accuracy by over four percentage points. We tried to reduce the number of layer-normalization by placing a layer-norm sub-layer in every alternate layers and observed the loss does not converge when there is an absence of normalization in each layer. Adding pre-norm to attention and feed-forward sub-layers was found to degrade the performance considerably. <ref type="table" target="#tab_2">Table 3</ref> shows the comparison of top-1 accuracy obtained by few models while using one optimizer (AdamW) and the performance gain obtained while using DualOpT (AdamW + SGD). We observed similar performance gains across all models trained in all the three datasets. The accuracy obtained by DualOpT was higher than what was obtained when we trained using only AdamW and only SGD. AdamW was shown to cause convergence during initial phase of training   <ref type="table">Table 4</ref>: Performance of CXV models using RandAugment and after further post-training using the un-augmented CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">DualOpT</head><p>and SGD was shown to be faster towards the final phase. While implementing DualOpT, it was observed across models that when shifting from AdamW to SGD, the training and testing accuracy jumps by a significant margin in the first epoch and the speed of convergence increases significantly. The improvement in performance using DualOpT was also observed while using RandAugment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Augmentation</head><p>We assessed the effect of data augmentation using RandAugment Cubuk et al.</p><p>[2019] on CXV models with number of augmentation (M ) and magnitude of augmentations (N ) set to one for CNV and CPV, and three for CLTV. The model parameters were same as used in Section 4.2. The accuracy increased significantly when RandAugment was used as shown in <ref type="table">Table 4</ref>.</p><p>It was observed during training that training accuracy was always stagnating at a lower value compared to its corresponding value when training without using RandAugment, even after test accuracy converged to its final value. So, after the final test accuracy was achieved, we trained the model using the dataset without RandAugment. This post-training on the un-augmented dataset caused a further increase in performance of the model as shown in <ref type="table">Table 4</ref>. Both training using RandAugment and post-training without RandAgument was done using DualOpT.</p><p>We conjecture that the increase in accuracy observed in post-training to be the fine-tuning effect created from the model seeing the un-augmented data for the first time which is equivalent to seeing a new dataset. This procedure of training improved the accuracy of CXV models by ? 3 percentage points. We believe that the results reported can be further improved with more extensive tuning of the hyper-parameters.</p><p>We introduced a new hybrid neural architecture for processing images that combines the inductive priors from convolutions and self-attention from transformers that requires smaller training data and less GPU memory for state-ofthe-art generalization. Our results using linear transformers for image classification agrees with previous studies <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref> that using linear attention mechanism can reduce the GPU consumption without affecting generalization. Adding convolutional sub-layers in our architecture ensured that our CXV models out-perform the current data-efficient architectures, such as CCT and CvT, considerably using one fourth the GPU memory for a given batch size. For our experiments we also constructed a new hybrid ViX model, the hybrid Vision Linear Transformer (ViLT), and showed that it consumes even lesser GPU RAM than other hybrid ViX models mentioned in <ref type="bibr" target="#b12">Jeevan and Sethi [2021]</ref>. Our CXV models use order of magnitude fewer parameters than convolutional models, such as ResNets while also requiring orders of magnitude lesser GPU RAM than transformer models. Our CXV models can be used for training computer vision models in GPU resource-limited settings (i.e., RAM, cores, power).</p><p>Additionally, we introduced a new training paradigm where we show that using two different optimizers at different phases of training can significantly improve model performance. This result was observed across different architectures, including transformers, ResNets, and token-mixers. DualOpT training needs to be investigated more using different initialisations, hyper-parameter tuning, and learning rates to shed more light into the differences in loss surfaces near and far away from the solution. Additionally, further study on which optimizers are better for initial and final phases also needs to be conducted.</p><p>Among the token-mixing architectures, we found that FNet and MLP-Mixer were not suitable for vision applications but WaveMix and ConvMixer perform at par with the transformer models while consuming less GPU RAM. Wavelets use 2D wavelet transform and ConvMixer uses depth-wise and spatial convolutions, providing inductive priors useful for processing images. This shows that if we use model architectures that can use image priors, it will improve the accuracy, lead to faster training with less data, and consume orders of magnitude less GPU.</p><p>Undoubtedly, further experiments with hyper-parameter tuning, warm up, different data-augmentations, larger datasets and different vision tasks can provide new directions for improving the accuracy of these models in low data, smaller GPU regime. Our results confirm the utility of many of the paths taken towards developing alternatives to purely convolutional or purely attention-based architectures for vision. The success of vision transformers with its impressive results in terms of accuracy comes with orders of magnitude higher costs in terms of training data and GPU RAM, essentially denying the possibility of training such models from scratch using resources available in most settings. The development of hybrid architectures and token-mixers can open up a path to develop alternative model architectures that can provide state-of-the-art results by exploiting the domain-specific inductive priors unique to images (and other specialized data, such as audio) with considerable savings in training data and GPU requirements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of CXV</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The CIFAR-10 (MIT License), CIFAR-100 (MIT License)<ref type="bibr" target="#b18">Krizhevsky [2009]</ref>, and the Tiny ImageNet (MIT License) Le and Yang[2015]  datasets were used in our experiments to compare the performance of different models in low resource scenario. We chose ResNet-18 and ResNet-34<ref type="bibr" target="#b6">He et al. [2015]</ref> from pure convolutional models; FNet Lee-Thorp et al. [2021], MLP-Mixer Tolstikhin et al. [2021], WaveMix P and Sethi [2022], and ConvMixer Anonymous [2022] from token mixing models; and ViT Dosovitskiy et al. [2021], Hybrid ViN, Hybrid ViP Jeevan and Sethi [2021], Hybrid ViLT, CCT Hassani et al. [2021], and CvT Wu et al. [2021] 2 from transformer models for comparison with our CXV. Except for ResNets and MLP Mixer, all other models with similar number of parameters (? 1.3 M) as the CXV were used in our experiments. The number of layers, heads and embedding dimensions were adjusted to keep the numbers of parameters same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of Top-1 Accuracy with GPU usage for various models on CIFAR-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of Hybrid ViLT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of top-1 accuracy and computational costs for all the models in image classification using different datasets. The GPU consumption shown is for a batch size of 32. Warm up and data augmentations were not used for training.</figDesc><table><row><cell>-18</cell><cell>11.2 M</cell><cell>0.6</cell><cell>0.56 G</cell><cell>86.29</cell><cell>59.15</cell><cell>43.02</cell></row><row><cell>ResNet-34</cell><cell>21.3 M</cell><cell>0.7</cell><cell>1.16 G</cell><cell>87.97</cell><cell>56.05</cell><cell>42.65</cell></row><row><cell>Mixing Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FNet-10</cell><cell>1.3 M</cell><cell>2.3</cell><cell>1.34 G</cell><cell>51.05</cell><cell>11.97</cell><cell>8.14</cell></row><row><cell>MLP Mixer-5</cell><cell>21.3 M</cell><cell>1.5</cell><cell>3.02 G</cell><cell>60.26</cell><cell>34.81</cell><cell>20.26</cell></row><row><cell>WaveMix-5</cell><cell>1.4 M</cell><cell>0.4</cell><cell>2.59 G</cell><cell>83.71</cell><cell>51.62</cell><cell>32.70</cell></row><row><cell>ConvMixer-16</cell><cell>1.3 M</cell><cell>3.5</cell><cell>1.37 G</cell><cell>88.46</cell><cell>61.80</cell><cell>45.39</cell></row><row><cell>Transformer Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-10/4</cell><cell>1.3 M</cell><cell>14.7</cell><cell>1.34 G</cell><cell>57.53</cell><cell>30.80</cell><cell>23.18</cell></row><row><cell>Hybrid ViN-6/8</cell><cell>1.3 M</cell><cell>5.3</cell><cell>1.41 G</cell><cell>77.96</cell><cell>50.06</cell><cell>36.84</cell></row><row><cell>Hybrid ViP-6/8</cell><cell>1.3 M</cell><cell>5.9</cell><cell>1.31 G</cell><cell>77.54</cell><cell>57.22</cell><cell>38.85</cell></row><row><cell>Hybrid ViLT-6/8</cell><cell>1.3 M</cell><cell>2.6</cell><cell>0.33 G</cell><cell>78.34</cell><cell>52.38</cell><cell>37.17</cell></row><row><cell>CCT-6/4</cell><cell>1.3 M</cell><cell>13.6</cell><cell>1.32 G</cell><cell>82.66</cell><cell>58.34</cell><cell>35.21</cell></row><row><cell>CvT-5/4</cell><cell>1.3 M</cell><cell>9.4</cell><cell>1.32 G</cell><cell>77.90</cell><cell>57.82</cell><cell>39.81</cell></row><row><cell cols="2">Convolutional Xformers for Vision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CNV-5/4</cell><cell>1.3 M</cell><cell>3.1</cell><cell>1.39 G</cell><cell>89.56</cell><cell>59.23</cell><cell>49.56</cell></row><row><cell>CLTV-5/4</cell><cell>1.3 M</cell><cell>1.8</cell><cell>1.38 G</cell><cell>86.99</cell><cell>60.11</cell><cell>46.69</cell></row><row><cell>CPV-5/4</cell><cell>1.3 M</cell><cell>3.2</cell><cell>1.37 G</cell><cell>91.42</cell><cell>57.34</cell><cell>47.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Percentage increase in the top-1 accuracy of few models while using DualOpT compared to accuracy when only one optimizer is used across different datasets.</figDesc><table><row><cell>Model</cell><cell>Accuracy with RandAugment (%)</cell><cell>Accuray after Post-training (%)</cell></row><row><cell>CNV-5/4</cell><cell>91.98</cell><cell>92.26</cell></row><row><cell>CPV-5/4</cell><cell>92.21</cell><cell>94.46</cell></row><row><cell>CLTV-5/4</cell><cell>88.16</cell><cell>91.40</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our Code is available at https://github.com/pranavphoenix/CXV</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Base code: https:github.com/lucidrains/vit-pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Base code: https://github.com/lucidrains/linear-attention-transformer</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fnet: Mixing tokens with fourier transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Wavemix: Multi-resolution token mixing for images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tBoSm4hUWV" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Patches are all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TVHS5Y4dNvM.underreview" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision xformers: Efficient attention for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran?ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from adam to sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirish</forename><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<editor>Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>On layer normalization in the transformer architecture</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salazar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Normformer: Improved transformer pretraining with extra normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ya Le and X. Yang. Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
