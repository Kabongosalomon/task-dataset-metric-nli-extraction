<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Vision-and-Language Tasks via Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
							<email>jielei@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
							<email>haotan@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unifying Vision-and-Language Tasks via Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent taskspecific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized singletask models. Our code is publicly available at: https://github.com/j-min/VL-T5</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mirroring the success of the pretraining-finetuning paradigm with transformer language models <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, recent vision-and-language transformers ; <ref type="bibr" target="#b32">Lu et al. (2019)</ref>; <ref type="bibr" target="#b6">Chen et al. (2020)</ref>; <ref type="bibr">Li et al. (2020b)</ref>, Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</p><p>"vqa: what is the man jumping over?"</p><p>"image text match: A cat is lying on a bed" "visual grounding: yellow fire hydrant" "span prediction: A &lt;text_1&gt; is &lt;text_2&gt; over fire hydrant" "&lt;text_1&gt; man &lt;text_2&gt; jumping" "fire hydrant" "&lt;vis_3&gt;"  <ref type="figure">Figure 1</ref>. Our unified framework for learning vision-and-language tasks. While existing methods require designing task-specific architectures for different tasks, our framework unifies them together as generating text labels conditioned on multimodal inputs.</p><p>inter alia) have also been adopted in a wide range of visionand-language tasks. These models are firstly pretrained on large image-text corpus (e.g., COCO Caption <ref type="bibr" target="#b5">(Chen et al., 2015)</ref>), then finetuned on downstream tasks (e.g., visual question answering  and referring expression comprehension <ref type="bibr" target="#b34">(Mao et al., 2016)</ref>), which outperformed many previous non-pretraining-finetuning methods.</p><p>For each pretraining or downstream task, existing visionand-language transformers typically require designing taskspecific, separately-parameterized architectures on top of the transformer encoder (e.g., multi-label sigmoid classifier for visual question answering, and softmax classifier for referring expression comprehension). However, the reasoning skills required by these tasks overlap significantly. Consider the example in <ref type="figure">Fig. 1</ref>. Both answering the question "What is the man jumping over?" and grounding an image region corresponding to the phrase "yellow fire hydrant" require recognizing the object "fire hydrant". In addition, the labels for these tasks can be easily expressed in text. For instance, we can assign a region id (e.g., "&lt;vis 3&gt;", a special text arXiv:2102.02779v2 [cs.CL] 23 May 2021 token) to a specific region in the image, and then the referring expression comprehension task can be expressed as generating the correct region id. For visual question answering, the labels are already in text, although existing approaches <ref type="bibr" target="#b1">(Anderson et al., 2018;</ref><ref type="bibr" target="#b6">Chen et al., 2020)</ref> tackle the task as learning a multi-label classifier over a fixed set of frequent answers (See <ref type="figure">Fig. 3</ref>).</p><p>Hence, in order to alleviate these hassles of designing taskspecific architectures, we propose a unified framework for vision-and-language learning via generating labels in text. Specifically, we extend off-the-shelf pretrained language models T5 <ref type="bibr" target="#b44">(Raffel et al., 2020)</ref> and BART <ref type="bibr" target="#b25">(Lewis et al., 2020)</ref> with visual understanding ability, named 'VL-T5' and 'VL-BART'. In contrast to existing methods that train different architectures for each pretraining and downstream task, our models tackle all tasks with the same language modeling head. To learn a new task, we can simply rewrite its input and output in text, without the need of adding extra parameters or designing new architectures and objectives. In addition, we can leverage the text generation ability of pretrained language models when making predictions. This is especially helpful when we answer open-ended questions that require non-trivial answers, where discriminative methods can only answer from a predefined set of frequent candidates, while our models can generate open-ended natural language answers.</p><p>To evaluate the effectiveness of our generative modeling approach, we compare our models against recent visionand-language transformers on a diverse set of 7 downstream benchmarks, including visual question answering on VQA  and GQA <ref type="bibr">(Hudson &amp; Manning, 2019)</ref>, referring expression comprehension on <ref type="bibr">Ref-COCOg (Mao et al., 2016)</ref>, natural language visual reasoning on NLVR 2 <ref type="bibr" target="#b50">(Suhr et al., 2019)</ref>, visual commonsense reasoning on VCR <ref type="bibr" target="#b66">(Zellers et al., 2019)</ref>, image captioning on COCO Caption <ref type="bibr" target="#b5">(Chen et al., 2015)</ref>, and multimodal machine translation on Multi30K <ref type="bibr" target="#b10">(Elliott et al., 2016)</ref>. Our unified generative method reaches comparable performance to recent state-of-the-art vision-and-language pretraining methods. This is especially interesting because we use the same unified language modeling architecture with the same maximum likelihood estimation (MLE) objective for all the tasks, while existing approaches use task-specific architectures and objective functions. In addition, we found that our generative models have better generalization ability compared to the discriminative versions in the rare-answer scenario on visual question answering, when ground truth answers for given questions are rarely seen during training. Finally, we also experiment with our unified framework under the multi-task learning setup on all 7 downstream tasks. With a single architecture and a single set of weights, our model achieves similar performance to separately optimized single-task models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Vision-and-Language pretraining: Large-scale language pretraining with transformers <ref type="bibr" target="#b54">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Devlin et al., 2019;</ref><ref type="bibr" target="#b30">Liu et al., 2019;</ref><ref type="bibr" target="#b23">Lan et al., 2020;</ref><ref type="bibr" target="#b8">Clark et al., 2020;</ref><ref type="bibr" target="#b61">Yang et al., 2019;</ref><ref type="bibr" target="#b44">Raffel et al., 2020)</ref> have achieved remarkable success for many natural language understanding tasks <ref type="bibr" target="#b45">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b65">Zellers et al., 2018;</ref><ref type="bibr" target="#b56">Wang et al., 2018;</ref><ref type="bibr" target="#b57">Williams et al., 2017)</ref>. Following this success, image+text pretraining models <ref type="bibr" target="#b6">Chen et al., 2020;</ref><ref type="bibr" target="#b16">Huang et al., 2020;</ref><ref type="bibr">Li et al., 2020b;</ref><ref type="bibr">Cho et al., 2020;</ref><ref type="bibr" target="#b43">Radford et al., 2021;</ref><ref type="bibr" target="#b67">Zhang et al., 2021)</ref> and video+text pretraining models <ref type="bibr" target="#b52">(Sun et al., 2019b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b26">Li et al., 2020a;</ref><ref type="bibr" target="#b70">Zhu &amp; Yang, 2020;</ref><ref type="bibr" target="#b36">Miech et al., 2020)</ref> have also shown to perform better than previous nonpretraining approaches <ref type="bibr" target="#b1">Anderson et al., 2018;</ref><ref type="bibr" target="#b21">Kim et al., 2018;</ref><ref type="bibr" target="#b64">Yu et al., 2018b)</ref> in a wide range of discriminative <ref type="bibr">Hudson &amp; Manning, 2019;</ref><ref type="bibr" target="#b24">Lei et al., 2018;</ref><ref type="bibr" target="#b34">Mao et al., 2016;</ref><ref type="bibr" target="#b60">Xu et al., 2016;</ref><ref type="bibr" target="#b68">Zhou et al., 2018)</ref> and generative tasks <ref type="bibr" target="#b5">(Chen et al., 2015;</ref><ref type="bibr" target="#b60">Xu et al., 2016;</ref><ref type="bibr" target="#b68">Zhou et al., 2018)</ref>. In this work, we focus on image+text tasks. While existing image+text models mostly use task-specific architectures and objectives, we seek to design a unified framework across different tasks.</p><p>Unified frameworks: One line of work focus on solving natural language processing tasks in a unified format, such as question answering <ref type="bibr" target="#b35">(Mccann et al., 2018)</ref>, span prediction <ref type="bibr" target="#b19">(Keskar et al., 2019)</ref>, or text generation <ref type="bibr" target="#b44">(Raffel et al., 2020;</ref><ref type="bibr" target="#b3">Brown et al., 2020;</ref><ref type="bibr" target="#b20">Khashabi et al., 2020)</ref>. These unified frameworks provide efficient knowledge sharing among different tasks and make it easy to leverage pretrained language models. In relation to these works, we propose to unify previously separately modeled vision-and-language tasks in a single unified format, via text generation, conditioned on multimodal inputs from the image and the textual context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>We propose a new framework that unifies vision-andlanguage problems as multimodal conditional text generation. We introduce VL-T5 and VL-BART based on two pretrained transformer language models: T5 Base <ref type="bibr" target="#b44">(Raffel et al., 2020)</ref> and BART Base <ref type="bibr" target="#b25">(Lewis et al., 2020)</ref>. Specifically, we extend their text encoders to multimodal encoders by incorporating image region embeddings as additional input. The overall architecture of our framework is shown in <ref type="figure">Fig. 2</ref>. Since the architecture differences between VL-T5 and VL-BART are minor, we use VL-T5 as an example to illustrate our framework in details in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual Embeddings</head><p>We represent an input image v with n=36 object regions from a Faster R-CNN <ref type="bibr" target="#b46">(Ren et al., 2015)</ref>   <ref type="figure">Figure 2</ref>. An illustration of our VL-T5 and VL-BART architectures for visual grounding task. Instead of task-specific architectures, our models use text prefixes to adapt to different tasks. The green block in (a) refers to visual embeddings. (b) shows the components of visual embedding. Note that we reuse the text embeddings of visual sentinel tokens (ex. &lt;vis 3&gt;) as region id embeddings, which allows our models to tackle many discriminative vision-language tasks as text generation, including visual grounding.</p><p>Genome <ref type="bibr" target="#b22">(Krishna et al., 2016)</ref> for object and attribute classification <ref type="bibr" target="#b1">(Anderson et al., 2018)</ref>. As shown in <ref type="figure">Fig. 2</ref>   <ref type="bibr">vlin et al., 2019)</ref>. Image ids are used to discriminate regions from different images, and is used when multiple images are given to the model (i.e., in NLVR 2 <ref type="bibr" target="#b50">(Suhr et al., 2019)</ref>, models take two input images). The final visual embeddings are denoted as e v ={e v 1 , . . . , e v n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Text Embeddings</head><p>Instead of designing task-specific architectures, we add different prefixes to the original input text to adapt to different tasks, as shown in <ref type="table">Table.</ref> 1 1 . This augmented input text x is then tokenized as {x 1 , . . . , x |x| } and encoded as learned embedding e x = {e x 1 , . . . , e x |x| }. The embedding parameters are shared by the encoder, decoder, and language modeling head <ref type="bibr" target="#b42">(Press &amp; Wolf, 2017)</ref>. Since the attention layers are permutation-invariant, BART learns positional embeddings <ref type="bibr" target="#b54">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Devlin et al., 2019)</ref> for absolute token positions and adds them to the token embeddings. In contrast, T5 adds relative position bias to each self-attention layer <ref type="bibr" target="#b47">(Shaw et al., 2018)</ref>. Our models follow the positional embedding configurations of their text backbone models.</p><p>In addition to the original vocabulary of T5 and BART, we introduce visual sentinel tokens {&lt;vis 1&gt;, . . . , &lt;vis n&gt;}, which corresponds to image regions. As illustrated in <ref type="figure">Fig. 2</ref>, we use the text embeddings of visual sentinel tokens as region id embeddings in Sec. 3.1. The embedding sharing enables our model to build the corre-1 Note that since we use simple prefixes (e.g., "vqa:" for VQA task), it is likely that engineering in text prompts  would improve the accuracy of our methods. As this is not the focus of this paper, we leave it as future works. spondence among query text, label text, and objects, which are useful in the grounding tasks (e.g., visual grounding and grounded captioning pretraining tasks in Sec. 4, referring expression comprehension in Sec. 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Encoder-Decoder Architecture</head><p>We use transformer encoder-decoder architecture <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> to encode visual and text inputs and generate label text. Our bidirectional multimodal encoder is a stack of m transformer blocks, consisting of a self-attention layer and a fully-connected layer with residual connections. Our decoder is another stack of m transformer blocks similar to the multimodal encoder, where each block has an additional cross-attention layer. As shown in <ref type="figure">Fig. 2 (a)</ref>, the encoder takes the concatenation of text and visual embeddings as input and outputs their contextualized joint representations</p><formula xml:id="formula_0">h = {h x 1 , . . . , h x |x| , h v 1 , . . . , h v n } = Enc(e x , e v ).</formula><p>Then the decoder iteratively attends to previously generated tokens y &lt;j (via self-attention) and the encoder outputs h (via crossattention), then predicts the probability of future text tokens P ? (y j |y &lt;j , x, v) = Dec(y &lt;j , h). We suggest readers to check <ref type="bibr" target="#b44">Raffel et al. (2020)</ref>; <ref type="bibr" target="#b25">Lewis et al. (2020)</ref> for more details of our backbone models. For both pretraining (Sec. 4) and downstream tasks (Sec. 5), we train our model parameters ? by minimizing the negative log-likelihood of label text y tokens given input text x and image v:</p><formula xml:id="formula_1">L GEN ? = ? |y| j=1 log P ? (y j |y &lt;j , x, v)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Task-Specific Methods vs. Our Unified Framework</head><p>We compare our unified framework with existing vision-andlanguage transformers on two popular tasks: visual question answering  and referring expression comprehension <ref type="bibr" target="#b34">(Mao et al., 2016)</ref>.</p><p>Visual question answering requires a model to answer a question to a given context image. As shown in <ref type="figure">Fig.3</ref>   <ref type="figure">Figure 3</ref>. Comparison between existing methods and our framework on visual question answering and referring expression comprehension (visual grounding) tasks. While existing methods use task-specific architectures and objectives, our models use the same language modeling architecture and maximum likelihood estimation on label text for all tasks.</p><p>2019; Chen et al., 2020) typically introduce a multilayer perceptron (MLP) multi-label classifier head on top of h x [CLS] , which is trained together with the transformer backbone through a binary cross-entropy loss, and weighted with VQA score  </p><formula xml:id="formula_2">2 : L VQA ? = ? K k=1 score(a k , x, v) log P VQA ? (correct|a k , x, v).</formula><p>Referring expression comprehension requires models to localize a target region in an image that is described by a given referring expression. Previous methods tackle this task as multi-class  or binary  classification over image regions. For example, UNITER  introduces an MLP region scoring head on top of the output representations of regions, as shown in <ref type="figure">Fig. 3</ref>(b). This region scoring head is jointly trained with the encoder by minimizing negative loglikelihood of target region r * :</p><formula xml:id="formula_3">L REF ? = ? log P REF ? (r * |x, v)</formula><p>. In contrast to existing methods that develop task-specific architectures and objectives (e.g., equations above), our unified framework is free from extra model designs for new tasks. As shown in <ref type="figure">Fig. 3 (c,d)</ref> and <ref type="table">Table 1</ref>, we formulate the task labels to corresponding text, and we learn these different tasks by predicting label text with the same language modeling objective (Eq. 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Pretraining</head><p>In this section, we describe how we pretrain our VL-T5 and VL-BART models (Sec. 3). We start with the details of the pretraining data and illustrate how we formulate diverse vision-and-language pretraining tasks as multimodal conditional text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pretraining Data</head><p>We aggregate pretraining data from MS COCO <ref type="bibr" target="#b5">Chen et al., 2015)</ref> and Visual Genome (VG; <ref type="bibr" target="#b22">Krishna et al. (2016)</ref>) images 3 . The captioning data from these two datasets are used in the multimodal language modeling task. The COCO captions are also used in the image-text matching task to learn cross-modal alignment. Besides the captions, we also use three visual question answering datasets (VQA v2.0 , GQA balanced version <ref type="bibr">(Hudson &amp; Manning, 2019)</ref>, and Visual7W ) as in , but only used them for the visual question answering task. Details of these pretraining tasks are in Sec. 4.2. Overall, our pretraining dataset contains 9.18M image-text pairs on 180K distinct images. We show more details of the pretraining data in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pretraining Tasks</head><p>We pretrain our models under a multi-task setup with diverse pretraining tasks, including multimodal language modeling, visual question answering, image-text matching, visual grounding, and grounded captioning. <ref type="table">Table 1</ref> shows input and output examples of our pretraining tasks. The training data for each of these tasks are summarized in appendix. In the rest of this section, we explain these tasks in detail.</p><p>Multimodal language modeling: We follow <ref type="bibr" target="#b44">Raffel et al. (2020)</ref> and <ref type="bibr" target="#b25">Lewis et al. (2020)</ref> to construct the language modeling pretraining task. For VL-T5, we mask 15% of input text tokens and replace contiguous text span with sentinel tokens (e.g., &lt;text 1&gt;). For VL-BART, we mask 30% of input text tokens with &lt;mask&gt; tokens. Then we predict the masked text. See <ref type="table">Table 1</ref> for examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual question answering:</head><p>We include visual question answering in our pretraining tasks as in . While previous methods <ref type="bibr" target="#b32">Lu et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020)</ref> tackle the task as classification over predefined answer candidates (illustrated in <ref type="figure">Fig. 3</ref>), we directly generate answers in their original text format.</p><p>Image-text matching: In this task, the model needs to verify whether a text corresponds to an image. We consider an image and its captions 4 as positive pairs. With a probability <ref type="table">Table 1</ref>. Input-output formats for pretraining (Sec. 4) and downstream tasks (Sec. 5). a We use different prefixes ("vqa:", "gqa:", "visual7w:") for questions from different datasets. b NLVR 2 takes two images as visual input, for brevity, we only show one here. of 50%, we randomly sample another training image's caption to create a negative pair. The model then predicts the correspondence with "true" or "false" as shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual grounding:</head><p>We develop an object-text matching task to endow the model with grounding ability, which is required in several tasks (e.g., referring expression comprehension and VCR). We give the model a region description and let it predict the id of the related object region. With the help of the visual sentinel token (e.g., &lt;vis 3&gt; in <ref type="table">Table 1)</ref>, this task fits naturally into our text generation objective. We make the region descriptions from the predictions of the object detector that we use for visual embeddings (see Sec.</p><p>3.1). Concretely, we sample an object region out of n region predictions. Then we concatenate its object name and attribute (e.g., attribute: "yellow" + object: "fire hydrant" ? "yellow fire hydrant"). This approach does not need extra annotation and could be extended to images without dense annotations (e.g., COCO images).</p><p>Grounded captioning: To teach the model with objectlevel information, we also use grounded captioning as an inverse task of visual grounding. As shown in <ref type="table">Table 1</ref>, given a visual sentinel token (which indicates an image region) as text input, the model is asked to generate a corresponding textual description of the image region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pretraining Implementation Details</head><p>For both VL-T5 and VL-BART, it takes 4 days for 30epoch pretraining with mixed precision training <ref type="bibr" target="#b37">(Narang et al., 2018)</ref> on 4 RTX 2080 Ti GPUs. We use batch size 320 and 600 for VL-T5 and VL-BART, respectively. We use AdamW <ref type="bibr" target="#b31">(Loshchilov &amp; Hutter, 2019)</ref> with (? 1 , ? 2 ) = (0.9, 0.999) and learning rate 1e-4 with 5% linear warmup schedule. Our code is based on PyTorch <ref type="bibr" target="#b40">(Paszke et al., 2017)</ref> and Huggingface Transformers <ref type="bibr" target="#b58">(Wolf et al., 2019)</ref>.</p><p>descriptions of an image (e.g., 'what is in the image?').</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Downstream Tasks and Results</head><p>In this section, we compare our generative architectures VL-T5 and VL-BART on a diverse set of 7 downstream tasks (details in Appendix) with existing vision-and-language pretrained transformers <ref type="bibr" target="#b32">Lu et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020;</ref><ref type="bibr" target="#b69">Zhou et al., 2020;</ref><ref type="bibr">Li et al., 2020b;</ref><ref type="bibr" target="#b59">Xia et al., 2020)</ref>. As summarized in <ref type="table" target="#tab_4">Table 2</ref>, our unified generative approach (with the input-output format in <ref type="table">Table 1)</ref> shows performance close to the task-specific models, most of which are discriminative. In the rest of this section, we provide detailed comparisons w.r.t. the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Visual Question Answering: VQA and GQA</head><p>The visual question answering task requires models to answer a question to a given context image. <ref type="table" target="#tab_4">Table 2</ref> compares our models VL-T5 and VL-BART with existing methods on VQA  and GQA <ref type="bibr">(Hudson &amp; Manning, 2019)</ref>. For both tasks, our models achieve comparable performance to existing approaches.</p><p>Generative vs. Discriminative model: Modern approaches <ref type="bibr" target="#b32">Lu et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020;</ref><ref type="bibr" target="#b69">Zhou et al., 2020;</ref><ref type="bibr">Li et al., 2020b)</ref> are discriminative models, where they tackle visual question answering tasks as multi-label classification over a predefined set of answer candidates. This strategy achieves strong performance but not generalizes to real-world open-ended scenarios. To quantitatively compare the existing discriminative approaches and our generative approach, we break down VQA questions into in-domain and out-of-domain questions, in terms of whether the best answer a * for each question is included in the top-K (K=3, 129) answer candidates A topk . After this split, the in-domain subset contains 24,722 questions, and the out-of-domain subset contains 1,558 questions. <ref type="table" target="#tab_5">Table 3</ref> shows the performance. For discriminative baselines, we introduce a sigmoid MLP classifier on top of the decoder representation of start-of-sequence token &lt;s&gt;, following  LXMERT and UNITER. Comparing models with the same backbone, we notice the generative models improve upon the discriminative baselines across all the subsets. This improvement is more significant on the out-of-domain subset, where the generative VL-T5 and VL-BART achieve 6 and 6.2 points improvement over their discriminative counterparts, showing the effectiveness of using generative modeling. Compared to the strong discriminative baseline UNITER Base (pretrained with 4M extra images), our generative models still show comparable overall performance while significantly outperform it on the out-of-domain subset (about 3 points).</p><p>Dataset-specific prefixes: As shown in recent works <ref type="bibr" target="#b48">Shin et al., 2020;</ref><ref type="bibr" target="#b28">Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b43">Radford et al., 2021)</ref>, different text prompts could result in different finetuning results. We thus experiment with a single prefix 'vqa' for both VQA and GQA in VL-T5 pretraining/finetuning. Interestingly, we found slight performance increases from the original dataset-specific prefix: VQA This shows that a single model can successfully handle multiple VQA tasks without dataset-specific prefixes (similar results were observed in text QA <ref type="bibr" target="#b20">(Khashabi et al., 2020)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Natural Language Visual Reasoning: NLVR 2</head><p>The task of NLVR 2 <ref type="bibr" target="#b50">(Suhr et al., 2019)</ref> is to determine whether a natural language statement is true about two images. To apply our model to this task, we concatenate region features from the two images and use different image id embeddings to disambiguate the regions from the two images. Then our model learns to generate text labels "true" and "false". This is similar to the Triplet setting described in UNITER . In <ref type="figure" target="#fig_1">Fig. 4</ref>, we illustrate three common encoding settings for NLVR 2 .  tional attention added to Pair. UNITER shows that one can improve performance with a more complex encoding setting, i.e., Pair-biattn achieves better performance than Pair, which is again better than the simplest Triplet. Note that both the Pair and the Pair-biattn settings approximately double the computational cost compared to that of the Triplet setting. While there's the gap between our models and baselines in Pair and Pair-biattn setting, VL-T5 shows comparable performance to UNITER in Triplet setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Referring Expression Comprehension: RefCOCOg</head><p>Referring expression comprehension requires a model to correctly localize an object described by a given phrase (e.g., 'the car on the left'). In this work, we evaluate models on the RefCOCOg <ref type="bibr" target="#b34">(Mao et al., 2016)</ref> dataset. Similar to the visual grounding pretraining task in Sec. 4, we give our model a referring phrase and candidate region features from the image, the model then generates the visual sentinel token (e.g., &lt;vis 1&gt;) of the region corresponding to the phrase. Following previous works UNITER and MAttNet , we use region detections from Mask R-CNN <ref type="bibr" target="#b14">(He et al., 2017)</ref> as candidates and mark a selected region to be correct if its intersection over union (IoU) with the ground truth region is greater than 0.5. <ref type="table" target="#tab_4">Table 2</ref> compares our models with discriminative baselines. With pretraining, VL-T5 significantly outperforms the strong modular model MAttNet, and achieves a reasonable performance compared to the UNITER model that has been pretrained on a much larger corpus. While our method did not achieve state-of-the-art performance, these results suggest that referring expression comprehension can be effectively formulated as a text-generation task, rather than previously <ref type="bibr" target="#b6">Chen et al., 2020</ref>) formulated classification task over a set of visual regions, allowing more flexible architecture design. We hope our work would inspire future works in this direction. We also observe that our experiments with VL-BART on RefCOCOg diverges. One reason might be the difference in positional encoding methods of T5 and BART. During training, BART adds learned absolute positional embedding to text token embedding, whereas T5 uses relative position biases in selfattention layers instead. We hypothesize that VL-BART found strong correspondence by memorizing the positions of each training object (we observe high training accuracy, but low validation accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Visual Commonsense Reasoning: VCR</head><p>Visual Commonsense Reasoning (VCR) <ref type="bibr" target="#b66">(Zellers et al., 2019</ref>) is a multiple-choice question answering task that requires commonsense reasoning beyond object or action recognition. Each VCR question (Q) has 4 answers (A) and 4 rationales (R), and it can be decomposed into two multiple choice sub-tasks: question answering (Q?A), and answer justification (QA?R). The overall task (Q?AR) requires a model to not only select the correct answer to the question, but also the correct rationale for choosing the answer. Similar to <ref type="bibr" target="#b38">Nogueira et al. (2020)</ref> that leverages language model for document ranking, we concatenate context (image+question) with each candidate choice, and let our models generate "true" for the correct choice and generate "false" otherwise, as shown in <ref type="table">Table 1</ref>, During inference, we use P (true) P (true)+P (f alse) to rank the choices and select the one with the highest score.</p><p>UNITER  has shown that a second-stage in-domain pretraining (with the same pretraining objectives as generic-domain pretraining) on the VCR dataset would significant improve VCR task performance. This is likely due to the domain difference between VCR and the generic-domain pretraining corpus (e.g., COCO Captions), e.g., the input text (concatenation of multiple sentences:</p><formula xml:id="formula_4">[Q]+[A]+[R]</formula><p>) in VCR is much longer than in genericdomain pretraining. In <ref type="table" target="#tab_9">Table 6</ref>, we show the experiment results with second stage pretraining on VCR. On VCR val split, comparing to the base models that do not pretrain, we find that both Stage 1 generic-domain pretraining and Stage 2 in-domain pretraining help improve the VCR task performance, which is consistent with the findings in UNITER. On VCR test split, we notice that our best model VL-T5 achieves a comparable (slightly better) performance to UNITER, while significantly higher performance when compared to ViLBERT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Image Captioning: COCO Caption</head><p>We evaluate automatic caption generation performance on MS COCO Caption dataset <ref type="bibr" target="#b5">(Chen et al., 2015)</ref>. We use Karparthy split <ref type="bibr">(Karpathy &amp; Fei-Fei, 2015)</ref>, which re-splits train2014 and val2014 images  into 113,287 / 5000 / 5000 for train / validation / test. While some methods use reinforcement learning-based optimization on CIDEr, we only compare with methods using cross-entropy loss. Note that image captioning is the only task in our experiments where textual context is not meaningful, which results in a notable difference in pretraining and finetuning w.r.t. the input format. Inspired by Oscar <ref type="bibr" target="#b26">(Li et al., 2020a)</ref>, we also experiment with using object tags as additional text inputs during finetuning. We use BLEU <ref type="bibr" target="#b39">(Papineni et al., 2002)</ref>, CIDEr , METEOR <ref type="bibr" target="#b2">(Banerjee &amp; Lavie, 2005)</ref>, SPICE <ref type="bibr" target="#b0">(Anderson et al., 2016)</ref> as evaluation In <ref type="table">Table 7</ref>, we compare our models with baselines in different settings: use of vision-and-language pretraining and use of object tag as additional text inputs. With and without vision-and-language pretraining, our models show comparable performance to baselines. Since the use of object tags requires significant extra computation, we only use it for finetuning. Using tags gives a comparable or slightly improved performance for both models, and the improvement is significant (2.5) in CIDEr for VL-BART. We expect object tag augmentation during pretraining like Oscar would further boost the performance of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Multimodal Machine Translation: Multi30K</head><p>We evaluate multimodal machine translation performance on Multi30K dataset <ref type="bibr" target="#b10">(Elliott et al., 2016)</ref>, where a model <ref type="table">Table 9</ref>. Single-task vs. Multi-task finetuning results on 7 tasks. With a single set of parameters, our multi-task model achieves similar performance to separately optimized single-task models. We denote the number of parameters of single VL-T5 model as P. translates English text to German text given context images. We report BLEU score using SacreBLEU <ref type="bibr">(Post, 2018) 6</ref> . We compare our method with state-of-the-art transformer models: Multimodal self-attention (MSA) <ref type="bibr" target="#b62">(Yao &amp; Wan, 2020)</ref>, MeMAD <ref type="bibr" target="#b13">(Gr?nroos et al., 2018)</ref>. <ref type="table" target="#tab_10">Table 8</ref> shows that our T5-based models outperform the baselines that use strong data augmentation (e.g., back-translation) on all three test splits. Our vision-and-language models improve the textonly backbones although we did not observe improvement with vision-and-language pretraining. This might be because the source text in Multi30K contains sufficient information for translation as discussed in <ref type="bibr">Caglayan et al. (2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Multi-Task Finetuning</head><p>Single-task vs. Multi-task Finetuning: While our framework has unified the architecture for different downstream tasks, the parameters are separately optimized. To see whether we can go further, we finetune a single VL-T5 model for 20 epochs, where it tackles 7 different tasks with the same set of weights. At each finetuning step, we sample a mini-batch of examples from one of the 7 tasks in a round-robin fashion. For a fair comparison, we use singletask baselines without augmentations (e.g., no 2nd stage pretraining for VCR, no object tags for COCO Captioning). <ref type="table">Table 9</ref> shows that our multi-task model achieves comparable performance to the separately optimized single-task models on all 7 tasks with a single set of parameters.</p><p>Single shared head vs. Task-specific heads: We also experiment with the multi-task finetuning setup of ViLBERT-MT <ref type="bibr" target="#b33">(Lu et al., 2020)</ref>, where a task-specific head is finetuned for each of 7 downstream tasks while sharing backbone. The head parameters are initialized from the pretrained LM head and separately updated during finetuning. The 7 task-specific heads (7H) add 7 ? 32K(vocab size) ? 768(embedding size) = 172M parameters, which is 80% of original VL-T5's 220M parameters (P), resulting around 400M parameters in total. Since the increased parameters make the training slow, we compare both models by 5th epoch checkpoints. <ref type="table" target="#tab_12">Table 10</ref> shows that VL-T5 with single shared head achieves almost equal performance with task-6 https://github.com/mjpost/sacrebleu specific heads, while having much fewer total parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed VL-T5 and VL-BART which tackle vision-and-language tasks with a unified text generation objective. Experiments show VL-T5 and VL-BART can achieve comparable performance with state-of-the-art vision-and-language transformers on diverse vision-andlanguage tasks without hand-crafted architectures and objectives. Especially, we demonstrate our generative approach is better suited for open-ended visual question answering.</p><p>In addition, we also showed it is possible to train seven different tasks simultaneously using a single architecture with single parameters without not losing much performance. It would be an interesting future work to further explore this direction by adding even more tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with Baselines</head><p>In <ref type="table" target="#tab_13">Table 11</ref>, we compare the baseline vision-and-language transformers with our VL-T5 and VL-BART in detail, including their pretraining datasets, architecture, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In <ref type="table" target="#tab_4">Table 12</ref> and <ref type="table" target="#tab_5">Table 13</ref>, we show the detailed statistics of our pretraining and downstream datasets and tasks. In <ref type="table" target="#tab_6">Table 14</ref>, we show the hyperparameters that we used in our pretraining and downstream task experiments. We provide the links to download pretraining and downstream datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Pretraining Data</head><p>Overall, our pretraining dataset contains 9.18M image-text pairs on 180K distinct images. We carefully split our pretraining data to avoid any intersection between our training data and the validation/test sets of the downstream tasks (e.g., COCO Captioning, RefCOCOg). In this process, around 10K images are excluded from the training sets of COCO 7 and Visual Genome 8 . We use COCO Karpathy val split <ref type="bibr">(Karpathy &amp; Fei-Fei, 2015)</ref> with 5,000 images as our validation set to monitor pretraining performance.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Different encoding settings for NLVR 2 . Pair and Pairbiattn approximately double the computational cost over Triplet, which our models are based on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>trained on Visual</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;vis_3&gt;</cell><cell>&lt;/s&gt;</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RoI</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bidirectional</cell><cell cols="2">Autoregressive</cell><cell></cell><cell></cell><cell></cell><cell>Box</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Multimodal Encoder</cell><cell cols="2">Text Decoder</cell><cell>+</cell><cell>+</cell><cell>+</cell><cell>coordinates</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>Image ids</cell></row><row><cell>visual</cell><cell>grounding</cell><cell>:</cell><cell>fire</cell><cell>hydrant</cell><cell>&lt;s&gt;</cell><cell>&lt;vis_3&gt;</cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>&lt;vis_1&gt;</cell><cell>&lt;vis_2&gt;</cell><cell>&lt;vis_3&gt;</cell><cell>Region ids</cell></row><row><cell></cell><cell>Prefix</cell><cell></cell><cell></cell><cell>Visual embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a) Our vision-and-language framework</cell><cell></cell><cell></cell><cell cols="3">(b) Visual embedding</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Multimodal LM (VL-T5) span prediction: A &lt;text 1&gt; is &lt;text 2&gt; over a fire hydrant. &lt;text 1&gt; man &lt;text 2&gt; jumping Multimodal LM (VL-BART) denoise: A &lt;mask&gt; is &lt;mask&gt; over a fire hydrant. A man is jumping over a fire hydrant</figDesc><table><row><cell>Tasks</cell><cell>Input image</cell><cell>Input text</cell><cell>Target text</cell></row><row><cell>Pretraning tasks (Sec. 4)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a Visual question answering</cell><cell></cell><cell>vqa: what is the color of the man's shirt?</cell><cell>blue</cell></row><row><cell>Image-text matching</cell><cell></cell><cell cols="2">image text match: A man with blue shirt is jumping over fire hydrant. true</cell></row><row><cell>Visual grounding</cell><cell></cell><cell>visual grounding: yellow fire hydrant</cell><cell>&lt;vis 3&gt;</cell></row><row><cell>Grounded captioning</cell><cell></cell><cell>caption region: &lt;vis 3&gt;</cell><cell>yellow fire hydrant</cell></row><row><cell>Downstream tasks (Sec. 5)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VQA</cell><cell></cell><cell>vqa: [Q]</cell><cell>[A]</cell></row><row><cell>GQA</cell><cell></cell><cell>gqa: [Q]</cell><cell>[A]</cell></row><row><cell>b NLVR 2</cell><cell></cell><cell>nlvr: [text]</cell><cell>true/false</cell></row><row><cell>VCR Q?A</cell><cell></cell><cell>vcr qa: question [Q] answer: [A]</cell><cell>true/false</cell></row><row><cell>VCR QA?R</cell><cell></cell><cell>vcr qar: question [Q] answer: [A] rationale: [R]</cell><cell>true/false</cell></row><row><cell>RefCOCOg</cell><cell></cell><cell>visual grounding: [referring expression]</cell><cell>[region id]</cell></row><row><cell>COCO captioning</cell><cell></cell><cell>caption:</cell><cell>[caption]</cell></row><row><cell>COCO captioning (w/ object tags)</cell><cell></cell><cell>caption with tags: [Tag1 Tag2 ..]</cell><cell>[caption]</cell></row><row><cell>Multi30K En-De translation</cell><cell></cell><cell>translate English to German: [English text]</cell><cell>[German text]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Single model performance on downstream tasks. Note that the baseline models adopt task-specific objectives and architectures, whereas our models tackle all tasks, including discriminative tasks (e.g., RefCOCOg), as text generation with a single architecture and objective. See our discussion in Sec.5.3.</figDesc><table><row><cell>Method</cell><cell># Pretrain</cell><cell>VQA</cell><cell>GQA</cell><cell cols="3">Discriminative tasks NLVR 2 RefCOCOg VCR Q? AR</cell><cell cols="2">Generative tasks COCO Cap Multi30K En-De</cell></row><row><cell></cell><cell>Images</cell><cell cols="2">test-std test-std</cell><cell>test-P</cell><cell>test d</cell><cell>test</cell><cell>Karpathy test</cell><cell>test 2018</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>CIDEr</cell><cell>BLEU</cell></row><row><cell>LXMERT</cell><cell>180K</cell><cell>72.5</cell><cell>60.3</cell><cell>74.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ViLBERT</cell><cell>3M</cell><cell>70.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>54.8</cell><cell>-</cell><cell>-</cell></row><row><cell>UNITER Base</cell><cell>4M</cell><cell>72.9</cell><cell>-</cell><cell>77.9</cell><cell>74.5</cell><cell>58.2</cell><cell>-</cell><cell></cell></row><row><cell>Unified VLP</cell><cell>3M</cell><cell>70.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>117.7</cell><cell>-</cell></row><row><cell>Oscar Base</cell><cell>4M</cell><cell>73.4</cell><cell>61.6</cell><cell>78.4</cell><cell>-</cell><cell>-</cell><cell>123.7</cell><cell>-</cell></row><row><cell>XGPT</cell><cell>3M</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>120.1</cell><cell>-</cell></row><row><cell>MeMAD</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.5</cell></row><row><cell>VL-T5</cell><cell>180K</cell><cell>70.3</cell><cell>60.8</cell><cell>73.6</cell><cell>71.3</cell><cell>58.9</cell><cell>116.5</cell><cell>38.6</cell></row><row><cell>VL-BART</cell><cell>180K</cell><cell>71.3</cell><cell>60.5</cell><cell>70.3</cell><cell>22.4</cell><cell>48.9</cell><cell>116.6</cell><cell>28.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="3">In-domain Out-of-domain Overall</cell></row><row><cell>Discriminative</cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNITER Base</cell><cell>74.4</cell><cell>10.0</cell><cell>70.5</cell></row><row><cell>VL-T5</cell><cell>70.2</cell><cell>7.1</cell><cell>66.4</cell></row><row><cell>VL-BART</cell><cell>69.4</cell><cell>7.0</cell><cell>65.7</cell></row><row><cell>Generative</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VL-T5</cell><cell>71.4</cell><cell>13.1</cell><cell>67.9</cell></row><row><cell>VL-BART</cell><cell>72.1</cell><cell>13.2</cell><cell>68.6</cell></row></table><note>VQA Karpathy-test split accuracy using generative and discriminative methods. We break down the questions into two subsets in terms of whether the best-scoring answer a* for each question is included in the top-K answer candidates A topk . In- domain: a* ? A topk , Out-of-domain: a* / ? A topk .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Setting</cell><cell>dev test-P</cell></row><row><cell cols="2">UNITER Base Triplet</cell><cell>73.0 73.9</cell></row><row><cell cols="2">UNITER Base Pair</cell><cell>75.9 75.8</cell></row><row><cell cols="3">UNITER Base Pair-biattn 77.2 77.9</cell></row><row><cell>LXMERT</cell><cell>Pair</cell><cell>74.9 74.5</cell></row><row><cell>Oscar Base</cell><cell>Pair</cell><cell>78.1 78.4</cell></row><row><cell>VL-T5</cell><cell>Triplet</cell><cell>74.6 73.6</cell></row><row><cell>VL-BART</cell><cell>Triplet</cell><cell>71.7 70.3</cell></row><row><cell cols="3">Karpathy-test (67.9 ? 69.3); GQA test-dev (60.0 ? 60.2).</cell></row></table><note>NLVR 2 performance comparison under different encod- ing settings. Note that Triplet takes lower computational cost than Pair and Pair-biattn. See also Fig. 4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">True / False</cell><cell></cell></row><row><cell></cell><cell cols="3">True / False</cell><cell cols="2">True / False</cell><cell></cell><cell cols="3">Multi-head Attention</cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell cols="4">Model Model</cell><cell cols="4">Model Model</cell></row><row><cell>Image 1</cell><cell>Image 2</cell><cell>Text</cell><cell>Image 1</cell><cell>Text</cell><cell>Image 2</cell><cell>Text</cell><cell>Image 1</cell><cell>Text</cell><cell>Image 2</cell><cell>Text</cell></row><row><cell cols="2">(a) Triplet</cell><cell></cell><cell></cell><cell cols="2">(b) Pair</cell><cell></cell><cell cols="3">(c) Pair-biattn</cell><cell></cell></row></table><note>shows the model results on NLVR 2 under differ- ent encoding settings: (i) Triplet: joint encoding of image pairs and text; (i) Pair: the concatenation of individual em- bedding of each image-text pair; (iii) Pair-biattn: bidirec-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>RefCOCOg performance comparison.</figDesc><table><row><cell>Method</cell><cell>V&amp;L PT val d test d</cell></row><row><cell>MattNet</cell><cell>66.9 67.3</cell></row><row><cell>UNITER Base</cell><cell>74.3 74.5</cell></row><row><cell>VL-T5</cell><cell>63.4 62.9</cell></row><row><cell>VL-T5</cell><cell>71.2 71.3</cell></row><row><cell>VL-BART</cell><cell>21.8 23.0</cell></row><row><cell>VL-BART</cell><cell>23.6 22.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>VCR accuracy. Stage 1 refers to the original generic-domain pretraining and Stage 2 refers to the in-domain pretraining on VCR. Stage 2 Q ? A QA ? R Q ? AR Q ? A QA ? R Q ? ARTable 7. COCO captioning scores on Karparthy-test split. All models are trained with cross-entropy loss. PT and FT refer to the use of object tags during pretraining and finetuning, respectively.</figDesc><table><row><cell cols="4">V&amp;L PT Stage 1 ViLBERT Method</cell><cell>69.3</cell><cell>VCR val 71.0</cell><cell>49.5</cell><cell>-</cell><cell>VCR test -</cell><cell>-</cell></row><row><cell cols="2">ViLBERT</cell><cell></cell><cell></cell><cell>72.4</cell><cell>74.4</cell><cell>54.0</cell><cell>73.3</cell><cell>74.6</cell><cell>54.8</cell></row><row><cell cols="2">UNITER Base</cell><cell></cell><cell></cell><cell>72.4</cell><cell>73.7</cell><cell>53.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UNITER Base</cell><cell></cell><cell></cell><cell>72.8</cell><cell>75.3</cell><cell>54.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UNITER Base</cell><cell></cell><cell></cell><cell>74.6</cell><cell>77.0</cell><cell>57.8</cell><cell>75.0</cell><cell>77.2</cell><cell>58.2</cell></row><row><cell cols="2">VL-T5</cell><cell></cell><cell></cell><cell>71.1</cell><cell>73.6</cell><cell>52.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">VL-T5</cell><cell></cell><cell></cell><cell>72.9</cell><cell>75.0</cell><cell>54.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">VL-T5</cell><cell></cell><cell></cell><cell>74.6</cell><cell>77.0</cell><cell>57.5</cell><cell>75.3</cell><cell>77.8</cell><cell>58.9</cell></row><row><cell cols="2">VL-BART</cell><cell></cell><cell></cell><cell>65.4</cell><cell>68.1</cell><cell>44.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">VL-BART</cell><cell></cell><cell></cell><cell>67.0</cell><cell>67.4</cell><cell>45.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">VL-BART</cell><cell></cell><cell></cell><cell>69.2</cell><cell>69.9</cell><cell>48.6</cell><cell>69.8</cell><cell>69.8</cell><cell>48.9</cell></row><row><cell>Method</cell><cell>V&amp;L PT Object tags</cell><cell>B</cell><cell cols="2">COCO Captioning C M</cell><cell>S</cell><cell></cell></row><row><cell>Oscar</cell><cell>PT+FT</cell><cell cols="4">36.5 123.7 30.3 23.1</cell><cell></cell></row><row><cell>VL-T5</cell><cell>FT</cell><cell cols="4">34.5 116.5 28.7 21.9</cell><cell></cell></row><row><cell>VL-BART</cell><cell>FT</cell><cell cols="4">35.1 116.6 28.7 21.5</cell><cell></cell></row><row><cell>Oscar</cell><cell></cell><cell cols="4">34.5 115.6 29.1 21.9</cell><cell></cell></row><row><cell>Unified VLP</cell><cell></cell><cell cols="4">36.5 117.7 28.4 21.3</cell><cell></cell></row><row><cell>XGPT</cell><cell></cell><cell cols="4">37.2 120.1 28.6 21.8</cell><cell></cell></row><row><cell>VL-T5</cell><cell></cell><cell cols="4">34.6 116.1 28.8 21.9</cell><cell></cell></row><row><cell>VL-BART</cell><cell></cell><cell cols="4">34.2 114.1 28.4 21.3</cell><cell></cell></row><row><cell>Unified VLP</cell><cell></cell><cell cols="4">35.5 114.3 28.2 21.0</cell><cell></cell></row><row><cell>XGPT</cell><cell></cell><cell cols="4">34.4 113.0 27.8 20.8</cell><cell></cell></row><row><cell>BUTD</cell><cell></cell><cell cols="4">36.2 113.5 27.0 20.3</cell><cell></cell></row><row><cell>VL-T5</cell><cell></cell><cell cols="4">32.6 109.4 28.2 21.0</cell><cell></cell></row><row><cell>VL-BART</cell><cell></cell><cell cols="4">33.8 112.4 28.5 21.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Multi30K En-De multimodal translation BLEU scores. ? and * refer to data augmentation and ensemble, respectively. We use gray color for the ensemble model it is not fairly comparable.</figDesc><table><row><cell>Method</cell><cell cols="3">V&amp;L PT test2016 test2017 test2018</cell></row><row><cell>MSA</cell><cell>38.7</cell><cell>-</cell><cell>-</cell></row><row><cell>MeMAD</cell><cell>38.9</cell><cell>32.0</cell><cell>-</cell></row><row><cell>MSA  ?</cell><cell>39.5</cell><cell>-</cell><cell>-</cell></row><row><cell>MeMAD  ?</cell><cell>45.1</cell><cell>40.8</cell><cell>-</cell></row><row><cell>MeMAD  ? *</cell><cell>45.5</cell><cell>41.8</cell><cell>38.5</cell></row><row><cell>T5 (text only)</cell><cell>44.6</cell><cell>41.6</cell><cell>39.0</cell></row><row><cell>VL-T5</cell><cell>45.3</cell><cell>42.4</cell><cell>39.5</cell></row><row><cell>VL-T5</cell><cell>45.5</cell><cell>40.9</cell><cell>38.6</cell></row><row><cell>BART (text only)</cell><cell>41.2</cell><cell>35.4</cell><cell>33.3</cell></row><row><cell>VL-BART</cell><cell>41.3</cell><cell>35.9</cell><cell>33.2</cell></row><row><cell>VL-BART</cell><cell>37.7</cell><cell>29.7</cell><cell>28.1</cell></row><row><cell cols="2">metrics using COCOEvalCap 5 .</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Multi-task finetuning with single/task-specific heads. While three tasks are included for brevity, the rest of the tasks also show the minimal differences between two setups.</figDesc><table><row><cell></cell><cell></cell><cell>VQA</cell><cell>GQA</cell><cell>COCO Caption</cell></row><row><cell>Method</cell><cell># Params</cell><cell cols="2">Karpathy test test-dev</cell><cell>Karpathy test</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>Acc</cell><cell>CIDEr</cell></row><row><cell>Single shared head</cell><cell>P</cell><cell>68.3</cell><cell>59.3</cell><cell>110.6</cell></row><row><cell cols="2">Task-specific heads P+7H=1.8P</cell><cell>68.5</cell><cell>59.3</cell><cell>110.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>Summary of baseline vision-and-language transformers. a Since not all models report exact parameter numbers, we provide rough estimates compared to BERTBase (86M; noted as P), where word embedding parameters are excluded. b LXMERT and XGPT are not initialized from pretrained language models. LXMERT authors found pretraining from scratch was more effective than initialization from BERTBase in their experiments. XGPT uses text pretraining on Conceptual captions and COCO captions with Masked LM<ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> and Masked Seq2Seq<ref type="bibr" target="#b49">(Song et al., 2019)</ref> objectives before V&amp;L pretraining. c LXMERT (text+visual+cross-modal) and ViLBERT (cross-modal) use dual-stream encoders. ViLBERT uses 768/1024-dim hidden states for text/visual streams respectively. XGPT uses AoA module<ref type="bibr" target="#b15">(Huang et al., 2019)</ref> as visual encoder. Rest of the models use single-stream encoders. d For generation tasks, Unified VLP and Oscar use causal mask and reuse encoder as decoder similar to UniLM. e XGPT also uses shared parameters for encoder and decoder, but its decoder is right-shifted and predicts next tokens. f Unified VLP is initialized from UniLM, which is initialized from BERTLarge. g Oscar uses object tags as additional text inputs.</figDesc><table><row><cell></cell><cell>V&amp;L Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Hyperparameters</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dataset</cell><cell cols="3"># Imgs Arch. type Backbone</cell><cell cols="5"># Layers # Params a Hidden dim # Regions Position Emb</cell></row><row><cell>LXMERT</cell><cell>COCO+VG</cell><cell cols="2">180K Encoder</cell><cell>-b</cell><cell>9+5+5 c</cell><cell>2P</cell><cell>768</cell><cell>36</cell><cell>absolute</cell></row><row><cell>ViLBERT</cell><cell>CC</cell><cell>3M</cell><cell>Encoder</cell><cell>BERT Base</cell><cell>12 c</cell><cell>2.5P</cell><cell>768/1024 c</cell><cell>10?36</cell><cell>absolute</cell></row><row><cell cols="2">UNITER Base CC+SBU+COCO+VG</cell><cell>4M</cell><cell>Encoder</cell><cell>BERT Base</cell><cell>12</cell><cell>P</cell><cell>768</cell><cell>10?100</cell><cell>absolute</cell></row><row><cell>Unified VLP</cell><cell>CC</cell><cell>3M</cell><cell>Encoder d</cell><cell>UniLM f</cell><cell>12</cell><cell>P</cell><cell>768</cell><cell>100</cell><cell>absolute</cell></row><row><cell>Oscar Base</cell><cell>CC+SBU+COCO+VG+Flickr30K</cell><cell>4M</cell><cell>Encoder d</cell><cell>BERT Base</cell><cell>12</cell><cell>P</cell><cell>768</cell><cell>50 g</cell><cell>absolute</cell></row><row><cell>XGPT</cell><cell>CC+COCO</cell><cell>3M</cell><cell>Enc-Dec e</cell><cell>-b</cell><cell>1 c +12+12</cell><cell>P</cell><cell>768</cell><cell>100</cell><cell>absolute</cell></row><row><cell>VL-T5</cell><cell>COCO+VG</cell><cell cols="2">180K Enc-Dec</cell><cell>T5 Base</cell><cell>12+12</cell><cell>2P</cell><cell>768</cell><cell>36</cell><cell>relative</cell></row><row><cell>VL-BART</cell><cell>COCO+VG</cell><cell cols="2">180K Enc-Dec</cell><cell>BART Base</cell><cell>6+6</cell><cell>P</cell><cell>768</cell><cell>36</cell><cell>absolute</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 .</head><label>12</label><figDesc>Pretraining tasks used in our vision-and-language pretraining. The images that have any intersection with evaluation set of downstream tasks (e.g., COCO caption, RefCOCOg) and the held-out validation set for pretraining are excluded.</figDesc><table><row><cell>Task</cell><cell cols="2">Image source Text source</cell><cell># Examples</cell></row><row><cell cols="2">Multimodal language modeling COCO, VG</cell><cell cols="2">COCO caption, VG caption 4.9M (# captions)</cell></row><row><cell>Visual question answering</cell><cell>COCO, VG</cell><cell>VQA, GQA, Visual7W</cell><cell>2.5M (# questions)</cell></row><row><cell>Image-text matching</cell><cell>COCO</cell><cell>COCO caption</cell><cell>533K (# captions)</cell></row><row><cell>Visual grounding</cell><cell>COCO, VG</cell><cell>object&amp;attribute tags</cell><cell>163K (# images)</cell></row><row><cell>Grounded captioning</cell><cell>COCO, VG</cell><cell>object&amp;attribute tags</cell><cell>163K (# images)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 .</head><label>13</label><figDesc>Statistics of the datasets used in downstream tasks. The data that are not used for training/validation (e.g., COCO test2015 images) and data for leaderboard submissions (e.g., test-dev/test-std for VQA, test for GQA) are excluded.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Image source # Images (train)</cell><cell># Text (train)</cell><cell>Metric</cell></row><row><cell>VQA</cell><cell>COCO</cell><cell>123K (113K)</cell><cell>658K (605K)</cell><cell>VQA-score</cell></row><row><cell>GQA</cell><cell>VG</cell><cell cols="3">82.7K (82.3K) 1.08M (1.07M) Accuracy</cell></row><row><cell>NLVR 2</cell><cell>Web Crawled</cell><cell>238K (206K)</cell><cell>100K (86K)</cell><cell>Accuracy</cell></row><row><cell>RefCOCOg</cell><cell>COCO</cell><cell>26K (21K)</cell><cell>95K (80K)</cell><cell>Accuracy</cell></row><row><cell>VCR</cell><cell>Movie Clips</cell><cell>110K (80K)</cell><cell>290K (212K)</cell><cell>Accuracy</cell></row><row><cell>COCO Caption</cell><cell>COCO</cell><cell>123K (113K)</cell><cell>616K (566K)</cell><cell>BLEU,CIDEr,METEOR,SPICE</cell></row><row><cell cols="2">Multi30K En-De Flickr30K</cell><cell>31K (29K)</cell><cell>31K (29K)</cell><cell>BLEU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 .</head><label>14</label><figDesc>Hyperparameters for pretraining and downtream tasks</figDesc><table><row><cell>Model</cell><cell>Task</cell><cell cols="3">Learning rate Batch size Epochs</cell></row><row><cell></cell><cell>Pretraining</cell><cell>1e-4</cell><cell>320</cell><cell>30</cell></row><row><cell></cell><cell>VCR Pretraining</cell><cell>5e-5</cell><cell>80</cell><cell>20</cell></row><row><cell></cell><cell>VQA</cell><cell>5e-5</cell><cell>320</cell><cell>20</cell></row><row><cell></cell><cell>GQA</cell><cell>1e-5</cell><cell>240</cell><cell>20</cell></row><row><cell>VL-T5</cell><cell>NLVR 2</cell><cell>5e-5</cell><cell>120</cell><cell>20</cell></row><row><cell></cell><cell>RefCOCOg</cell><cell>5e-5</cell><cell>360</cell><cell>20</cell></row><row><cell></cell><cell>VCR</cell><cell>5e-5</cell><cell>16</cell><cell>20</cell></row><row><cell></cell><cell>COCO Caption</cell><cell>3e-5</cell><cell>320</cell><cell>20</cell></row><row><cell></cell><cell>Multi30K En-De</cell><cell>5e-5</cell><cell>120</cell><cell>20</cell></row><row><cell></cell><cell>Pretraining</cell><cell>1e-4</cell><cell>600</cell><cell>30</cell></row><row><cell></cell><cell>VCR Pretraining</cell><cell>5e-5</cell><cell>120</cell><cell>20</cell></row><row><cell></cell><cell>VQA</cell><cell>5e-5</cell><cell>600</cell><cell>20</cell></row><row><cell></cell><cell>GQA</cell><cell>1e-5</cell><cell>800</cell><cell>20</cell></row><row><cell>VL-BART</cell><cell>NLVR 2</cell><cell>5e-5</cell><cell>400</cell><cell>20</cell></row><row><cell></cell><cell>RefCOCOg</cell><cell>5e-5</cell><cell>1200</cell><cell>20</cell></row><row><cell></cell><cell>VCR</cell><cell>5e-5</cell><cell>48</cell><cell>20</cell></row><row><cell></cell><cell>COCO Caption</cell><cell>3e-5</cell><cell>520</cell><cell>20</cell></row><row><cell></cell><cell>Multi30K En-De</cell><cell>5e-5</cell><cell>320</cell><cell>20</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">UNC Chapel Hill. Correspondence to: Jaemin Cho &lt;jmin-cho@cs.unc.edu&gt;.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">score(a, x, v)= min((#humans that gave answer a) * 0.3, 1)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Existing vision-and-language transformers are trained with different datasets and computational budgets, thus their results may not be directly comparable to each other. We show the number of their pretraining images inTable 2.4  We only use captions from COCO for this task, since many short captions from VG and visual questions are nondistinctive</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/tylin/coco-caption</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Hyounghun Kim, Zineng Tang, Swarnadeep Saha, Xiang Zhou, and anonymous reviewers for their comments and suggestions. This work was supported by NSF-CAREER Award 1846185, ARO-YIP Award W911NF-18-1-0336, DARPA MCS Grant N66001-19-2-4031, Google Focused Research Award, and Bloomberg Data Science Ph.D. Fellowship. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic Propositional Image Caption Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.07998" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">METEOR : An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amodei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2005.14165" />
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probing the Need for Visual Context in Multimodal Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1422</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL, 2019. ISBN 9781950737130</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1504.00325" />
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNITER: UNiversal Image-TExt Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.11740" />
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.707</idno>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1810.04805" />
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi30K : Multilingual English-German Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Making Pre-trained Language Models Better Few-shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2012.15723" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>15731405. doi: 10.1007/ s11263-018-1116-0</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The MeMAD Submission to the WMT18 Multimodal Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Gr?nroos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sj?berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sulubacak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Troncy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>V?zquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iccv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00473</idno>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4633" to="4642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.00849" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00686</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2019. ISBN 9781728132938</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2598339</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2015. ISBN 9781467369640</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unifying Question Answering and Text Classification via Span Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.09286" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unified QA : Crossing Format Boundaries with a Single QA System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jia-Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-016-0981-7</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-T</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HERO: Hierarchical Encoder for Video+Language Omnirepresentation Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oscar</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2004.06165" />
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coco</forename><surname>Microsoft</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-148</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vilbert</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.02265" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-Task Vision and Language Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1912.02315" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generation and Comprehension of Unambiguous Object Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The Natural Language Decathlon : Multitask Learning as Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mixed Precision Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1gs9JgRZ" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Document Ranking with a Pretrained Sequenceto-Sequence Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">R</forename><surname>Mar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
		<ptr target="http://portal.acm.org/citation.cfm?doid=1073083.1073135http://dl.acm.org/citation.cfm?id=1073135" />
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=BJJsrmfCZ" />
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Call for Clarity in Reporting BLEU Scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using the Output Embedding to Improve Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aditya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sandhini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.10683" />
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faster R-Cnn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1506.01497" />
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L L</forename><surname>Iv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoprompt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Masked Sequence to Sequence Pre-training for Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mass</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.02450" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Corpus for Reasoning About Natural Language Grounded in Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.00491" />
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Contrastive Bidirectional Transformer for Temporal Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.05743" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">VideoBERT: A Joint Model for Video and Language Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1904.01766" />
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Cross-Modality Encoder Representations from Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lxmert</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.07490" />
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based Image Description Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1411.5726" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A broadcoverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">HuggingFace&apos;s Transformers: Stateof-the-art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brew</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.03771" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">XGPT : Cross-modal Generative Pre-Training for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.01473" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multimodal Transformer for Multimodal Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.400</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4346" to="4350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">MAttNet : Modular Attention Network for Referring Expression Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.08186" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">From Recognition to Cognition: Visual Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1811.10830" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Making Visual Representations Matter in Vision-Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinvl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.11059" />
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local videotext representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visual7w</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.540</idno>
		<ptr target="http://arxiv.org/abs/1511.03416" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">COCO caption For both VQA and COCO captioning tasks, we follow Karparthy split</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
			<affiliation>
				<orgName type="collaboration">B.2. Downstream Tasks VQA 9</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">which re-splits train2014 and val2014 COCO images</title>
		<editor>Karpathy &amp; Fei-Fei</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">113</biblScope>
		</imprint>
	</monogr>
	<note>287 / 5,000 / 5,000 images for train / validation / test</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">we use GQA-balanced version. We use train and val splits for training and use test-dev split for validation</title>
		<ptr target="https://cocodataset.org/#download8http://visualgenome.org/api/v0/api_home.html9https://visualqa.org/download.html10https://cs.stanford.edu/people/dorarad/gqa/download" />
	</analytic>
	<monogr>
		<title level="m">GQA 10 Following LXMERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>html dev splits consist of 943,000 / 132,062 / 12,578 questions, respectively</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">NLVR 2 11 Train / val / test-P splits consist of 86,373 / 6982 / 6967 sentences, respectively. We train our model on train split and use val split for validation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">VCR 12 Train / val / test splits consist of 212</title>
		<imprint>
			<biblScope unit="volume">923</biblScope>
			<biblScope unit="page">534</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">/ 25,263 questions, respectively. We train our model on train split and use val split for validation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">RefCOCOg 13 We use umd split, which consists of train / val / test sets with 42,226 / 2,573 / 5,023 sentences, respectively. Following UNITER (Chen et al., 2020) and MAttNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">we use ground truth COCO boxes for training, and use the detected boxes from an off-the-shelf Mask R-CNN 14 as candidates during inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Multi30K En-De 15 The train / val / test2016 / test2017 / test2018 splits consist of 29,000 / 1,014 / 1,000 / 1,000 / 1,017 English-German sentence pairs</title>
		<ptr target="http://lil.nlp.cornell.edu/nlvr/12https://visualcommonsense.com/download/13https://github.com/lichengunc/refer14https://github.com/lichengunc/MAttNet#pre-computed-detectionsmasks15https://github.com/multi30k/dataset" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
