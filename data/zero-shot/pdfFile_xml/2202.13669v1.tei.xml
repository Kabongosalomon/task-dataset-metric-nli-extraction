<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IntSig Information Co</orgName>
								<address>
									<settlement>Ltd, Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">INTSIG-SCUT Joint Laboratory of Document Recognition and Understanding</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pretrained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding offthe-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structured document understanding (SDU) aims at reading and analyzing the textual and structured information contained in scanned/digital-born documents. With the acceleration of the digitization process, it has been regarded as a crucial part of intelligent document processing and required by many real-world applications in various industries such as finance, medical treatment and insurance.</p><p>Recently, inspired by the rapid development of pre-trained language models of plain texts <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Liu et al., 2019b;</ref><ref type="bibr" target="#b4">Bao et al., 2020;</ref><ref type="bibr" target="#b5">Chi et al., 2021)</ref>, many researches on structured document pre-training <ref type="bibr" target="#b39">(Xu et al., , 2021a</ref><ref type="bibr">Li et al., 2021a,b,c;</ref><ref type="bibr" target="#b1">Appalaraju et al., 2021)</ref> have also * Corresponding author.</p><p>(a) A form.</p><p>(b) A receipt. <ref type="figure">Figure 1</ref>: The substitution of language does not appear obviously unnatural when the layout structure remains unchanged, as shown in a (a) form/(b) receipt. The detailed content has been re-synthesized to avoid the sensitive information leak. Best viewed in zoomed-in.</p><p>pushed the limit of a variety of SDU tasks. However, almost all of them only focus on pre-training and fine-tuning on the documents in a single language, typically English. This is extremely limited for other languages, especially in the case of lacking pre-training structured document data.</p><p>In this regard, we consider how to make the SDU tasks enjoy language-independent benefit from the pre-training of document layout structure. Here, we give an observation as shown in <ref type="figure">Figure 1</ref>. When the layout structure remains unchanged, the substitution of language does not make obvious unnaturalness. It fully motivates us to decouple and reuse the layout invariance among different languages.</p><p>Based on this inspiration, in this paper, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. In our framework, the text and layout information are first decoupled and jointoptimized during pre-training, and then re-coupled for fine-tuning. To ensure that the two modalities have sufficient language-independent interaction, we further propose a novel bi-directional attention complementation mechanism (BiACM) to enhance the cross-modality cooperation. Moreover, we present the key point location (KPL) and crossmodal alignment identification (CAI) tasks, which are combined with the widely-used masked visual-language modeling (MVLM) to serve as our pretraining objectives. During fine-tuning, the layout flow (LiLT) can be separated and combined with the off-the-shelf pre-trained textual models (such as RoBERTa <ref type="bibr" target="#b26">(Liu et al., 2019b)</ref>, XLM-R <ref type="bibr" target="#b6">(Conneau et al., 2020)</ref>, InfoXLM <ref type="bibr" target="#b5">(Chi et al., 2021)</ref>, etc) to deal with the downstream tasks. In this way, our method decouples and learns the layout knowledge from the monolingual structured documents before generalizing it to the multilingual ones.</p><p>To the best of our knowledge, the only preexisting multilingual SDU model is LayoutXLM <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref>. It scraps multilingual PDF documents of 53 languages from a web crawler and introduces extra pre-processing steps to clean the collected data, filter the low-quality documents, and classify them into different languages. After this, it utilizes a heuristic distribution to sample 22 million multilingual documents, which are further combined with the 8 million sampled English ones from the IIT-CDIP <ref type="bibr" target="#b19">(Lewis et al., 2006)</ref> dataset (11 million English documents), resulting 30 million for pre-training with the LayoutLMv2 <ref type="bibr" target="#b39">(Xu et al., 2021a)</ref> framework. However, this process is timeconsuming and laborious. On the contrary, LiLT can be pre-trained with only IIT-CDIP and then adapted to other languages. In this respect, LiLT is the first language-independent method for structured document understanding.</p><p>Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which substantially benefits numerous real-world SDU applications. Our main contributions can be summarized as follows:</p><p>? We introduce a simple yet effective languageindependent layout Transformer called LiLT for monolingual/multilingual structured document understanding.</p><p>? We propose BiACM to provide languageindependent cross-modality interaction, along with an effective asynchronous optimization strategy for textual and non-textual flows in pre-training. Moreover, we present two new pre-training objectives, namely KPL and CAI.</p><p>? LiLT achieves competitive or even superior performance on various widely-used downstream benchmarks of different languages under different settings, which fully demonstrates its effectiveness. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall illustration of our method. Given an input document image, we first use off-the-shelf OCR engines to get text bounding boxes and contents. Then, the text and layout information are separately embedded and fed into the corresponding Transformer-based architecture to obtain enhanced features. Bi-directional attention complementation mechanism (BiACM) is introduced to accomplish the cross-modality interaction of text and layout clues. Finally, the encoded text and layout features are concatenated and additional heads are added upon them, for the self-supervised pre-training or the downstream fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LiLT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Architecture</head><p>The whole framework can be regarded as a parallel dual-stream Transformer. The layout flow shares a similar structure as text flow, except for the reduced hidden size and intermediate size to achieve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Text Embedding</head><p>Following the common practice <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref>, in the text flow, all text strings in the OCR results are first tokenized and concatenated as a sequence S t by sorting the corresponding text bounding boxes from the top-left to bottom-right. Intuitively, the special tokens [CLS] and [SEP] are also added at the beginning and end of the sequence respectively. After this, S t will be truncated or padded with extra [PAD] tokens until its length equals the maximum sequence length N . Finally, we sum the token embedding E token of S t and the 1D positional embedding P 1D to obtain the text embedding E T ? R N ?d T as:</p><formula xml:id="formula_0">E T = LN(E token + P 1D ),<label>(1)</label></formula><p>where d T is the number of text feature dimension and LN is the layer normalization <ref type="bibr">(Ba et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Layout Embedding</head><p>As for the layout flow, we construct a 2D position sequence S l with the same length as the token sequence S t using the corresponding text bounding boxes. To be specific, we normalize and discretize all box coordinates to integers in the range [0, 1000], and use four embedding layers to generate x-axis, y-axis, height, and width features separately. Given the normalized bounding boxes B = (x min , x max , y min , y max , width, height), the 2D  positional embedding P 2D ? R N ?d L (where d L is the number of layout feature dimension) is constructed as follows:</p><formula xml:id="formula_1">MatMul MatMul MaskOut SoftMax Q T K T V T Transformer Layer i MatMul MatMul MaskOut SoftMax Q L K L V L</formula><formula xml:id="formula_2">O O H Q A Q O A - - - - - - - - - - - - - - - - - - - - - - - - - - 0 - - - - - - - 0 1 - - - - - - - - - - - - - - 0 0 0 - - - - - 0 0 0 1 - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Extraction</head><formula xml:id="formula_3">P 2D = Linear(CAT(E x min , E xmax , E y min , E ymax ,E width , E height )). (2)</formula><p>Here, the Es are embedded vectors. Linear is a linear projection layer and CAT is the channelwise concatenation operation. The special tokens [CLS], [SEP] and [PAD] are also attached with (0,0,0,0,0,0), (1000,1000,1000,1000,0,0) and (0,0,0,0,0,0) respectively. It is worth mentioning that, for each token, we directly utilize the bounding box of the text string it belongs to, because the fine-grained token-level information is not always included in the results of some OCR engines. Since Transformer layers are permutationinvariant, here we introduce the 1D positional embedding again. The resulting layout embedding E L ? R N ?d L can be formulated as:</p><formula xml:id="formula_4">E L = LN(P 2D + P 1D ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">BiACM</head><p>The text embedding E T and layout embedding E L are fed into their respective sub-models to generate high-level enhanced features. However, it will considerably ignore the cross-modal interaction process if we simply combine the text and layout features at the encoder output only. The network also needs to comprehensively analyse them at earlier stages. In view of this, we propose a new bi-directional attention complementation mechanism (BiACM) to strengthen the cross-modality interaction across the entire encoding pipeline. Experiments in Section 3.2 will further verify its effectiveness. The vanilla self-attention mechanism in Transformer layers captures the correlation between query x i and key x j by projecting the two vectors and calculating the attention score as:</p><formula xml:id="formula_5">? ij = (x i W Q )(x j W K ) ? d h .<label>(4)</label></formula><p>Here, the description is for a single head in a single self-attention layer with hidden size of d h and projection metrics W Q , W K for simplicity. Given ? T ij and ? L ij of the text and layout flows located in the same head of the same layer, BiACM shares them as common knowledge, which is formulated as:</p><formula xml:id="formula_6">? T ij = ? L ij + ? T ij ,<label>(5)</label></formula><formula xml:id="formula_7">? L ij = ? L ij + DETACH(? T ij ) if Pre-train, ? L ij + ? T ij if Fine-tune.<label>(6)</label></formula><p>In order to maintain the ability of LiLT to cooperate with different off-the-shelf text models in finetuning as much as possible, we heuristically adopt the detached ? T ij for ? L ij , so that the textual stream will not be affected by the gradient of non-textual one during pre-training, and its overall consistency can be preserved. Finally, the modified attention scores are used to weight the projected value vectors for subsequent modules in both flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-training Tasks</head><p>We conduct three self-supervised pre-training tasks to guide the model to autonomously learn joint representations with cross-modal cooperation. The details are introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Masked Visual-Language Modeling</head><p>This task is originally derived from <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. MVLM randomly masks some of the input tokens and the model is asked to recover them over the whole vocabulary using the output encoded features, driven by a cross-entropy loss. Meanwhile, the non-textual information remains unchanged. MVLM improves model learning on the language side with cross-modality information. The given layout embedding can also help the model better capture both inter-and intra-sentence relationships. We mask 15% text tokens, among which 80% are replaced by the special token [MASK], 10% are replaced by random tokens sampled from the whole vocabulary, and 10% remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Key Point Location</head><p>We propose this task to make the model better understand layout information in the structured documents. KPL equally divides the entire layout into several regions (we set 7?7=49 regions by default) and randomly masks some of the input bounding boxes. The model is required to predict which regions the key points (top-left corner, bottom-right corner, and center point) of each box belong to using separate heads. To deal with it, the model is required to fully understand the text content and know where to put a specific word/sentence when the surrounding ones are given. We mask 15% boxes, among which 80% are replaced by (0,0,0,0,0,0), 10% are replaced by random boxes sampled from the same batch, and 10% remain the same. Crossentropy loss is adopted.</p><p>Since there may exist detection errors in the output of OCR engines, we let the model predict the discretized regions (as mentioned above) instead of the exact location. This strategy can moderately relax the punishment criterion while improving the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Cross-modal Alignment Identification</head><p>We collect those encoded features of token-box pairs that are masked and further replaced (misaligned) or kept unchanged (aligned) by MVLM and KPL, and build an additional head upon them to identify whether each pair is aligned. To achieve this, the model is required to learn the cross-modal perception capacity. CAI is a binary classification task, and a cross-entropy loss is applied for it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization Strategy</head><p>Utilizing a unified learning rate for all model parameters to perform the end-to-end training process is the most common optimization strategy. While in our case, it will cause the layout flow to continuously update in the direction of coupling with the evolving text flow in the pre-training stage, which is harmful to the ability of LiLT to cooperate with different off-the-shelf textual models during fine-tuning. Based on this consideration, we explore multiple ratios to greatly slow down the pre-training optimization of the text stream. We also find that an appropriate reduction ratio is better than parameter freezing.</p><p>Note that, we adopt a unified learning rate for end-to-end optimization during fine-tuning. The DETACH operation of BiACM is also canceled at this time, as shown in Equation 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training Setting</head><p>We pre-train LiLT on the IIT-CDIP Test Collection 1.0 <ref type="bibr" target="#b19">(Lewis et al., 2006)</ref>, which is a large-scale scanned document image dataset and contains more than 6 million documents with more than 11 million scanned document images. We use TextIn API 1 to obtain the text bounding boxes and strings for this dataset.</p><p>In this paper, we initialize the text flow from the existing pre-trained English RoBERTa BASE <ref type="bibr" target="#b26">(Liu et al., 2019b)</ref>   in the number of parameters as 6.1M. The maximum sequence length N is set as 512.</p><p>LiLT BASE is pre-trained using Adam optimizer (Kingma and <ref type="bibr" target="#b17">Ba, 2015;</ref><ref type="bibr" target="#b27">Loshchilov and Hutter, 2018)</ref>, with the learning rate 2?10 ?5 , weight decay 1?10 ?2 , and (? 1 , ? 2 ) = (0.9, 0.999). The learning rate is linearly warmed up over the first 10% steps and then linearly decayed. We set the batch size as 96 and train LiLT BASE for 5 epochs on the IIT-CDIP dataset using 4 NVIDIA A40 48GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>Considering the complete pre-training takes a long time, we pre-train LiLT BASE with 2M documents randomly sampled from IIT-CDIP for 5 epochs to conduct ablation experiments, as shown in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We first evaluate the effect of introducing Bi-ACM. In setting (a)#1, the text and layout features are concatenated at the model output without any further interaction. Compared with (a)#6, we find that such a plain design results in a much   worse performance than using the text flow alone. From (a)#1 to (a)#3, the significant improvement demonstrates that it is the novel BiACM that makes the transfer from "monolingual" to "multilingual" successful. Beside this, we have also tried to replace BiACM with the co-attention mechanism <ref type="bibr" target="#b28">(Lu et al., 2019)</ref> which is widely adopted in dualstream Transformer architecture. It can be seen as a "deeper" cross-modal interaction, since the keys and values from each modality are passed as input to the other modality's dot-product attention calculation. However, severe drops are observed as shown in (a)#2 vs (a)#1#3. We attribute it to the damage of such a "deeper" interaction to the overall consistency of the text flow in the pre-training optimization. In contrast, BiACM can maintain LiLT's cross-model cooperation ability on the basis of providing cross-modal information. Moreover, the necessity of DETACH in pre-training is proved in (a)#4 vs (a)#3. Compared (a)#3 to (a)#5, we can also infer that removing DETACH in fine-tuning leads to a better performance.</p><p>Then, we compare the proposed KPL and CAI tasks. As shown in Table 1(b), both tasks improve the model performance substantially, and the proposed CAI benefits the model more than KPL. Using both tasks together is more effective than using either one alone.  <ref type="table">Table 3</ref>: Comparison on the semantic entity recognition (SER) task of CORD <ref type="bibr" target="#b29">(Park et al., 2019)</ref> dataset. 1 <ref type="bibr" target="#b12">(Garncarek et al., 2021)</ref>; 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Precision Recall F1</head><p>BiLSTM+CRF  Finally, we explore the most suitable slow-down ratio for the pre-training optimization of the text flow. A ratio equal to 1 in (c)#1 means there is no slow-down and a unified learning rate is adopted. It can be found that the F1 scores keep rising with the growth of slow-down ratios and begin to fall when the ratio is greater than 1000. Consequently, we set the slow-down ratio as 1000 by default.</p><formula xml:id="formula_8">1 - - 0.8910 GraphIE 2 - - 0.9026 GCN-based 3 - - 0.9255 TRIE 4 - - 0.9321 VIES 5 - - 0.9523 MatchVIE 6 - - 0.9687 TCPN 7 - - 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparisons with the SOTAs</head><p>To demonstrate the performance of LiLT, we conduct experiments on several widely-used monolingual datasets and the multilingual XFUND benchmark <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref>. In addition to the experiments involving typical language-specific finetuning, we also follow the two settings designed  in <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref> to demonstrate the ability to transfer knowledge among different languages, which are zero-shot transfer learning and multitask fine-tuning, for fair comparisons. Specifically, (1) language-specific fine-tuning refers to the typical fine-tuning paradigm of fine-tuning on language X and testing on language X. (2) Zero-shot transfer learning means the models are fine-tuned on English data only and then evaluated on each target language.</p><p>(3) Multitask fine-tuning requires the model to fine-tune on data in all languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Language-specific Fine-tuning</head><p>We first evaluate LiLT on four widely-used monolingual datasets -FUNSD <ref type="bibr" target="#b15">(Jaume et al., 2019)</ref>, CORD <ref type="bibr" target="#b29">(Park et al., 2019)</ref>, EPHOIE  and RVL-CDIP <ref type="bibr" target="#b19">(Lewis et al., 2006)</ref>, and the results are shown in <ref type="table">Table 2</ref>, 3, 4 and 5. We have found that (1) LiLT is flexible since it can work with monolingual or multilingual plain text models to deal with downstream tasks.</p><p>(2) Although LiLT is designed for the transfer from "monolingual" to "multilingual", it can surprisingly cooperate with monolingual textual models to achieve competitive or even superior performance (especially on the FUNSD dataset with only a few training samples available), compared with existing languagespecific SDU models such as LayoutLMv2 and  <ref type="table">Table 6</ref>: Language-specific fine-tuning F1 accuracy on FUNSD and XFUND (fine-tuning on X, testing on X). "SER" denotes the semantic entity recognition and "RE" denotes the relation extraction. [] indicates the off-theshelf textual model used as the text flow of LiLT.</p><p>DocFormer.</p><p>(3) On these datasets which are widely adopted for monolingual evaluation, LiLT generally performs better than LayoutXLM. This fully demonstrates the effectiveness of our pre-training framework and indicates that the layout and text information can be successfully decoupled in pretraining and re-coupled in fine-tuning. Then we evaluate LiLT on language-specific fine-tuning tasks of FUNSD and the multilingual XFUND <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref>, and the results are shown in <ref type="table">Table 6</ref>. Compared with the plain text models (XLM-R/InfoXLM) or the LayoutXLM model pre-trained with 30M multilingual structured documents, LiLT achieves the highest F1 scores on both the SER and RE tasks of each language while using 11M monolingual data. This significant improvement shows LiLT's capability to transfer language-independent knowledge from pre-training to downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Zero-shot Transfer Learning</head><p>The results of cross-lingual zero-shot transfer are presented in <ref type="table" target="#tab_12">Table 7</ref>. It can be observed that the LiLT model transfers the most knowledge from English to other languages, and significantly outperforms its competitors. This fully verifies that LiLT can capture the common layout invariance among different languages. Moreover, LiLT has never seen non-English documents before evaluation under this setting, while the LayoutXLM model has been pre-trained with them. This is to say, LiLT faces a stricter cross-lingual zero-shot transfer scenario but achieves better performance. <ref type="table" target="#tab_13">Table 8</ref> shows the results of multitask learning. In this setting, the pre-trained LiLT model is simultaneously fine-tuned with all eight languages and evaluated for each specific language. We observe that this setting further improves the model performance compared to the language-specific finetuning, which confirms that SDU can benefit from commonalities in the layout of multilingual structured documents. In addition, LiLT once again outperforms its counterparts by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Multi-task Fine-tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>During the past decade, deep learning methods became the mainstream for document understanding tasks <ref type="bibr" target="#b42">(Yang et al., 2017;</ref><ref type="bibr" target="#b2">Augusto Borges Oliveira et al., 2017;</ref><ref type="bibr" target="#b33">Siegel et al., 2018)</ref>. Grid-based methods <ref type="bibr" target="#b16">(Katti et al., 2018;</ref><ref type="bibr" target="#b10">Denk and Reisswig, 2019;</ref><ref type="bibr" target="#b24">Lin et al., 2021)</ref> were proposed for 2D document representation where text pixels were encoded using character or word embeddings and classified into specific field types, using a convolutional neural network. GNN-based approaches <ref type="bibr" target="#b25">(Liu et al., 2019a;</ref><ref type="bibr" target="#b43">Yu et al., 2021;</ref><ref type="bibr" target="#b35">Tang et al., 2021)</ref> adopted multi-modal features of text segments as nodes to model the document graph, and used graph neural networks to propagate information between neighboring nodes to attain a richer representation.</p><p>In recent years, self-supervised pre-training has achieved great success. Inspired by the development of the pre-trained language models in various NLP tasks, recent studies on structured document pre-training <ref type="bibr" target="#b39">(Xu et al., , 2021a</ref><ref type="bibr">Li et al., 2021a,b,c;</ref><ref type="bibr" target="#b1">Appalaraju et al., 2021)</ref> have pushed the limits. LayoutLM  modified the BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> architecture by adding 2D spatial coordinate embeddings. In comparison, our LiLT can be regarded as a more powerful and flexible solution for structured document understanding. LayoutLMv2 <ref type="bibr" target="#b39">(Xu et al., 2021a)</ref>    Nevertheless, the aforementioned SDU approaches mainly focus on a single language -typically English, which is extremely limited with respect to multilingual application scenarios. To the best of our knowledge, LayoutXLM <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref> was the only pre-existing multilingual SDU model, which adopted the multilingual textual model InfoXLM <ref type="bibr" target="#b5">(Chi et al., 2021)</ref> as the initialization, and adapted the LayoutLMv2 <ref type="bibr" target="#b39">(Xu et al., 2021a)</ref> framework to multilingual structured document pre-training. However, it required a heavy process of multilingual data collection, cleaning and pre-training. On the contrary, our LiLT can deal with the multilingual structured documents by pre-training on the monolingual IIT-CDIP Test Collection 1.0 <ref type="bibr" target="#b19">(Lewis et al., 2006)</ref> only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present LiLT, a languageindependent layout Transformer that can learn the layout knowledge from monolingual structured documents and then generalize it to deal with multilingual ones. Our framework successfully first decouples the text and layout information in pre-training and then re-couples them for finetuning. Experimental results on eight languages under three settings (language-specific, cross-lingual zero-shot transfer, and multi-task fine-tuning) have fully illustrated its effectiveness, which substantially bridges the language gap in real-world structured document understanding applications. The public availability of LiLT is also expected to promote the development of document intelligence.</p><p>For future research, we will continue to follow the pattern of transferring from "monolingual" to "multilingual" and further unlock the power of LiLT. In addition, we will also explore the generalized rather than language-specific visual information contained in multilingual structured documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Fine-tuning Details</head><p>Fine-tuning for Semantic Entity Recognition We conduct the semantic entity recognition task on FUNSD, CORD, EPHOIE and XFUND. We build a token-level classification layer above the output representations to predict the BIO tags for each entity field.</p><p>Fine-tuning for Document Classification This task depends on high-level visual information, thereby we leverage the image features explicitly in the fine-tuning stage, following LayoutLMv2 <ref type="bibr" target="#b39">(Xu et al., 2021a)</ref>. We pool the visual feature of the ResNeXt101-FPN <ref type="bibr" target="#b38">(Xie et al., 2017;</ref><ref type="bibr" target="#b23">Lin et al., 2017)</ref> backbone into a global feature, concatenate it with the [CLS] output feature, and feed them into the final classification layer.</p><p>Fine-tuning for Relation Extraction We build the additional head for relation extraction on the FUNSD and XFUND datasets following <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref> for fair comparison. We first incrementally construct the set of relation candidates by producing all possible pairs of given semantic entities. For every pair, the representation of the head/tail entity is the concatenation of the first token vector in each entity and the entity type embedding obtained with a specific type embedding layer. After respectively projected by two FFN layers, the representations of head and tail are concatenated and then fed into a bi-affine classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The overall illustration of our framework. Text and layout information are separately embedded and fed into the corresponding flow. BiACM is proposed to accomplish the cross-modality interaction. At the model output, text and layout features are concatenated for the self-supervised pre-training or the downstream fine-tuning. N l is the number of Transformer layers. The red * M /* R indicates the randomly masked/replaced item for pre-training. t, b and r represent token, box and region, respectively. Best viewed in zoomed-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2: Comparison on the semantic entity recognition (SER) task of FUNSD<ref type="bibr" target="#b15">(Jaume et al., 2019)</ref> dataset. Bold indicates the SOTA and underline indicates the second best. "EN-R" is short for English RoBERTa.The multilingual model. [] denotes the off-the-shelf textual model used as the text flow of LiLT. 1<ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>; 2<ref type="bibr" target="#b26">(Liu et al., 2019b)</ref>; 3<ref type="bibr" target="#b4">(Bao et al., 2020)</ref>; 4; 5<ref type="bibr" target="#b14">(Hong et al., 2020)</ref>; 6<ref type="bibr" target="#b21">(Li et al., 2021b)</ref>; 7<ref type="bibr" target="#b39">(Xu et al., 2021a)</ref>; 8<ref type="bibr" target="#b22">(Li et al., 2021c)</ref>; 9<ref type="bibr" target="#b1">(Appalaraju et al., 2021)</ref>; 10 (Xu et al., 2021b); 11<ref type="bibr" target="#b5">(Chi et al., 2021)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>? Scale The Text Flow (RoBERTa/XLM-R/InfoXLM/...) The Layout Flow (LiLT) || Pre-training Objectives</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Transformer Layer i</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>t3</cell><cell>-</cell><cell>t5</cell><cell>-</cell><cell>t7</cell><cell>-</cell><cell>Masked Visual-Language Modeling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>r2</cell><cell>-</cell><cell>r4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>r8 Key Point Location</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>0</cell><cell>-</cell><cell>1</cell><cell>0</cell><cell>-</cell><cell>1</cell><cell>-</cell><cell>Cross-modal Alignment Identification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(0:Mis-aligned, 1:Aligned)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Fine-tuning Tasks</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Semantic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entity Recognition</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(H:Header, Q:Question,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A:Answer, O:Other)</cell></row><row><cell>Token Embedding</cell><cell>t1</cell><cell>t2</cell><cell>tM</cell><cell>t4</cell><cell>tR</cell><cell>t6</cell><cell>t7</cell><cell>t8</cell><cell>b1</cell><cell>bR</cell><cell>b3</cell><cell>b4</cell><cell>b5</cell><cell>b6</cell><cell>b7</cell><cell>bM</cell><cell>2D Position Embedding</cell></row><row><cell></cell><cell cols="8">+ + + + + + + +</cell><cell cols="8">+ + + + + + + +</cell><cell></cell></row><row><cell>1D Position Embedding</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>1D Position Embedding</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for our document pre-training, and combine LiLT BASE with the pre-trained InfoXLM BASE (Chi et al., 2021)/a new pre-trained RoBERTa BASE for multilingual/monolingual finetuning. They have an equal number of selfattention layers, attention heads and maximum sequence length, which ensures that BiACM can work normally. In this BASE setting, LiLT has a 12-layer encoder with 192 hidden size, 768 feedforward filter size and 12 attention heads, resulting</figDesc><table><row><cell cols="4"># Inter-modal Operation</cell><cell></cell><cell>Average F1</cell></row><row><cell cols="2">1 CAT</cell><cell></cell><cell></cell><cell></cell><cell>0.6751</cell></row><row><cell cols="5">2 CAT+Co-Attention (Lu et al., 2019)</cell><cell>0.6276</cell></row><row><cell cols="3">3 CAT+BiACM</cell><cell></cell><cell></cell><cell>0.7963</cell></row><row><cell cols="5">4 CAT+BiACM?DETACH in pre-training</cell><cell>0.7682</cell></row><row><cell cols="5">5 CAT+BiACM+DETACH in fine-tuning</cell><cell>0.7822</cell></row><row><cell>6</cell><cell cols="4">The text flow alone (InfoXLM BASE , as shown in Table 6)</cell><cell>0.7207</cell></row><row><cell cols="6">(a) BiACM. CAT is short for concatenation.</cell></row><row><cell>#</cell><cell></cell><cell>MVLM</cell><cell>KPL</cell><cell>CAI</cell><cell>Average F1</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7616</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7748</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7809</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7963</cell></row><row><cell></cell><cell></cell><cell cols="3">(b) Pre-training tasks.</cell></row><row><cell></cell><cell>#</cell><cell cols="2">Slow-down Ratio</cell><cell></cell><cell>Average F1</cell></row><row><cell></cell><cell>1</cell><cell cols="2">1 (No Slow-down)</cell><cell></cell><cell>0.7840</cell></row><row><cell></cell><cell>2</cell><cell>500</cell><cell></cell><cell></cell><cell>0.7901</cell></row><row><cell></cell><cell>3</cell><cell>800</cell><cell></cell><cell></cell><cell>0.7947</cell></row><row><cell></cell><cell>4</cell><cell>1000</cell><cell></cell><cell></cell><cell>0.7963</cell></row><row><cell></cell><cell>5</cell><cell>1200</cell><cell></cell><cell></cell><cell>0.7935</cell></row><row><cell></cell><cell>6</cell><cell cols="3">+? (Parameter Freezing)</cell><cell>0.7893</cell></row></table><note>(c) Slow-down ratios.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Ablation study of LiLT BASE combined with InfoXLM BASE (Chi et al., 2021) on the FUNSD and XFUND datasets (8 languages in total). The average F1 accuracy of language-specific semantic entity recogni- tion (SER) task is given. (a) BiACM. (b) Pre-training tasks. (c) Slow-down ratios of the pre-training opti- mization for the text flow.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Comparison on the semantic entity recognition</cell></row><row><cell>(SER) task of EPHOIE (Wang et al., 2021a) dataset.</cell></row><row><cell>"ZH-R" is short for Chinese RoBERTa. 1 (Lample et al.,</cell></row><row><cell>2016); 2 (Qian et al., 2019); 3 (Liu et al., 2019a); 4 (Zhang</cell></row><row><cell>et al., 2020); 5 (Wang et al., 2021a); 6 (Tang et al.,</cell></row><row><cell>2021);</cell></row></table><note>7 (Wang et al., 2021b); 8 (Cui et al., 2020).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison on the document classification (DC) task of RVL-CDIP (Harley et al., 2015) dataset.</figDesc><table /><note>1 (Afzal et al., 2017); 2 (Das et al., 2018); 3 (Szegedy et al., 2017); 4 (Sarkhel and Nandi, 2019); 5 (Dauphinee et al., 2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>BASE English only 11M 0.8415 0.8938 0.7964 0.7911 0.7953 0.8376 0.8231 0.8220 0.8251</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="2">Pre-training Docs</cell><cell>FUNSD</cell><cell></cell><cell></cell><cell></cell><cell>XFUND</cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>Language</cell><cell>Size</cell><cell>EN</cell><cell>ZH</cell><cell>JA</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>DE</cell><cell>PT</cell></row><row><cell></cell><cell>XLM-RoBERTa BASE</cell><cell>-</cell><cell>-</cell><cell cols="7">0.6670 0.8774 0.7761 0.6105 0.6743 0.6687 0.6814 0.6818 0.7047</cell></row><row><cell>SER</cell><cell>InfoXLM BASE LayoutXLM BASE</cell><cell cols="9">-Multilingual 30M 0.7940 0.8924 0.7921 0.7550 0.7902 0.8082 0.8222 0.7903 0.8056 -0.6852 0.8868 0.7865 0.6230 0.7015 0.6751 0.7063 0.7008 0.7207</cell></row><row><cell cols="2">XLM-RoBERTa BASE InfoXLM BASE LiLT[InfoXLM] RE LayoutXLM BASE</cell><cell cols="9">--Multilingual 30M 0.5483 0.7073 0.6963 0.6896 0.6353 0.6415 0.6551 0.5718 0.6432 -0.2659 0.5105 0.5800 0.5295 0.4965 0.5305 0.5041 0.3982 0.4769 -0.2920 0.5214 0.6000 0.5516 0.4913 0.5281 0.5262 0.4170 0.4910</cell></row><row><cell></cell><cell cols="10">LiLT[InfoXLM] BASE English only 11M 0.6276 0.7297 0.7037 0.7195 0.6965 0.7043 0.6558 0.5874 0.6781</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Cross-lingual zero-shot transfer F1 accuracy on FUNSD and XFUND (fine-tuning on FUNSD, testing on X). ? indicates that LiLT faces a stricter zero-shot transfer scenario compared with LayoutXLM, since it has never seen non-English documents before evaluation, even during pre-training.</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="2">Pre-training Docs</cell><cell>FUNSD</cell><cell></cell><cell></cell><cell></cell><cell>XFUND</cell><cell></cell><cell></cell><cell></cell><cell>Avg.</cell></row><row><cell></cell><cell></cell><cell>Language</cell><cell>Size</cell><cell>EN</cell><cell>ZH</cell><cell>JA</cell><cell>ES</cell><cell>FR</cell><cell>IT</cell><cell>DE</cell><cell>PT</cell></row><row><cell></cell><cell>XLM-RoBERTa BASE</cell><cell>-</cell><cell>-</cell><cell>0.6633</cell><cell>0.8830</cell><cell>0.7786</cell><cell>0.6223</cell><cell>0.7035</cell><cell>0.6814</cell><cell>0.7146</cell><cell>0.6726</cell><cell>0.7149</cell></row><row><cell>SER</cell><cell>InfoXLM BASE LayoutXLM BASE</cell><cell cols="3">-Multilingual 30M 0.7924 -0.6538</cell><cell>0.8741 0.8973</cell><cell>0.7855 0.7964</cell><cell>0.5979 0.7798</cell><cell>0.7057 0.8173</cell><cell>0.6826 0.8210</cell><cell>0.7055 0.8322</cell><cell>0.6796 0.8241</cell><cell>0.7106 0.8201</cell></row><row><cell></cell><cell cols="4">LiLT[InfoXLM] BASE English only 11M 0.8574</cell><cell>0.9047</cell><cell>0.8088</cell><cell>0.8340</cell><cell>0.8577</cell><cell>0.8792</cell><cell>0.8769</cell><cell>0.8493</cell><cell>0.8585</cell></row><row><cell></cell><cell>XLM-RoBERTa BASE</cell><cell>-</cell><cell>-</cell><cell>0.3638</cell><cell>0.6797</cell><cell>0.6829</cell><cell>0.6828</cell><cell>0.6727</cell><cell>0.6937</cell><cell>0.6887</cell><cell>0.6082</cell><cell>0.6341</cell></row><row><cell>RE</cell><cell>InfoXLM BASE LayoutXLM BASE</cell><cell cols="3">-Multilingual 30M 0.6671 -0.3699</cell><cell>0.6493 0.8241</cell><cell>0.6473 0.8142</cell><cell>0.6828 0.8104</cell><cell>0.6831 0.8221</cell><cell>0.6690 0.8310</cell><cell>0.6384 0.7854</cell><cell>0.5763 0.7044</cell><cell>0.6145 0.7823</cell></row><row><cell></cell><cell cols="4">LiLT[InfoXLM] BASE English only 11M 0.7407</cell><cell cols="2">0.8471 0.8345</cell><cell>0.8335</cell><cell>0.8466</cell><cell>0.8458</cell><cell cols="2">0.7878 0.7643</cell><cell>0.8125</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Multitask fine-tuning F1 accuracy on FUNSD and XFUND (fine-tuning on 8 languages all, testing on X).</figDesc><table><row><cell>tures as separate tokens. Furthermore, additional</cell></row><row><cell>pre-training tasks were explored to improve the uti-</cell></row><row><cell>lization of unlabeled document data. SelfDoc (Li</cell></row><row><cell>et al., 2021b) established the contextualization over</cell></row><row><cell>a block of content, while StructuralLM (Li et al.,</cell></row><row><cell>2021a) proposed cell-level 2D position embeddings</cell></row><row><cell>and the corresponding pre-training objective. Re-</cell></row><row><cell>cently, StrucTexT (Li et al., 2021c) introduced a</cell></row><row><cell>unified solution to efficiently extract semantic fea-</cell></row><row><cell>tures from different levels and modalities to handle</cell></row><row><cell>the entity labeling and entity linking tasks. Doc-</cell></row><row><cell>Former (Appalaraju et al., 2021) designed a novel</cell></row><row><cell>multi-modal self-attention layer capable of fusing</cell></row><row><cell>textual, vision and spatial features.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.textin.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is supported in part by NSFC (Grant No.: 61936003)  and GD-NSF (No.  2017A030312006).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A Dataset Details FUNSD FUNSD <ref type="bibr" target="#b15">(Jaume et al., 2019)</ref> is an English dataset for form understanding in noisy scanned documents. It contains 199 real, fully annotated, scanned forms where 9,707 semantic entities are annotated above 31,485 words. The 199 samples are split into 149 for training and 50 for testing. We directly use the official OCR annotations. The semantic entity recognition (SER) task is assigning to each word a semantic entity label from a set of four predefined categories: question, answer, header, or other. The entity-level F1 score is used as the evaluation metric <ref type="table">(Table 2)</ref>.</p><p>CORD CORD <ref type="bibr" target="#b29">(Park et al., 2019)</ref> is an English receipt dataset for key information extraction. Its publicly available subset includes 800 receipts for the training set, 100 for the validation set, and 100 for the test set. A photo and a list of OCR annotations are equipped for each receipt. The dataset defines 30 fields under 4 categories and the task aims to label each word to the right field. The evaluation metric is the entity-level F1 score, as shown in <ref type="table">Table 3</ref>. We use the official OCR annotations.</p><p>EPHOIE EPHOIE  is collected from actual Chinese examination papers with the diversity of text types and layout distribution. The 1,494 samples are divided into a training set with 1,183 images and a testing set with 311 images, respectively. It defines ten entity categories, and we provide the entity-level F1 score for RoBERTa, LayoutXLM and LiLT in <ref type="table">Table 4</ref>. The official OCR annotations are adopted.</p><p>RVL-CDIP RVL-CDIP (Harley et al., 2015) consists of 400,000 gray-scale images of English documents, with 8:1:1 for the training set, validation set, and test set. A multi-class single-label classification task is defined on RVL-CDIP. The images are categorized into 16 classes, with 25,000 images per class. The evaluation metric is the overall classification accuracy as shown in <ref type="table">Table 5</ref>. Text and layout information are extracted by TextIn API.</p><p>XFUND XFUND <ref type="bibr" target="#b41">(Xu et al., 2021b</ref>) is a multilingual form understanding dataset that contains 1,393 fully annotated forms with seven languages including Chinese (ZH), Japanese (JA), Spanish (ES), French (FR), Italian (IT), German (DE), and Portuguese (PT). Each language includes 199 forms, where the training set includes 149 forms, and the test set includes 50 forms. We focus on the semantic entity recognition (SER) and relation extraction (RE) tasks defined in the original paper <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref>. Relation extraction aims to predict the relation between any two given semantic entities, and we mainly focus on the key-value relation extraction. We use the official OCR results, and the same F1 accuracy evaluation metric as in LayoutXLM <ref type="bibr" target="#b41">(Xu et al., 2021b)</ref> for <ref type="table">Table 6</ref>, 7 and 8.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cutting the error by half: Investigation of very deep CNN and advanced training strategies for document image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Muhammad Zeshan Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheraz</forename><surname>K?lsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="883" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Doc-Former: End-to-end Transformer for document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikar</forename><surname>Appalaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavan</forename><surname>Jasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusheng</forename><surname>Bhargava Urala Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast CNNbased document layout analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario Augusto Borges</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1173" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UniLMv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In-foXLM: An information-theoretic framework for cross-lingual language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He-Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3576" to="3588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting pretrained models for Chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="657" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Document image classification with intra-domain transfer learning and stacked generalization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swapan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICPR</title>
		<imprint>
			<biblScope unit="page" from="3180" to="3185" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modular multimodal architecture for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Dauphinee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rashidi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04376</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERTgrid: Contextualized embedding for 2D document representation and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reisswig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Document Intelligence at NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional Transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LAMBERT: Layout-aware (language) modeling using BERT for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Garncarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Stanis?awek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartosz</forename><surname>Topolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Halama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Grali?ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluation of deep convolutional nets for document image classification and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BROS: A pre-trained language model for understanding texts in document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teakgyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonseok</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FUNSD: A dataset for form understanding in noisy scanned documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Jaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chargrid: Towards understanding 2D documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Anoop R Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordula</forename><surname>Reisswig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Guder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Brarda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>H?hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baptiste Faddoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4459" to="4469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a test collection for complex document information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gady</forename><surname>Agam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Frieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferson</forename><surname>Heard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="665" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Struc-turalLM: Structural pre-training for form understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SelfDoc: Self-supervised document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfu</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5652" to="5660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">StrucTexT: Structured text understanding with multi-modal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiameng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
	<note>Feature pyramid networks for object detection</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ViBERTgrid: A jointly trained multi-modal 2D document representation for key information extraction from documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph convolution for multimodal information extraction from visually rich documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huasha</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CORD: A consolidated receipt dataset for post-OCR parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bado</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeheung</forename><surname>Surh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwalsuk</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Document Intelligence at NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going full-TILT boogie on document understanding with text-image-layout Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Powalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Jurkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Pietruszka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Pa?ka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GraphIE: A graph-based framework for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="751" to="761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deterministic routing between layout abstractions for multi-scale classification of visually rich documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritesh</forename><surname>Sarkhel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Nandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3360" to="3366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extracting scientific figures with distantly supervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MatchVIE: Exploiting match relevancy between entities for visual information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1039" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards robust visual information extraction in real world: New dataset and novel solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaitao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxiang</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2738" to="2745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tag, copy or predict: A unified weaklysupervised learning framework for visual information extraction using sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guozhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1082" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LayoutLMv2: Multi-modal pre-training for visually-rich document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LayoutLM: Pretraining of text and layout for document image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIGKDD</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1192" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">LayoutXLM: Multimodal pre-training for multilingual visually-rich document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinei</forename><surname>Florencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08836</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to extract semantic structure from documents using multimodal fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Asente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Kraley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5315" to="5324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PICK: Processing key information extraction from documents using improved graph learning-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4363" to="4370" />
		</imprint>
	</monogr>
	<note>Ping Gong, and Rong Xiao</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">TRIE: End-to-end text reading and information extraction for document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanzhan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1413" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
