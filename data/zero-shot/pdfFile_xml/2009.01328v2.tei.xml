<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Internal Cluster Validity Index Using a Distance- based Separability Measure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyue</forename><surname>Guan</surname></persName>
							<email>frankshuyueguan@gwu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Loew</surname></persName>
							<email>loew@gwu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">The George Washington University Washington DC</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering The George</orgName>
								<orgName type="institution">Washington University</orgName>
								<address>
									<addrLine>Washington DC</addrLine>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Internal Cluster Validity Index Using a Distance- based Separability Measure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*Corresponding author</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>cluster validity</term>
					<term>cluster validity index evaluation</term>
					<term>clustering analysis</term>
					<term>separability measure</term>
					<term>distance-based separability index</term>
					<term>sequence comparison</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To evaluate clustering results is a significant part of cluster analysis. There are no true class labels for clustering in typical unsupervised learning. Thus, a number of internal evaluations, which use predicted labels and data, have been created. They are also named internal cluster validity indices (CVIs). Without true labels, to design an effective CVI is not simple because it is similar to create a clustering method. And, to have more CVIs is crucial because there is no universal CVI that can be used to measure all datasets, and no specific method for selecting a proper CVI for clusters without true labels. Therefore, to apply more CVIs to evaluate clustering results is necessary. In this paper, we propose a novel CVI -called Distance-based Separability Index (DSI), based on a data separability measure. We applied the DSI and eight other internal CVIs including early studies from Dunn (1974) to most recent studies CVDD (2019) as comparison. We used an external CVI as ground truth for clustering results of five clustering algorithms on 12 real and 97 synthetic datasets. Results show DSI is an effective, unique, and competitive CVI to other compared CVIs. In addition, we summarized the general process to evaluate CVIs and created a new method -rank difference -to compare the results of CVIs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As a typical unsupervised learning method, cluster analysis plays a critical role in machine learning. Without any prior knowledge, a clustering algorithm could divide a dataset into clusters <ref type="bibr" target="#b0">[1]</ref> based on the distribution structure of the data. Clustering is a main study of data mining <ref type="bibr" target="#b1">[2]</ref> and widely used in many fields, including pattern recognition <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, image segmentation <ref type="bibr" target="#b4">[5]</ref>, bioinformatics <ref type="bibr" target="#b5">[6]</ref>, and data compression <ref type="bibr" target="#b6">[7]</ref>. For some machine learning applications, such as medical image analysis, one problem is the shortage of training data because labeling is expensive <ref type="bibr" target="#b7">[8]</ref>; the same problem arises in applications of big data <ref type="bibr" target="#b8">[9]</ref>. Cluster analysis is a promising solution to the lack of labels problem. Clustering methods can be categorized into centroid-based (e.g. k-means), distribution-based (e.g. EM algorithm <ref type="bibr" target="#b9">[10]</ref>), density-based (e.g. DBSCAN <ref type="bibr" target="#b10">[11]</ref>), hierarchical (e.g. Ward linkage <ref type="bibr" target="#b12">[12]</ref>), spectral clustering <ref type="bibr" target="#b13">[13]</ref> and others. However, none of the clustering methods can perform well with all datasets <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>. In other words, a clustering method performing well for some types of datasets will perform poorly for some others. Hence, many clustering methods have been created for various types of datasets. As a result, to evaluate which method performs well for a dataset, effective measures of clustering quality (clustering validations) are required <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>. Meanwhile, these measures are also employed to tune parameters of clustering algorithms.</p><p>Clustering evaluations have two categories: internal and external evaluations. External evaluations use the true class labels and predicted labels, and internal evaluations use predicted labels and data. Since external evaluations require true labels, they are applied for supervised learning. For absolute unsupervised learning tasks, there are no true class labels, thus we can apply only the internal evaluations for clusters <ref type="bibr" target="#b18">[18]</ref>. In fact, to measure clustering results by internal evaluations is as difficult as to analyze clustering itself <ref type="bibr" target="#b19">[19]</ref> because measurements have no more information than the clustering methods. Therefore, designing an internal cluster validity index (CVI) is similar to creating an optimizing function for a clustering algorithm. The difference is that the optimizing function will provide a value (loss) with the ability to update clustering results but CVI delivers only a value for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Related Work</head><p>A variety of CVIs have been designed to deal with the many types of datasets <ref type="bibr" target="#b20">[20]</ref>. Here, we select some representative CVIs for comparison. Since external evaluations use the true class labels and predicted labels, we choose an external CVI -the adjusted Rand index (ARI) <ref type="bibr" target="#b21">[21]</ref>, as the ground truth for comparison.</p><p>Internal CVIs can be categorized in two types: center and non-center by their representatives for calculation <ref type="bibr" target="#b22">[22]</ref>. Center based CVIs use information of clusters. For example, the Davies-Bouldin index (DB) <ref type="bibr" target="#b23">[23]</ref> involves cluster diameters and the distance between cluster centroids. Non-center CVIs use information of data points. For example, the Dunn index <ref type="bibr" target="#b24">[24]</ref> uses the minimum and maximum distances between two data points. Besides the DB and Dunn indexes, two more traditional CVIs are applied for comparison in our study: Calinski-Harabasz index (CH) <ref type="bibr" target="#b25">[25]</ref> and Silhouette coefficient (Sil) <ref type="bibr" target="#b26">[26]</ref>. For recently developed CVIs, we select the I index <ref type="bibr" target="#b27">[27]</ref>, WB index <ref type="bibr" target="#b28">[28]</ref>, clustering validation index based on nearest neighbors (CVNN) <ref type="bibr" target="#b18">[18]</ref>, and cluster validity index based on density-involved distance (CVDD) <ref type="bibr" target="#b22">[22]</ref>.</p><p>In summary, we select one external CVI as the ground truth and eight typical internal CVIs for comparison with our proposed CVI. The compared CVIs range from early studies from <ref type="bibr" target="#b24">Dunn (1974)</ref> to the most recent studies CVDD (2019). Unless specifically stated otherwise, the CVIs that appear in the rest paper mean internal CVIs. And the single external CVI is named ARI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distance-based Separability Measure</head><p>Generally, the goal of clustering is to separate a dataset into clusters. From a macro perspective, we consider that the degree of separability of clusters could indicate how well a dataset has been separated. A clustering algorithm assigns every data point a class label. In the worst case, if all labels are assigned randomly, the data points of different classes could be seen having the same distribution. This is the most difficult situation for separation of the dataset. To analyze how the classes of data are distributed, we propose the distance-based separability index (DSI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If two classes and have have ,</head><p>points, we define that the Intra-Class distance (ICD) set is the set of distances between any two points in the same class :</p><formula xml:id="formula_0">{ } = ?? ? ? 2 | , ? ; ? ? If | | = , then |{ }| = 1 2 ( ? 1).</formula><p>And the Between-class distance (BCD) set is the set of distances between any two points from different classes and :</p><formula xml:id="formula_1">? , ? = ?? ? ? 2 | ? ; ? ? If | | = , | | = , then ?? , ?? = .</formula><p>It can be shown 1 that: if and only if two classes and have sufficient data points with the same distribution, the distributions of ICD and BCD sets are nearly identical. Hence, if the distributions of the ICD and BCD sets are nearly identical, all labels are assigned randomly and thus the dataset has the worst separability. The metric of distance is Euclidean ( 2norm). The time cost for computing ICD and BCD sets is linear with the dimensionality, and quadratic with the number, of observations.</p><p>To compute the DSI of the two classes and , first, we compute the ICD sets of and : { }, ? ? and the BCD set:? , ?. To examine the similarity of the distributions of ICD and BCD sets, we apply the Kolmogorov-Smirnov (KS) test <ref type="bibr">[29]</ref>. Although there are many statistical measures to compare two distributions, such as Bhattacharyya distance, Kullback-Leibler divergence and Jensen-Shannon divergence, most require that the two sets have the same number of observations. It is easy to show that the|{ }|, ?? ??and?? , ?? cannot be the same. The similarities between the ICD and BCD sets are then computed using the KS test: = ?{ }, ? , ?? and = ?? ?, ? , ??. Since there are two classes, the DSI is the average of the two KS similarities:</p><p>({ , }) = ? + ? 2 ? . The ?{ }, ? ?? is not considered because it shows only the difference of distribution shapes, not their location information. For example, two distributions that have the same shape, but no overlap will have similarities between ICD:</p><p>?{ }, ? ?? equal to zero. In general, for an n-class dataset (DSI Algorithm):</p><p>? Compute n ICD sets for each class: ? ?; = 1,2, ? , .</p><p>? For the i-th class of data , the BCD set is the set of distances between any two points in and ? (other classes, not ): ? , ? ?. The KS similarity between ICD and BCD set is = ?? ?, ? , ? ??.</p><p>? The DSI of this dataset is  <ref type="figure" target="#fig_0">Figure 1</ref> displays a two-class dataset. If the labels are assigned correctly, the distributions of ICD sets will be different from the BCD set; the DSI will reach the maximum value for this dataset (see <ref type="figure" target="#fig_0">Figure 1a</ref>) because the two clusters are well separated. For incorrect clustering <ref type="figure" target="#fig_0">(Figure 1b)</ref>, the difference between distributions of ICD and BCD sets becomes smaller so that the DSI value decreases. For an extreme situation ( <ref type="figure" target="#fig_0">Figure  1c</ref>), if all labels are randomly assigned, the distributions of the ICD and BCD sets will be nearly the same. This is the worst case of separation; DSI is close to zero. Therefore, the DSI could well reflect the separability of clusters. DSI ranges from 0 to 1; the greater value means the dataset is clustered better.</p><formula xml:id="formula_2">({ }) = (? )? .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>The goal of CVIs is to evaluate the clustering results. In this study, we employ several internal CVIs including our proposed DSI to examine the clustering results from several clustering methods (algorithms). For a given dataset, different clustering algorithms may return different clusters; CVIs are used to find the best clusters. We choose an external CVI -the adjusted Rand index (ARI) as the ground truth for comparison because ARI involves true labels (clusters) of the dataset. Eight commonly used CVIs are selected to compare with the proposed DSI and are shown in TABLE I. They include classical and recent CVIs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synthetic datasets</head><p>The synthetic datasets for clustering are from the Tomas Barton repository. This repository contains 122 artificial datasets <ref type="bibr" target="#b1">2</ref> . Each dataset has hundreds to thousands of objects with several to tens of classes in two or three dimensions (features). We selected 97 datasets for experiment; the 25 unused datasets have too many objects to run the clustering processing in reasonable time. The names of the 97 used synthetic datasets are shown in   <ref type="table" target="#tab_0">2d-10c  ds2c2sc13  rings  square5  aggregation  2d-20c-no0  ds3c3sc6  shapes  st900  2d-3c-no123  threenorm  ds4c2sc8  simplex  target  dense-disk-3000  triangle1  2d-4c  sizes1  tetra  dense-disk-5000  triangle2  2dnormals  sizes2  curves1  elliptical_10_2  dartboard1  engytime  sizes3  curves2  elly-2d10c13s  dartboard2  flame  sizes4  D31  2sp2glob</ref> 2d-4c-no4 fourty sizes5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-spiral</head><formula xml:id="formula_3">twenty cure-t0-2000n-2D 2d-4c-no9 gaussians1 smile1 aml28 cure-t1-2000n-2D diamond9</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real datasets</head><p>In this study, the 12 real datasets used for clustering are from three sources: the sklearn.datasets package 4 , UC Irvine Machine Learning Repository <ref type="bibr" target="#b29">[30]</ref> and Tomas Barton's repository (real world datasets). Unlike the synthetic datasets, the dimensions (feature numbers) of most selected real datasets are greater than three. Hence, to evaluate their clustering results we must use CVIs rather than plotting clusters as for 2D or 3D synthetic datasets. Details about the 12 real datasets appear in TABLE III. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Evaluations</head><p>In summary, the steps to verify CVIs are:</p><p>? To run different clustering algorithms on a dataset to obtain different clustering results.</p><p>? To compute CVIs of these clustering results and their ARI with real labels.</p><p>? Then to compare the values of CVI with ARI (ground truth) <ref type="bibr" target="#b21">[21]</ref>.</p><p>? Repeat the former three steps for a new dataset.</p><p>Five clustering algorithms from various categories are used, they are: k-means, Ward linkage, spectral clustering, BIRCH <ref type="bibr" target="#b30">[31]</ref> and EM algorithm (Gaussian Mixture). CVIs used for verification and comparison are shown in TABLE I and the datasets used are shown <ref type="table" target="#tab_0">in TABLE II and TABLE III.</ref> The main problem is how to compare the values of CVIs with ground truth ARI. There are two plans:</p><p>1) Hit-the-best. Clustering results of different clustering algorithms on a dataset would have different CVIs and ARI; if a CVI gives the best score to a clustering result that also has the best ARI score, this CVI is considered to be hit-the-best (correct prediction). TABLE IV shows an example of hit-the-best. For the "wine" dataset, k-means receives the best ARI score and Dunn, DB, WB, I, CVNN and DSI give k-means the best score; and thus, the six CVIs are hit-the-best. For the hit-the-best plan, however, the best score can be relatively unstable and random in some cases. For example, in TABLE IV, the ARI score of EM is very close to that of kmeams and the Silhouette score of EM is also very close to that of k-meams. If these values fluctuated a little and changed the best cases, the comparison outcome for this dataset will be changed. Another drawback of this plan is that it only concerns one best case and ignores others. A hit-the-best plan does not evaluate the whole picture for one dataset. It might be a more strict criterion but lacks robustness. It is vulnerable to extreme cases such as when scores of different clustering results are very close to each other. Hence, we create another plan to compare the score sequences of CVIs and ARI through their orders.</p><p>2) Rank difference. This comparison plan fixes the two problems in the hit-the-best plan. One is instablility for similar scores and the other one is the bias on only one case.</p><p>We apply quantization to solve the problem of similar scores. Every score in the score sequence of a CVI (i.e., a row in TABLE IV) will be assigned a rank number and similar scores have high probability to be allocated the same rank number. The procedure is:</p><p>? Find the minimum and maximum values of scores from one sequence.</p><p>? Uniformly divide [ , ] into ? 1 intervals.</p><p>? Label intervals from to by 1,2, ? , ? 1.</p><p>? If a score is in the -th interval, its rank number is .</p><p>? Define rank number of is 1, and intervals are left open and right closed: (left, right]. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of converting a score sequence to rank numbers (a rank sequence). The rank number of scores 9 and 8 is 1 because they are in the 1st interval. For the same reason, the rank number of scores 1 and 2 is 4. Such quantization is better than assigning rank numbers by ordering because it avoids the assignment of different rank numbers to very close scores in most cases (it is still possible to use different rank numbers for very close scores; for example, in the <ref type="figure" target="#fig_1">Figure 2</ref> case, if scores 8 and 6 changed to 7.1 and 6.9, their rank numbers will still be 1 and 2 even they are very close).</p><p>For two score sequences (e.g. CVI and ARI), after quantizing them to two rank sequences, we will compute the difference of two rank sequences (called the rank difference), which is simply defined as the summation of absolute difference between two rank sequences. For example, if two rank sequences are: Summation of absolute difference is:</p><formula xml:id="formula_4">|4 ? 1| + |1 ? 3| + |4 ? 2| + |1 ? 4| + |2 ? 4| = 12</formula><p>Smaller rank difference means the distance of two sequences is closer. That two sequences of CVI and ARI are closer indicates a better prediction. It is not difficult to show that rank difference for two N-length score sequences ranges: [0, ( ? 2)].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>As discussed before, for one dataset and a CVI, an evaluation result can be computed by using the hit-the-best or rank difference comparison. In other words, one result is obtained by comparing one CVI row in TABLE IV with the graybackground row (ARI). The outcome of hit-the-best comparison is either 0 or 1. The 1 means that the best clusters predicted by CVI are the same as ARI; otherwise the outcome is 0. The outcome of the rank difference comparison is a value in the range [0, ( ? 2)]. As TABLE IV shows, the length of score sequences is 5; hence, the range of rank difference is [0, 15]. The smaller value means the CVI predicts better.</p><p>We applied this evaluation to all datasets (real and synthetic) and selected CVIs <ref type="table" target="#tab_0">. TABLE V and TABLE VII are hit-the-best  comparison results for real and synthetic datasets 5 . TABLE VI  and TABLE VIII</ref> are rank difference comparison results for real and synthetic datasets. To compare across data sets, we summed all results in the last row. For the hit-the-best comparison, the larger total value is better because more hits appear. For the rank difference comparison, the smaller total value is better because results of the CVI are closer to that of ARI. Finally, ranks in the last row uniformly indicate CVIs' performances. The smaller rank number means better performance.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Although DSI obtains only one top-1 rank <ref type="table" target="#tab_6">(TABLE V)</ref> in these experiments, having no last rank means it performed better than some other CVIs. It is worth emphasizing that all compared CVIs are excellent and widely used. Therefore, experiments show that DSI can join them as a new promising CVI. Actually, by examining those CVI evaluation results, we confirm that no one CVI performs well for all datasets. And thus, it is better to measure clustering results by using more effective CVIs. The DSI provides another CVI option. Also, DSI is unique: no other CVIs perform the same as DSI. For example, in TABLE V, for the "vehicle" dataset, only CVNN and DSI predicted correctly. But for "zoo" dataset, CVNN was wrong and DSI was correct. For another example in TABLE VI, for the "sonar" dataset, DSI performed better than Dunn, CVNN, and CVDD; but for the "cancer" dataset, Dunn, CVNN, and CVDD performed better than DSI. More examples of the diversity of CVI are shown in TABLE IX and their plots with true labels are shown in <ref type="figure" target="#fig_3">Figure  3</ref> (the "atom" dataset has three features; the others have two features).</p><p>The former examples show the need for employing more CVIs because each is different and every CVI may have its special capability. That capability, however, is difficult to describe clearly. Some CVIs' definitions show them to be categorized into center/non-center representative <ref type="bibr" target="#b22">[22]</ref> or density-representative. Similarly, the DSI is a separabilityrepresentative CVI. That is, DSI performs better for clusters having high separability with true labels (like the "atom" dataset in in <ref type="figure" target="#fig_3">Figure 3)</ref>; otherwise, if real clusters have low separability, the incorrectly predicted clusters may have a higher DSI score <ref type="figure">(Figure 4</ref>). Clusters in datasets have great diversity so that the diversity of clustering methods and CVIs is necessary. Since the preferences of CVIs are difficult to analyze precisely and quantitatively, more studies for selecting a proper CVI to measure clusters without true labels need to be done in the atom disk-4000n disk-1000n D31 flame square3 future. More CVIs expand the options. And, before that breakthrough, it is meaningful to provide more effective CVIs and apply more than one CVIs to evaluate clustering results.</p><p>In addition, to evaluate CVIs is also an important work. The general process is to:</p><p>? Create different clusters from datasets;</p><p>? Compute external CVI with true labels as ground truth and internal CVIs;</p><p>? Compare results of internal CVIs with the ground truth.</p><p>Results from an effective internal CVI should be close to the results of an external CVI.</p><p>In this paper, we generated different clusters using different clustering methods. This step can also be achieved through changing parameters of clustering algorithms (like the k in kmeans clustering) or taking subsets of datasets. The comparison step also has alternative methods. For example, besides the two plans we used, to compare the optimal number of clusters recognized by CVIs <ref type="bibr" target="#b31">[32]</ref> is another feasible plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Since there is no universal CVI for all datasets and no specific method for selecting a proper CVI to measure clusters without true labels, to apply more CVIs to evaluate clustering results is inevitable. In this paper, we propose a novel CVI, called DSI, based on a data separability measure. The goal of clustering is to separate a dataset into clusters; and we hypothesize that better clustering could cause these clusters to have a higher separability.</p><p>We applied nine internal CVIs including the proposed DSI, and an external CVI (ARI) as ground truth to clustering results of five clustering algorithms on various datasets. The results show DSI is an effective, unique, and competitive CVI to other compared CVIs. We summarized the general process to evaluate CVIs and used two methods to compare the results of CVIs with ground truth scores. The second comparison method -rank difference -which we created, avoids two disadvantages of the hit-the-best method, which is commonly used in CVI evaluation. Both DSI and the new comparison method can be helpful in clustering analysis and CVI studies in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Two clusters (classes) datasets with different label assignments. Each histogram indicates the relative frequency of the value of each of the three distance measures (indicated by color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An example of rank numbers assignment.1: {4, 1, 4, 1, 2}; 2: {1, 3, 2, 4, 4};</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>value is better (smaller rank).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Examples for rank differences of synthetic datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARED</head><label>I</label><figDesc>CVIS.</figDesc><table><row><cell>Name</cell><cell>Optimal a</cell><cell>Reference</cell></row><row><cell>Dunn index</cell><cell>Max</cell><cell>(Dunn, J.,1973) [24]</cell></row><row><cell cols="2">Calinski-Harabasz Index Max</cell><cell>(Calinski &amp; Harabasz, 1974) [25]</cell></row><row><cell>Davies-Bouldin index</cell><cell>min</cell><cell>(Davies &amp; Bouldin, 1979) [23]</cell></row><row><cell>Silhouette Coefficient</cell><cell>Max</cell><cell>(Rousseeuw, 1987) [26]</cell></row><row><cell>I</cell><cell>Max</cell><cell>(U. Maulik, 2002) [27]</cell></row><row><cell>WB</cell><cell>min</cell><cell>(Zhao Q., 2009) [28]</cell></row><row><cell>CVNN</cell><cell>min</cell><cell>(Yanchi L., 2013) [18]</cell></row><row><cell>CVDD</cell><cell>Max</cell><cell>(Lianyu H., 2019) [22]</cell></row><row><cell>DSI</cell><cell>Max</cell><cell>Proposed</cell></row></table><note>a. Optimal column means the CVI for best case has the minimum or maximum value.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE</head><label></label><figDesc>II and illustrated in Tomas Barton's homepage 3 . https://github.com/deric/clustering-benchmark/tree/master/src/main/resources/datasets/artificial 3 https://github.com/deric/clustering-benchmark 4 https://scikit-learn.org/stable/datasets/index.html#</figDesc><table /><note>2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II NAMES</head><label>II</label><figDesc></figDesc><table /><note>OF THE USED SYNTHETIC DATASETS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III THE</head><label>III</label><figDesc>DESCRIPTION OF USED REAL DATASETS.</figDesc><table><row><cell>Name</cell><cell>Title</cell><cell cols="3">Object# Feature# Class#</cell></row><row><cell>Iris</cell><cell>Iris plants dataset</cell><cell>150</cell><cell>4</cell><cell>3</cell></row><row><cell>digits</cell><cell>Optical recognition of handwritten digits dataset</cell><cell>5620</cell><cell>64</cell><cell>10</cell></row><row><cell>wine</cell><cell>Wine recognition dataset</cell><cell>178</cell><cell>13</cell><cell>3</cell></row><row><cell>cancer</cell><cell>Breast cancer Wisconsin (diagnostic) dataset</cell><cell>569</cell><cell>30</cell><cell>2</cell></row><row><cell>faces</cell><cell>Olivetti faces data-set</cell><cell>400</cell><cell>4096</cell><cell>40</cell></row><row><cell>vertebral</cell><cell>Vertebral Column Data Set</cell><cell>310</cell><cell>6</cell><cell>3</cell></row><row><cell cols="2">haberman Haberman's Survival Data</cell><cell>306</cell><cell>3</cell><cell>2</cell></row><row><cell>sonar</cell><cell>Sonar, Mines vs. Rocks</cell><cell>208</cell><cell>60</cell><cell>2</cell></row><row><cell>tae</cell><cell>Teaching Assistant Evaluation</cell><cell>151</cell><cell>5</cell><cell>3</cell></row><row><cell>thy</cell><cell>Thyroid Disease Data Set</cell><cell>215</cell><cell>5</cell><cell>3</cell></row><row><cell>vehicle</cell><cell>Vehicle silhouettes</cell><cell>946</cell><cell>18</cell><cell>4</cell></row><row><cell>zoo</cell><cell>Zoo Data Set</cell><cell>101</cell><cell>16</cell><cell>7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV CLUSTERING</head><label>IV</label><figDesc>RESULTS AND THEIR CVI SCORES ON THE WINE RECOGNITION DATASET.</figDesc><table><row><cell>Clustering Validity a method</cell><cell>KMeans</cell><cell>Linkage Ward</cell><cell>Clustering Spectral</cell><cell>BIRCH</cell><cell>EM</cell></row><row><cell>ARI b +</cell><cell>0.913 c</cell><cell>0.757</cell><cell>0.880</cell><cell>0.790</cell><cell>0.897</cell></row><row><cell>Dunn +</cell><cell>0.232</cell><cell>0.220</cell><cell>0.177</cell><cell>0.229</cell><cell>0.232</cell></row><row><cell>CH +</cell><cell>70.885</cell><cell>68.346</cell><cell>70.041</cell><cell>67.647</cell><cell>70.940</cell></row><row><cell>DB -</cell><cell>1.388</cell><cell>1.390</cell><cell>1.391</cell><cell>1.419</cell><cell>1.389</cell></row><row><cell>Silhouette +</cell><cell>0.284</cell><cell>0.275</cell><cell>0.283</cell><cell>0.277</cell><cell>0.285</cell></row><row><cell>WB -</cell><cell>3.700</cell><cell>3.841</cell><cell>3.748</cell><cell>3.880</cell><cell>3.700</cell></row><row><cell>I +</cell><cell>5.421</cell><cell>4.933</cell><cell>5.326</cell><cell>4.962</cell><cell>5.421</cell></row><row><cell>CVNN -</cell><cell>21.859</cell><cell>22.134</cell><cell>21.932</cell><cell>22.186</cell><cell>21.859</cell></row><row><cell>CVDD +</cell><cell>31.114</cell><cell>31.141</cell><cell>29.994</cell><cell>30.492</cell><cell>31.114</cell></row><row><cell>DSI +</cell><cell>0.635</cell><cell>0.606</cell><cell>0.629</cell><cell>0.609</cell><cell>0.634</cell></row></table><note>a. CVI for best case has the minimum (-) or maximum (+) value.b. Gray background is the only row of ARI as ground truth; other rows are CVIs.c. Bold value: the best case by the measure of this row.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V HIT</head><label>V</label><figDesc></figDesc><table /><note>-THE-BEST RESULTS FOR REAL DATASETS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI RANK</head><label>VI</label><figDesc>DIFFERENCE RESULTS FOR REAL DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Dunn</cell><cell>CH</cell><cell>DB</cell><cell>Silhouette</cell><cell>WB</cell><cell>I</cell><cell>CVNN</cell><cell>CVDD</cell><cell>DSI</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII HIT</head><label>VII</label><figDesc>-THE-BEST RESULTS FOR 97 SYNTHETIC DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Dunn</cell><cell>CH</cell><cell>DB</cell><cell>Silhouette</cell><cell>WB</cell><cell>I</cell><cell>CVNN</cell><cell>CVDD</cell><cell>DSI</cell></row><row><cell>3-spiral</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell>aggregation</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>zelnik5</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell>zelnik6</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Total a (rank)</cell><cell>46 (2)</cell><cell>30 (8)</cell><cell>35 (4)</cell><cell>35 (4)</cell><cell>29 (9)</cell><cell>31 (7)</cell><cell>35 (4)</cell><cell>50 (1)</cell><cell>40 (3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">a. Larger value is better (smaller rank).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII RANK</head><label>VIII</label><figDesc>DIFFERENCE RESULTS FOR 97 SYNTHETIC DATASETS.</figDesc><table><row><cell>Dataset</cell><cell>Dunn</cell><cell>CH</cell><cell>DB</cell><cell>Silhouette</cell><cell>WB</cell><cell>I</cell><cell>CVNN</cell><cell>CVDD</cell><cell>DSI</cell></row><row><cell>3-spiral</cell><cell>2</cell><cell>12</cell><cell>14</cell><cell>13</cell><cell>14</cell><cell>12</cell><cell>13</cell><cell>1</cell><cell>13</cell></row><row><cell>aggregation</cell><cell>3</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>4</cell><cell>5</cell><cell>2</cell><cell>5</cell><cell>3</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>zelnik5</cell><cell>4</cell><cell>10</cell><cell>12</cell><cell>10</cell><cell>11</cell><cell>11</cell><cell>10</cell><cell>4</cell><cell>11</cell></row><row><cell>zelnik6</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>5</cell><cell>2</cell><cell>2</cell></row><row><cell>Total a (rank)</cell><cell>406 (2)</cell><cell>541 (6)</cell><cell>547 (7)</cell><cell>489 (4)</cell><cell>583 (9)</cell><cell>554 (8)</cell><cell>504 (5)</cell><cell>337 (1)</cell><cell>415 (3)</cell></row></table><note>a. Smaller value is better (smaller rank).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX RANK</head><label>IX</label><figDesc>DIFFERENCE RESULTS FOR SELECTED SYNTHETIC DATASETS.</figDesc><table><row><cell cols="3">Dataset Dunn CH</cell><cell>DB</cell><cell>Sil</cell><cell>WB</cell><cell>I</cell><cell cols="3">CVNN CVDD DSI</cell></row><row><cell>atom</cell><cell>0</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>15</cell><cell>14</cell><cell>4</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">disk-4000n 10</cell><cell>0</cell><cell>7</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>11</cell><cell>12</cell><cell>1</cell></row><row><cell>disk-1000n</cell><cell>6</cell><cell>12</cell><cell>15</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>8</cell><cell>14</cell></row><row><cell>D31</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>10</cell><cell>2</cell><cell>0</cell></row><row><cell>flame</cell><cell>10</cell><cell>6</cell><cell>11</cell><cell>7</cell><cell>7</cell><cell>8</cell><cell>12</cell><cell>11</cell><cell>7</cell></row><row><cell>square3</cell><cell>11</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>7</cell><cell>0</cell><cell>11</cell><cell>0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We do not show the proof here because it is detailed and not relevant to the main topic. It will appear in another forthcoming publication, which can be found in author's website linked up with the ORCID: https://orcid.org/0000-0002-3779-9368. Since the statement is intuitive, we provide an informal explanation here: points in and having the same distribution and covering the same region can be considered to have been sampled from one distribution . Hence, both ICDs of and , and BCDs between and are actually ICDs of . Consequently, the distributions of ICDs and BCDs are identical.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The code can be found in author's website linked up with the ORCID: https://orcid.org/0000-0002-3779-9368</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a. Smaller value is better (smaller rank).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<idno type="DOI">10.1145/331499.331504</idno>
		<ptr target="https://doi.org/10.1145/331499.331504" />
		<imprint>
			<date type="published" when="1999-09-01" />
			<publisher>Association for Computing Machinery</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Data mining: a tutorial-based primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Roiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A shape-based clustering method for pattern recognition of residential electricity consumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clean. Prod</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page" from="475" to="488" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Application of unsupervised learning to hyperspectral imaging of cardiac ablation lesions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarvazyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loew</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JMI.5.4.046003</idno>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46003</biblScope>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on image segmentation methods using clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhanachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Eng. Res. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning-based clustering approaches for bioinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Karim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief. Bioinform</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial data compression via adaptive dispersion clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cressie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="138" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoo-Chang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2016.2528162</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Clustering Methods for Big Data Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nasraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-E</forename><forename type="middle">B</forename><surname>N&amp;apos;cir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The EM Algorithm: Theory, Applications and Related Methods</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kdd</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="226" to="231" />
			<date type="published" when="1996-03-25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<ptr target="http://www.aaai.org/Papers/KDD/1996/KDD96-037" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Impossibility Theorem for Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Becker, S. Thrun, and K. Obermayer</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustering: Science or art?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="65" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measures of Clustering Quality: A Working Set of Axioms for Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>21, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">To cluster, or not to cluster: An analysis of clusterability methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adolfsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Brownstein</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2018.10.026</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="13" to="26" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding and Enhancement of Internal Clustering Validation Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSMCB.2012.2220543</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="982" to="994" />
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Characterization and evaluation of similarity measures for pairs of clusterings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfitzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leibbrandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10115-008-0150-6</idno>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clustering indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desgraupes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Paris Ouest-Lab Modal&apos;X</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the Use of the Adjusted Rand Index as a Metric for Evaluating Supervised Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Embrechts</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04277-5_18</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks -ICANN 2009</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Internal Validity Index Based on Density-Involved Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2906949</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="40038" to="40051" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Cluster Separation Measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Bouldin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.1979</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
			<date type="published" when="1979-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Well-Separated Clusters and Optimal Fuzzy Partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
		</author>
		<idno type="DOI">10.1080/01969727408546059</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cybern</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="1974-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A dendrite method for cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cali?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Stat.-Theory Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance evaluation of some clustering algorithms and validity indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Maulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2002.1114856</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1650" to="1654" />
			<date type="published" when="2002-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">WB-index: A sum-of-squares based index for cluster validity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fr?nti</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.datak.2014.07.008</idno>
		<ptr target="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kstest.html" />
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
	<note>29] &quot;scipy.stats.kstest -SciPy v0.14.0 Reference Guide</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dheeru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Taniskidou</surname></persName>
		</author>
		<ptr target="https://archive.ics.uci.edu/ml/index.php" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BIRCH: an efficient data clustering method for very large databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<idno type="DOI">10.1145/235968.233324</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="1996-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Novel Cluster Validity Index Based on Local Cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2853710</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="999" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
