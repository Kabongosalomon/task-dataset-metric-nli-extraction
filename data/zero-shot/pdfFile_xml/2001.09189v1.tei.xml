<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning a distance function with a Siamese network to localize anomalies in videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">North Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
							<email>mjones@merl.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Mitsubishi Electric Research Labs (MERL)</orgName>
								<address>
									<addrLine>201 Broadway, 8th floor</addrLine>
									<postCode>02478</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga</forename><forename type="middle">Raju</forename><surname>Vatsavai</surname></persName>
							<email>rrvatsav@ncsu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">North Carolina State University Raleigh</orgName>
								<address>
									<postCode>27695</postCode>
									<region>NC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning a distance function with a Siamese network to localize anomalies in videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work introduces a new approach to localize anomalies in surveillance video. The main novelty is the idea of using a Siamese convolutional neural network (CNN) to learn a distance function between a pair of video patches (spatiotemporal regions of video). The learned distance function, which is not specific to the target video, is used to measure the distance between each video patch in the testing video and the video patches found in normal training video. If a testing video patch is not similar to any normal video patch then it must be anomalous. We compare our approach to previously published algorithms using 4 evaluation measures and 3 challenging target benchmark datasets. Experiments show that our approach either surpasses or performs comparably to current state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video anomaly detection is the task of localizing (spatially and temporally) anomalies in videos, where anomalies refer simply to unusual activity. Unusual activity is scene dependent; what is unusual in one scene may be normal in another. In order to define what is normal, video of normal activity from the scene is provided. In the formulation of video anomaly detection that we focus on in this paper, we assume both the normal training video as well as the testing video come from the same single fixed camera, the most common surveillance setting. In this application, normal video (i.e. not containing any anomalies) is simple to gather while anomalous video is not. This is why it makes sense to provide normal video (and only normal video) for training. Given this formulation, the problem becomes one of building a model of normal activity from the normal training video and then detecting large deviations from the model in testing video of the same scene as anomalous.</p><p>Most previous methods have limitations that can be attributed to one or more of the following, which serve as the motivation for our approach: <ref type="bibr" target="#b0">(1)</ref> The features used in many methods are hand-crafted. Examples include spatiotemporal gradients <ref type="bibr" target="#b23">[24]</ref>, dynamic textures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref>, histogram of gradients <ref type="bibr" target="#b11">[12]</ref>, histogram of flows <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref>, flow fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and foreground masks <ref type="bibr" target="#b28">[29]</ref>.</p><p>(2) Almost every method requires a computationally expensive model building phase requiring expert knowledge which may not be practical for real applications. (3) Many previous works focus on detecting only specific deviations from normality as anomalous.</p><p>To overcome these limitations, we propose an exemplarbased nearest neighbor approach to video anomaly detection that uses a distance function learned by a Siamese CNN to measure how similar activity in testing video is to normal activity. Our approach builds on the work of <ref type="bibr" target="#b28">[29]</ref>, in which normal video is used to create a model of normal activity consisting of a set of exemplars for each spatial region of the video. An exemplar is a feature vector representing a video patch, i.e., a spatio-temporal block of video of fixed size H ? W ? T where H, W and T are the height, width and temporal depth of a video patch <ref type="bibr" target="#b7">[8]</ref>. The exemplars for a spatial region of video represent all of the unique video patches that occur in the normal video in that region. Exemplars are region-specific because of the simple fact that anomalies are region-specific. To detect anomalies, video patches from a particular spatial region in testing video are compared against the exemplars for that region, and the anomaly score is the distance to the nearest exemplar. If a testing video patch is dissimilar to every exemplar video patch, then it is anomalous.</p><p>In <ref type="bibr" target="#b28">[29]</ref>, hand-crafted features (either foreground masks or flow fields) were used to represent video patches and a predefined distance function (either L 2 or normalized L 1 ) was used to compute distances between feature vectors. We propose learning a better feature vector and distance function by training a Siamese CNN to measure the distance between pairs of video patches. Our CNN is not specific to a particular scene, but is trained from video patches from several different source video anomaly detection datasets. This idea is similar in spirit to the work on learning a CNN for matching patches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref>, except extended to video. Experiments show that our method either surpasses or performs comparably to the current state of the art on the UCSD Ped1, Ped2 <ref type="bibr" target="#b25">[26]</ref> and CUHK Avenue <ref type="bibr" target="#b23">[24]</ref> test sets. In summary, our major contributions are: 1. Our approach transforms the problem of training a CNN to classify video patches as normal or anomalous (which cannot be done since we have no anomalous training examples) to the problem of training a CNN that computes the distance between two video patches (a problem for which we can generate plenty of examples). We use the same parameters for training the CNN from source datasets regardless of the target dataset.</p><p>2. This approach allows task-specific feature learning, allows for efficient exemplar model building from normal video and detects a wide variety of deviations from normality as anomalous.</p><p>3. By shifting the complexity of the problem to the dis-tance function learning task, the simple 1-NN distance-toexemplar anomaly detection becomes highly interpretable.</p><p>To the best of our knowledge, our paper is the first to take this approach to anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Due to space constraints, we cannot do justice to the complete literature. We focus here on video anomaly detection methods that follow the formulation of the problem outlined previously. A number of methods such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b34">35]</ref> use other formulations of the video anomaly detection problem which we do not discuss here, although we organize this section similar to <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Distance-based approaches</head><p>Distance-based approaches involve creating a model from a training partition and measuring deviations from this model to determine anomaly scores in the test partition.</p><p>The authors in <ref type="bibr" target="#b31">[32]</ref> use the insight that 'optimal decision rules to determine local anomalies are local irrespective of normal behavior exhibiting statistical dependencies at the global scale' to collapse the large ambient data dimension. They propose local nearest neighbor based statistics to approximate these optimal decision rules to detect anomalies.</p><p>In <ref type="bibr" target="#b39">[40]</ref>, stacked denoising auto-encoders are used to learn both appearance and motion representations of video patches which are used with one-class SVMs to perform anomaly detection.</p><p>The authors in <ref type="bibr" target="#b29">[30]</ref> derive an anomaly score map by consolidating the change in image features from a pre-trained  CNN over the length of a video block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Probabilistic approaches</head><p>Probabilistic approaches are similar to distance-based approaches, except that the model has a probabilistic interpretation, for example as a probabilistic graphical model or a high-dimensional probability distribution.</p><p>The authors in <ref type="bibr" target="#b0">[1]</ref> use multiple fixed-location monitors to extract optical flow fields and compute the likelihood of an observation given the distribution stored in that monitor's buffer.</p><p>In <ref type="bibr" target="#b25">[26]</ref>, the authors propose a representation comprising a mixture of dynamic textures (MDT), modeling a generative process for MDTs and discriminant saliency hypothesis test for anomaly detection. In <ref type="bibr" target="#b37">[38]</ref>, they build off the MDT representation to detect anomalies at multiple scales in a conditional random field framework.</p><p>Authors in <ref type="bibr" target="#b1">[2]</ref> contend that anomaly detection should try to "explain away" the normality in the test data using information learned from the training data. To this end, they use foreground object hypotheses and take a video parsing approach, treating those object hypotheses at test time which are necessary to explain the foreground but not explained by the exemplar training hypotheses are anomalous. In <ref type="bibr" target="#b2">[3]</ref>, they further build on this idea by extending the atomic unit of processing from an image patch to a video pipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Reconstruction approaches</head><p>Reconstruction approaches aim to break down inputs into their common constituent pieces and put them back together to reconstruct the input, minimizing "reconstruction error". <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> are examples of methods that use this approach. In our experience, reconstruction based approaches seem to be naively biased against reconstructing faster motion, for the simple reason that absence of motion is much more common and easier to reconstruct.</p><p>A subset of reconstruction approaches, sparse reconstruction approaches have an additional constraint in that the reconstruction must be minimialistic, that is, using only a few essential features from a dictionary to perform the reconstruction. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref> are examples of methods that use this approach.</p><p>Many of the methods mentioned above use deep networks. All of the previous papers that use deep networks for video anomaly detection that we are aware of use them in one of two techniques: (1) either to provide higher level features to represent video frames or (2) to learn to reconstruct only normal video frames. Much of the previous work builds on the basic idea of using a CNN, either pre-trained on image classification or other tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> or trained on the training partitions of each video anomaly detection dataset <ref type="bibr" target="#b39">[40]</ref>, to provide a feature vector for representing video frames. The CNN feature maps provide higher level features than raw pixels. The other major theme of deep network approaches is to learn an auto-encoder <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5]</ref> or generative adversarial network <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21]</ref> to learn to reconstruct or predict only normal video frames. Reconstruction error is then used as an anomaly score. Our method follows neither of these previous techniques and instead presents a new way to take advantage of the power of deep networks for video anomaly detection. Namely, we use a CNN to learn a distance function between pairs of video patches. Thus, ours is a novel distance-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>By building on the exemplar-based nearest neighbor approach of <ref type="bibr" target="#b28">[29]</ref>, our main problem is to learn a distance function for comparing video patches from testing video to exemplar video patches that represent all of the unique video patches found in the normal video. To do this we use a Siamese network (see <ref type="figure" target="#fig_0">Figure 1</ref>) similar to the one first introduced by Bromley and LeCun <ref type="bibr" target="#b3">[4]</ref>. In essence, by making the anomaly detection task itself a rather simple nearest neighbor distance computation (see <ref type="figure" target="#fig_1">Figure 2</ref>), we seek to offload the burden of modeling the complexity in this prob-lem to the task of learning a distance function. This learning problem can be done offline and has a large amount of training data available from source datasets. Ideally this can be done once and the resulting feature representation and distance function used on a wide variety of different target datasets.</p><p>In this section, we go into more detail in each of the steps shown in <ref type="figure" target="#fig_0">Figures 1 and 2</ref>, provide justifications for our design decisions and setup some language essential for the Experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating training video patch pairs</head><p>The main difficulty with training a Siamese network to estimate the distance between a pair of video patches is determining how to generate the training set of similar and dissimilar video patch pairs. One training example consists of a pair of video patches plus a binary label indicating whether the two video patches are similar or dissimilar (see <ref type="figure" target="#fig_0">Figure 1</ref> part 1). Video patch pairs should be selected to correctly correspond to their ground truth labels of "similar" or "dissimilar". Pairs should also be picked such that coverage of the possible domain of inputs to the CNN during test time is high. This is to ensure that the CNN is not asked to operate on out-of-domain inputs at test time.</p><p>How can we determine whether two video patches are similar or dissimilar and how can we select a varied set of video patch pairs that are relevant to video anomaly detection? An important insight is that we can use existing video anomaly detection datasets to do this. We use a source set of labeled video anomaly detection datasets to generate similar and dissimilar video patch pairs. The labeled datasets used to generate training examples should of course be disjoint from the target video anomaly detection dataset on which testing will eventually be done. The basic insight is as follows: for each source dataset, (1) A non-anomalous video patch from the test partition is similar to at least one video patch from the same spatial region in the train partition. If it were not similar to any normal video patches it would be anomalous.</p><p>(2) An anomalous video patch from the test partition is dissimilar to all possible patches from the same spatial region in the train partition. Moreover, it is dissimilar to even the most similar video patch.</p><p>The first rule generates a single pair for each normal video patch in a test video, although since there are many normal video patches in any test video, this rule can generate many similar pairs. The second rule generates many different dissimilar pairs for each anomalous video patch in a test video. The first rule requires a distance function to find the most similar train video patch to a test video patch. It is also useful in the second rule to have a distance function to know which dissimilar pairs are the most difficult (i.e. similar) since these are the most useful for training. We use a simple normalized L1 distance as our distance function along with the representation of video patches described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A reasonable concern about using a predefined distance function to help select training examples is that the Siamese network might simply learn this distance function. This does not happen for a few reasons. One is that the label for each example pair is not the L1 distance, but rather a 0 or 1 indicating whether the pair is similar or dissimilar, respectively. Secondly, it is possible for the L1 distance between two similar pairs to be larger than the L1 distance between two dissimilar pairs.</p><p>One important point to note is that normalized L1 distance is far from ideal to measure distance between video patches. For example, this distance does not take into account many variations in natural images such as scale, illumination and pose of objects. Because these variations mostly exist across different regions in the camera's field of view, we determine an adaptive threshold on normalized L1 distance below which to perform these pairings. The threshold for a region is determined by taking into account the above rules in combination with inspecting the distribution of nearest neighbor distances in a given region. Specifically, an adaptive threshold for a given region in the camera frame is determined simply as ?+? * ? where ? is the mean of nearest neighbor distances between testing video patches and training video patches, ? is the corresponding standard deviation and ? is determined by identifying an elbow in the distribution of nearest neighbor distances (we set it to 0.2 consistently in experiments). The adaptive threshold is common across the source datasets but different for similar and dissimilar pairs. Notice that dissimilar pairs that have large distances are more likely to be easy to discriminate for the Siamese network; on the other hand, we require some of these pairings despite this property to achieve high domain converage. Thus, we include candidate pairs with probability inversely proportional to the distance between them, achieving high domain coverage, but also a sufficient number of examples close to the decision boundary. We also include as similar pairs random video patches paired with slightly augmented (random translation and/or central scaling) versions of them. Our final video patch pair dataset consists of an equal number of similar and dissimilar pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning a distance function</head><p>Choice of representation: At this point, it is important to choose how video patches are represented, such that the learned distance function will perform well in the anomaly detection task. Our choice of representation consists of a H ? W ? C cuboid. In light of all anomalies being appearance or motion based, we adopt a multi-modal representation. In all our experiments that follow, the first channel is a grayscale image patch and the next 12 channels are image conv1 <ref type="bibr">(B, 20, 20, 13)</ref> conv2 <ref type="bibr">(B, 20, 20, 32)</ref>   Network architecture and training: <ref type="figure" target="#fig_2">Figure 3</ref> outlines our network architecture. Each video patch in a pair is first processed independently using conv-relu-batchnorm operations with 2 ? 2 max-pooling after every other convolution in what we call convolutional twin "tails". Weight tying between the tails guarantees that two extremely similar video patches could not possibly have very different intermediate representations because each tail computes the same function. Finally, flattened feature vectors from the two twin tails (conv5, conv5 5) are subtracted element-wise and processed consequently in a typical classification pipeline minimizing a cross-entropy loss. All convolutions use 3 ? 3 filters with a stride of 1. We find that subtracting the feature maps at conv5 produces faster optimization when compared to concatenation. We think this is because element-wise subtraction induces a stronger structural prior on the network architecture. Let B represent minibatch size, where i indexes the minibatch and y(x </p><formula xml:id="formula_0">L(x (i) 1 , x (i) 2 ) = ?? * y(x (i) 1 , x (i) 2 ) log p(x (i) 1 , x (i) 2 ) ?(1 ? y(x (i) 1 , x (i) 2 )) log (1 ? p(x (i) 1 , x (i) 2 )) (1) where p(x (i) 1 , x (i) 2 )</formula><p>is the probability of the patches being dissimilar as output by the softmax function. Note that in the loss, we set class weight for the dissimilar class ? as 0.2 to penalize incorrectly classified dissimilar pairs less than incorrectly classified similar pairs. This further serves our objective at the anomaly detection phase to have low false positive rates at high true positive rates (where anomalies are denoted positive class). For training, the objective is combined with the standard backpropagation algorithm with the Adam optimizer <ref type="bibr" target="#b17">[18]</ref>, saving the best network weights by testing on the validation set (a set of heldout training examples) periodically. The gradient is additive across the twin tails due to tied weights. We use a batch size of 128 with an initial learning rate of 0.001 and train for a maximum of 500 iterations. Xavier-Glorot weight initialization <ref type="bibr" target="#b9">[10]</ref> sampling from a normal distribution is used in tandem with ReLU activations in all layers. One important point to note is that, rather than save the network weights that maximize validation accuracy or minimize validation loss, we save that which maximizes validation area under the receiver operating characteristic curve (AUC) for false positive rates up to 0.3. This ROC curve is obtained by plotting true positive rate as a function of false positive rate, where the dissimilar class is denoted positive. By maximizing this AUC, the network that orders distances in a way that achieves high true positive rate at low false positive rates is preferred, the behavior we would like to see when it comes time for the anomaly detection phase. We use label smoothing regularization <ref type="bibr" target="#b35">[36]</ref> set to 0.1 to aid generalization. We find that adding label smoothing regularization is helpful for two reasons. The first is that the video patch pairing process has to in a sense guess what a future learned function should call similar and different in order to achieve good performance on anomaly detection, so it produces a dataset with noisy labels. The second arises from the ob-  <ref type="bibr" target="#b31">[32]</ref> 92.7%/16.0% --/---/-Detection at 150 FPS <ref type="bibr" target="#b23">[24]</ref> 91.8%/15.0% 63.8% -/---/-Sparse reconstruction <ref type="bibr" target="#b5">[6]</ref> 86.0%/19.0% 45.3% -/---/-HMDT CRF <ref type="bibr" target="#b37">[38]</ref> -/17.8% 82.7% -/18.5% --/-ST video parsing <ref type="bibr" target="#b2">[3]</ref> 93  <ref type="table">Table 1</ref>. Traditional frame-level and pixel-level evaluation criteria on the UCSD Ped1, UCSD Ped2 and CUHK Avenue benchmark datasets from related literature, ordered chronologically, complied from this same list. Our approach either surpasses or performs comparably on these evaluation criteria when compared to previous methods. *Some of the earlier works unfortunately use only a partially annotated subset available at the time to report performance.</p><p>servation that minimizing the cross entropy is equivalent to maximizing the log-likelihood of the correct label, which makes the network try to increase the logit corresponding to the correct label and make it much larger than the other logits, causing it to overfit to the training data and become too confident about its predictions. Label smoothing helps with both of these by making the network less confident about its predictions. We also use dropout <ref type="bibr" target="#b33">[34]</ref> of 0.3 on the activations of the second to last fully connected layer (fc1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exemplar learning and anomaly detection on target dataset</head><p>Detecting anomalies on a target dataset involves two stages: exemplar model building using the train partition of the dataset and anomaly detection on the test partition. Both stages use the previously trained Siamese network to measure distance between video patches. This is done by simply treating the softmax of the logit value that corresponds to the video patches being different as a measure of distance between the patches. Because the softmax output can also be interpreted as a probability, the distance measured can also be interpreted as the probability of patches being different. We emphasize that the training of the Siamese network is independent of the exemplar model building and anomaly detection stages. The Siamese network is trained on a dif-ferent set of source datasets than the target video anomaly detection dataset.</p><p>Exemplar learning on train partition of target dataset: Since videos contain a large amount of temporal redundancies, we use the exemplar learning approach of <ref type="bibr" target="#b16">[17]</ref> to build a model of normal activity in the target dataset. The exemplar model consists of sets of region-specific exemplar video patches from the videos in the train partition using a sliding spatio-temporal window with spatial stride (H/2, W/2) and temporal stride of 1. The point of exemplar learning is to represent the set of all video patches in the train partition using a smaller set of unique, representative video patches. The feature vector learned by the Siamese network is used to represent a video patch and the distance function learned by the Siamese network measures the distance between two feature vectors. A video patch is added to the exemplar set for a particular spatial region if its distance to the nearest exemplar for that region is above a threshold, which we set to 0.3 for all experiments. <ref type="figure" target="#fig_1">Figure  2</ref> illustrates a subset of exemplar video patches extracted from one region of the camera's field of view in the UCSD Ped1 dataset by our CNN. One big advantage of the exemplar learning approach is that updating the exemplar set in a streaming fashion is possible. This makes the approach scalable and adaptable to environmental changes over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly detection on test partition of target dataset:</head><p>At test time, overlapping patches with spatial stride (H/2, W/2) and temporal stride of 1 are extracted from the test partition and distances to nearest exemplars produce anomaly scores (see <ref type="figure" target="#fig_1">Figure 2</ref>). In both the exemplar learning and anomaly scoring phases, we achieve additional speedup by ignoring video patches that contain little or no motion. Specifically, a video patch is ignored if under 20% of its pixels across the channel dimension do not satisfy a threshold on flow magnitude or a threshold on the raw pixel value difference between the current and the previous frame. Furthermore, the brute-force nearest neighbor search used in the experiments could be replaced by a fast approximate nearest neighbors algorithm <ref type="bibr" target="#b27">[28]</ref> for further speed-up. Anomaly scores are stored and aggregated in a pixel map and the final anomaly score of a pixel is simply the mean of all anomaly scores it received as part of patches it participated in (due to overlap of patches in space and time). The anomaly detection is region-specific, so a patch is only compared to exemplars extracted from the same region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup -Datasets and evaluation measures</head><p>Datasets: We perform experiments on 3 benchmark datasets: UCSD Ped1, UCSD Ped2 <ref type="bibr" target="#b25">[26]</ref> and CUHK Avenue <ref type="bibr" target="#b23">[24]</ref>. Each of these datasets includes pre-defined train and test partitions from a single static camera where train partitions contain sequences of normal activity only and test partitions contain sequences with both normal and anomalous activity, and with spatial anomaly annotations per frame.</p><p>Evaluation measures: To compare against other works we use the widely-used frame-level and pixel-level area under the curve (AUC) and equal error rate (EER) criteria proposed in <ref type="bibr" target="#b25">[26]</ref>.</p><p>In addition, we report performance using two new criteria presented in <ref type="bibr" target="#b28">[29]</ref>, which are more representative of real-world performance as argued in that paper. The first is a region-based criterion: A true positive occurs if a ground truth annotated region has a minimum intersection over union (IOU) of 0.1 with a detection region. Detected regions are formed as connected components of detected pixels. The total number of positives is correspondingly the total number of anomalous regions in the test data. A false positive occurs if a detected region simply does not satisfy the minimum IOU threshold of 0.1 with any ground truth region. The region-based ROC curve plots the true positive rate (which is the fraction of ground truth anomalous regions detected) versus the false positive rate per frame. The second is a track-based criterion: A true positive occurs if at least 10% of the frames comprising a ground truth anomaly's track satisfy the region-based criterion. The to- tal number of positives is the number of ground truth annotated tracks in the test data. False positives are counted identically to the region-based criterion. The track-based ROC curve plots the true positive rate (which is the fraction of ground truth anomalous tracks detected) versus the false positive rate per frame. AUCs for both criteria are calculated for false positive rates from 0.0 up to 1.0. Because the track-based criterion requires ground truth annotations to have a track ID, we relabeled the Ped1, Ped2, and Avenue test sets with bounding boxes that include a track ID. These new labels will be made publicly available. Old labels are used for the frame and pixel-level criteria.  <ref type="table">Table 3</ref>. Track and region-based criteria, area under the curve for false positive rates up to 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison against state of the art</head><p>To evaluate our approach, we compare against results reported on the traditional evaluation measures by papers in the recent literature. For each of our experiments, a new CNN was trained using only datasets other than the target dataset to curate the training data for the Siamese network (see <ref type="table">Table 2</ref>), but each newly trained network used the same aforementioned regularization parameters. A simple heuristic was used to choose which source datasets should be used for a given target dataset -those datasets in which the scale of objects roughly match that in the target dataset for a H ? W image patch. In future work, we plan to use more labeled videos to train a single Siamese network that works well across many different target datasets. <ref type="table">Table 1</ref> presents frame and pixel-level AUC measures on the UCSD Ped1, UCSD Ped2 and CUHK Avenue datasets. Our approach sets new state of the art on UCSD Ped2 pixellevel AUC by around 4% as well as on CUHK Avenue frame EER by around 6%. Upon visualizing the detections, we find that our approach finds it particularly difficult to detect anomalies at very small scales that exist in the UCSD Ped1 test set. Also, our method, like most others in <ref type="table">Table 1</ref>, is unable to detect loitering anomalies present in the CUHK Avenue dataset. This is mainly due to our use of a "motion check" that ignores video patches with little or no motion for efficiency reasons. This could be replaced by a more sophisticated background model that is slower to absorb stationary objects.</p><p>Further, we report AUC for false positive rates up to 1.0 for the track and region based criteria in <ref type="table">Table 3</ref>. We reimplemented the work of <ref type="bibr" target="#b28">[29]</ref> for these results. Clearly, our approach surpasses that of <ref type="bibr" target="#b28">[29]</ref>, meaning we detect more anomalous events (tracks and regions) while also producing fewer false positives per frame overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UCSD Ped1</head><p>UCSD Ped2 CUHK Avenue   <ref type="figure">Figure 5</ref>. Anomaly score as a function of frame number for CUHK Avenue Test sequence number 6. Green shading on the plot denotes ground truth anomalous frames.</p><p>These ROC curves and AUC measures do not completely capture the behavior of video anomaly detection approaches. In <ref type="bibr" target="#b22">[23]</ref>, the authors present an excellent analysis of the problems with an evaluation measure such as AUC. Thus, we present a set of qualitative results here. <ref type="figure" target="#fig_4">Figure  4</ref> shows some detection results at a fixed anomaly score threshold. We notice that the quality of false positives in our approach is high, and often we are able to attribute reasons for these errors. For example, the false positive shown in the figure for UCSD Ped1 dataset is due to the fact that a person is never seen walking across the grass in this specific manner in the train partition. A similar argument explains the false positives shown for the other two datasets as well. This could either indicate that the train partition is incomplete, or highlight the subjectivity involved in ground truth annotation processes. <ref type="figure">Figure 5</ref> illustrates how anomaly score per frame, computed as the maximum of anomaly scores of pixels in the frame, varies for one test sequence of CUHK Avenue. The high variance in anomaly scores during the "bag throwing" anomaly even indicates how this event might intersperse normal and anomalous frames, seeming normal when the bag leaves the camera frame and vice versa.  <ref type="table">Table 4</ref>. Ablation study on the choice of source datasets for a particular target dataset. 'Y' denotes that the dataset was used in the source pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study on source datasets used</head><p>We perform an ablation study to understand the effect of picking source datasets for a particular target dataset. Since it is prohibitive to perform a complete ablation study, for this study we set the target to be UCSD Ped2 and vary all non-empty subsets of source datasets from the set {UCSD Ped1, CUHK Avenue, ShanghaiTech (cameras 06 and 10)}, training only once. The results presented in <ref type="table">Table 4</ref> show that while there is some sensitivity to the choice of source datasets, on both the frame and pixel level measures, we see a variation of &lt; 5%. This variation is from a combination of variation due to stochasticity during training (batching, random initialization, dropout) and choice of source datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel approach to video anomaly detection that introduces a new way to use a deep network for this problem. We substitute the problem of classifying a video patch as anomalous or not for the problem of estimating a distance between two video patches, for which we can generate plenty of labeled training data. The learned distance function (which also learns a feature vector to represent a video patch) can then be used in a straightforward video anomaly detection method that measures the distance from each testing video patch to the nearest exemplar video patch for that region. We have shown that our approach either surpasses or performs comparably to the previous state of the art without any training of the Siamese network on data from the target dataset. Our approach also possesses some favorable properties in being a plug-and-play method (learned distance function can be used out-of-the-box on target dataset), and in being scalable and resistant to environmental changes (updation of the exemplar set is easy). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Understanding the distance function learned</head><p>We also tried to gain some insight into what properties the distance function learned by the CNN possesses. To this end, we recorded the video patch pairs on which the CNN makes large errors, that is, either classifying similar pairs as dissimilar or vice versa, with high predicted probability. <ref type="figure" target="#fig_6">Figure 6</ref> is a visualization of 4 such video patch pairs when the target dataset is UCSD Ped1. Remarkably, the CNN seems to find it hard to correctly classify examples that are conceivably hard for humans. Specifically, the dissimilar pairs that have been misclassified seem to contain a skateboarder moving only slightly faster than a pedestrian would, and the similar pairs that have been misclassified exhibit some distinct differences in their flow fields.  <ref type="figure" target="#fig_0">12</ref> show the ROC curves for our CNN approach (denoted "CNN distance") as well as that of <ref type="bibr" target="#b28">[29]</ref>'s FG masks (denoted "FG L2 distance") and flow (denoted "Flow L1 distance") methods on all 3 datasets. Overall, it appears that our approach of using a learned representation and learned distance function is able to achieve better detection performance, demonstrated by higher true positive rates at low false positive rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Track and region based ROC curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">More detection result visualizations</head><p>Figures 13 through 30 present additional true positive, false positive and false negative detection results from our approach for all 3 datasets. As in the submission document, the green bounding boxes refer to ground truth anomalies and the red regions our detections at a fixed threshold on anomaly scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">More frame-level anomaly score visualizations</head><p>Figures 31 through 36 provide additional frame-level anomaly score visualizations for some test sequences using our approach from all 3 datasets. As in the submission document, green shading on the plot indicates ground truth   anomalous frames and we also show detection visualiza-        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Visualizations of learned representations for video patch pairs</head><p>Figures 37 through 41 show select video patch pairs from UCSD Ped2, their learned representations and the distance measured between them by our CNN. To generate this set of figures, we used the CNN corresponding to the scenario where the target dataset was UCSD Ped2 to give a realistic idea of distance measurement at 'test time'. Each group of 3 rows is a visualization of the feature maps of the first video patch before element-wise subtraction (1st row), the second video patch before element-wise subtraction (2nd row), and the element-wise subtraction layer's output (3rd row). All 128 feature maps are shown on columns, wrapping around to the next row when necessary. Specific feature maps could exhibit high activations for features such as speed, direction, velocity, shape, texture and illumination among others.              </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of the scenario where UCSD Ped2, ShanghaiTech and CUHK Avenue are used as source datasets to learn a distance function from. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An illustration of using the learned distance function to perform exemplar extraction and anomaly scoring on the target UCSD Ped1 dataset. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of the Siamese neural network that learns a distance function between video patches. Best viewed in electronic form in color; color coding denotes unique structure.patches from absolute values of x and y directional gradients of dense optical flow fields (we use<ref type="bibr" target="#b19">[20]</ref>) between the subsequent 6 pairs of image patches. This sets C = 13 and we set H = 20 and W = 20 for all experiments. SeeFigure 1(part 2) for an illustration.Pre-processing: Data augmentation of a random amount is performed on every video patch pair x 1 , x 2 during training in order to improve the robustness of the learned distance function to these variations. The data augmentation involves randomly flipping left to right, centrally scaling in [0.7, 1] and brightness jittering of the first channel in [-0.2, 0.2] in a stochastic manner on both video patches in a pair. Pre-processing also involves linearly scaling intensity values of each video patch from [0, 255] to [-1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 )</head><label>2</label><figDesc>be a length-B vector which contains the labels for the mini-batch, where we assume y(x (i) 1 , x (i) 2 ) = 0 whenever x 1 and x 2 are similar video patches and y(x (i) 1 , x (i) 2 ) = 1 otherwise. The crossentropy loss is of the form:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Examples of true positives (first row) and false positives (second row) from our detector on all 3 datasets. Green bounding box annotations denote ground truth anomalies and red regions our model's detections (intersections are orange-ish).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Examples of large prediction errors made by our model on UCSD Ped1. Classes 0 and 1 refer to similar and dissimilar pairs respectively. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figures 7 through</head><label></label><figDesc>Figures 7 through 12 show the ROC curves for our CNN approach (denoted "CNN distance") as well as that of [29]'s FG masks (denoted "FG L2 distance") and flow (denoted "Flow L1 distance") methods on all 3 datasets. Overall, it appears that our approach of using a learned representation and learned distance function is able to achieve better detection performance, demonstrated by higher true positive rates at low false positive rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Track-based ROC curves on UCSD Ped1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Region-based ROC curves on UCSD Ped1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Track-based ROC curves on UCSD Ped2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Region-based ROC curves on UCSD Ped2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Track-based ROC curves on CUHK Avenue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Region-based ROC curves on CUHK Avenue. tions at select frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>True positive in UCSD Ped1 -a biker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 .</head><label>14</label><figDesc>True positive in UCSD Ped1 -a skateboarder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>False positive in UCSD Ped1 -camera fault.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 .</head><label>16</label><figDesc>False positive in UCSD Ped1 -seemingly random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 .</head><label>17</label><figDesc>False negative in UCSD Ped1 -biker not yet fully in the camera frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 .</head><label>18</label><figDesc>False negative in UCSD Ped1 -skateboarder moving slowly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 .</head><label>19</label><figDesc>True positive in UCSD Ped2 -2 bikers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 .</head><label>20</label><figDesc>True positive in UCSD Ped2 -a biker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 21 .</head><label>21</label><figDesc>False positive in UCSD Ped2 -seemingly random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 .</head><label>22</label><figDesc>False positive in UCSD Ped2 -unusual movement in this region of the camera frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 23 .</head><label>23</label><figDesc>False negative in UCSD Ped2 -occluded, slow-moving skateboarder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 24 .</head><label>24</label><figDesc>False negative in UCSD Ped2 -biker partially left the camera frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 25 .</head><label>25</label><figDesc>True positive in CUHK Avenue -person running.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 26 .</head><label>26</label><figDesc>True positive in CUHK Avenue -person interacting with a bag on the grass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 27 .</head><label>27</label><figDesc>False positive in CUHK Avenue -seemingly random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 28 .</head><label>28</label><figDesc>False positive in CUHK Avenue -unusual movement in this region of the camera frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 29 .</head><label>29</label><figDesc>False negative in CUHK Avenue -still, unattended bag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 30 .</head><label>30</label><figDesc>False negative in CUHK Avenue -start of an anomalous event that is seemingly normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 31 .Figure 32 .Figure 33 .Figure 34 .Figure 35 .Figure 36 .Figure 37 . 24 Figure 38 .Figure 39 .Figure 40 .Figure 41 .</head><label>313233343536372438394041</label><figDesc>Per-frame anomaly score visualization of UCSD Ped1 Test sequence 006.normal skateboarder normal Per-frame anomaly score visualization of UCSD Ped1 Test sequence 025.normal biker Per-frame anomaly score visualization of UCSD Ped2 Test sequence 002. Per-frame anomaly score visualization of UCSD Ped2 Test sequence 004.normal running running Per-frame anomaly score visualization of CUHK Avenue Test sequence 004. Per-frame anomaly score visualization of CUHK Avenue Test sequence 020. Learned representations and their element-wise difference between 2 video patches in UCSD Ped2, visualized.Distance measured by CNN = 0.Learned representations and their element-wise difference between 2 video patches in UCSD Ped2, visualized. Learned representations and their element-wise difference between 2 video patches in UCSD Ped2, visualized. Learned representations and their element-wise difference between 2 video patches in UCSD Ped2, visualized. Learned representations and their element-wise difference between 2 video patches in UCSD Ped2, visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3. Exemplar learning from training video and anomaly scoring on testing video</figDesc><table><row><cell></cell><cell cols="2">Exemplar set per region</cell><cell></cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell></row><row><cell>Video patch</cell><cell>Siamese CNN</cell><cell>Testing video patch</cell><cell>Video patch</cell></row><row><cell></cell><cell></cell><cell>Anomaly score =</cell><cell></cell></row><row><cell>Training video -UCSD Ped1</cell><cell>distance</cell><cell>minimum distance over all exemplars</cell><cell>Testing video -UCSD Ped1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust Real-Time Unusual Event Detection using Multiple Fixed-Location Monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Anti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06235</idno>
		<title level="m">Spatio-temporal Video Parsing for Abnormality Detection</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01546</idno>
		<idno>arXiv: 1701.01546. 3</idno>
		<title level="m">Abnormal Event Detection in Videos using Spatiotemporal Autoencoder</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Abnormal event detection in crowded scenes using sparse representation. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1851" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Discriminative Framework for Anomaly Detection in Large Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>VS-PETS</publisher>
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep event models for crowd anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="548" to="556" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research (PMLR)</title>
		<meeting>Machine Learning Research (PMLR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Temporal Regularity in Video Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unmasking the Abnormal Events in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2914" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting Abnormal Events in Video Using Narrowed Normality Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exemplar learning for extremely efficient anomaly detection in realvalued time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nikovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery (DMKD)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1454" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video Anomaly Detection With Compact Feature Sets for Online Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leyva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3463" to="3478" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Beyond Pixels: Exploring New Representations and Applications for Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">MIT PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Classifier Two-Sample Test for Video Anomaly Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AUC: a misleading measure of the performance of predictive distribution models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jim?nez-Valverde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Ecology and Biogeography</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Australia</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Revisit of Sparse Coding Based Anomaly Detection in Stacked RNN Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Application VISSAPP&apos;09)</title>
		<imprint>
			<publisher>INSTICC Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Street Scene: A new dataset and evaluation protocol for video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Appearance Features for Abnormal Behavior Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing (ICIAP)</title>
		<editor>S. Battiato, G. Gallo, R. Schettini, and F. Stanco</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Real-World Anomaly Detection in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convex Polytope Ensembles for Spatio-Temporal Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Turchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schettini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Image Analysis and Processing (ICIAP)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="174" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anomaly Detection and Localization in Crowded Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
