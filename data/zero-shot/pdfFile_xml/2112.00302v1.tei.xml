<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Module for Temporal Action Localization in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Graph Convolutional Module for Temporal Action Localization in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Temporal Action Localization</term>
					<term>Graph Convolutional Networks</term>
					<term>Video Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal action localization, which requires a machine to recognize the location as well as the category of action instances in videos, has long been researched in computer vision. The main challenge of temporal action localization lies in that videos are usually long and untrimmed with diverse action contents involved. Existing state-of-the-art action localization methods divide each video into multiple action units (i.e., proposals in two-stage methods and segments in one-stage methods) and then perform action recognition/regression on each of them individually, without explicitly exploiting their relations during learning. In this paper, we claim that the relations between action units play an important role in action localization, and a more powerful action detector should not only capture the local content of each action unit but also allow a wider field of view on the context related to it. To this end, we propose a general graph convolutional module (GCM) that can be easily plugged into existing action localization methods, including two-stage and one-stage paradigms. To be specific, we first construct a graph, where each action unit is represented as a node and their relations between two action units as an edge. Here, we use two types of relations, one for capturing the temporal connections between different action units, and the other one for characterizing their semantic relationship. Particularly for the temporal connections in two-stage methods, we further explore two different kinds of edges, one connecting the overlapping action units and the other one connecting surrounding but disjointed units. Upon the graph we built, we then apply graph convolutional networks (GCNs) to model the relations among different action units, which is able to learn more informative representations to enhance action localization. Experimental results show that our GCM consistently improves the performance of existing action localization methods, including two-stage methods (e.g., CBR [15] and R-C3D [47]) and one-stage methods (e.g., D-SSAD [22]), verifying the generality and effectiveness of our GCM. Moreover, with the aid of GCM, our approach significantly outperforms the state-of-the-art on THUMOS14 (50.9% versus 42.8%). Augmentation experiments on ActivityNet also verify the efficacy of modeling the relationships between action units.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>U NDERSTANDING human actions from raw videos is a longstanding research goal of computer vision, owing to its various applications in security surveillance, human behavior analysis and many other areas <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Joining the success of deep learning, video-based action classification <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref> has exhibited fruitful progress in recent years. Nevertheless, this task assumes a tacit approval of addressing videos that are trimmed and short, which limits its practical potential. Temporal action localization, in contrast, targets on untrimmed and long videos to localize the start and end times of every action instance of interest as well as to predict the corresponding label. Taking the sports video in <ref type="figure">Figure 1</ref> as an example, the detector should determine where the action event is occurring and identify which class the event belongs to. The lower restriction in video collection and preprocessing makes temporal action localization a more compelling yet challenging task in video analytics.</p><p>A variety of studies have been performed on temporal action localization over the last few years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b55">[56]</ref>. In general, existing methods are categorized into two types: the two-stage paradigm <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b55">[56]</ref> and the one-stage paradigm <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>. For the two-stage methods, they first generate a set of action proposals and then individually perform classification and temporal boundary regression on each proposal. In terms of one-stage methods, they divide each video into segments of equal number and then predict the labels and boundary offsets of the anchors mounted to each segment. Despite their difference in whether they use external proposals or not, these two paradigms share the similar spirit of independently conducting classification/regression on each action unit-a general concept corresponds to the proposal in two-stage methods and the segment in one-stage methods. Processing each action unit separately, however, will inevitably neglect the relations in-between and potentially lose critical cues for action localization. For example, the adjacent action units around the target unit can provide temporal context for localizing its temporal boundary. Meanwhile, two distant action units might also provide indicative hints of action recognition to each other if they are semantically similar.</p><p>Based upon the intuition above, this paper investigates the relationships between action units from two perspectives, namely the temporal relationship and the semantic relationship. To illustrate this, we revisit the example in <ref type="figure">Figure 1</ref>, where we have generated five action units. units p 1 , p 2 and p 3 overlapping with each other describe different parts of the same action instance (i.e., the start period, main body and end period). Conventional action localization methods perform prediction on p 1 by using its feature alone, which we think is insufficient to deliver complete knowledge. If we additionally consider the features of p 2 and p 3 , we will obtain more contextual information around p 1 , which is advantageous especially for the temporal boundary regression of p 1 . On the other hand, p 4 describes the background (i.e., the sport field), and its content is also helpful in identifying the action label of p 1 , since what is happening on the sports field is likely to be sports action (e.g., "riding bicycle") but not the action that occurs elsewhere (e.g., "kissing"). In other words, the classification of p 1 can be partly guided by the content of p 4 since they are temporally related even disjointed. 2) Semantic relationship: p 5 is distant from p 1 , but it describes the same action type as p 1 ("riding bicycle") in a different view. We can acquire more complete information for predicting the action category of p 1 if we additionally leverage the content of p 5 . To model the interactions between action units, one possible way is to employ the self-attention mechanism <ref type="bibr" target="#b38">[39]</ref>, as what has been conducted previously in language translation <ref type="bibr" target="#b38">[39]</ref> and object detection <ref type="bibr" target="#b18">[19]</ref>, to capture the pair-wise similarity between action units. A self-attention module can affect an individual action unit by aggregating information from all other action units with the automatically learned aggregation weights. However, this method is computationally expensive as querying all action unit pairs has a quadratic complexity of the node number (note that each video can contain more than thousands of action units). In contrast, graph convolutional networks (GCNs), which generalize convolutions from grid-like data (e.g.images) to non-grid structures (e.g.social networks), have received increasing interest in the machine learning domain <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b49">[50]</ref>. GCNs can affect each node by aggregating information from the adjacent nodes, and thus are very suitable for leveraging the relations between action units. More importantly, unlike the self-attention strategy, applying GCNs enables us to aggregate information from only the local neighborhoods for each action unit, and thus can help remarkably decrease the computational complexity.</p><p>In this paper, we propose a general graph convolutional module (GCM) that can be easily plugged into existing action localization methods to exploit the relations between action units. In this module, we first regard the action units as the nodes of a specific graph and represent their relations as edges. To construct the graph, we investigate three kinds of edges between action units, including: 1) the contextual edges to incorporate the contextual information for each proposal instance (e.g., detecting p 1 by accessing p 2 and p 3 in <ref type="figure">Figure 1)</ref>; 2) the surrounding edges to query knowledge from nearby but distinct action units (e.g., querying p 4 for p 1 ); 3) the semantic edges to involve the content of the semantically similar units for enhanced action recognition (e.g., recognizing p 1 by considering p 5 ). Then, we perform graph convolutions on the constructed graph. Although the information is aggregated from local neighbors in each layer, message passing between distant nodes is still possible if the depth of the GCNs increases. Moreover, to avoid the overwhelming computational cost, we further devise a sampling strategy to train the GCNs efficiently while still preserving the desired detection performance. We evaluate our proposed method by incorporating GCM with existing action localization methods on two popular benchmarks for temporal action detection, i.e., THUMOS14 <ref type="bibr" target="#b22">[23]</ref> and AcitivityNet1.3 <ref type="bibr" target="#b4">[5]</ref>.</p><p>In summary, our contributions are as follows:</p><p>? To the best of our knowledge, we are the first to exploit the relationships between action units for temporal action localization in videos. ? To model the interactions between action units, we propose a general graph convolutional module (GCM) to construct a graph of action units by establishing the edges based on our valuable observations and then apply GCNs for message aggregation among action units. Our GCM can be plugged into existing two-stage and one-stage methods. ? Experimental results show that GCM consistently improves the performance of SSN <ref type="bibr" target="#b55">[56]</ref>, R-C3D <ref type="bibr" target="#b46">[47]</ref>, CBR <ref type="bibr" target="#b14">[15]</ref> and D-SSAD <ref type="bibr" target="#b21">[22]</ref> on two benchmarks, demonstrating the generality and effectiveness of our proposed GCM. On THUMOS14 especially, our method obtains a mAP of 50.9% when tIoU = 0.5, which significantly outperforms the stateof-the-art, i.e., 42.8% by <ref type="bibr" target="#b6">[7]</ref>. Augmentation experiments on ActivityNet also verify the efficacy of modeling action proposal relationships. This paper extends our preliminary version <ref type="bibr" target="#b54">[55]</ref> that was published in ICCV 2019 in the following several aspects. 1) We integrate graph construction and graph convolution into a general graph convolutional module (GCM) so that the proposed module can be plugged into any of the two-stage temporal action localization methods (e.g., SSN, R-C3D and CBR) and the onestage methods (e.g., D-SSAD). 2) In addition to the temporal relationships leveraged in our ICCV paper, we further explore semantic relationships to learn more discriminative representations. Experimental results reveal that the semantic relationships provide more valuable information for action recognition. 3) We conduct more ablation studies (e.g., analysis of semantic edges, runtime comparison with the baseline methods, and comparisons for onestage methods) to verify the effectiveness and efficiency of the proposed method. 4) We achieve clearly better action localization results over our ICCV version on THUMOS14 (50.9% vs. 49.1%) and ActivityNet 1.3 (31.45% vs. 31.11%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Temporal action localization. Recently, great progress has been achieved in deep learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b41">[42]</ref>, which facilitates the development of temporal action localization. Approaches on this task can be grouped into three categories: (1) methods performing frame or segment-level classification, which requires a post-processing step (e.g., smoothing and merging) to obtain the temporal boundaries of the action instances <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b33">[34]</ref>;</p><p>(2) approaches employing a two-stage framework similar to the two-stage object detection methods in images. They often involve proposal generation, proposal classification and boundary refinement <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b55">[56]</ref>; (3) methods that integrate proposal generation and classification (and/or boundary regression) into end-to-end architectures, which are often called one-stage action localization methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p><p>Our work can be used to help both two-stage and one-stage action localization paradigms, where each video is divided into multiple action units and each action unit is processed individually. Following the two-stage paradigm, Shou et al. <ref type="bibr" target="#b34">[35]</ref> proposed generating a set of proposal candidates from sliding windows and classifying them by using deep neural networks. Xu et al. <ref type="bibr" target="#b46">[47]</ref> exploited the 3D convolutional networks and proposed a framework inspired by Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>. Following the onestage paradigm, Lin et al. <ref type="bibr" target="#b25">[26]</ref> divided the video into segments and used convolutional layers to obtain video features, which were further processed by an anchor layer for temporal action localization. Huang et al. <ref type="bibr" target="#b21">[22]</ref> decoupled the localization and classification in a one-stage scheme. However, the above methods neglect the contextual information of action units. To address this issue, some attempts have been developed to incorporate the context to enhance the proposal feature <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b55">[56]</ref>. They show encouraging improvements by extracting features on the extended receptive field (i.e., boundary) of the proposal. Despite their success, they all process each action unit individually. In contrast, our method considered the relations between action units. Graph-based relation modeling. Relation modeling has proven to be very helpful in many computer vision tasks like object detection <ref type="bibr" target="#b18">[19]</ref>, visual reasoning <ref type="bibr" target="#b8">[9]</ref> and image classification <ref type="bibr" target="#b43">[44]</ref>. For instance, the performance of object detection can be improved by considering the object relations since objects in an image are often highly correlated <ref type="bibr" target="#b18">[19]</ref>. Recently, Kipf et al. <ref type="bibr" target="#b23">[24]</ref> proposed graph convolutional network (GCN) to define convolutions on nongrid structures. Due to its effectiveness in relation modeling, GCN has been widely applied to several research areas in computer vision, such as skeleton-based action recognition <ref type="bibr" target="#b49">[50]</ref>, object detection <ref type="bibr" target="#b47">[48]</ref> and video classification <ref type="bibr" target="#b44">[45]</ref>. Wang et al. <ref type="bibr" target="#b44">[45]</ref> used a graph to represent the spatiotemporal relations between objects for the action classification task. Xu et al. <ref type="bibr" target="#b47">[48]</ref> constructed an object graph relying on the spatial configurations between objects for object detection. Our work considers both the temporal and semantic relations between action units for a more challenging temporal action localization task, where both action classification and localization are required. Recently, Xu et al. <ref type="bibr" target="#b48">[49]</ref> proposed a one-stage action localization method with a graph to exploit the relations between video segments. Our work is able to model the relations between action units (i.e., video segments or proposals) and is more general since it can be easily plugged into existing action localization methods, including two-stage and one-stage paradigms. Graph sampling strategy. For real-world applications, the graph can be large and directly using GCNs is inefficient. Therefore, several attempts have been made for efficient training by virtue of the sampling strategy, such as the node-wise method SAGE <ref type="bibr" target="#b16">[17]</ref>, layer-wise model FastGCN <ref type="bibr" target="#b7">[8]</ref> and its layer-dependent variant AS-GCN <ref type="bibr" target="#b20">[21]</ref>. In this paper, considering the flexibility and implementability, we adopt the SAGE method as the sampling strategy in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation and preliminaries</head><p>We denote an untrimmed video as V = {I t ? R H?W ?3 } T t=1 , where I t denotes the frame at the time slot t with height H and width W .</p><formula xml:id="formula_0">Within each video V , let P = {p i | p i = (x i , (t i,s , t i,e ))} N i=1</formula><p>be the action units of interest, where the action unit can be a proposal in two-stage action localization methods (e.g., SSN <ref type="bibr" target="#b55">[56]</ref>) or a video segment in one-stage methods (e.g., SSAD <ref type="bibr" target="#b25">[26]</ref>). Let t i,s and t i,e be the start and end times of an action unit, respectively. In addition, given action unit p i , let x i ? R d be the feature extracted by a certain feature extractor (e.g., the I3D network <ref type="bibr" target="#b5">[6]</ref>) from frames between I ti,s and I ti,e .</p><p>Let G(V, E) be a graph of N nodes with nodes v i ? V and edges e ij = (v i , v j ) ? E. Furthermore, let A ? R N ?N be the adjacency matrix associated with G. In this paper, we seek to exploit graphs G(P, E) on action units in P to better model the interactions between action units in videos. Here, each action unit is treated as a node, and the edges in E are used to represent the relations between nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">General scheme of our approach</head><p>We focus on solving the problem that existing temporal action localization methods neglect the relation between action units, which, however, is able to significantly improve the localization accuracy. Thus, we propose a general graph convolutional module (GCM) that can be inserted into existing action localization methods in a plug-and-play manner. In particular, GCM uses a graph G(P, E) to present the relations between action units and then applies GCN on the graph to exploit the relations and learn powerful representations for action units. The intuition is that when performing graph convolution, each node aggregates information from its neighborhoods. In this way, the feature of each action unit is enhanced by other action units, which helps eventually improve the detection performance. The schematic of our approach is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Without loss of generality, we assume the action units have been obtained beforehand by some methods (e.g., the TAG method in <ref type="bibr" target="#b55">[56]</ref>). Given the features of the action units {x i } N i=1 and their initial temporal boundaries {(t i,s , t i,e ))} N i=1 , our GCM constructs a graph G according to the temporal and semantic relations between action units. Then, we apply a K-layer GCN in the GCM to exploit the relations and obtain the relation-aware features Y of action units. For the k-th layer (1 ? k ? K), the graph convolution is implemented by</p><formula xml:id="formula_1">X (k) = AX (k?1) W (k) .<label>(1)</label></formula><p>Here, A is the adjacency matrix, W (k) ? R d k ?d k is the parameter matrix to be learned, X (k) ? R N ?d k are the hidden features for all action units at layer k, and X (0) ? R N ?d are the input features. We apply an activation function (i.e., ReLU) after each convolution layer before the features are forwarded to the next layer. In addition, our experiments find it more effective by further combining the hidden features with the input features in the last layer, namely, where the summation is performed in an element-wise manner. The relation-aware action unit features Y are then used to jointly predict the action category? i and temporal position (t i,s ,t i,e ) for each action unit p i by calculating</p><formula xml:id="formula_2">Y = X (K) + X (0) ,<label>(2)</label></formula><formula xml:id="formula_3">{(? i , (t i,s ,t i,e ))} N i=1 = F (Y),<label>(3)</label></formula><p>where F denotes any action localization methods, such as SSN <ref type="bibr" target="#b55">[56]</ref>, R-C3D <ref type="bibr" target="#b46">[47]</ref>, CBR <ref type="bibr" target="#b14">[15]</ref> and D-SSAD <ref type="bibr" target="#b21">[22]</ref>. In the following sections, we aim to answer two questions: (1) how to construct a graph to represent the relations between action units, and (2) how to insert our GCM into the existing action localization methods, including the two-stage paradigm and one-stage paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Action unit graph construction</head><p>For the graph G(P, E) of each video, the nodes are instantiated as the action units, while the edges E between action units are demanded to be characterized specifically to better model the relations. One way for constructing edges is linking all action units with each other, which yet leads to overwhelming computations for going through all action unit pairs. It also incurs redundant or noisy information for action localization, as some unrelated action units should not be connected. In this paper, we devise a smarter approach by exploiting the temporal relevance/distance and the semantic relationships between action units instead. Specifically, we introduce three types of edges, the contextual edges, the surrounding edges and the semantic edges, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Contextual edges</head><p>We establish an edge between action units p i and p j if r(p i , p j ) &gt; ? ctx , where ? ctx is a certain threshold. Here, r(p i , p j ) represents the relevance between action units and is defined by the tIoU metric, i.e.,</p><formula xml:id="formula_4">r(p i , p j ) = tIoU (p i , p j ) = I(p i , p j ) U (p i , p j ) ,<label>(4)</label></formula><p>where I(p i , p j ) and U (p i , p j ) compute the temporal intersection and union of the two action units, respectively. If we focus on the proposal p i , establishing the edges by computing r(p i , p j ) &gt; ? ctx will select its neighborhoods as those that have high overlaps with it. Obviously, the non-overlapping portions of the highly-overlapping neighborhoods can provide rich contextual information for p i . As already demonstrated in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, exploring the contextual information is of great help in refining the detection boundary and eventually increasing the detection accuracy. Here, by our contextual edges, all overlapping action units automatically share the contextual information with each other, and this information is further processed by the graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Surrounding edges</head><p>The contextual edges connect the overlapping action units that usually correspond to the same action instance. Actually, surrounding but disjointed action units (including the background items) can also be correlated, and the message passing among them will facilitate the detection of each other. For example, in <ref type="figure">Figure 1</ref>, the background p 4 provides guidance on identifying the action class of action unit p 1 (e.g., more likely to be sports actions). To handle such kind of correlations, we first utilize r(p i , p j ) = 0 to query the disjointed action units, and then compute the following distance</p><formula xml:id="formula_5">d(p i , p j ) = |c i ? c j | U (p i , p j ) ,<label>(5)</label></formula><p>to add the edges between nearby action units if d(p i , p j ) &lt; ? sur , where ? sur is a certain threshold. In Eq. (5), c i (or c j ) represents the center coordinate of p i (or p j ). As a complement to the contextual edges, the surrounding edges enable the message to pass across distinct action instances and thereby provide more temporal cues for the detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Semantic edges</head><p>The above contextual and surrounding edges aim to exploit the temporal context for each action unit, which, however, still neglects the semantic information between action units. It is worth noting that one untrimmed video often contains multiple action instances (e.g., each video on THUMOS14 dataset <ref type="bibr" target="#b22">[23]</ref> contains more than 15 action instances on average), and the instances in one video often belong to the same or semantically similar action category. For example, the actions CricketBowling and CricketShot often occur in the same video on THUMOS14. Although their categories are different when performing action localization, it is intuitive that the semantics of CricketBowling are helpful for recognizing CricketShot from other actions (e.g., CliffDiving). Therefore, the proposal that locates at a distance from an action but containing similar semantic content might provide indicative hints for detecting the action.</p><p>To exploit such semantic information for action localization, we add a semantic edge between the action units that share similar semantics. In particular, we first define an action unit set S i for the i-th action unit as</p><formula xml:id="formula_6">S i = {p j |r(p i , p j ) = 0, j ? N l (i)},<label>(6)</label></formula><p>where N l (i) is the index set of the l nearest neighborhoods of proposal p i and N l (i) is constructed in the feature space relying on the cosine similarity between action unit features x i and x j . Then, we establish a semantic edge between p i and the action units in S i . Note that the action unit feature x i can be the high-level appearance or motion feature containing rich semantic information.</p><p>In other words, the action units sharing similar appearance (e.g., some similar places) or motions (e.g., the same action performed by different actors) can be used to help the recognition of action units. To summarize, the edge e ij between nodes p i and p j can be formulated as</p><formula xml:id="formula_7">e ij = ? ? ? ? ? ? ? ? ? 1, if r(p i , p j ) &gt; ? ctx ; 1, if r(p i , p j ) = 0, d(p i , p j ) &lt; ? sur ; 1, if r(p i , p j ) = 0, j ? N l (i); 0, else.<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Adjacency matrix</head><p>In Eq. (1), we need to compute the adjacency matrix A. Here, we design the adjacency matrix by assigning specific weights to edges. For example, we can apply the cosine similarity to estimate the weights of edge e ij by</p><formula xml:id="formula_8">A ij = x T i xj xi 2? xj 2 , e ij = 1; 0, e ij = 0.<label>(8)</label></formula><p>In the above computation, we compute A ij relying on the feature vector x i . We can also map the feature vectors into an embedding space using a learnable linear mapping function as in <ref type="bibr" target="#b43">[44]</ref> before the cosine computation. We leave the discussion in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GCM for two-stage action localization methods</head><p>Due to the residual nature of GCM (see Eq. <ref type="formula" target="#formula_2">(2)</ref>), the proposed GCM can be easily plugged into existing two-stage action localization methods, which typically involve the following steps:</p><p>Step 1: generates a set of proposal candidates, which may contain action instances;</p><p>Step 2: uses some certain feature extractors, which can be off-the-shelf <ref type="bibr" target="#b14">[15]</ref> or trained in an end-to-end manner <ref type="bibr" target="#b46">[47]</ref>, to obtain the proposal features;</p><p>Step 3: processes the proposal features using an action classifier and a boundary regressor, which are often implemented as fully-connected layers;</p><p>Step 4:</p><p>performs duplicate removal, which is usually achieved by using non-maximum suppression (NMS). In this paper, our proposed GCM is used between Step 2 and Step 3. Given a set of proposals, our GCM first constructs a proposal graph according to Equation <ref type="bibr" target="#b6">(7)</ref>. Then, the relation-aware proposal features are obtained by performing graph convolution on the constructed graph via Equations <ref type="formula" target="#formula_1">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref>. Joining the previous work SSN <ref type="bibr" target="#b55">[56]</ref>, we find that it is beneficial to predict the action label and temporal boundary separately by virtue of two GCMs-one conducted on the original proposal features x i and the other one on the extended proposal features x i . The first GCM is formulated as</p><formula xml:id="formula_9">{? i } N i=1 = softmax(FC 1 (GCM 1 ({x i } N i=1 ))),<label>(9)</label></formula><p>where we apply a fully-connected (FC) layer with soft-max operation on top of GCM 1 to predict the action label? i . The second GCM can be formulated as</p><formula xml:id="formula_10">{(t i,s ,t i,e )} N i=1 = FC 2 (GCM 2 ({x i } N i=1 )),<label>(10)</label></formula><formula xml:id="formula_11">{? i } N i=1 = FC 3 (GCM 2 ({x i } N i=1 )),<label>(11)</label></formula><p>where the graph structure G(P, E) is the same as that in Eq. <ref type="formula" target="#formula_9">(9)</ref> but the input proposal feature is different. The extended feature x i is attained by first extending the temporal boundary of p i with 1 2 of its length on both the left and right sides and then extracting the feature within the extended boundary. Here, we adopt two FC layers on top of GCM 2 , one for predicting the boundary (t i,s ,t i,e ) and the other one for predicting the completeness score? i , which indicates whether the proposal is complete or not. It has been demonstrated by <ref type="bibr" target="#b55">[56]</ref> that, incomplete action units that have low tIoU with the ground-truths can have high classification scores, and thus it will make mistakes when using the classification score alone to rank the proposal for the mAP test; further applying the completeness score enables us to avoid this issue. For other two-stage action localization methods (e.g., CBR <ref type="bibr" target="#b14">[15]</ref>, R-C3D <ref type="bibr" target="#b46">[47]</ref>) that do not rely on the two-stream pipeline such as SSN, we only insert one GCM into them. Specifically, GCM takes the original proposal features x i as input and outputs the relationaware features, which are further processed by two individual FC layers for predicting the action classification and boundary regression, respectively. Formally, the action localization process can be formulated as</p><formula xml:id="formula_12">{(t i,s ,t i,e )} N i=1 = FC 4 (GCM 3 ({x i } N i=1 )), {? i } N i=1 = softmax(FC 5 (GCM 3 ({x i } N i=1 ))).<label>(12)</label></formula><p>where FC * denotes the fully-connected (FC) layers, whose inputs are the same relation-aware features produced by GCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GCM for one-stage action localization methods</head><p>Our proposed GCM is a general module for exploiting the relationships between action units, which can be the segments in one-stage action localization methods, as discussed in Section 1.</p><p>Existing one-stage methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref> are inspired by the singleshot object detection methods in images <ref type="bibr" target="#b27">[28]</ref>. A three-step pipeline is used in these methods, as summarized below.</p><p>Step 1: evenly divides the input video into T segments and extracts a C-dim feature vector for each segment, thus leading to a 1D feature map F ? R T ?C ;</p><p>Step 2: obtain 1D feature maps with multiple temporal scales (i.e., different temporal granularity) relying on F;</p><p>Step 3: predict the action category and boundary offsets of the anchors mounted to each location on the 1D feature maps. For better readability, we call the feature vector at each location as a feature unit.</p><p>Our proposed GCM is used between Step 2 and Step 3. Although the boundaries of feature units are non-overlapping, we can incorporate our GCM to exploit the relations between feature units with a minor modification. In particular, we only consider the surrounding and semantic edges to link the feature units and perform graph convolution to aggregate messages. The intuition is that the feature units can be regarded as a special case of proposals. Specifically, each feature unit corresponds to a segment in the videos with a certain duration, and these segments are non-overlapping. By adding the GCM to the 1D feature maps, we are able to exploit the relationship between the feature units in a 1D feature map. It is worth mentioning that our module can be inserted one or multiple times throughout the network to model the feature relationships at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Loss functions</head><p>Our proposed method not only predicts the action category and the completeness score (when inserting our GCM into SSN <ref type="bibr" target="#b55">[56]</ref>) of each proposal but also refines the temporal boundary of action units by location regression. To train our model, we define the following loss functions: Classification Loss. We define the training loss function for the action classifier as follows:</p><formula xml:id="formula_13">L cls = 1 N N i=1 L 1 (y i ,? i ),<label>(13)</label></formula><p>where y i and? i are the ground truth and the prediction of the i-th action unit, respectively. We use the cross-entropy loss as L 1 , and N is the number of action units in a mini-batch. Completeness Loss. Given the predicted completeness score? i and the ground truth e i of the i-th action unit, we use the following loss function to train the completeness predictor:</p><formula xml:id="formula_14">L com = 1 N com N i=1 1 i com L 2 (e i ,? i ),<label>(14)</label></formula><p>where we use hinge loss as L 2 and N com is the number of completeness training samples. 1 i com is the indicator function, being 1 if y i ? 1 (i.e., the action unit is not considered as part of the background) and 0 otherwise. Regression Loss. We devise a set of location regressors {R m } N class m=1 , each for an action category. For an action unit, we regress the boundary using the closest ground-truth instance as the target. Our method predicts the offset? i = (? i,c ,? i,l ) relative to the action unit , where? i,c and? i,l are the offset of center coordinate and length, respectively. The ground-truth offset is denoted as o i = (o i,c , o i,l ) and parameterized by:</p><formula xml:id="formula_15">o i,c = (c i ? c gt )/l i , o i,l = log(l i /l gt ),<label>(15)</label></formula><p>where c i and l i denote the original center coordinate and length of the action unit, respectively. c gt and l gt are the center coordinate and length of the closest ground truth, respectively. To train the regressor, we define the following loss function:</p><formula xml:id="formula_16">L reg = 1 N reg N i=1 1 i reg L 3 (o i ,? i ),<label>(16)</label></formula><p>Algorithm 1 Training details of our method.</p><p>Input: Action unit set P = {p i | p i = (x i , (t i,s , t i,e ))} N i=1 ; original action unit features {x</p><formula xml:id="formula_17">(0) i } N i=1 ; extended action unit features {x i (0) } N i=1</formula><p>; graph depth K; sampling size N s Parameter: Weight matrices W (k) , ?k ? {1, . . . , K} 1: instantiate the nodes by the action units p i , ?p i ? P 2: establish edges between nodes using Eq. <ref type="formula" target="#formula_7">(7)</ref> 3: obtain an action unit graph G(P, E) 4: calculate adjacent matrix using Eq. (8) <ref type="bibr">5:</ref> while not converges do <ref type="bibr">6:</ref> for k = 1 . . . K do <ref type="bibr">7:</ref> for p ? P do <ref type="bibr">8:</ref> sample N s neighborhoods of p predict completeness score {? i } N i=1 using Eq. (11) <ref type="bibr">15:</ref> compute L total using Eq. (17) <ref type="bibr">16:</ref> update parameters via stochastic gradient descent 17: end while where N reg is the number of regression training samples. 1 i reg is the indicator function, being 1 if y i ? 1 and e i = 1 (i.e., the proposal is a foreground sample) and 0 otherwise. We use the smooth-L1 loss as L 3 because it is less sensitive to outliers. Multi-task Loss. We train the whole model by using the following multi-task loss function:</p><formula xml:id="formula_18">L total = L cls + ? 1 L com + ? 2 L reg ,<label>(17)</label></formula><p>where ? 1 and ? 2 are hyper-parameters to trade-off these losses. We set ? 1 = ? 2 = 0.5 in all the experiments and find that it works well across all of them. It is worth mentioning that we consider the completeness loss only when we plug our GCM into the SSN method <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Efficient training by sampling</head><p>Typical action unit generation methods usually produce thousands of action units for each video. Applying the aforementioned graph convolution (Eq. (1)) on all action units demands many computations and large memory footprints. To accelerate the training of GCNs, several approaches <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b20">[21]</ref> have been proposed based on neighborhood sampling. Here, we adopt the SAGE method <ref type="bibr" target="#b16">[17]</ref> in our method for its flexibility. The SAGE method uniformly samples the fixed-size neighborhoods of each node layer-by-layer in a top-down passway. In other words, the nodes of the (k ? 1)-th layer are formulated as the sampled neighborhoods of the nodes in the k-th layer. After all nodes of all layers are sampled, SAGE performs the information aggregation in a bottom-up manner. Here, we specify the aggregation function to be a sampling form of Eq. (1), namely,</p><formula xml:id="formula_19">x (k) i = ? ? 1 N s Ns j=1 A ij x (k?1) j + x (k?1) i ? ? W (k) ,<label>(18)</label></formula><p>where node j is sampled from the neighborhoods of node i, i.e., j ? N (i), and N s is the sampling size and is much less than the total number N . The summation in Eq. (18) is further normalized by N s , which empirically makes the training more stable. In addition, we also enforce the self-addition of its feature for node i in Eq. <ref type="bibr" target="#b17">(18)</ref>. We do not perform any sampling when testing. For better readability, Algorithm 1 depicts the algorithmic flow of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>THUMOS14 <ref type="bibr" target="#b22">[23]</ref> is a standard benchmark for action localization. Its training set, known as the UCF-101 dataset, consists of 13320 videos. The validation, testing and background sets contain 1010, 1574 and 2500 untrimmed videos, respectively. The temporal action localization task of THUMOS14, which contains videos over 20 hours from 20 sports classes, is very challenging since each video has more than 15 action instances and its 71% frames are occupied by background items. Following the common setting in <ref type="bibr" target="#b22">[23]</ref>, we apply 200 videos in the validation set for training and conduct evaluation on the 213 annotated videos from the testing set. ActivityNet <ref type="bibr" target="#b4">[5]</ref> is another popular benchmark for action localization on untrimmed videos. We evaluate our method on ActivityNet v1.3, which contains approximately 10K training videos and 5K validation videos corresponding to 200 different activities. Each video has an average of 1.65 action instances. Following the standard practice, we train our method on the training videos and test it on the validation videos. In our experiments, we contrast our method with the state-of-the-art methods on both THUMOS14 and ActivityNet v1.3, and perform ablation studies on THUMOS14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Evaluation metrics. We use the mean average precision (mAP) as the evaluation metric. A proposal is considered to be correct if its temporal IoU with the ground-truth instance is larger than a certain threshold and the predicted category is the same as this ground-truth instance. On THUMOS14, the tIOU thresholds are chosen from {0.1, 0.2, 0.3, 0.4, 0.5}; on ActivityNet v1.3, the IoU thresholds are from {0.5, 0.75, 0.95}, and we also report the average mAP of the IoU thresholds between 0.5 and 0.95 with the step of 0.05. Graph construction. We construct the graph by fixing the values of ? ctx as 0.7 and ? sur as 1 for both streams, which are selected by grid search. We adopt a 2-layer GCN since we observed no clear improvement with more than 2 layers but the model complexity is increased. For more efficiency, we choose N s = 4 in Eq. (18) for neighborhood sampling unless otherwise specified.</p><p>Training. The initial learning rate is 0.001 for the RGB stream and 0.01 for the flow stream. During training, the learning rates are divided by 10 every 15 epochs. The dropout ratio is 0.8. Testing. We do not perform neighborhood sampling (i.e., Eq. (18)) for testing. The predictions of the RGB and flow streams are fused using a ratio of 2:3. We multiply the classification score with the completeness score as the final score for calculating mAP. We then use non-maximum suppression (NMS) to obtain the final predicted temporal action units for each action class separately. We use 800 and 100 action units per video for computing mAPs on THUMOS14 and ActivityNet v1.3, respectively. Action units and features for two-stage methods. The action units in two-stage methods refer to the action proposals. Our model is implemented under the two-stream strategy <ref type="bibr" target="#b35">[36]</ref>: RGB frames and optical-flow fields. 1) For SSN <ref type="bibr" target="#b55">[56]</ref>, we first uniformly divide each input video into 64-frame RGB/optical-flow segments and adopt a two-stream I3D model pre-trained on Kinetics <ref type="bibr" target="#b5">[6]</ref> to obtain a 1024-dimensional feature vector for each segment. Upon the I3D features, we further apply max pooling across segments to obtain one 1024-dimensional feature vector for each proposal that is obtained by the BSN method <ref type="bibr" target="#b26">[27]</ref>. Note that we do not finetune the parameters of the I3D model in our training phase. In addition to the I3D features and BSN proposals, our ablation studies in Section 5.4 also explore other types of features (e.g., 2D features <ref type="bibr" target="#b26">[27]</ref>) and proposals (e.g., TAG action units <ref type="bibr" target="#b55">[56]</ref>). 2) For CBR <ref type="bibr" target="#b14">[15]</ref>, we use the two-stream model <ref type="bibr" target="#b45">[46]</ref> pre-trained on the ActivityNet v1.3 training set as the feature extractor. We use the proposals obtained from the proposal stage in <ref type="bibr" target="#b14">[15]</ref> to perform action localization. 3) For R-C3D <ref type="bibr" target="#b46">[47]</ref>, we use a 3D ConvNet modified from C3D <ref type="bibr" target="#b37">[38]</ref> to extract proposal features. We adopt the proposals generated by the proposal subnet in <ref type="bibr" target="#b46">[47]</ref> for a fair comparison. Action units and features for one-stage methods. The action units in one-stage methods refer to the video segments. We follow <ref type="bibr" target="#b21">[22]</ref> to use two-stream networks <ref type="bibr" target="#b35">[36]</ref> pre-trained on Kinetics <ref type="bibr" target="#b5">[6]</ref> to extract spatial and temporal feature representations for each video clip with length 512. We keep other settings (e.g., the learning rate, anchor settings) the same as those are used in <ref type="bibr" target="#b21">[22]</ref> for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with state-of-the-art results</head><p>THUMOS14. Our method is compared with the state-of-the-art methods in <ref type="table" target="#tab_0">Table 1</ref>. GCM consistently boosts the performance of both two-stage methods (e.g., SSN <ref type="bibr" target="#b55">[56]</ref>, R-C3D <ref type="bibr" target="#b46">[47]</ref>, CBR <ref type="bibr" target="#b14">[15]</ref>) and one-stage methods (e.g., D-SSAD <ref type="bibr" target="#b21">[22]</ref>) on THUMOS14, demonstrating the generality and effectiveness of our proposed GCM. With the aid of GCM, our method (i.e., SSN+GCM) reaches the highest mAP over all thresholds, implying that our method can recognize and localize actions much more accurately than any other method. In particular, our method outperforms the previously best method (i.e., TAL-Net <ref type="bibr" target="#b6">[7]</ref>) by 8.1% absolute improvement and the second-best result <ref type="bibr" target="#b15">[16]</ref> by more than 13.5%, when tIoU = 0.5. When using the proposals of higher quality (i.e., BMN proposals <ref type="bibr" target="#b24">[25]</ref>), our method (i.e., SSN+GCM ? ) lifts the mAP to 51.9% when tIoU = 0.5. ActivityNet v1.3. We report the action localization results of various methods in <ref type="table">Table 2</ref>. Regarding the average mAP, our method (i.e., SSN+GCM) outperforms SSN <ref type="bibr" target="#b55">[56]</ref>, and CDC <ref type="bibr" target="#b33">[34]</ref> by 2.83% and 3.40%, respectively. We observe that BSN <ref type="bibr" target="#b26">[27]</ref> and BMN <ref type="bibr" target="#b24">[25]</ref> perform promisingly on this dataset. Note that these two methods were originally designed for generating class-agnostic proposals, and thus rely on external video-level action labels (from UntrimmedNet <ref type="bibr" target="#b40">[41]</ref>) for action localization. In contrast, our method is self-contained and is able to perform action localization without any external label. Actually, our method can be modified to take external labels into account. To achieve this, we replace the predicted action classes in Eq. (9) with the external action labels. Specifically, given an input video, we use UntrimmedNet to predict the top-2 videolevel classes and assign these classes to all the proposals in this video. Thus, each proposal has two predicted action classes. To compute mAP, we follow <ref type="bibr" target="#b26">[27]</ref> to obtain the score of each proposal by calculating s prop = s gcm * s bsn/bmn * s unet , where s gcm is the proposal score predicted by our model (i.e., SSN+GCM), s bsn/bmn is the confidence score produced by BSN (or BMN) and s unet denotes the action score predicted by UntrimmedNet. As summarized in <ref type="table">Table 2</ref>, our enhanced version (i.e., SSN*+GCM) consistently outperforms BSN and BMN when using the same proposals. Moreover, SSN*+GCM outperforms GTAD <ref type="bibr" target="#b48">[49]</ref> even though GTAD uses additional video classification scores from <ref type="bibr" target="#b40">[41]</ref>. These results further demonstrate the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION RESULTS ON TWO-STAGE METHODS</head><p>In this section, we will perform complete and in-depth ablation studies to evaluate the impact of each component of our model. Setting mAP@IoU=0.5 Gain CBR <ref type="bibr" target="#b14">[15]</ref> 31.00 -CBR + GCM 32.24 1.24 R-C3D <ref type="bibr" target="#b46">[47]</ref> 28.90 -R-C3D + GCM 30.85 1.95</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effectiveness and generality of GCM</head><p>In this section, we incorporate our GCM into two popular two-stage action localization methods (i.e., CBR <ref type="bibr" target="#b14">[15]</ref> and R-C3D <ref type="bibr" target="#b46">[47]</ref>) to validate the effectiveness and generality of GCM. In the following, we present the implementation details and the results. Cascaded Boundary Regression (CBR) <ref type="bibr" target="#b14">[15]</ref>. The CBR method adopts a cascaded framework to iteratively regress the boundary of the action units. In the proposal stage, CBR uses a deep model to obtain the initial action units by refining the boundary of the sliding windows. In the detection stage, CBR uses another deep model to learn better representations of the action units. Last, these action units are forwarded to fully-connected layers for action classification and boundary regression. In our experiments, we insert our GCM in the detection stage. The outputs of the GCM are forwarded to the action classifier and regressor. For a fair comparison with CBR, we use two-stream features and unitlevel offsets. As shown in <ref type="table" target="#tab_1">Table 3</ref>, our GCM helps to lift the action localization results over all IoU thresholds, demonstrating its effectiveness. Region Convolutional 3D Network (R-C3D) <ref type="bibr" target="#b46">[47]</ref>. Inspired by the faster-RCNN <ref type="bibr" target="#b31">[32]</ref> approach in object detection, Xu et al.</p><p>proposed an end-to-end R-C3D network for activity detection. The network encodes the frames with fully-convolutional 3D layers and then uses a proposal subnet to generate activity segments (i.e., action units). Last, they use a classification subnet to classify and refine the action units based on the RoI-pooled features. Our GCM takes the pooled features as input and enhances the features by constructing a graph and performing graph convolution. The outputs of the GCM are forwarded to the action classifier and regressor. We follow the same settings in <ref type="bibr" target="#b46">[47]</ref> for a fair comparison. From <ref type="table" target="#tab_1">Table 3</ref>, the action localization performance of R-C3D is significantly improved with the help of our GCM. More critically, the performance gain on two action localization methods demonstrates the generality of our module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How do proposal-proposal relations help?</head><p>As illustrated in Section 3.4, we apply two GCMs for action classification and boundary regression separately. Here, we implement the baseline with a 2-layer multilayer-perceptron (MLP). The MLP baseline shares the same structure as GCM except that we remove the adjacent matrix A in Eq. (1). Specifically, for the k-th layer, the propagation in Eq. (1) becomes X k = X k?1 W k , where W k are the trainable parameters. Without using A, MLP processes each proposal feature independently. By comparing the performance of MLP with GCN, we can justify the importance of message passing along action units. To do so, we replace each GCM with an MLP and have the following variants of our model including: (1) MLP 1 + GCM 2 where GCN 1 is replaced; (2) GCM 1 + MLP 2 where GCM 2 is replaced; and (3) MLP 1 + MLP 2 where both GCMs are replaced. <ref type="table" target="#tab_2">Table 4</ref> shows that all these variants decrease the performance of our model, thus verifying the effectiveness of GCNs The temporal boundary of the input proposal is not precise (i.e., some portions of the corresponding ground truth have not been detected). Our proposed GCM helps to aggregates contextual information from other proposals and lastly predicts the action category correctly and refines the temporal boundary of the input proposal precisely.</p><p>for both action classification and boundary regression. Overall, our method significantly outperforms the MLP protocol (i.e.MLP 1 + MLP 2 ), validating the importance of considering the relations between action units in temporal action localization. The MLP baseline is indeed a particular implementation of SSN <ref type="bibr" target="#b55">[56]</ref>. We compare the runtime between GCM and MLP baseline in <ref type="table" target="#tab_3">Table 5</ref>.</p><p>In detail, we train each model with 200 iterations on a Titan X GPU and report the average processing time per video per iteration (note that proposal generation and feature extraction are excluded for each model). It reads that GCM only incurs a relatively small additional runtime compared with the MLP baseline but is able to improve the performance significantly. Visualization of the constructed graph. To understand how proposal-proposal relations help improve the action localization performance, we visualize an example of the graph constructed by our proposed method in <ref type="figure" target="#fig_3">Figure 3</ref>. Specifically, given an input proposal p 1 , we choose K = 8 proposals with the largest weights among all connected proposals. The temporal boundary of the input proposal p 1 is not precise (i.e., the ending period of the corresponding ground truth action instance has not been detected in p 1 ). The contextual and surrounding edges connect four proposals (p 2 , p 3 , p 4 and p 5 ) that can provide a wider receptive field for p 1 to detect the ending period of actions. Interestingly, the semantic edges connect not only two proposals (p 6 and p 7 ) that provide action information from other action instances in the same video but also two proposals (p 8 and p 9 ) with background scenes related to the action instance. Lastly, the temporal boundary of p 1 is refined to match the corresponding ground truth and the action category is correctly predicted by our method. Clearly, our proposed GCM is able to exploit contextual information to improve the action localization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">How does the graph convolution help?</head><p>In addition to graph convolutions, performing mean pooling among proposal features is another way to enable information dissemination between action units. We thus conduct another baseline by first adopting MLP on the action unit features and then conducting mean pooling on the output of MLP over adjacent  We report the results in <ref type="table">Table 6</ref>. The models with two GCMs outperform all MP variants, demonstrating the superiority of graph convolution over mean pooling in capturing between-proposal connections. The protocol MP 1 + MP 2 in <ref type="table">Table 6</ref> performs better than MLP 1 + MLP 2 in <ref type="table" target="#tab_2">Table 4</ref>, which again reveals the benefit of modeling the relations between action units, even though we pursue it using the naive mean pooling.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Influences of different backbones</head><p>Our framework is general and compatible with different backbones (i.e., action units and features). In addition to the backbones applied above, we further perform experiments on TAG action units <ref type="bibr" target="#b55">[56]</ref> and 2D features <ref type="bibr" target="#b26">[27]</ref>. We try different combinations: (1) BSN+I3D, (2) BSN+2D, (3) TAG+I3D, and (4) TAG+2D, and report the results of SSN and SSN+GCM in <ref type="figure" target="#fig_4">Figure 4</ref>. In comparison with MLP, our method leads to significant and consistent improvements in all types of features and action units. These results conclude that our method is generally effective and is not limited to the specific feature or proposal type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">The weights of edge and self-addition</head><p>We have defined the weights of edges in Eq. (8), where the cosine similarity (cos-sim) is applied. This similarity can be further extended by first embedding the features before the cosine computation. We call the embedded version as embed-cos-sim, and compare it with cos-sim in <ref type="table" target="#tab_5">Table 7</ref>. No obvious improvement is attained by replacing cos-sim with embed-cos-sim (the mAP difference between them is less than 0.3%). Eq. (18) has considered the self-addition of the node feature. We also investigate the importance of this term in <ref type="table" target="#tab_5">Table 7</ref>. It suggests that the self-addition leads to at least 1.06% absolute improvements on both RGB and flow streams. Comparisons with learned weights. To further verify the effectiveness of our graph construction strategy, we conduct an experiment by using learned weights of edge. Specifically, we first construct a fully-connected graph and then follow the "scaled dot-product attention" mechanism in <ref type="bibr" target="#b38">[39]</ref> to obtain the adjacent matrix by computing and W 2 are learnable parameters and N is the number of proposals in one video. Note that one video often contains thousands of proposals, and thus using a fully-connected graph will inevitably incur large computation cost when aggregating information from all other proposals. From <ref type="table" target="#tab_6">Table 8</ref>, our GCM outperforms the baseline using learned weights. This is probably because the fullyconnected graph may introduce noise from irrelevant proposals, which may even make the training unstable. In contrast, our GCM passes messages only from the temporally adjacent and semantically correlated action units, and thus may eliminate noisy information from irrelevant action units and yield better performance. Moreover, using learned weights is able to lift the action localization performance of the baseline (49.7% vs 49.3%). These results reveal that exploiting action unit relations helps localize actions more precisely, and they also justify our motivation for considering the relations between action units.</p><formula xml:id="formula_20">A ij = e (W 1 x i ) T (W 2 x j ) N n=1 e (W 1 x i ) T (W 2 xn ) , where W 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Is it necessary to consider three types of edges?</head><p>To evaluate the necessity of formulating three types of edges, we perform experiments on three variants of our method, each of which removes one type of edge in the graph construction stage. From <ref type="table">Table 9</ref>, the result drops remarkably when any kind of edge is removed. Another crucial point is that our method still improves the MLP baseline when only the surrounding edges remain. The rationale behind this could be that actions in the same video are correlated and exploiting the surrounding relation enables more accurate action classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">The efficiency of our sampling strategy</head><p>We train our model efficiently based on the neighborhood sampling in Eq. <ref type="bibr" target="#b17">(18)</ref>. Here, we are interested in how the sampling size N s affects the final performance. <ref type="table" target="#tab_0">Table 10</ref> reports the testing mAPs corresponding to different N s values varying from 1 to 5 (and also 10). The training time per iteration is also added in <ref type="table" target="#tab_0">Table 10</ref>. We  observe that when N s = 4, the model achieves higher mAP than the full model (i.e., N s = 10) while reducing the training time by 76% for each iteration. This is interesting, as sampling fewer nodes yields even better results. We conjecture that the neighborhood sampling can bring in more stochasticity and guide our model to escape from the local minimal during training, thus delivering better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATION RESULTS ON ONE-STAGE METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Effectiveness of GCM</head><p>Decoupled single-shot temporal action detection (D-SSAD) <ref type="bibr" target="#b21">[22]</ref>. Huang et al. decoupled the localization and classification in a one-stage scheme. In particular, D-SSAD consists of three main components: a base feature network, an anchor network, and a classification/regression module. The base feature network extracts representations of each video segment to form feature maps. Then, a multi-branch anchor network takes the feature maps as input and produces multiple anchors at each location on the feature maps. Last, the anchors are processed by the classification and regression module. In our experiments, we add our GCM to the feature maps before generating anchors. As discussed in Section 3.5, the GCM can be inserted one or multiple times throughout the network to model the feature relationships at different scales. Therefore, we add the GCM to feature maps with multiple scales (from 1 to 3). From <ref type="table" target="#tab_0">Table 11</ref>, the performance of D-SSAD is improved by using our GCM to enhance the features. As more GCMs are inserted, the action localization results increase, which demonstrates that our GCM is general and compatible with the one-stage action localization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">How much does each type of edge help?</head><p>To evaluate the effectiveness of surrounding and semantic edges, we perform experiments by gradually adding one type of edge to our GCM. From <ref type="table" target="#tab_0">Table 12</ref>, adding the surrounding edge and semantic edge to the baseline (i.e., without both types of edges) results in at least 0.61% improvements in terms of action localization mAP. When considering both surrounding and semantic edges simultaneously, the performance is further improved to 44.77%, which strongly supports the necessity of constructing two types of edges in our proposed GCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">QUALITATIVE RESULTS</head><p>Given the significant improvements, we also attempt to find out in what cases our method improves over the baseline method. We visualize the qualitative results on THUMOS14 in <ref type="figure">Figure 5</ref>. In these examples, the baseline method (i.e., SSN <ref type="bibr" target="#b55">[56]</ref>) is able to predict the action category correctly, while failing to precisely predict the location of actions. With the help of our proposed GCM, we predict a more precise temporal boundary, which demonstrates the effectiveness of GCM for temporal action localization. Failure case analysis. Our method achieves state-of-the-art performance on two benchmark action localization datasets, but like other methods, it is still not sufficiently capable of detecting actions when they share the similar contents. For example, in <ref type="figure">Figure 6</ref>, our method correctly detects the locations of actions but misclassifies the action Pole Vault into the action Javelin Throw since both these actions share similar contents (i.e.an athlete runs when holding a pole). Another failure case is the misclassification between Cliff Diving and Diving. While this is a common challenge in temporal action localization, exploiting more advanced feature extraction methods may solve it to some extent, which will be left for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this paper, we have exploited the relationships between action units to address the task of temporal action localization in videos. Specifically, we have proposed to construct a graph of action units based on the temporal context and semantic information, and apply GCNs to enable message passing among action units. In this way, we enhanced the action unit features and eventually improved the action localization performance. More critically, we have integrated the above graph construction and graph convolution processes into a general graph convolutional module (GCM), which can be easily inserted into existing action localization methods, including the onestage paradigm and two-stage paradigm. Experimental results show that our GCM is compatible with other action localization methods and helps to consistently improve their action localization accuracy.</p><p>With the aid of GCM, our method outperforms the state-of-the-art methods by a large margin on two benchmarks, i.e., THUMOS14 and ActivithNet v1.3. It would be interesting to extend our method for object detection in images and we leave it for our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 )Fig. 1 .</head><label>11</label><figDesc>Temporal relationship: the action arXiv:2112.00302v1 [cs.CV] 1 Dec 2021 Schematic depiction of our approach. We apply graph convolutional networks to model the interactions between action units and boost the temporal action localization performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An example of two-stage action localization methods with GCM (c) An example of one-stage action localization methods with GCM Schematic of our method. (a) Given a set of action units (e.g., proposals in two-stage methods and segments in one-stage methods), our graph convolutional module (GCM) instantiates the nodes in the graph by each action unit. Then, we establish three kinds of edges among nodes to model the relations between action units and employ GCNs on the constructed graph. Lastly, our GCM module outputs relation-aware features. (b) For two-stage action localization methods, our GCM can be used in the second stage to enhance the proposal features, which are used for action classification and boundary regression. (c) For one-stage action localization methods, our GCM can be exploited to enhance the video features before the anchor layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization results of the graph constructed by our GCM on THUMOS14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Action localization results on THUMOS14 with different backbones, measured by mAP@tIoU=0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Action localization results on THUMOS14, measured by mAP (%) at different tIoU thresholds ?. ( ? ) indicates the method that uses BMN proposals [25]. Huang et al. [22] 66.4 64.7 59.8 53.4 43.2 Huang et al. + GCM 66.4 65.2 61.4 54.7 44.8</figDesc><table><row><cell cols="2">Paradigm tIoU</cell><cell cols="5">0.1 0.2 0.3 0.4 0.5</cell></row><row><cell></cell><cell>Yeung et al. [52]</cell><cell>-</cell><cell>-</cell><cell cols="3">36.0 26.4 17.1</cell></row><row><cell></cell><cell>Lin et al. [26]</cell><cell>-</cell><cell>-</cell><cell cols="3">43.0 35.0 24.6</cell></row><row><cell>One-Stage</cell><cell>Buch et al. [2]</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>-</cell><cell>29.2</cell></row><row><cell></cell><cell>Wang et al. [40]</cell><cell cols="5">18.2 17.0 14.0 11.7 8.3</cell></row><row><cell></cell><cell>Caba et al. [4]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.5</cell></row><row><cell></cell><cell>Escorcia et al. [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>13.9</cell></row><row><cell></cell><cell>Oneata et al. [30]</cell><cell cols="5">36.6 33.6 27.0 20.8 14.4</cell></row><row><cell></cell><cell>Richard et al. [33]</cell><cell cols="5">39.7 35.7 30.0 23.2 15.2</cell></row><row><cell></cell><cell>Yeung et al. [52]</cell><cell cols="5">48.9 44.0 36.0 26.4 17.1</cell></row><row><cell></cell><cell>Yuan et al. [54]</cell><cell cols="5">51.0 45.2 36.5 27.8 17.8</cell></row><row><cell></cell><cell>Yuan et al. [53]</cell><cell cols="5">51.4 42.6 33.6 26.1 18.8</cell></row><row><cell></cell><cell>Shou et al. [35]</cell><cell cols="5">47.7 43.5 36.3 28.7 19.0</cell></row><row><cell></cell><cell>Hou et al. [18]</cell><cell>51.3</cell><cell>-</cell><cell>43.7</cell><cell>-</cell><cell>22.0</cell></row><row><cell></cell><cell>Buch et al. [3]</cell><cell>-</cell><cell>-</cell><cell>37.8</cell><cell>-</cell><cell>23.0</cell></row><row><cell></cell><cell>Shou et al. [34]</cell><cell>-</cell><cell>-</cell><cell cols="3">40.1 29.4 23.3</cell></row><row><cell></cell><cell>Dai et al. [10]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">33.3 25.6</cell></row><row><cell>Two-Stage</cell><cell>Gao et al. [14] Huang et al. [20]</cell><cell cols="5">54.0 50.9 44.1 34.9 25.6 ----27.7</cell></row><row><cell></cell><cell>Yang et al. [51]</cell><cell>-</cell><cell>-</cell><cell cols="3">44.1 37.1 28.2</cell></row><row><cell></cell><cell>Zhao et al. [56]</cell><cell cols="5">66.0 59.4 51.9 41.0 29.8</cell></row><row><cell></cell><cell>Gao et al. [13]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.9</cell></row><row><cell></cell><cell>Alwassel et al. [1]</cell><cell>-</cell><cell>-</cell><cell cols="3">51.8 42.4 30.8</cell></row><row><cell></cell><cell>Lin et al. [27]</cell><cell>-</cell><cell>-</cell><cell cols="3">53.5 45.0 36.9</cell></row><row><cell></cell><cell>Gleason et al. [16]</cell><cell cols="5">52.1 51.4 49.7 46.1 37.4</cell></row><row><cell></cell><cell>Chao et al. [7]</cell><cell cols="5">59.8 57.1 53.2 48.5 42.8</cell></row><row><cell></cell><cell>Xu et al. [47]</cell><cell cols="5">54.5 51.5 44.8 35.6 28.9</cell></row><row><cell></cell><cell>Xu et al. + GCM</cell><cell cols="5">56.3 53.5 47.0 37.9 30.9</cell></row><row><cell></cell><cell>Gao et al. [15]</cell><cell cols="5">60.1 56.7 50.1 41.3 31.0</cell></row><row><cell></cell><cell>Gao et al. + GCM</cell><cell cols="5">61.2 57.8 50.3 42.4 32.2</cell></row><row><cell></cell><cell cols="6">Zhao et al. [56] (I3D) 69.7 67.5 64.6 58.3 49.3</cell></row><row><cell></cell><cell>Zhao et al. + GCM</cell><cell cols="5">70.5 68.6 65.2 59.8 50.9</cell></row><row><cell></cell><cell>Zhao et al. + GCM  ?</cell><cell cols="5">72.5 70.9 66.5 60.8 51.9</cell></row><row><cell></cell><cell cols="2">TABLE 2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Action localization results on ActivityNet v1.3 (val), measured by mAP</cell></row><row><cell cols="7">(%) at different tIoU thresholds and the average mAP of IoU thresholds</cell></row><row><cell cols="7">from 0.5 to 0.95. (*) indicates the method that uses the external video</cell></row><row><cell></cell><cell cols="4">labels/scores from UntrimmedNet [41].</cell><cell></cell><cell></cell></row><row><cell>tIoU</cell><cell></cell><cell>0.5</cell><cell>0.75</cell><cell cols="3">0.95 Average</cell></row><row><cell cols="2">Singh et al. [37]</cell><cell>34.47</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">Wang et al. [43]</cell><cell>43.65</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">Shou et al. [34]</cell><cell cols="3">45.30 26.00 0.20</cell><cell></cell><cell>23.80</cell></row><row><cell cols="2">Dai et al. [10]</cell><cell cols="3">36.44 21.15 3.90</cell><cell></cell><cell>-</cell></row><row><cell cols="2">Xu et al. [47]</cell><cell>26.80</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell cols="2">Zhao et al. [56]</cell><cell cols="3">39.12 23.48 5.49</cell><cell></cell><cell>23.98</cell></row><row><cell cols="2">Chao et al. [7]</cell><cell cols="3">38.23 18.30 1.30</cell><cell></cell><cell>20.22</cell></row><row><cell cols="2">Lin et al. [27] (BSN*)</cell><cell cols="3">46.45 29.96 8.02</cell><cell></cell><cell>30.03</cell></row><row><cell cols="2">Xu et al. [49] (GTAD*)</cell><cell cols="3">50.36 34.60 9.02</cell><cell></cell><cell>34.09</cell></row><row><cell cols="2">Lin et al. [25] (BMN*)</cell><cell cols="3">50.07 34.78 8.29</cell><cell></cell><cell>33.85</cell></row><row><cell cols="2">SSN (BSN prop [27])</cell><cell cols="3">38.59 24.53 4.57</cell><cell></cell><cell>24.37</cell></row><row><cell cols="2">SSN + GCM (BSN prop [27])</cell><cell cols="3">42.55 28.27 2.84</cell><cell></cell><cell>27.20</cell></row><row><cell cols="2">SSN* + GCM (BSN prop [27])</cell><cell cols="3">47.92 32.91 4.16</cell><cell></cell><cell>31.45</cell></row><row><cell cols="5">SSN* + GCM (BMN prop [25]) 51.03 35.17 7.44</cell><cell></cell><cell>34.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell>Ablation study of GCM on CBR and R-C3D, measured by mAP (%) when</cell></row><row><cell>tIoU=0.5 on THUMOS14.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">Comparison between our model and the MLP baseline on THUMOS14,</cell></row><row><cell cols="5">measured by mAP (%) when tIoU=0.5..</cell></row><row><cell>mAP@tIoU=0.5</cell><cell>RGB</cell><cell>Gain</cell><cell>Flow</cell><cell>Gain</cell></row><row><cell>MLP 1 + MLP 2</cell><cell>36.82</cell><cell>-</cell><cell>46.74</cell><cell>-</cell></row><row><cell>MLP 1 + GCM 2</cell><cell>38.11</cell><cell>1.29</cell><cell>47.39</cell><cell>0.65</cell></row><row><cell>GCM 1 + MLP 2</cell><cell>37.87</cell><cell>1.05</cell><cell>48.14</cell><cell>1.40</cell></row><row><cell>GCM 1 + GCM 2</cell><cell>39.38</cell><cell>2.56</cell><cell>48.76</cell><cell>2.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5</head><label>5</label><figDesc>Comparison with MLP baseline in terms of runtime, computation complexity in FLOPs, and action localization mAP on THUMOS14. MP 1 + MP 2 ; (2) MP 1 + GCM 2 ; and (3) GCM 1 + MP 2 .</figDesc><table><row><cell>Method</cell><cell cols="2">Runtime</cell><cell>FLOPs</cell><cell cols="2">mAP@tIoU=0.5 RGB Flow</cell></row><row><cell>MLP 1 + MLP 2</cell><cell></cell><cell>0.376s</cell><cell cols="2">16.57M 36.82</cell><cell>46.74</cell></row><row><cell>GCM 1 + GCM 2</cell><cell></cell><cell>0.404s</cell><cell cols="2">17.70M 39.38</cell><cell>48.76</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 6</cell><cell></cell></row><row><cell cols="6">Comparison between our model and mean-pooling (MP) on THUMOS14,</cell></row><row><cell cols="6">measured by mAP (%) when tIoU=0.5.</cell></row><row><cell cols="2">mAP@tIoU=0.5</cell><cell>RGB</cell><cell>Gain</cell><cell>Flow</cell><cell>Gain</cell></row><row><cell>MP 1 + MP 2</cell><cell></cell><cell>37.12</cell><cell>-</cell><cell>46.96</cell><cell>-</cell></row><row><cell>MP 1 + GCM 2</cell><cell></cell><cell>38.32</cell><cell>1.20</cell><cell>47.66</cell><cell>0.80</cell></row><row><cell>GCM 1 + MP 2</cell><cell></cell><cell>38.38</cell><cell>1.26</cell><cell>47.93</cell><cell>1.07</cell></row><row><cell cols="2">GCM 1 + GCM 2</cell><cell>39.38</cell><cell>2.26</cell><cell>48.76</cell><cell>1.80</cell></row><row><cell cols="6">action units. The adjacent connections are formulated by using the</cell></row><row><cell cols="6">same graph as GCN. We term this baseline as MP below. Similar</cell></row><row><cell cols="6">to the setting in Section 5.2, we have three variants of our model</cell></row><row><cell>including: (1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">Comparison of different types of edge functions on THUMOS14,</cell></row><row><cell cols="3">measured by mAP (%) when tIoU=0.5.</cell></row><row><cell>mAP@tIoU=0.5</cell><cell>RGB</cell><cell>Flow</cell></row><row><cell>cos-sim</cell><cell cols="2">38.32 47.62</cell></row><row><cell>cos-sim + self-add</cell><cell cols="2">39.38 48.76</cell></row><row><cell cols="3">embed-cos-sim + self-add 39.27 48.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8</head><label>8</label><figDesc>Comparisons between our GCM and the baseline using learned weights on THUMOS14. 67.5 64.6 58.3 49.3 SSN + Learned weights 70.3 68.3 64.5 58.2 49.7 (?0.4) SSN + GCM (ours) 70.5 68.6 65.2 59.8 50.9 (?1.6)</figDesc><table><row><cell>Method</cell><cell></cell><cell>0.1</cell><cell cols="4">mAP at different tIoUs 0.2 0.3 0.4 0.5</cell></row><row><cell>SSN [56]</cell><cell></cell><cell cols="2">69.7 TABLE 9</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Comparison of three types of edge on THUMOS14, measured by mAP</cell></row><row><cell></cell><cell cols="4">(%) when tIoU=0.5.</cell><cell></cell><cell></cell></row><row><cell cols="2">mAP@tIoU=0.5</cell><cell></cell><cell>RGB</cell><cell>Gain</cell><cell>Flow</cell><cell>Gain</cell></row><row><cell cols="2">w/ all edges</cell><cell></cell><cell>39.38</cell><cell>-</cell><cell>48.76</cell><cell>-</cell></row><row><cell cols="7">w/o surrounding edges 38.80 -0.58 47.69 -0.56</cell></row><row><cell cols="3">w/o contextual edges</cell><cell cols="4">38.28 -1.10 47.57 -0.68</cell></row><row><cell cols="2">w/o semantic edges</cell><cell></cell><cell cols="4">39.02 -0.36 47.38 -0.87</cell></row><row><cell cols="2">w/o edges (MLP)</cell><cell></cell><cell cols="4">36.82 -2.56 46.74 -2.02</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE 10</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Comparison of different sampling sizes and training time for each</cell></row><row><cell cols="7">iteration on THUMOS14, measured by mAP@tIoU=0.5.</cell></row><row><cell>Ns</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>10</cell></row><row><cell>mAP</cell><cell cols="6">48.28 48.47 48.54 48.76 48.34 48.30</cell></row><row><cell cols="2">Time(s) 0.10</cell><cell>0.23</cell><cell>0.33</cell><cell>0.41</cell><cell>0.48</cell><cell>1.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 11</head><label>11</label><figDesc>Ablation study of our GCM on D-SSAD, measured by mAP (%) when tIoU=0.5 on THUMOS14.</figDesc><table><row><cell>Setting</cell><cell cols="2">mAP@IoU=0.5 Gain</cell></row><row><cell>D-SSAD [22] (our impl.)</cell><cell>43.21</cell><cell>-</cell></row><row><cell>D-SSAD + GCM ? 1</cell><cell>43.47</cell><cell>0.26</cell></row><row><cell>D-SSAD + GCM ? 2</cell><cell>44.29</cell><cell>1.08</cell></row><row><cell>D-SSAD + GCM ? 3</cell><cell>44.77</cell><cell>1.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 12</head><label>12</label><figDesc>Comparison of two types of edge on THUMOS14, conducted on D-SSAD<ref type="bibr" target="#b21">[22]</ref> with GCM.</figDesc><table><row><cell cols="2">Settings surrounding edges semantic edges</cell><cell cols="2">mAP@tIoU=0.5 Gain</cell></row><row><cell>?</cell><cell>?</cell><cell>43.21</cell><cell>-</cell></row><row><cell></cell><cell>?</cell><cell>43.82</cell><cell>0.61</cell></row><row><cell>?</cell><cell></cell><cell>44.19</cell><cell>0.98</cell></row><row><cell></cell><cell></cell><cell>44.77</cell><cell>1.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>9 Pole Vault Pole Vault Javelin Throw Ground Truth Our Predictions time Pole Vault Pole Vault</head><label></label><figDesc>Fig. 6. Examples of failure cases. Top: Our method predicts the beginning portion of Pole Vault as Javelin Throw since these two actions have similar contents (i.e., an athlete running with a pole). Bottom: Our method mis-classifies the action Cliff Diving into the action Diving without recognizing the background cliff.</figDesc><table><row><cell></cell><cell></cell><cell>55.0s</cell><cell></cell><cell>63.0s</cell><cell>94.4s</cell><cell></cell><cell>105.3s</cell></row><row><cell></cell><cell></cell><cell>55.1s</cell><cell>58.7s 59.7s</cell><cell>63.3s</cell><cell>96.3s</cell><cell></cell><cell>104.8s</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>time</cell></row><row><cell>Ground Truth</cell><cell cols="2">48.28s</cell><cell>Cliff Diving</cell><cell>53.72s</cell><cell>94.8s</cell><cell>Cliff Diving</cell><cell>100.24s</cell></row><row><cell cols="2">Our Predictions</cell><cell>47.12s</cell><cell>Cliff Diving</cell><cell>53.52s</cell><cell>94.92s</cell><cell>Diving</cell><cell>99.12s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Action search: Spotting actions in videos and its application to temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end, single-stream temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sst: Singlestream temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast temporal activity proposals for efficient detection of human actions in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1914" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iterative visual reasoning beyond convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7239" to="7248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal context network for activity localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Qiu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Daps: Deep action proposals for action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ctap: Complementary temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="68" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turn tap: Temporal unit regression network for temporal action proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3628" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascaded boundary regression for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A proposal-based solution to spatio-temporal action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schwarcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time temporal action localization in untrimmed videos by sub-action discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sap: Self-adaptive proposal model for temporal action detection based on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupling localization and classification in single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1288" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-23" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<title level="m">Temporal activity detection in untrimmed videos with recurrent neural networks. 1st NIPS Workshop on Large Scale Computer Vision Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The lear submission at thumos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal gaussian mixture layer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5152" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Untrimmed video classification for activity detection: submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ActivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">THUMOS14 Action Recognition Challenge</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<title level="m">Uts at activitynet 2016. ActivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<title level="m">Cuhk &amp; ethz &amp; siat submission to activitynet challenge 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial-aware graph relation network for large-scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">G-tad: Subgraph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10156" to="10165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring temporal preservation networks for precise temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2678" to="2687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Temporal action localization with pyramid of score distribution features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal action localization by structured maximal sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal action detection with structured segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
