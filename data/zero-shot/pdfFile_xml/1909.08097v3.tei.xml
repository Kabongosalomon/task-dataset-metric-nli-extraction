<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble Knowledge Distillation for Learning Improved and Efficient Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Asif</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harrer</surname></persName>
						</author>
						<title level="a" type="main">Ensemble Knowledge Distillation for Learning Improved and Efficient Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensemble models comprising of deep Convolutional Neural Networks (CNN) have shown significant improvements in model generalization but at the cost of large computation and memory requirements. In this paper, we present a framework for learning compact CNN models with improved classification performance and model generalization. For this, we propose a CNN architecture of a compact student model with parallel branches which are trained using ground truth labels and information from high capacity teacher networks in an ensemble learning fashion. Our framework provides two main benefits: i) Distilling knowledge from different teachers into the student network promotes heterogeneity in learning features at different branches of the student network and enables the network to learn diverse solutions to the target problem. ii) Coupling the branches of the student network through ensembling encourages collaboration and improves the quality of the final predictions by reducing variance in the network outputs. Experiments on the well established CIFAR-10 and CIFAR-100 datasets show that our Ensemble Knowledge Distillation (EKD) improves classification accuracy and model generalization especially in situations with limited training data. Experiments also show that our EKD based compact networks outperform in terms of mean accuracy on the test datasets compared to other knowledge distillation based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ensemble methods have shown considerable improvements in model generalization and produced state of the art results in several machine learning competitions (e.g., Kaggle) <ref type="bibr" target="#b3">[4]</ref>. These ensemble methods typically contain multiple deep Convolutional Neural Networks (CNN) as sub-networks which are pre-trained on large-scale datasets to extract discriminative features from the input data. The size of an ensemble is not constrained by training because the sub-networks can be trained independently, and their outputs can be computed in parallel. However, in many applications limited training data is not sufficient to effectively train deep CNN models compared to small or compact networks. For instance in healthcare applications, the amount of available data is constrained by the number of patients. Therefore, improving generalization capability of compact network without requiring large-scale annotated datasets is of utmost importance. Furthermore, today's high performing deep CNN based ensemble models have Giga-FLOPS compute and Giga-Bytes storage requirements <ref type="bibr" target="#b11">[12]</ref>, making them prohibitive in resource constrained systems (e.g., mobile-or edge-devices) which have stringent requirements on memory, latency and computational power. To overcome these challenges, model compression techniques such as parameter pruning <ref type="bibr" target="#b23">[24]</ref> is a common way to reduce model size with trade-offs between accuracy and efficiency. Other techniques include hand crafting efficient CNN architectures such as SqueezeNets <ref type="bibr" target="#b12">[13]</ref>, MobileNets <ref type="bibr" target="#b10">[11]</ref>, and ShuffleNets <ref type="bibr" target="#b25">[26]</ref>. Recently, neural network search showed an effective way to generate efficient CNN architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> by extensively tuning parameters such as network width, depth, filter types and sizes. These models showed better efficiency than hand-crafted networks but, at the cost of extremely large tuning cost. Another stream of work in building efficient networks for resource constrained scenarios is through Knowledge distillation <ref type="bibr" target="#b9">[10]</ref>. It enables small low memory footprint networks to mimic the behavior of large complex networks by training small networks using the predictions of large networks as soft labels in addition to the ground truth hard labels.</p><p>In this paper we also explore Knowledge Distillation (KD) based strategies to improve model generalization and classification performance for applications with memory and compute restrictions. For this, we present a CNN architecture with parallel branches which distill high level features from different teacher networks during training and maintains low computational overhead during inference. Our architecture provides two main benefits: i) It combines a student network with different teacher networks and distills diverse feature representations into the student network during training. This promotes heterogeneity in feature learning and enables the student network to mimic diverse high-level feature spaces produced by the teacher networks. ii) It combines the distilled information through parallel-branches in an ensembling manner. This reduces variance in the branch-level outputs and improves the quality of the final predictions of the student network. In summary, the main contributions of this paper are as follows:</p><p>1. We present an Ensemble Knowledge Distillation (EKD) framework which improves classification performance and model generalization of small and compact networks by distilling knowledge from multiple teacher networks into a compact student network using an ensemble architecture. 2. We present a novel training objective function to distill ensemble knowledge into a single student network. Our objective function optimizes the parameters of the student network with a goal of learning mappings between input data and ground truth labels, and a goal of minimizing the difference between high level features of the teacher networks and the student network. 3. We perform ablation study of our framework on CIFAR-10 and CIFAR-100 datasets in terms of different CNN architectures, varying ensemble sizes, and limited training data scenarios. Experiments show that by encouraging heterogeneity in feature learning through the proposed ensemble distillation, our EKD-based compact networks produce superior accuracy compared to the networks without using knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we discuss related work on model compression and knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Compression</head><p>Network pruning is a popular approach to reduce a heavy network to obtain a light-weight form by removing redundancy in the heavy network. In this approach, a complex over-parameterized network is first trained, then pruned based on come criterions, and finally fine-tuned to achieve comparable performance with reduced parameters. In this context, methods such as <ref type="bibr" target="#b23">[24]</ref> compress large networks through the reduction of connections based on weight magnitudes or importance scores. Other methods used quantization of the weights to 8 bits <ref type="bibr" target="#b6">[7]</ref>, filter pruning <ref type="bibr" target="#b14">[15]</ref> and channel pruning <ref type="bibr" target="#b15">[16]</ref> to reduce network sizes. However, the trimmed models are generally sub-graphs of the original networks and there is less flexibility in changing the original architecture design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>Knowledge Distillation (KD) aims at learning a light-weight student network such that it can mimic the behavior of a complicated teacher network. In this context, the work of <ref type="bibr" target="#b0">[1]</ref> was the first to introduce knowledge distillation by minimizing L2 distance between the features from the last layers of two networks. Later, the work of <ref type="bibr" target="#b9">[10]</ref> showed that the predicted class probabilities from the teacher are informative for the student and the probabilities can be used as a supervision signal in addition to the regular labeled training data during training. Romero et. al. <ref type="bibr" target="#b18">[19]</ref> bridged the intermediate layers of the student and teacher networks in addition to the class probabilities and used L2 loss to supervise the student network. The method of <ref type="bibr" target="#b4">[5]</ref> minimized the difference between teacher and student derivatives of the loss combined with the divergence from the teacher predictions. Other methods explored knowledge distillation using activation maps <ref type="bibr" target="#b8">[9]</ref>, attention maps <ref type="bibr" target="#b24">[25]</ref>, Jacobians <ref type="bibr" target="#b19">[20]</ref>, and unsupervised feature factors <ref type="bibr" target="#b13">[14]</ref>.</p><p>Ensembling is a promising technique to improve model generalization compared to the performance of individual models. Since different CNN architectures can achieve diverse distributions of errors due to the presence of several local minima, the combination of the outputs of individually trained networks leads to improved performance and better generalization to unseen test data. In the light of these studies, methods such as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6]</ref> combined ensemble learning and knowledge distillation to improve model generalization. For instance, the method of <ref type="bibr" target="#b21">[22]</ref> trained an ensemble of 16 CNN models and compressed the learned function into shallow multi-layer perceptrons containing 5 layers. The work of <ref type="bibr" target="#b5">[6]</ref> presented an iterative technique to transform a student model into the teacher model at each iteration. At the end of the iterations, all the student outputs were combined to form an ensemble. Our work also follows ensemble learning coupled with knowledge distillation however, compared to <ref type="bibr" target="#b5">[6]</ref>, we train a compact student network through knowledge distillation in a single iteration. Furthermore, our ensemble architecture distills knowledge from different teacher networks into the student network. This increases heterogeneity in student feature learning and enables the student network to mimic diverse feature representations produced by different teacher networks. Consequently, our EKDbased compact networks demonstrate better generalization capability compared to conventional KD methods <ref type="bibr" target="#b9">[10]</ref>. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the overall architecture of our Ensemble Knowledge Distillation (EKD) framework. It consists of two main modules: i) A compact student network (CompNet) which is composed of Ns branches connected in parallel ( <ref type="figure" target="#fig_1">Fig. 1-A)</ref>. The branches follow a common architecture constituting convolutional and pooling layers. ii) A Teacher Ensemble Network (TeachNet) which is composed of Nt CNN models with different architectures or layer configurations ( <ref type="figure" target="#fig_1">Fig. 1-B</ref>). In the following, we describe in detail the individual modules of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Proposed Compact Network (CompNet)</head><p>Our compact network is composed of Ns branches connected in parallel. The branches follow a common architecture where each branch is composed of multiple convolutional layers interconnected through residual connections. The branch outputs are fed into linear layers (Q s ) to produce probabilistic distributions of the input data with respect to the target classes. Our branch architecture is composed of ResNet structure which contain multiple residual blocks. Specifically, the branch architecture starts with a 3?3 convolution followed by Batch Normalization (BN), and a Rectified Linear Unit (ReLU). Next, there are three residual blocks, where each residual block consists of two convolution layers with skip connections, followed by a pooling layer. Each convolution in the residual block is followed by BN, and a ReLU operation. Each branch ends with global average pooling and produces Y ? R 1?M ?dimensional feature maps which are then fed to a linear layer of 1 ? K dimensions to produce probabilitic distributions (Qs ? R 1?K ) with respect to K target classes. Mathematically, the output of a linear layer can be written as:</p><formula xml:id="formula_0">Q s = Y * W s + B s<label>(1)</label></formula><p>where, W s and B s represent weights and bias matrices, respectively. Finally, the outputs of the linear layers are summed to produce a combined feature representation Ps as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>-A. It is given by:</p><formula xml:id="formula_1">Ps = Ns i=1 Q s i (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Proposed Teacher Ensemble Network (TeachNet)</head><p>Our teacher ensemble is composed of multiple CNN models which act as independent classifiers. The teacher sub-networks should use different architectures or layer configurations in order to produce diverse feature representations at the final convolutional layers. Similar to our CompNet architecture, the teacher outputs are first fed into linear layers to produce probabilistic distributions (Q t ) of the input data with respect to the target classes, and finally summed together to produce a combined feature representation Pt as shown in <ref type="figure" target="#fig_1">Fig. 1-B</ref>. It is given by:  In the following we describe our specialized training objective function which optimizes the parameters of our CompNet using ground truth labels as well as high-level feature representations from the teacher ensemble.</p><formula xml:id="formula_2">Pt = N t i=1 Q t i<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Proposed Ensemble Knowledge Distillation (EKD)</head><p>Consider </p><p>Our training function Ltrain is a weighted combination of three loss terms. A CrossEntropy loss term LCE which is applied on the outputs of the teacher ensemble and the CompNet model with respect to the ground truth labels (y), and a distillation loss term LKD which matches the outputs of the sub-networks of the teacher ensemble and the outputs of the branches of the CompNet model. Mathematically, Ltrain can be written as:</p><formula xml:id="formula_4">Ltrain = ? ? LCE(Pt, y) + ? ? LCE(Ps, y) + ? ? LKD,<label>(5)</label></formula><p>where Pt = ft(x) and Ps = fs(x) represent the logits (the inputs to the SoftMax) of the teacher ensemble and the CompNet model, respectively. The terms ? ? [0, 0.5, 1], ? ? [0, 0.5, 1], and ? ? [0, 0.5, 1] are the hyper-parameters which balance the individual loss terms. Mathematically, the CrossEntropy loss LCE can be written as:</p><formula xml:id="formula_5">LCE(Pt, y) = K k=1 I(k = y) log ?(Pt, y),<label>(6)</label></formula><p>where I is the indicator function and ? is the SoftMax operation. It is given by:</p><formula xml:id="formula_6">?(z) = exp(z) K k=1 exp(z k ) .<label>(7)</label></formula><p>Our KD-based loss LKD in Eq. 5 is composed of Kullback-Leibler (KL) divergence loss (LKL) and Mean-Squared-Error loss (LMSE). Mathematically, LKD can be written as:</p><formula xml:id="formula_7">LKD = LKL(Ps, Pt/T ) + LMSE(Ps, Pt)+ Ns i=1 (LKL(Q s i , Q t i /T ) + LMSE(Q s i , Q t i )),<label>(8)</label></formula><p>where i indexes the sub-networks of the teacher ensemble and the branches of the CompNet model. The term T in Eq. 8 is a temperature hyper-parameter which controls the softening of the output of the teacher sub-networks. A higher value of T produces a softer probability distribution over the target classes. The KL divergence loss LKL is defined between log-probabilities computed from the outputs of a student network and probabilities computed from the outputs of a teacher network. Mathematically, it can be written as:</p><formula xml:id="formula_8">LKL(Q s 1 , Q t 1 ) = K k=1 ?(Q s 1 ) ? log(?(Q s 1 )) ? ?(Q t 1 ) ,<label>(9)</label></formula><p>where ? represents the SoftMax operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section we describe the details of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architectures</head><p>We evaluated our ensemble knowledge distillation using ResNet8 structure as the student network as used in other knowledge distillation based studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>. For the teacher ensemble, we considered up to 7 sub-networks based on ResNet14, ResNet20, ResNet26, ResNet32, ResNet44, ResNet56, and ResNet110 architectures as used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training and Implementation</head><p>For training, we initialized the weights of the convolutional and the fully connected layers from zero-mean Gaussian distributions. The  standard deviations were set to 0.01, the biases were set to 0, and a parameter decay of 0.0005 was set on the weights and biases. The teacher ensemble was first trained independently from the scratch, and then fine-tuned simultaneously and collaboratively with the student network. The distillation from the teacher sub-networks to the student network was performed throughout the whole training process by optimizing the training objective function in Eq. 5. Specifically, we trained the networks for 500 epochs starting with a learning rate of 0.01 which was divided by 10 at 50% and 75% of the total number of epochs. Our implementation is based on the autogradient computation framework of the Torch library <ref type="bibr" target="#b17">[18]</ref>. Training was performed by ADAM optimizer with a batch size of 128 using 2 nvidia V100 GPU hardware. For hyper-parameter optimization, we used the toolkit of <ref type="bibr" target="#b1">[2]</ref> to tune the loss weighing parameters ? = 0.5, ? = 0.5, ? = 0.6, and the temperature parameter T = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>We evaluated our framework on three well calibrated image classification datasets CIFAR-10, CIFAR-100, and Tiny ImageNet 4 . CIFAR-10 and CIFAR-100 consist of 60,000 RGB images distributed into 10 and 100 classes, respectively. Specifically, the training set contains 50,000 images and the test set contains 10,000 images of sizes 64 ? 64 pixels. Tiny ImageNet is a subset of the Ima-geNet dataset. It contains 100k training images and 10k test images </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ensemble Distillation Improves Model Performance</head><p>Here, we evaluate our compact student networks with and without the proposed Ensemble Knowledge Distillation (EKD) on the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. <ref type="table" target="#tab_1">Table 1</ref> shows the results of these experiments for different ensemble sizes. From <ref type="table" target="#tab_1">Table 1</ref> we see that EKD based networks improve accuracy for all the tested ensemble sizes on all the target datasets. For instance, EKD with an ensemble size of 7 improves accuracy by upto 4%, 7%, and 4% on the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ensemble Learning Improves Knowledge Distillation</head><p>Here, we evaluate the performance of our EKD based student networks by varying the size of the ensemble to explore the benefits of ensembling for knowledge distillation. <ref type="table" target="#tab_1">Table 1</ref> shows the results of these experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. The results show that the accuracy increases with the increase in ensemble size (ES) at the cost of increase in the number of parameters and the number of FLOPS. For instance, a student network with 7 branches improves accuracy by around 2%, 7%, and 7% <ref type="figure">Figure 3</ref>: Comparison of mean accuracy of a 7-branch ResNet8 model with and without the proposed EKD for different sizes of data used for training the models on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. The comparison shows that compared to ResNet8 without using knowledge distillation, our EKD-based ResNet8 produced considerably higher mean accuracy for all the tested data sizes. Our EKD-based 7-branch ResNet8 also produced higher mean accuracy compared to ResNet110 when limited data was used to train the models. compared to a 1-branch student network on the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, respectively. <ref type="figure" target="#fig_3">Fig. 2</ref> shows a comparison of TSNE embeddings of features produced by ResNet8 models with different ensemble sizes. From the figure, it is clear that the embeddings produced by the 7-branch model with EKD shows better seperation of the target classes compared to the embeddings produced by the 1-branch model without distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Generalization Performance</head><p>Here we evaluate the generalization performance of the proposed ensemble knowledge distillation. For this, we conducted experiments for different sizes of the data used for training the teacher and the student networks. <ref type="figure">Fig. 3</ref> and <ref type="figure" target="#fig_5">Fig. 4</ref> show the results of these experiments using ResNet8 as the student network. The results show that the performance gap increases for all the tested networks as the size of the dataset is reduced. For instance, the accuracy drop by around 25%, 40%, and 30% when 10% of the data was used to train the networks without the proposed EKD on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, respectively as shown in <ref type="figure">Fig. 3. Fig. 3</ref> and <ref type="figure" target="#fig_5">Fig. 4</ref> also show that using the proposed EKD, ResNet8 based models improve test accuracy for all the tested sizes of training data with considerable margins compared to ResNet8 models without distilla-tion and the other networks used as teachers (ResNet14, ResNet20, ResNet26, ResNet32, ResNet44, ResNet56, and ResNet110). For instance, EKD-based ResNet8 models produced improvements of upto 12%, 10%, and 25% when 10% of data was used for training on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets, respectively compared to networks without distillation as shown in <ref type="figure">Fig. 3</ref>. These improvements are attributed to our ensemble distillation which promotes diversity in feature learning by transfering knowledge from different teachers into the student network and improves model generalization to test data. These experiments show that for situations where non-KD methods fail to achieve generalization due to insufficient data, the proposed ensemble distillation achieves considerable performance improvements, thereby demonstrating potentials for uses in applications with limited-data constraints. <ref type="table" target="#tab_2">Table 2</ref> shows a comparison between our EDK-based 7-branch student network and the teacher networks on the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets when 10% of the data was used for training. The results show that our EKD based 7-branch ResNet8 produced the best accuracy on the test datasets with 3? less number of parameters, 2.8? less number of FLOPS, and faster inference speed compared to the ResNet110 network. We also conducted experiments to visualize the features space learnt by our EKD based networks. <ref type="figure" target="#fig_6">Fig. 5</ref> shows the 2-dimensional TSNE-embeddings gen- erated using the features produced by the teacher networks and our 7-branch ResNet8 models with and without using the proposed ensemble distillation under different sizes of data used during training. These experiments show that training a student network using highlevel features from multiple teacher networks enables the student network to produce embeddings which exhibit better separation of the target classes compared to the embeddings produced by the models without using distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with Other Knowledge Distillation Methods</head><p>Here, we compare our Ensemble Knowledge Distillation (EKD) with some of the recent state-of-the-art knowledge distillation based methods including: activation based attention transfer (AAT) <ref type="bibr" target="#b24">[25]</ref>, the method of <ref type="bibr" target="#b22">[23]</ref> (FSP), the method of <ref type="bibr" target="#b9">[10]</ref>, hint based transfer (Fit-Net) <ref type="bibr" target="#b18">[19]</ref>, the method of <ref type="bibr" target="#b7">[8]</ref>(BSS), the method of deep mutual learning <ref type="bibr" target="#b26">[27]</ref>(MUTUAL), and the method of <ref type="bibr" target="#b16">[17]</ref> (TAKD). For a fair comparison, we used exactly the same setting for CIFAR-10 experiments, and a ResNet8 based student network as used in the baseline  studies. <ref type="table">Table 3</ref> shows that our EKD based ResNet8 improved accuracy on the tested dataset compared to the other KD methods. This improved performance is attributed to the proposed ensemble distillation architecture where the proposed training objective function enabled the student network to successfully mimic diverse feature embeddings produced by different teachers and improve generalization to unseen test data. Furthermore, the combination of distilled information through ensembling reduced variance in the outputs and improved the quality of the final predictions of the student network.</p><formula xml:id="formula_9">(C) ResNet110</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Conclusion and Future Work</head><p>Recently, deep CNN based ensemble methods have shown state-ofthe-art performance in image classification but at the cost of high computation cost and large memory requirements. In this paper, we show that knowledge distillation using an ensemble architecture can improve classification accuracy and model generalization especially with fewer training data for small and compact networks. Unlike traditional ensembling techniques which reduce variance in outputs by combining independently trained networks, we show that Ensemble Knowledge Distillation (EKD) encourages heterogeneity in student <ref type="table">Table 3</ref>: Comparison of our ensemble distillation using 1-branch ResNet8 base student network and other Knowledge Distillation (KD) methods on the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dataset Accuracy</p><p>Hinton <ref type="bibr" target="#b9">[10]</ref> CIFAR-10 86.66 FITNET <ref type="bibr" target="#b16">[17]</ref> CIFAR-10 86.73 Attention <ref type="bibr" target="#b24">[25]</ref> CIFAR-10 86.86 FSP <ref type="bibr" target="#b22">[23]</ref> CIFAR-10 87.07 BSS <ref type="bibr" target="#b7">[8]</ref> CIFAR-10 87.32 MUTUAL <ref type="bibr" target="#b26">[27]</ref> CIFAR-10 87.71 TAKD <ref type="bibr" target="#b16">[17]</ref> CIFAR-10 88.01 (this work) ResNet8+EKD (ES=1) CIFAR-10 89.66 feature learning through collaboration between different teachers and the student network. This enables student networks to learn more discriminative and diverse feature representations while maintaining small memory and compute requirements. Experiments on wellestablished CIFAR-10, CIFAR-100, and Tiny ImageNet datasets show that compact networks trained through the proposed ensemble distillation improved classification accuracy and model generalization especially in situations with fewer training data. In future, we plan to explore a fully data-driven automated ensemble selection. We also plan to evaluate our framework for video classification tasks to gain more insights into the benefits of ensemble distillation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>IBM Research Australia, email: umarasif@au1.ibm.com 2 IBM Research Australia, email: jbtang@au1.ibm.com 3 IBM Research Australia, email: sharrer@au1.ibm.com</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of our framework which consists of a Compact Network (CompNet)-A and a teacher ensemble network (TeachNet)-B. CompNet is composed of parallel branches with similar architecture topology. During training, the branches are coupled with the sub-networks of the teacher ensemble and the parameters of the CompNet model are optimized with respect to the ground truth labels as well as the high-level features produced by the teacher ensemble. During testing, the branches of CompNet are executed in parallel (to increase inference speed) and their outputs are summed before Softmax to produce final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a training dataset of images and labels (x, y) ? (X , Y), where each sample belongs to one of the K classes (Y = 1, 2, ..., K). To learn the mapping fs(x) : X ? Y, we train our CompNet parameterized by fs(x, ? * ), where ? * are the learned parameters obtained by minimizing a training objective function Ltrain: ? * = arg min ? Ltrain(y, fs(x, ?))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of TSNE visualizations of 2-dimensional embeddings generated using features produced by a 7-branch ResNet8 model with EKD (A), a 7-branch ResNet8 model without distillation (B), a 1-branch ResNet8 model with EKD (C), and a 1-branch ResNet8 model without distillation (D) on the test data of CIFAR-10 dataset. The comparison shows that the embeddings produced by our EKD based models show better separation of the target classes compared to the embeddings produced by models trained without distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4</head><label></label><figDesc>https://tiny-imagenet.herokuapp.com/ distributed into 200 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of mean accuracy of our EKD-based ResNet8 model and teacher networks for different sizes of data used for training on CIFAR-10 and CIFAR-100 datasets. The comparison shows that the proposed EKD-based ResNet8 produces higher accuracy compared to the models trained without knowledge distillation under limited training data cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of TSNE visualizations of 2-dimensional embeddings generated using features produced by the proposed teacher networks (A, B, and C), our EKD-based ResNet8 (D), and a ResNet8 model without distillation (E), on the test data of CIFAR-10 dataset. The comparison shows that the embeddings produced by our EKD based models show better separation of the target classes especially in cases with limited training data compared to the embeddings produced by models trained without distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablation study of our ensemble distillation based student network in terms of the Ensemble Size (ES), number of training parameters, number of FLOPS, and mean test accuracy on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets for ResNet8 as the student network architecture.</figDesc><table><row><cell>Model</cell><cell cols="2">ES Teachers</cell><cell cols="6">CIFAR10 no EKD with EKD no EKD with EKD no EKD with EKD CIFAR100 Tiny ImageNet</cell><cell cols="2">No. of param No. of FLOPS (million) (million)</cell></row><row><cell></cell><cell>1</cell><cell>T1</cell><cell>87.51</cell><cell>89.66</cell><cell>60.03</cell><cell>60.57</cell><cell>34.98</cell><cell>36.84</cell><cell>0.08</cell><cell>12.75</cell></row><row><cell></cell><cell>2</cell><cell>T1-T2</cell><cell>86.60</cell><cell>91.11</cell><cell>60.58</cell><cell>62.79</cell><cell>37.85</cell><cell>40.36</cell><cell>0.16</cell><cell>25.50</cell></row><row><cell></cell><cell>3</cell><cell>T1-T3</cell><cell>87.41</cell><cell>91.40</cell><cell>62.78</cell><cell>64.09</cell><cell>37.99</cell><cell>41.58</cell><cell>0.23</cell><cell>38.25</cell></row><row><cell>ResNet8</cell><cell>4</cell><cell>T1-T4</cell><cell>87.74</cell><cell>91.72</cell><cell>63.25</cell><cell>65.45</cell><cell>38.00</cell><cell>41.86</cell><cell>0.31</cell><cell>51.01</cell></row><row><cell></cell><cell>5</cell><cell>T1-T5</cell><cell>87.81</cell><cell>92.15</cell><cell>64.24</cell><cell>66.54</cell><cell>38.16</cell><cell>42.63</cell><cell>0.39</cell><cell>63.76</cell></row><row><cell></cell><cell>6</cell><cell>T1-T6</cell><cell>88.14</cell><cell>92.24</cell><cell>60.75</cell><cell>67.36</cell><cell>39.45</cell><cell>42.59</cell><cell>0.47</cell><cell>76.51</cell></row><row><cell></cell><cell>7</cell><cell>T1-T7</cell><cell>88.05</cell><cell>92.33</cell><cell>60.83</cell><cell>67.78</cell><cell>39.06</cell><cell>43.89</cell><cell>0.55</cell><cell>89.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of a 7-branch ResNet8 model (ES=7) with and without the proposed EKD and the proposed teacher networks in terms of number of training parameters, number of FLOPS, and inference speed. The results show that the proposed EKD based ResNet8 produces higher average accuracy compared to the other networks without knowledge distillations on all the tested datasets.</figDesc><table><row><cell>Model</cell><cell cols="6">CIFAR-10 data size 10% data size 10% data size 10% CIFAR-100 Tiny ImageNet No. of param. No. of FLOPS Inference time (million) (million) (ms)</cell></row><row><cell>ResNet14</cell><cell>64.39</cell><cell>20.77</cell><cell>15.89</cell><cell>0.19</cell><cell>27.09</cell><cell>2</cell></row><row><cell>ResNet20</cell><cell>64.38</cell><cell>22.98</cell><cell>16.67</cell><cell>0.28</cell><cell>41.42</cell><cell>5</cell></row><row><cell>ResNet26</cell><cell>63.61</cell><cell>23.13</cell><cell>14.79</cell><cell>0.38</cell><cell>55.75</cell><cell>5</cell></row><row><cell>ResNet32</cell><cell>62.74</cell><cell>24.48</cell><cell>13.69</cell><cell>0.48</cell><cell>70.07</cell><cell>6</cell></row><row><cell>ResNet44</cell><cell>63.95</cell><cell>22.74</cell><cell>14.80</cell><cell>0.67</cell><cell>98.73</cell><cell>9</cell></row><row><cell>ResNet56</cell><cell>62.33</cell><cell>21.48</cell><cell>16.14</cell><cell>0.87</cell><cell>127.39</cell><cell>10</cell></row><row><cell>ResNet110</cell><cell>56.66</cell><cell>20.75</cell><cell>12.51</cell><cell>1.74</cell><cell>256.34</cell><cell>18</cell></row><row><cell>ResNet8 (ES=7)</cell><cell>69.65</cell><cell>31.98</cell><cell>17.49</cell><cell>0.55</cell><cell>89.26</cell><cell>11</cell></row><row><cell>ResNet8+EKD (ES=7)</cell><cell>79.97</cell><cell>56.13</cell><cell>31.32</cell><cell>0.55</cell><cell>89.26</cell><cell>11</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bal?zs</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?gl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware&apos;, arXiv preprint</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sobolev training for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<title level="m">Born again neural networks&apos;</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving knowledge distillation with supporting adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05532</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3779" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved knowledge distillation via teacher assistant: Bridging the gap between student and teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed-Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03393</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Knowledge transfer with jacobian matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00443</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjie</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caruana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05691</idno>
		<title level="m">Do deep convolutional nets really need to be deep and convolutional?&apos;, arXiv preprint</title>
		<editor>Abdelrahman Mohamed, Matthai Philipose, and Matt Richardson</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nisp: Pruning networks using neuron importance score propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruichi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Hsin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9194" to="9203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shufflenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
