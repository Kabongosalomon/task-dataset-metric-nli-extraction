<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Robust Dynamic Hand Gesture Recognition via Key Frames Extraction and Feature Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Lingxi Artificial Intelligence Co</orgName>
								<address>
									<settlement>Ltd, Shen Zhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Robust Dynamic Hand Gesture Recognition via Key Frames Extraction and Feature Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hand gesture recognition</term>
					<term>Key frames extraction</term>
					<term>Feature fusion</term>
					<term>Fast</term>
					<term>Robust</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gesture recognition is a hot topic in computer vision and pattern recognition, which plays a vitally important role in natural human-computer interface. Although great progress has been made recently, fast and robust hand gesture recognition remains an open problem, since the existing methods have not well balanced the performance and the efficiency simultaneously. To bridge it, this work combines image entropy and density clustering to exploit the key frames from hand gesture video for further feature extraction, which can improve the efficiency of recognition. Moreover, a feature fusion strategy is also proposed to further improve feature representation, which elevates the performance of recognition. To validate our approach in a "wild" environment, we also introduce two new datasets called HandGesture and Action3D datasets. Experiments consistently demonstrate that our strategy achieves competitive results on Northwestern University, Cambridge, HandGesture and Action3D hand gesture datasets. Our code and datasets will release at https://github.com/Ha0Tang/HandGestureRecognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Gesture recognition is to recognize category labels from an image or a video which contains gestures made by the user. Gestures are expressive, meaningful body motions involving physical movements of the fingers, hands, arms, head, face, or body with the intent of: conveying meaningful information or interacting with the environment.</p><p>Hand gesture is one of the most expressive, natural and common type of body language for conveying attitudes and emotions in human interactions. For example, in a television control system, hand gesture has the following attributes: "Pause","Play", "Next Channel", "Previous Channel", "Volume Up", "Volume Down" and "Menu Item". While in a recommendation system, hand gesture can express "Like" or "Dislike" emotions of users. Thus, it is one of the most fundamental problems in computer vision and pattern recognition, and has a wide range of applications such as virtual reality systems <ref type="bibr" target="#b0">[1]</ref>, interactive gaming platforms <ref type="bibr" target="#b1">[2]</ref>, recognizing sign language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, enabling very young children to interact with computers <ref type="bibr" target="#b5">[6]</ref>, controlling robot <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, practicing music conducting <ref type="bibr" target="#b8">[9]</ref>, television control <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, automotive interfaces <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, learning and teaching assistance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and hand gesture generation <ref type="bibr" target="#b15">[16]</ref>. There has been significant progress in hand gesture recognition, however, some key problems e.g., fast and robust are still challenging. Prior work usually puts emphasis on using whole data series, which always contain redundant information, resulting in degraded performance. For examples, Wang et al. <ref type="bibr" target="#b0">[1]</ref> present a superpixel-based hand gesture recognition system based on a novel superpixel earth mover's distance metric. Ren et al. <ref type="bibr" target="#b1">[2]</ref> focus on building a robust part-based hand gesture recognition system. Hikawa and Kaida <ref type="bibr" target="#b2">[3]</ref> propose a posture recognition system with a hybrid network. Moreover, there are many approaches are also proposed for action or video recognition task, such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Liu and Shao <ref type="bibr" target="#b16">[17]</ref> introduce an adaptive learning methodology to extract spatio-temporal features, simultaneously fusing the RGB and depth information, from RGB-D video data for visual recognition tasks. Liu et al. <ref type="bibr" target="#b25">[26]</ref> propose to combine the Salient Depth Map (SDM) and the Binary Shape Map (BSM) for human action recognition task. Simonyan et al. <ref type="bibr" target="#b26">[27]</ref> propose a two-stream ConvNet architecture which incorporates spatial and temporal networks to extract spatial and temporal features. Feichtenhofer et al. <ref type="bibr" target="#b27">[28]</ref> study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. In sum, all these efforts endeavor to decrease the computation burden in each solo frame, while overlooking all processing schemes in the whole frames would incur more computation burden than a few selected representative frames, which is a fundamental way to decrease the computation burden, greatly. This paper is devoted to bridge the gap between fast and robust hand gesture recognition, simply using solo popular cue e.g., RGB, which ensures great potential in practical use.</p><p>Key frames, also known as representative frames, extract the main content of a data series, which could greatly reduce the amount of processing data. In <ref type="bibr" target="#b28">[29]</ref>, the key frames of the video sequence are selected by their discriminative power and represented by the local motion features detected in them and integrated from their temporal neighbors. Carlsson and Sullivan <ref type="bibr" target="#b29">[30]</ref> demonstrate that specific actions can be recognized in long video sequence by matching shape information extracted from individual frames to stored prototypes representing key frames of the action. However, we regard every frame in a video as a point in the 2-D coordinate space. Since we are focusing on distinguishing dynamic gesture from a data series while not reconstructing it, we simply introduce a measure to find which frames are more important for distinguishing and which are not. In consideration of information entropy <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> could be a useful measurement to quantify the information each frame contains, we introduce frame entropy as an quantitative feature to describe each frame and then map these values into a 2-D coordinate space. How to describe this 2-D space is a hard nut to crack for its uneven distribution. Therefore, we further propose an integrated strategy to extract key frames using local extreme points and density clustering. Local extreme points includes the local maximum and local minimum points, which represent the most discriminative points of frame entropy. Shao and Ji <ref type="bibr" target="#b33">[34]</ref> also propose a key frame extraction method based on entropy. However, the differences between <ref type="bibr" target="#b33">[34]</ref> and the proposed method are two-folder: (i) The entropy in <ref type="bibr" target="#b33">[34]</ref> is calculated on motion histograms of each frame, while the proposed method directly calculate on each frame. (ii) <ref type="bibr" target="#b33">[34]</ref> simply to find peaks in the curve of entropy and use histogram intersection to output final key frames, while the proposed method first selects the local peaks of entropy and then use density clustering to calculate the cluster centers as the final key frames. Density clustering <ref type="bibr" target="#b34">[35]</ref> is the approach based on the local density of feature points, which is able to detect local clusters, while previous clustering approaches such as dynamic delaunay clustering <ref type="bibr" target="#b35">[36]</ref>, k-means clustering <ref type="bibr" target="#b36">[37]</ref>, spectral clustering <ref type="bibr" target="#b37">[38]</ref> and graph clustering <ref type="bibr" target="#b38">[39]</ref> cannot detect local clusters due to the fact that they only rely on the distance between feature points to do clustering.</p><p>In order to promote the accuracy, we also present a novel feature fusion method that combines appearance and motion cues. After extracting key frames, we replace the original video sequence with the key frames sequence, which could greatly enhance the time efficiency at the cost of accuracy. This feature fusion strategy takes advantage of both the motion and the appearance information in the spatiotemporal activity context under the hierarchical model. The experimental results show that the method proposed is accurate and effective for dynamic hand gesture recognition on four datasets. To summarize, the main contributions of this paper are:</p><p>? A novel key frames extraction method is proposed, which improves efficiency of hand gesture processing.</p><p>? A feature strategy is presented in which appearance and motion cues are fused to elevate the accuracy of recognition.</p><p>? Experiments demonstrate that our method achieves the balance between efficiency and accuracy simultaneously in four datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Key Frames Extraction and Feature Fusion Strategy for Hand Gesture Recognition</head><p>In this section, we will introduce the proposed key frames extraction and feature fusion strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Key Frames Extraction</head><p>Key frames extraction is the key technology for video abstraction, which can remove the redundant information in the video greatly. The algorithm for key frames extraction will affect the reconstruction of video content. If a frame in video V can be represented by f i , where i is (1, 2, ..., n) and n is the total number of frames in video V . Hence, the key frames set S Keyframes is defined as follows:</p><formula xml:id="formula_0">S Keyframes = f Keyframes (V ),<label>(1)</label></formula><p>where f Keyframes denotes the key frames extraction procedure.</p><p>In this paper, a method of key frames extraction based on image entropy and density clustering is proposed, as we can see from <ref type="figure">Figure 1</ref>. Our key frames extraction methods are mainly divided into three steps, namely, 1) calculating image entropy, 2) finding local extreme points and 3) executing density cluster. The following section would expand upon on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Image Entropy.</head><p>In this section, we try to find a proper descriptive index to evaluate each frame in a video, facilitating key frame extraction. Informative frames could better summarize the whole video where they reside, while how to quantify the information each frame contains is a hard-nut to crack. Firstly, we calculate image entropy of each frame, and then map them into a two-dimensional coordinate space, as shown in <ref type="figure">Figure 1</ref>(b). Entropy is a nice way of representing the impurity or unpredictability of a set of data since it is dependent on the context in which the measurement is taken. As for a single video frame, the gray-scale color/intensity distribution of this frame can be seen as (a) A hand gesture sequence sample from the Northwestern University hand gesture dataset, which contains 26 frames. The key frames obtained by our method are in green boxes, which are the 2, 9, 14, 20 and 26 frames. (e) Select the number of clustering. In this case, we choose 5 clusters.</p><p>(f) The final results of clustering. The points of 2, 5, 8, 9 and 10 are the clustering centers, therefore, the corresponding frames <ref type="bibr">(2, 9, 14, 20 and 26)</ref> are the key frames of original sequence.</p><p>(g) The key frames are the 2, 9, 14, 20 and 26 frames. Now we use this sequence to replace the original sequences for the next step. <ref type="figure">Figure 1</ref>: The framework of the proposed key frames extraction method. p = {p 1 , p 2 , ..., p n }. For the image frames f i , their image entropy can be defined as:</p><formula xml:id="formula_1">E(f i ) = ? j p fi (j)logp fi (j),<label>(2)</label></formula><p>where p fi (j) denotes the probability density function of frame f i , which could be obtained by normalizing their histogram of gray-scale pixel intensities. Next we map the value E(f i ) to a two-dimensional coordinate space (the E(f i ) vs. i plot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Local Extreme Points.</head><p>Secondly, we pick the local extreme points in the twodimensional coordinate space, illustrated by <ref type="figure">Figure 1</ref>(c). Local extreme points include the local maximum points and local minimum points. Local maximum points can be calculated as follows:</p><formula xml:id="formula_2">P max = E(f i ), if E(f i ) &gt; E(f i+1 ) &amp; E(f i ) &gt; E(f i?1 ). remove, else.</formula><p>(3) Local minimum points can also be calculated by the following formula:</p><formula xml:id="formula_3">P min = E(f i ), if E(f i+1 ) &gt; E(f i ) &amp; E(f i?1 ) &gt; E(f i ). remove, else.</formula><p>(4) where i = 1, 2, ..., n. Therefore, local extreme points P extreme can be united by:</p><formula xml:id="formula_4">P extreme = P max ? P min .<label>(5)</label></formula><p>Local extreme points could further extract representative frames from the original video sequence. This procedure could be viewed as finding local representatives to roughly describe the original sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Density Clustering.</head><p>After obtaining the extreme points, as shown in <ref type="figure">Figure  1</ref>(c), we try to cluster these points into N (N is a predefined constant for all the datasets) categories, as, {1, 2}, {3, 4, 5, 6, 7}, {8}, {9} and {10}. The distribution of these extreme points have the characteristics, the cluster centers are surrounded by neighbors with a lower local density and that they are at a relatively large distance from any points with a higher local density.</p><p>Therefore, we adopt density clustering <ref type="bibr" target="#b34">[35]</ref> to further cluster these extreme points P extreme , as shown in <ref type="figure">Figure  1</ref>(d-f). Density clustering could better catch the delicate structure of 2-D space where extreme points reside than traditional clustering strategies, e.g. K-means. First, we search for a local density maximum point as a cluster center, and then spread the cluster label from high density to low-density points sequentially. For each data point P k , we compute two quantities: corresponding local density (neighborhood has a density, not the data point) ? P k and its distance ? P k from points of higher density. Both these quantities depend only on the distances d P k P l between data points, which are assumed to satisfy the triangular inequality. The local density ? P k of data point P k is defined as:</p><formula xml:id="formula_5">? P k = P l ?(d P k P l ? d c ),<label>(6)</label></formula><p>where ?(x) = 1 if x &lt; 0 and ?(x) = 0 when otherwise, and d c is a cutoff distance. Basically, ? P k is equal to the number of points that are closer than d c to point P k . The algorithm is sensitive only to the relative magnitude of ? P k in different points, which implies that, the results of the analysis are robust with respect to the choice of d c for large datasets. A different way of defining ? P k as:</p><formula xml:id="formula_6">? P k = P l e ?( d P k P l dc ) 2 .<label>(7)</label></formula><p>? P k is measured by finding the minimum distance between the point P k and any other point with higher density:</p><formula xml:id="formula_7">? P k = min P l :?P l &gt;?P k (d P k P l ),<label>(8)</label></formula><p>which uses a Gaussian kernel to calculate. We can see from these two kernels, cutoff kernel is discrete value, while Gaussian kernel is a continuous value, which guarantees a smaller probability of conflict.</p><p>As we can see from <ref type="figure">Figure 1</ref>(d), we calculate ? and ? using Formula <ref type="formula" target="#formula_5">(6)</ref> and <ref type="bibr" target="#b7">(8)</ref>. Then select the number of clustering center N , namely, the N largest ? values, e.g., in <ref type="figure">Figure  1</ref>(e), we select 5 cluster centers. <ref type="figure">Figure 1</ref>(f) illustrates the final results of clustering, in which the points of 2, 5, 8, 9 and 10 are the clustering centers, therefore, the corresponding x-coordinates (the 2, 9, 14, 20 and 26 frames, shown in figure 1(g)) are the key frames S Keyframes in the original video V (shown in <ref type="figure">Figure 1(a)</ref>). The pipeline of the proposed key frames extraction method is summarized in Algorithm 1. Note that the proposed density clustering cannot handle the situation where the entropy of the video sequence is monotone increasing or decreasing since we need to select the local extreme points. While in our experiments, we observer that there is no one video sequence which frame entropy is monotone increasing or decreasing all the time, it means we can always obtain the local extreme points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Fusion Strategy</head><p>In view of each key frame and the relationship between each key frame, for better representing each key frame sequence, we not only try to describe each frame of key frame, but also the variation between the key frames. That is, we not only extract the most representative frames and map them into a simple 2-D space, but also describe how these frames move in this space. We believe this two-phase strategy could set up a "holographic" description of each hand gesture sequence. Hadid and Pietik?inen <ref type="bibr" target="#b39">[40]</ref> also Algorithm 1 The proposed key frames extraction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>The original hand gesture video V , as shown in <ref type="figure">Figure 1</ref>(a) and the number of key frames N (N is a pre-defined constant for all the datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensure:</head><p>The key frames S Keyframes in original video V , as shown in <ref type="figure">Figure 1</ref>(g). 1: Calculate image entropy E(fi) of each frame in V using Formula 2; 2: Map E(fi) to a two-dimensional coordinate space; <ref type="bibr">3:</ref> Find local maximum points Pmax in the two-dimensional coordinate space using Formula (3); 4: Find local minimum points Pmin in the two-dimensional coordinate space using Formula (4); 5: Obtain Pextreme by uniting the local maximum points Pmax and local minimum points Pmin using Formula (5); 6: Calculate ? for each point in Pextreme using Formula (6) or (7); 7: Calculate ? for each point in Pextreme using Formula (8); 8: Draw decision graph like <ref type="figure">Figure 1</ref> demonstrate that excellent results can be obtained combining appearance feature and motion feature for dynamic video analysis. Jain et al. <ref type="bibr" target="#b40">[41]</ref> propose a two-stream fully convolutional neural network which fuses together motion and appearance in a unified framework, and substantially improve the state-of-the-art results for segmenting unseen objects in videos. Xu et al. <ref type="bibr" target="#b41">[42]</ref> consider exploiting the appearance and motion information resided in the video with a attention mechanism for the image question answering task. For this purpose, we propose a feature fusion strategy to capture these two phases: appearance based approach can only be applied to each frame, which represents the differences of space merely; while, motion based method can describe the evolution with the time. Thus we combine appearance and motion feature for better describe image sequence. Meanwhile, to better weight these two feature, we also introduce an efficient strategy to balance them. Tang et al. <ref type="bibr" target="#b15">[16]</ref> also propose a feature fusion method which fuses features extracted from different sleeves to boost the recognition performance. <ref type="figure" target="#fig_3">Figure 2</ref> shows the whole proposed feature fusion procedure for the obtained key frames. After extracting key frames, we take the key frames sequence in place of the original sequence. We begin by extracting key frames from the original sequence, and then extract appearance and motion features (hist1 and hist2 in <ref type="figure" target="#fig_3">Figure 2</ref>) from the key frames sequence, respectively. For further increase the importance of the useful feature, we add weights to appearance and motion features. By feeding hist1 and hist2 to the SVM classifier separately, we obtain two classification accuracy R = {R a , R m }. Based on the assumption that the higher the rate is, the better representation becomes, we compute the weights as follows:</p><formula xml:id="formula_8">T = R ? min(R) (100 ? min(R))/10 .<label>(9)</label></formula><p>Finally, considering that the weight of the lowest rate is 1, the other weights can be obtained according to a linear relationship of their differences to that with the lowest rate. The final step is written as:</p><formula xml:id="formula_9">T 1 = round(T ) T 2 = T ? ((max(T 1) ? 1)) max(T ) + 1 W = round(T 2)<label>(10)</label></formula><p>in which W = {?, ?} is the weight vector corresponding to hist1 and hist2.</p><p>There are many existing descriptors for us to extract hist1 and hist2. In other words, the fusion strategy does not depend on specific descriptors, which guarantees its great potential in applications. In term of hist1, we can use Gist <ref type="bibr" target="#b42">[43]</ref>, rootSIFT <ref type="bibr" target="#b43">[44]</ref>, HSV, SURF <ref type="bibr" target="#b44">[45]</ref>, HOG <ref type="bibr" target="#b45">[46]</ref>, LBP <ref type="bibr" target="#b46">[47]</ref> or its variation CLBP <ref type="bibr" target="#b47">[48]</ref> to extract appearance cue of each image. As for hist2, LBP-TOP, VLBP <ref type="bibr" target="#b48">[49]</ref> and SIFT 3D <ref type="bibr" target="#b49">[50]</ref> are used to extract motion cues from the whole key frames sequence. We also use Bag-of-Feature (BoF) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b50">51]</ref> to represent these appearance and motion cues. At the end of the procedure, we concatenate weighted hist1 and hist2 to obtain the final representation hist (as shown in <ref type="figure" target="#fig_3">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Hand Gesture Recognition Framework</head><p>The hand gesture recognition framework based on key frames and feature fusion is composed of two stages, training and testing, which is summarized in Algorithm 2. In the training stage, we first extract key frames S Keyframes using Algorithm 1 (step 3). Then we extract appearance features from each key frame using descriptors such as SURF, LBP, etc. After obtaining the appearance features, we employ BoF <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b50">51]</ref> to represent these features for hist1 q (step 4). Next we use LBP-TOP, VLBP or SIFT 3D to extract motion features from the whole key frames sequence, producing the corresponding histogram hist2 q (step 5). After that, hist1 q and hist2 q are fed to separate classifiers to obtain R = {R a , R m } (step 6). And then, we calculate ? and ? by Formula (9) and (10) (step 7). Then the training histogram hist q is constructed from ?hist1 q and ?hist2 q (step 8). In the end of the iteration, we obtain the training representation vector hist. Then hist and corresponding labels L label are fed to a SVM classifier (step 10). During the testing stage, testing hand gesture representation is obtained in the same way as the training stage (step 12). Thereby the trained SVM classifier is used to predict the gesture label t label (step 13).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require:</head><p>L hand gesture videos for training, as shown in <ref type="figure">Figure 1(a)</ref>, corresponds to the gesture labels L label ; Testing hand gesture video t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensure:</head><p>The hand gesture label t label . 1: TRAINING STAGE: 2: for q = 1 to L do ? and ? ? using by Formula (9) and (10); <ref type="bibr">8:</ref> hist q ? {?hist1, ?hist2}; 9: end for 10: Classifier ? hist ? L label ; 11: TESTING STAGE: <ref type="bibr">12:</ref> Obtain hand gesture representation hist t for testing t using the same method as the training stage; 13: Obtain t label by the classifier after calculation; <ref type="bibr">14:</ref> return t label .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and Settings</head><p>To evaluate the effectiveness of the proposed method, we conduct experiments on two publicly available datasets (Cambridge <ref type="bibr" target="#b51">[52]</ref> and Northwestern University Hand Gesture datasets <ref type="bibr" target="#b52">[53]</ref>) and two collected datasets (HandGesture and Action3D hand gesture datasets, both will be released after paper accepted). Some characteristics of these datasets are listed in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Cambridge Hand Gesture dataset is a commonly used benchmark gesture data set with 900 video clips of 9 hand gesture classes defined by 3 primitive hand shapes (i.e., flat, spread, V-shape) and 3 primitive motions (i.e., leftward, rightward, contract). For each class, it includes 100 sequences captured with 5 different illuminations, 10 arbitrary motions and 2 subjects. Each sequence is recorded in front of a fixed camera having coarsely isolated gestures in spatial and temporal dimensions.</p><p>Northwestern University Hand Gesture dataset is a more diverse data set which contains 10 categories of dynamic hand gestures in total: move right, move left, rotate up, rotate down, move downright, move right-down, clockwise circle, counterclockwise circle, "Z" and cross. This dataset is performed by 15 subjects and each subject contributes 70 sequences of these ten categories with seven postures (i.e., Fist, Fingers extended, "OK", Index, Side Hand, Side Index and Thumb).</p><p>These two datasets mentioned above are both with clear backgrounds, and sequences snipped tightly around the gestures. However, how well will this method work on videos from "the wild" with significant clutter, extraneous motion, continuous running video without pre-snipping? To validate our approach, we introduce two new datasets, called HandGesture and Action3D datasets.</p><p>HandGesture data set consists of 132 video sequences of 640 by 360 resolution, each of which recorded from a different subject (7 males and 4 females) with 12 different gestures ("0"-"9", "NO" and "OK").</p><p>We also acquired Action3D dataset which consisting of 1620 image sequences of 6 hand gesture classes (box, high wave, horizontal wave, curl, circle and hand up), which are defined by 2 different hands (right and left hand) and 5 situations (sit, stand, with a pillow, with a laptop and with a person). Each class contains 270 image sequences (5 different situations ? 2 different hands ? 3 times ? 9 subjects). Each sequence was recorded in front of a fixed camera having roughly isolated gestures in space and time. All video sequences were uniformly resized into 320 ? 240 in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parameter Analysis</head><p>Two parameters are involved in our framework: the number of key frames N and the dictionary number D in BoF. Firstly, we extract N = 3, 4, ..., 9 key frames from the original video, respectively. And then extract SURF features from each key frame. Every key point detected by SURF provides a 64-D vector describing the texture of it. Finally, we adopt BoF to represent each key frame with  <ref type="figure" target="#fig_6">Figure 3</ref> presents the accuracy results on the four datasets. From <ref type="figure" target="#fig_6">Figure  3</ref> (a) and (c), the accuracy first rises to the peak when D = 64 and then drops after reaching the peak. However, as shown in <ref type="figure" target="#fig_6">Figure 3</ref> (b) and (d), the accuracy reaching the peak when D = 16. Thus, we set D = 64 on the Cambridge and Northwestern datasets, and D = 16 on our two proposed datasets. It is observe that the more key frames we have, the more time will be consumed. Thus, to balance accuracy and efficiency, we set N = 5 on all the four datasets in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiment Evaluation</head><p>To evaluate the necessity and efficiency of the proposed strategy, we test it in multi-aspect: (1) necessity of key frames extraction; (2) different kernel tricks; (3) different fusion strategies; (4) different clustering methods; <ref type="bibr" target="#b4">(5)</ref> performance comparisons with the state-of-the-art; (6) efficiency. (1)-(4) demonstrate the rationality and validity of the methods. (5) compares the proposed method with others. (6) shows its efficiency.</p><p>(1) Comparison with Different Key Frames Extraction Methods. We discuss whether our key frames method is necessary or not here. For the Cambridge and Action3D dataset, we only extract LBP-TOP, and then concatenate the three orthogonal planes of LBP-TOP. For the Northwestern and HandGesture dataset, 200 points are randomly selected from each video using SIFT 3D. As we can see from <ref type="table" target="#tab_1">Table 2</ref>, our approach outperforms the other four methods on both accuracy and efficiency, thereby our approach is not only theoretical improvement, but also has an empirical advantage.</p><p>(2) Gaussian Kernel vs. Cutoff Kernel. We also compare the Gaussian and cutoff kernel. We adopt SURF to extract feature from each key frame. Comparison results are shown in the <ref type="table" target="#tab_2">Table 3</ref>. As we can observe that there is small different between using the Gaussian and cutoff kernel.</p><p>(3) Comparison with Different Feature Fusion Strategies. We demonstrate that combining appearance (hand posture) and motion (the way hand is moving) boosts hand gesture recognition task here. Moreover, we also compare different schemes based on appearance and motion, respectively. For feature fusion, we set ? and ? to 8 and 1, respectively. As shown in <ref type="table" target="#tab_3">Table 4</ref>, fusion strategies are much better than appearance or motion based methods, which demonstrates the necessity of our feature fusion strategy. Motion-based method achieve the worst results, which can illustrate that in our task spatial cues is more important than the temporal cues. The spatial cues represents/extracts the difference between different gesture classes, also called inter-class differences. While the temporal cues captures the difference among different frames in the same gesture sequences, also called intra-class differences. Inter-class differences are always greater than intra-class differences, which means the spatial cues can represent more discriminative feature than the temporal cues. In our task, we observe that the differences between different types of gestures are much greater than the differences between the same gesture sequence, which means the spatial cues is more discriminative than the temporal cues. However, if hand gesture moves fast and change hugely in one sequence, the temporal cues could be more important.</p><p>(4) Comparison with Different Clustering Methods. We also compare different clustering methods for the key frames extraction. As shown in <ref type="table" target="#tab_4">Table 5</ref>, we can see that density clustering is much better than K-means, OPTICS <ref type="bibr" target="#b53">[54]</ref> and DBSCAN <ref type="bibr" target="#b54">[55]</ref>. <ref type="bibr" target="#b4">(5)</ref> Comparison with State-of-the-Arts. For the Cambridge and Northwestern datasets, we compare our results with the state-of-the-art methods in <ref type="table" target="#tab_5">Tables 6 and 7</ref>. We achieve 98.23% ? 0.84% and 96.89% ? 1.08% recognition accuracy on the Cambridge and Northwestern dataset, both of which exceed the other baseline methods. (6) Efficiency. Finally, We investigate our approach in terms of computation time of classifying one test video. We run our experiment on a 3.40-GHz i7-3770 CPU with 8 GB memory. The proposed method is implemented using Matlab. Code and datasets will release after paper accepted. As we can see from <ref type="table" target="#tab_6">Table 8</ref> that the time of classifying one test sequence is 4.31s, 10.89s, 13.06s and 4.26s for the Cambridge, Northwestern, HandGesture and Action3D datasets. We observe that the proposed key frame extraction methods including entropy calculation and density clustering can be finished within around 1s on the Cambridge, Northwestern and Action3D datasets. While for the HandGesture dataset which contains about 200 frames in a single video, it only cost about 3s per video. We also note that the most time-consuming part is feature extraction, and we have two solutions to improve it, (i) we can reduce the size of images, we note that Cambridge and Action3D only consume about 4s, while Northwestern and HandGesture cost about 11s and 13s respectively.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cambridge</head><p>Methods Accuracy Wong and Cipolla <ref type="bibr" target="#b55">[56]</ref> Sparse Bayesian Classifier 44% Niebles et al. <ref type="bibr" target="#b56">[57]</ref> Spatial-Temporal Words 67% Kim et al. <ref type="bibr" target="#b51">[52]</ref> Tensor Canonical Correlation Analysis 82% Kim and Cipolla <ref type="bibr" target="#b57">[58]</ref> Canonical Correlation Analysis 82% Liu and Shao <ref type="bibr" target="#b58">[59]</ref> Genetic Programming 85% Lui et al. <ref type="bibr" target="#b59">[60]</ref> High Order Singular Value Decomposition 88% Lui and Beveridge <ref type="bibr" target="#b60">[61]</ref> Tangent Bundle 91% Wong et al. <ref type="bibr" target="#b61">[62]</ref> Probabilistic Latent Semantic Analysis 91.47% Sanin et al. <ref type="bibr" target="#b62">[63]</ref> Spatio-Temporal Covariance Descriptors 93% Baraldi et al. <ref type="bibr" target="#b63">[64]</ref> Dense Trajectories + Hand Segmentation 94% Zhao and Elgammal <ref type="bibr" target="#b28">[29]</ref> Information Theoretic 96.22% Ours Key Frames + Feature Fusion 98.23% ? 0.84% <ref type="table">Table 7</ref>: Comparison between the state-of-the-art methods and our method on the Northwestern University dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Northwestern</head><p>Methods Accuracy Liu and Shao <ref type="bibr" target="#b58">[59]</ref> Genetic Programming 96.1% Shen et al. <ref type="bibr" target="#b52">[53]</ref> Motion Divergence fields 95.8% Our method Key Frames + Feature Fusion 96.89% ? 1.08%  <ref type="bibr" target="#b58">[59]</ref> 6.45s 13.32s 15.32s 6.43s Zhao and Elgammal <ref type="bibr" target="#b28">[29]</ref> 5.34s 11.78s 14.98s 5.21s tion3D datasets. We re-implement both methods with the same running settings for fair comparison, including hardware platform and programming language. The results are shown in <ref type="table" target="#tab_6">Table 8</ref>, and we can see that the proposed method achieve better results than both methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In order to build a fast and robust gesture recognition system, in this paper, we present a novel key frames extraction method and feature fusion strategy. Considering the speed of recognition, we propose a new key frames extraction method based on image entropy and density clustering, which can greatly reduce the redundant information of original video. Moreover, we further propose an efficient feature fusion strategy which combines appearance and motion cues for robust hand gesture recognition. Experimental results show that the proposed approach outperforms the state-of-the-art methods on the Cambridge (98.23% ? 0.84%) and Northwestern (96.89% ? 1.08%) datasets. For evaluate our method on videos from "the wild" with significant clutter, extraneous motion and no pre-snipping, we introduce two new datasets, namely HandGesture and Action3D. We achieve accuracy of 99.21%? 0.88% and 98.98% ? 0.65% on the HandGesture and Ac-tion3D datasets, respectively. From the respect of the recognition speed, we also achieve better results than the state-of-the-art approaches for classifying one test sequence on the Cambridge, Northwestern, HandGesture and Ac-tion3D datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>$</head><label></label><figDesc>E-mail: {hao.tang, niculae.sebe}@unitn.it; hongliu@pku.edu.cn; xiaoweithu@163.com; * Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Calculate the image entropy.(c) Select the local peak points. (d) Calculate ? and ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(d);9:  Choose the N largest ? values as the clustering centers, as shown inFigure 1(e);10:  The corresponding x-coordinate of N clustering centers are the key frames S Keyframes . 11: return S Keyframes .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>The framework of the proposed feature extraction and fusion methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2</head><label>2</label><figDesc>The proposed hand gesture recognition framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 : 5 : 6 :R</head><label>456</label><figDesc>hist1 q ? (SURF or GIST, etc) ? BoF;hist2 q ? (VLBP or LBP-TOP or SIFT 3D, etc) ? BoF; = {Ra, Rm} ? training a classifier using hist1 q and hist2 q ; 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Parameters N and D selection on the four datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Characteristics of the datasets used in our hand gesture recognition experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="5"># categories # videos # training # validation # testing</cell></row><row><cell>Cambridge</cell><cell>9</cell><cell>900</cell><cell>450</cell><cell>225</cell><cell>225</cell></row><row><cell>Northwestern</cell><cell>10</cell><cell>1,050</cell><cell>550</cell><cell>250</cell><cell>250</cell></row><row><cell>HandGesture</cell><cell>12</cell><cell>132</cell><cell>66</cell><cell>33</cell><cell>33</cell></row><row><cell>Action3D</cell><cell>6</cell><cell>1,620</cell><cell>810</cell><cell>405</cell><cell>405</cell></row><row><cell cols="3">dictionary D = 1, 2, 4, ..., 4096, respectively. The number</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">of training set, validation set and testing set please refer</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">to Table 1. We repeat all the experiments 20 times with</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">different random spits of the training and testing samples</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">to obtain reliable results. The final classification accuracy</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">is reported as the average of each run.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between different key frames extraction methods on the Cambridge, Northwestern, HandGesture and Action3D datasets.</figDesc><table><row><cell>Method</cell><cell>Cambridge Accuracy</cell><cell>Time(s)</cell><cell cols="2">Northwestern Accuracy Time(s)</cell></row><row><cell>Original Sequence</cell><cell>35.26% ? 3.15%</cell><cell>20,803</cell><cell cols="2">21.65% ? 1.23% 108,029</cell></row><row><cell cols="2">5 evenly-spaced frames (in time) 56.13% ? 5.46%</cell><cell>1,189</cell><cell>58.79% ? 2.64%</cell><cell>26,303</cell></row><row><cell>Zhao and Elgammal [29]</cell><cell>58.14% ? 3.36%</cell><cell>1,432</cell><cell>61.45% ? 3.45%</cell><cell>27,789</cell></row><row><cell>Carlsson and Sullivan [30]</cell><cell>50.57% ? 4.78%</cell><cell>1,631</cell><cell>51.27% ? 3.86%</cell><cell>29,568</cell></row><row><cell>Ours key frames method</cell><cell>60.78% ? 2.21%</cell><cell>1,152</cell><cell>64.24% ? 2.15%</cell><cell>25,214</cell></row><row><cell>Method</cell><cell cols="2">HandGesture Accuracy Time(s)</cell><cell>Action3D Accuracy</cell><cell>Time(s)</cell></row><row><cell>Original Sequence</cell><cell>42.54% ? 4.61%</cell><cell>8,549</cell><cell cols="2">34.56% ? 2.65% 284,489</cell></row><row><cell cols="2">5 evenly-spaced frames (in time) 58.32% ? 3.88%</cell><cell>1,689</cell><cell>52.13% ? 2.31%</cell><cell>18,430</cell></row><row><cell>Zhao and Elgammal [29]</cell><cell>60.45% ? 4.56%</cell><cell>1,895</cell><cell>54.56% ? 1.97%</cell><cell>20,143</cell></row><row><cell>Carlsson and Sullivan [30]</cell><cell>50.78% ? 4.06%</cell><cell>2,154</cell><cell>46.34% ? 2.78%</cell><cell>23,768</cell></row><row><cell>Ours key frames method</cell><cell>65.18% ? 3.62%</cell><cell>1,645</cell><cell>56.13% ? 1.89%</cell><cell>16,294</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison between Gaussian kernel and cutoff kernel on the Cambridge, Northwestern, HandGesture and Action3D datasets. 37% ? 1.67% 90.33% ? 2.78% Northwestern 81.31% ? 1.49% 80.25% ? 1.86% HandGesture 96.32% ? 3.35% 94.54% ? 2.78% Action3D 96.26% ? 1.39% 93.65% ? 1.23% The reason is that the image size of Cambridge and Ac-tion3D is 320 ? 240, while the image size of Northwestern and HandGesture is 640 ? 480; (ii) We can further reduce the time for feature extraction by using a GPU like most deep learning methods do. Moreover, we also compare two methods ( [29] and [59] currently achieve best recognition results on the Cambridge and Northwestern datasets, respectively.) on the time for classifying a test sequence on the Cambridge, Northwestern, HandGesture and Ac-</figDesc><table><row><cell>Kernel</cell><cell>Gaussian Kernel</cell><cell>Cutoff Kernel</cell></row><row><cell>Cambridge</cell><cell>92.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with different features fusion strategies on the Cambridge, Northwestern, HandGesture and Action3D datasets. N and D denote the number of key frames and the dictionary number.</figDesc><table><row><cell>Appearance-based</cell><cell>Dimension</cell><cell>Cambridge</cell><cell>Northwestern</cell><cell>HandGesture</cell><cell>Action3D</cell></row><row><cell>SURF</cell><cell>N  *  D</cell><cell>92.37% ? 1.67%</cell><cell>81.31% ? 1.49%</cell><cell>96.32% ? 3.35%</cell><cell>96.26% ? 1.39%</cell></row><row><cell>GIST</cell><cell>N  *  D</cell><cell>88.15% ? 1.65%</cell><cell>78.41% ? 1.91%</cell><cell>92.64% ? 2.89%</cell><cell>91.23% ? 1.84%</cell></row><row><cell>Motion-based</cell><cell>-</cell><cell>Cambridge</cell><cell>Northwestern</cell><cell>HandGesture</cell><cell>Action3D</cell></row><row><cell>LBP-TOP</cell><cell>177</cell><cell>60.78% ? 2.21%</cell><cell>51.36% ? 2.16%</cell><cell>60.84% ? 2.61%</cell><cell>56.13% ? 1.89%</cell></row><row><cell>VLBP</cell><cell>16,386</cell><cell>50.36% ? 3.56%</cell><cell>42.11% ? 3.04%</cell><cell>49.78% ? 3.51%</cell><cell>44.26% ? 4.23%</cell></row><row><cell>SIFT 3D</cell><cell>N  *  D</cell><cell>68.94% ? 4.81%</cell><cell>64.24% ? 2.15%</cell><cell>65.18% ? 3.62%</cell><cell>62.04% ? 2.89%</cell></row><row><cell>Appearance + Motion</cell><cell>-</cell><cell>Cambridge</cell><cell>Northwestern</cell><cell>HandGesture</cell><cell>Action3D</cell></row><row><cell>SURF + LBP-TOP</cell><cell>N  *  D+177</cell><cell>95.75% ? 0.79%</cell><cell>93.54% ? 1.36%</cell><cell>97.25% ? 0.79%</cell><cell>98.53% ? 1.31%</cell></row><row><cell>SURF + VLBP</cell><cell>N  *  D+16,386</cell><cell>92.52% ? 1.27%</cell><cell>91.22% ? 0.95%</cell><cell>96.82% ? 0.95%</cell><cell>97.21% ? 0.94%</cell></row><row><cell>SURF + SIFT 3D</cell><cell>2  *  N  *  D</cell><cell cols="4">98.23% ? 0.84% 96.89% ? 1.08% 99.21% ? 0.88% 98.98% ? 0.65%</cell></row><row><cell>GIST + LBP-TOP</cell><cell>N  *  D+177</cell><cell>91.87% ? 1.65%</cell><cell>86.26% ? 0.94%</cell><cell>93.56% ? 1.35%</cell><cell>93.11% ? 0.89%</cell></row><row><cell>GIST + VLBP</cell><cell>N  *  D+16,386</cell><cell>90.56% ? 0.87%</cell><cell>82.87% ? 1.84%</cell><cell>92.88% ? 1.21%</cell><cell>92.63% ? 0.64%</cell></row><row><cell>GIST + SIFT 3D</cell><cell>2  *  N  *  D</cell><cell>93.52% ? 0.63%</cell><cell>88.54% ? 1.62%</cell><cell>94.16% ? 0.67%</cell><cell>94.21% ? 0.61%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with different clustering methods on the Cambridge, Northwestern, HandGesture and Action3D datasets.</figDesc><table><row><cell>Clustering Method</cell><cell>OPTICS [54]</cell><cell>DBSCAN [55]</cell><cell>K-means</cell><cell>Density Clustering [35]</cell></row><row><cell>Cambridge</cell><cell cols="3">88.15% ? 1.51% 90.34% ? 1.78% 86.26% ? 2.51%</cell><cell>98.23% ? 0.84%</cell></row><row><cell>Northwestern</cell><cell cols="3">86.34% ? 2.45% 88.35% ? 1.67% 83.65% ? 1.06%</cell><cell>96.89% ? 1.08%</cell></row><row><cell>HandGesture</cell><cell cols="3">84.56% ? 1.89% 85.98% ? 1.76% 84.69% ? 1.98%</cell><cell>99.21% ? 0.88%</cell></row><row><cell>Action3D</cell><cell cols="3">83.56% ? 1.56% 87.43% ? 1.63% 82.36% ? 1.46%</cell><cell>98.98% ? 0.65%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison with the state-of-the-art methods on the Cambridge dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Computation time for classifying a test sequence on the Cambridge, Northwestern, HandGesture and Action3D datasets.</figDesc><table><row><cell>Time</cell><cell cols="4">Cambridge Northwestern HandGesture Action3D</cell></row><row><cell>Entropy Calculation</cell><cell>0.93s</cell><cell>0.84s</cell><cell>3.21s</cell><cell>0.75s</cell></row><row><cell>Density Clustering</cell><cell>0.31s</cell><cell>0.34s</cell><cell>0.43s</cell><cell>0.38s</cell></row><row><cell>Feature Extraction</cell><cell>3.07s</cell><cell>9.71s</cell><cell>9.42s</cell><cell>3.13s</cell></row><row><cell>SVM Classification</cell><cell>0.60 ms</cell><cell>0.51ms</cell><cell>0.46ms</cell><cell>0.65 ms</cell></row><row><cell>Our Full Model</cell><cell>4.31s</cell><cell>10.89s</cell><cell>13.06s</cell><cell>4.26s</cell></row><row><cell>Liu and Shao</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by National Natural Science Foundation of China (NSFC, U1613209), Shenzhen Key Laboratory for Intelligent Multimedia and Virtual Reality (ZDSYS201703031405467), Scientific Research Project of Shenzhen City (JCYJ20170306164738129).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Superpixel-based hand gesture recognition with kinect depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust part-based hand gesture recognition using kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1110" to="1120" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Novel fpga implementation of hand sign recognition system with som-hebb classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kaida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="166" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hand gesture recognition with leap motion and kinect devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dominio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Real-time sign language recognition using a consumer depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IC-CVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contour model based hand-gesture recognition using kinect sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1935" to="1944" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prasuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oyamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mochizuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<title level="m">A hogbased hand gesture recognition system on a mobile device</title>
		<imprint>
			<publisher>ICIP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Realtime and continuous hand gesture spotting: an approach based on artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Norberto</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Moreira</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic time warping for music conducting gestures evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schramm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miranda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic user state recognition for hand gesture based low-cost television control system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCE</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Television control by hand gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weissman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>AFGRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in real time for automotive interfaces: A multimodal vision-based approach and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TITS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2368" to="2377" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The power is in your hands: 3d analysis of hand gestures in naturalistic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards automated understanding of student-tutor interactions using visual deictic gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathayanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Satzoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Salamanca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sathyanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<title level="m">Hand gestures for intelligent tutoring systems: Dataset, techniques &amp; evaluation</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gesturegan for hand gesture-to-gesture translation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning discriminative representations from rgb-d video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure-preserving binary representations for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1651" to="1664" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gender classification using pyramid segmentation for unconstrained back-facing video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<title level="m">Learning spatiotemporal features with 3d convolutional networks</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernelized multiview projection for robust action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer IJCV</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequential bag-of-words model for human action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="136" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zeroshot action recognition with error-correcting output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sdm-bsm: A fusing depth scheme for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Information theoretic key frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition by shape matching to key frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Models versus Exemplars in Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using spatial information as an aid to maximum entropy image threshold selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel method of determining parameters of clahe based on image entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Software Engineering and Its Applications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ship detection for complex background sar images based on a multiscale variance weighted image entropy method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="187" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Motion histogram analysis based key frame extraction for human action/activity representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CRV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Video key frame extraction through dynamic delaunay clustering with a structural constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kuanar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JVCIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1212" to="1227" />
			<date type="published" when="2013" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E F</forename><surname>De Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P B</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Da Luz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Albuquerque Ara?jo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="56" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Spatio-temporal featurebased keyframe detection from video shots using spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>V?zquez-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bandera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scalable video summarization using skeleton graph and random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kuanar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Combining appearance and motion for face and gender recognition from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<idno>PR 42</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="2818" to="2827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="145" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multiresolution grayscale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>M?enp??</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A completed modeling of local binary pattern operator for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1657" to="1663" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Dardas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3592" to="3607" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Tensor canonical correlation analysis for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Dynamic hand gesture recognition: An exemplar-based approach from motion divergence fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Elsevier</publisher>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="227" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Optics: ordering points to identify the clustering structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Real-time interpretation of hand motions using a sparse bayesian classifier on motion gradient orientation images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer IJCV</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis of video volume tensors for action categorization and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Synthesis of spatio-temporal descriptors for dynamic hand gesture recognition using genetic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>FGW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<title level="m">Action classification on product manifolds</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Tangent bundle for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>FG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning motion categories using both semantic and structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<title level="m">Spatiotemporal covariance descriptors for action and gesture recognition</title>
		<imprint>
			<publisher>WACV</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Gesture recognition in ego-centric videos using dense trajectories and hand segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Paci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
