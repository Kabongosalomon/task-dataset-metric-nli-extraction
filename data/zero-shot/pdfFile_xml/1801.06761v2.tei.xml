<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PU-Net: Point Cloud Upsampling Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
							<email>lqyu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
							<email>xzli@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
							<email>cwfu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
							<email>pheng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PU-Net: Point Cloud Upsampling Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning and analyzing 3D point clouds with deep networks is challenging due to the sparseness and irregularity of the data. In this paper, we present a data-driven point cloud upsampling technique. The key idea is to learn multilevel features per point and expand the point set via a multibranch convolution unit implicitly in feature space. The expanded feature is then split to a multitude of features, which are then reconstructed to an upsampled point set. Our network is applied at a patch-level, with a joint loss function that encourages the upsampled points to remain on the underlying surface with a uniform distribution. We conduct various experiments using synthesis and scan data to evaluate our method and demonstrate its superiority over some baseline methods and an optimization-based method. Results show that our upsampled points have better uniformity and are located closer to the underlying surfaces.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud is a fundamental 3D representation that has drawn increasing attention due to the popularity of various depth scanning devices. Recently, pioneering works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18]</ref> began to explore the possibility of reasoning point clouds by means of deep networks for understanding geometry and recognizing 3D structures. In these works, the deep networks directly extract features from the raw 3D point coordinates without using traditional features, e.g., normal and curvature. These works present impressive results for 3D object classification and semantic scene segmentation.</p><p>In this work we are interested in an upsampling problem: given a set of points, generate a denser set of points to describe the underlying geometry by learning the geometry of a training dataset. This upsampling problem is similar in spirit to the image super-resolution problem <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>; however, dealing with 3D points rather than a 2D grid of pixels * Equal contribution. poses new challenges. First, unlike the image space, which is represented by a regular grid, point clouds do not have any spatial order and regular structure. Second, the generated points should describe the underlying geometry of a latent target object, meaning that they should roughly lie on the target object surface. Third, the generated points should be informative and should not clutter together. Having said that, the generated output point set should be more uniform on the target object surface. Thus, simple interpolation between input points cannot produce satisfactory results.</p><p>To meet the above challenges, we present a data-driven point cloud upsampling network. Our network is applied at a patch-level, with a joint loss function that encourages the upsampled points to remain on the underlying surface with a uniform distribution. The key idea is to learn multilevel features per point, and then expand the point set via a multi-branch convolution unit implicitly in feature space. The expanded feature is then split to a multitude of features, which are then reconstructed to an upsampled point set.</p><p>Our network, namely PU-Net, learns geometry semantics of point-based patches from 3D models, and applies the learned knowledge to upsample a given point cloud. It should be noted that unlike previous network-based methods designed for 3D point sets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18]</ref>, the number of input and output points in our network are not the same.</p><p>We formulate two metrics, distribution uniformity and distance deviation from underlying surfaces, to quantitatively evaluate the upsampled point set, and test our method using variety of synthetic and real-scanned data. We also evaluate the performance of our method, and compare it to baseline and state-of-the-art optimization-based methods. Results show that our upsampled points have better uniformity, and are located closer to the underlying surfaces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch Extraction</head><p>. . . <ref type="figure">Figure 1</ref>. The architecture of PU-Net (better view in color). The input has N points, while the output has rN points, where r is the upsampling rate. Ci,C andCi represent the feature channel number. We restore different level features for the original N points with interpolation, and reduce all level features to a fixed dimension C with a convolution. The red color in the point feature embedding component shows the original and progressively subsampled points in hierarchical feature learning, while the green color indicates the restored features. We jointly adopt the reconstruction loss and repulsion loss in the end-to-end training of PU-Net.</p><p>reconstruction based on an L 1 median. The operator works well even when the input point set contains noise and outliers. Successively, Huang et al. <ref type="bibr" target="#b13">[14]</ref> propose an improved weighted LOP to address the point set density problem. Although these works have demonstrated good results, they make a strong assumption that the underlying surface is smooth, thus restricting the method's scope. Then, Huang et al. <ref type="bibr" target="#b14">[15]</ref> introduce an edge-aware point set resampling method by first resampling away from edges and then progressively approaching edges and corners. However, the quality of their results heavily relies on the accuracy of the normals at given points and careful parameter tuning. It is worth mentioning that Wu et al. <ref type="bibr" target="#b34">[35]</ref> propose a deep points representation method to fuse consolidation and completion in one coherent step. Since its main focus is on filling large holes, global smoothness is, however, not enforced, so the method is sensitive to large noise. Overall, the above methods are not data-driven, thus heavily relying on priors.</p><p>Related work: deep-learning-based methods. Points in a point cloud do not have any specific order nor follow any regular grid structure, so only a few recent works adopt a deep learning model to directly process point clouds. Most existing works convert a point cloud into some other 3D representations such as the volumetric grids <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref> and geometric graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> for processing. Qi et al. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> firstly introduced a deep learning network for point cloud classification and segmentation; in particular, the Point-Net++ uses a hierarchical feature learning architecture to capture both local and global geometry context. Subsequently, many other networks were proposed for high-level analysis problems with point clouds <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b27">28]</ref>. However, they all focus on global or mid-level attributes of point clouds. In another work, Guerrero et al. <ref type="bibr" target="#b9">[10]</ref> developed a network to estimate the local shape properties in point clouds, including normal and curvature. Other relevant networks focus on 3D reconstruction from 2D images <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref>. To the best of our knowledge, there are no prior works focusing on point cloud upsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Network Architecture</head><p>Given a 3D point cloud with point coordinates in nonuniform distributions, our network aims to output a denser point cloud that follows the underlying surface of the target object while being uniform in distribution. Our network architecture (see <ref type="figure">Fig. 1</ref>) has four components: patch extraction, point feature embedding, feature expansion, and coordinate reconstruction. First, we extract patches of points in varying scales and distributions from a given set of prior 3D models (Sec. 2.1). Then, the point feature embedding component maps the raw 3D coordinates to a feature space by hierarchical feature learning and multi-level feature aggregation (Sec. 2.2). After that, we expand the number of features using the feature expansion component (Sec. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Patch Extraction</head><p>We collect a set of 3D objects as prior information for training. These objects cover a rich variety of shapes, from smooth surface to shapes with sharp edges and corners. Essentially, for our network to upsample a point cloud, it should learn local geometry patterns from the objects. This motivates us to take a patch-based approach to train the network and to learn the geometry semantics.</p><p>In detail, we randomly select M points on the surface of these objects. From each selected point, we grow a surface patch on the object, such that any point on the patch is within a certain geodesic distance (d) from the selected point over the surface. Then, we use Poisson disk sampling to randomly generateN points on each patch as the referenced ground truth point distribution on the patch. In our upsampling task, both local and global context contribute to a smooth and uniform output. Hence, we set d with varying sizes, so that we can extract patches of points on the prior objects with varying scale and density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Point Feature Embedding</head><p>To learn both local and global geometry context from the patches, we consider the following two feature learning strategies, whose benefits complement each other:</p><p>Hierarchical feature learning. Progressively capturing features of growing scales in a hierarchy has been proved to be an effective strategy for extracting local and global features. Hence, we adopt the recently proposed hierarchical feature learning mechanism in PointNet++ <ref type="bibr" target="#b29">[30]</ref> as the very frontal part in our network. To adopt hierarchical feature learning for point cloud upsampling, we specifically use a relatively small grouping radius in each level, since generating new points usually involves more of the local context than the high-level recognition tasks in <ref type="bibr" target="#b29">[30]</ref>.</p><p>Multi-level feature aggregation. Lower layers in a network generally correspond to local features in smaller scales, and vice versa. For better upsampling results, we should optimally aggregate features in different levels. Some previous works adopt skip-connections for cascaded multi-level feature aggregation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref>. However, we found by experiments that such top-down propagation is not very efficient for aggregating features in our upsampling problem. Therefore, we propose to directly combine features from different levels and let the network learn the importance of each level <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Since the input point set on each patch (see point feature embedding in <ref type="figure">Fig. 1</ref>) is subsampled gradually in hierarchical feature extraction, we concatenate the point features from each level by first restoring the features of all original points from the downsampled point features by the interpolation method in PointNet++ <ref type="bibr" target="#b29">[30]</ref>. Specifically, the features of an interpolated point x in level is calculated by:</p><formula xml:id="formula_0">f ( ) (x) = 3 i=1 w i (x)f ( ) (x i ) 3 i=1 w i (x) ,<label>(1)</label></formula><p>where w i (x)=1/d(x, x i ), which is an inverse distance weight, and x i , x 2 , x 3 are the three nearest neighbors of x in level . We then use a 1?1 convolution to reduce the interpolated feature in different level to the same dimension, i.e., C. Finally, we concatenate the features from each level as the embedded point feature f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature Expansion</head><p>After the point feature embedding component, we expand the number of features in the feature space. This is equivalent to expanding the number of points, since points and features are interchangeable. Suppose the dimension of f is N ?C, N is the number of input points andC is the feature dimension of the concatenated embedded feature. The feature expansion operation would output a feature f with dimension rN ?C 2 , where r is the upsampling rate andC 2 is the new feature dimension. Essentially, this is similar to feature upsampling in image-related tasks, which can be done by deconvolution <ref type="bibr" target="#b24">[25]</ref> (also known as transposed convolution) or interpolation <ref type="bibr" target="#b6">[7]</ref>. However, it is nontrivial to apply these operations to point clouds due to the non-regularity and unordered properties of points.</p><p>We therefore propose an efficient feature expansion operation based on the sub-pixel convolution layer <ref type="bibr" target="#b32">[33]</ref>. This operation can be represented as:</p><formula xml:id="formula_1">f = RS( [ C 2 1 (C 1 1 (f )) , ... , C 2 r (C 1 r (f )) ] ) ,<label>(2)</label></formula><p>where C 1 i (?) and C 2 i (?) are two sets of separate 1?1 convolutions, and RS(?) is a reshape operation to convert an N ? rC 2 tensor to a tensor of size rN ?C 2 . We emphasize that the feature in the embedding space has already encapsulated the relative spatial information from the neighborhood points via the efficient multi-level feature aggregation, so we do not need to explicitly consider the spatial information when performing this feature expansion operation.</p><p>It is worth mentioning that the r feature sets generated from the first convolution C 1 i (?) in each set have a high correlation, and this would cause the final reconstructed 3D points to be located too close to one another. Hence, we further add another convolution (with separate weights) for each feature set. Since we train the network to learn the r different convolutions for the r feature sets, these new features can include more diverse information, thus reducing their correlations. This feature expansion operation can be implemented by applying separated convolutions to the r feature sets; see <ref type="figure">Fig. 1</ref>. It can also be implemented by more computation efficient grouped convolution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Coordinate Reconstruction</head><p>In this part, we reconstruct the 3D coordinates of output points from the expanded feature with the size of rN ?C 2 . Specifically, we regress the 3D coordinates via a series of fully connected layers on the feature of each point, and the final output is the upsampled point coordinates rN ? 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">End-to-End Network Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Data Generation</head><p>Point cloud upsampling is an ill-posed problem due to the uncertainty or ambiguity of upsampled point clouds. Given a sparse input point cloud, there are many feasible output point distributions. Therefore, we do not have the notion of "correct pairs" of input and ground truth. To alleviate this problem, we propose an on-the-fly input generation scheme. Specifically, the referenced ground truth point distribution of a training patch is fixed, whereas the input points are randomly sampled from the ground truth point set with a downsampling rate of r at each training epoch. Intuitively, this scheme is equivalent to simulating many feasible output point distributions for a given sparse input point distribution. Additionally, this scheme can further enlarge the training dataset, allowing us to depend on a relatively small dataset for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Joint Loss Function</head><p>We propose a novel joint loss function to train the network in an end-to-end fashion. As we mentioned earlier, the function should encourage the generated points to be located on the underlying object surfaces in a more uniform distribution. Therefore, we design a joint loss function that combines the reconstruction loss and repulsion loss.</p><p>Reconstruction loss. To put points on the underlying object surfaces, we propose to use the Earth Mover's distance (EMD) <ref type="bibr" target="#b7">[8]</ref> as our reconstruction loss to evaluate the similarity between the predicted point cloud S p ? R 3 and the referenced ground truth point cloud S gt ? R 3 :</p><formula xml:id="formula_2">L rec = d EM D (S p , S gt ) = min ?:Sp?Sgt xi?Sp x i ? ?(x i ) 2 ,<label>(3)</label></formula><p>where ? : S p ? S gt indicates the bijection mapping.</p><p>Actually, Chamfer Distance (CD) is another candidate for evaluating the similarity between two point sets. However, compared with CD, EMD can better capture the shape (see <ref type="bibr" target="#b7">[8]</ref> for more details) to encourage the output points to be located close to the underlying object surfaces. Hence, we choose to use EMD in our reconstruction loss.</p><p>Repulsion loss. Although training with the reconstruction loss can generate points on the underlying object surfaces, the generated points tend to be located near the original points. To distribute the generated points more uniformly, we design the repulsion loss, which is represented as:</p><formula xml:id="formula_3">L rep =N i=0 i ?K(i) ?( x i ? x i )w( x i ? x i ) ,<label>(4)</label></formula><p>whereN = rN is the number of output points, K(i) is the index set of the k-nearest neighbors of point x i , and ? is the L2-norm. ?(r) = ?r is called the repulsion term, which is a decreasing function to penalize x i if x i is located too close to other points in K(i). To penalize x i only when it is too close to its neighboring points, we add two restrictions: (i) we only consider points x i in the k-nearest neighborhood of x i ; and (ii) we use the fast-decaying weight function w(r) in the repulsion loss; that is, we follow <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b13">14]</ref> to set w(r) = e ?r 2 /h 2 in our experiments. Altogether, we train the network in an end-to-end manner by minimizing the following joint loss function:</p><formula xml:id="formula_4">L(?) = L rec + ?L rep + ? ? 2 ,<label>(5)</label></formula><p>where ? indicates the parameters in our network, ? balances the reconstruction loss and repulsion loss, and ? denotes the multiplier of weight decay. For simplicity, we ignore the index of each training sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Since there are no public benchmarks for point cloud upsampling, we collect a dataset of 60 different models from the Visionair repository <ref type="bibr" target="#b0">[1]</ref>, ranging from smooth non-rigid objects (e.g., Bunny) to steep rigid objects (e.g., Chair). Among them, we randomly select 40 for training, and use the rest for testing 1 . We crop 100 patches for each training object, and we use M =4000 patches to train the network in total. For testing objects, we use Monte-Carlo random sampling approach to sample 5000 points on each object as input. To further demonstrate the generalization ability of our network, we directly test our well-trained network on the SHREC15 <ref type="bibr" target="#b21">[22]</ref> dataset, which contains 1200 shapes from 50 categories. In detail, we randomly choose one model from each category for testing, considering that each category contains 24 similar objects in various poses. As for ModelNet40 <ref type="bibr" target="#b35">[36]</ref> and ShapeNet <ref type="bibr" target="#b3">[4]</ref>, we found it hard to extract patches from those objects due to the low mesh quality (e.g., holes, self-intersection, etc.). Therefore, we use them for testing; see the supplementary material for the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The default point numberN of each patch is 4096, and the upsampling rate r is 4. Therefore, each input patch has 1024 points. To avoid overfitting, we augment the data by randomly rotating, shifting and scaling the data. We use 4 levels with grouping radii 0.05, 0.1, 0.2 and 0.3 in the point feature embedding component, and the dimension C of the restored feature is 64. For details on other network architecture parameters, please see our supplementary material. Parameters k and h in repulsion loss are set as 5 and 0.03, respectively. The balancing weights ? and ? are set as 0.01 and ? = 10 ?5 , respectively. The implementation is based on TensorFlow 2 . For the optimization, we train the network for 120 epoch using the Adam <ref type="bibr" target="#b16">[17]</ref> algorithm with a minibatch size of 28 and a learning rate of 0.001. Generally, the training took about 4.5h on the NVIDIA TITAN Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metric</head><p>To quantitatively evaluate the quality of the output point sets, we formulate two metrics to measure the deviation between the output points and the ground truth meshes, as well as the distribution uniformity of the output points. For surface deviation, we find the closest pointx i on the mesh for each predicted point x i , and calculate the distance between them. Then we compute the mean and standard deviation over all the points as one of our metrics.</p><p>As for the uniformity metric, we randomly put D equalsize disks on the object surface (D = 9000 in our experiments) and calculate the standard deviation of the number of points inside the disks. We further normalize the density of each object and then compute the overall uniformity of the point sets over all the objects in the testing dataset. Therefore, we define the normalized uniformity coefficient (NUC) with disk area percentage p as:</p><formula xml:id="formula_5">avg = 1 K * D K k=1 D i=1 n k i N k * p , N U C = 1 K * D K k=1 D i=1 ( n k i N k * p ? avg) 2 ,<label>(6)</label></formula><p>where n k i is the number of points within the i-th disk of the k-th object, N k is the total number of points on the kth object, K is the total number of test objects, and p is the percentage of the disk area over the total object surface area. Note that we use geodesic distance rather than Euclidean distance to form the disks. can see, the proposed NUC metric can effectively reveal the point set uniformity: the lower the UNC value, the more uniform the point set distribution is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with Other Methods</head><p>Comparison with an optimization-based method. We compare our method with the Edge Aware Resampling (EAR) method <ref type="bibr" target="#b14">[15]</ref>, which is a state-of-art method for point cloud upsampling. The results are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, where the Chair is from our collected testing dataset and the Spider is from SHREC15. We color-code the point clouds to show the deviation from the ground truth meshes. There are 1024 points in the input and we do a 4X upsampling. Since EAR relies on the normal information, to be fair, we calculate the normal according to the ground truth mesh. We show two results of EAR with increasing radius, while setting other parameters to their default values. As we can see, the radius parameter has a great influence on EAR's performance. For relatively small radius, the output has low surface deviation but the added points are not uniform, while more outliers are introduced if the radius is large. In contrast, our method can better balance the deviation and uniformity without the need to carefully tune the parameters.</p><p>Comparison with deep learning-based methods. As far as we know, we are not aware of any deep learning-based method for point cloud upsampling, so we design some baseline methods for comparison. Since PointNet <ref type="bibr" target="#b28">[29]</ref> and PointNet++ <ref type="bibr" target="#b29">[30]</ref> are pioneers for 3D point cloud reasoning with deep learning techniques, we design the baselines based on them. Specifically, we adopt the semantic segmentation network architecture for point feature embedding and use one set of convolutions for feature expansion. Note that we consider two versions of PointNet++: basic PointNet++ and PointNet++ with multi-scale grouping (MSG) for handling non-uniform sampling density; hence, we have three  <ref type="table" target="#tab_0">Tables 1 and 2</ref> list the quantitative comparison results on our collected dataset and the SHREC15 dataset, respectively. Note that measuring NUC with small p shows local distribution uniformity in small regions, while measuring NUC with large p shows more global uniformity. Among the baselines, PointNet performs the worst, since it cannot capture local structure information. Compared with Point-Net++, PointNet++(MSG) can slightly improve the uniformity due to the explicit multi-scale information grouping. However, it involves more parameters, thus significantly prolonging the training and testing time. Overall, our PU-Net achieves the best performance with the lowest deviation from surface and the best distribution uniformity compared to the baselines, especially on the local uniformity. <ref type="figure" target="#fig_5">Fig. 4</ref> shows results for visual comparison, where points are colored by their distance deviations from surface. As we can see, the point clouds predicted by our method better match the underlying surface with lower deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Architecture Design Analysis</head><p>Analyzing the feature expansion. We compare our proposed feature expansion scheme with two interpolation-like schemes. The first one is similar to a naive point interpolation (denoted as interp1). After extracting the point feature of each point, we combine its own features and the features from the nearest neighboring points to generate the upsampled features. The second one introduces more randomness (denoted as interp2). Instead of using the features from the nearest neighbors, we use a radius based ball query to find the neighborhood and combine the features from these points to generate the upsampled features. We train these two networks with the reconstruction loss (also named as the EMD loss) and the results are listed in the top two rows of <ref type="table" target="#tab_1">Table 3</ref>. For fair comparison, we also train our network only with the EMD loss. The results are shown in the fourth row. Comparing with these two interpolation-like schemes, our proposed scheme can generate more uniform outputs with comparable surface distance deviation.</p><p>Comparing different loss functions. As mentioned above, the EMD can better capture the object shape than CD. Comparing their performance in <ref type="table" target="#tab_1">Table 3</ref>, we can see that the EMD loss improves the output uniformity with low surface distance deviation when comparing with the CD loss, meaning that the EMD loss can better encourage the output points to lie on the underlying surface. Furthermore, by comparing the last two rows in <ref type="table" target="#tab_1">Table 3</ref>, we can see that the repulsion loss can further improve the uniformity of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">More Experiments</head><p>Surface reconstruction from upsampled point sets. An important application of point cloud upsampling is to improve the surface reconstruction quality. Hence, we com-     <ref type="figure">Figure 6</ref>. Results of iterative upsampling. We color-code points by the depth information. Blue points are closer to us. pare the reconstruction results of different methods with the direct Poisson surface reconstruction method <ref type="bibr" target="#b15">[16]</ref> provided in MeshLab <ref type="bibr" target="#b4">[5]</ref>; see <ref type="figure" target="#fig_6">Fig. 5</ref>. We can observe that the reconstruction result from our method is the closest to the ground truth, while other methods either miss certain structures (e.g., the leg of the Horse) or overfill the hole.</p><p>Results of iterative upsampling. To study the ability of our network to handle varying number of input points, we design an iterative upsampling experiment, which takes the output of the previous iteration as the input of the next iteration. <ref type="figure">Fig. 6</ref> shows the results. The initial input point cloud has 1024 points and we increase fourfold in each iteration. From the results, we can see that our network can produce reasonable results for different number of input points. Furthermore, this iterative upsampling experiment also shows the anti-noise ability of our network to resist the accumulated errors introduced in the iterative upsampling.</p><p>Results from noisy input point sets. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the surface reconstruction results from noisy point clouds (Gaussian noise of 0.5% and 1% of object bounding box diagonal), which demonstrate that our network facilitates the production of better surfaces even with noisy inputs.</p><p>Results on real-scanned point clouds. Lastly, we evaluated the ability of our network to upsample real-scanned point clouds, which were downloaded from Visionair <ref type="bibr" target="#b0">[1]</ref>. In <ref type="figure" target="#fig_9">Fig. 8</ref>, the left-most column presents the real-scanned point clouds. Even though each real-scanned point cloud contains millions of points, the phenomenon of inhomogeneity still exists. For better visualization, we cut small patches from the original point clouds and show the patches in the middle column. We can observe that the real-scanned points tend to have line structures, while our network still has the ability to uniformly add points in the sparse regions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a deep network for point cloud upsampling, with the goal of generating a denser and uniform set of points from a sparser set of points. Our network is trained at a patch-level using a multi-level feature aggregation manner, thus capturing both local and global information. The design of our network bypasses the need for a prescribed order among the points, by operating on individual features that contain non-local geometry to allow a context-aware upsampling. Our experiments demonstrate the effectiveness of our method. As the first attempt using deep networks, our method still has a number of limitations. Firstly, it is not designed for completion, so our network can not fill large holes and missing parts. Besides, our network may not be able to add meaningful points for tiny structures that are severely undersampled.</p><p>In the future, we would like to investigate and develop more means to handle irregular and sparse data, both for regression purposes and for synthesis. One immediate step is to develop a downsampling method. Although, downsampling seems like a simpler problem, there is room to devise proper losses and architecture that maximize the preservation of information in the decimated point set. We believe that in general, the development of deep learning methods for irregular structures is a viable research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>In this supplementary material, we first provide more details about our collected dataset in Section B. Then, we show the details of our network architecture as well as the baseline networks employed in the experiments in Section C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of our Collected Dataset</head><p>We collect 60 different 3D models to form our training and testing datasets. The specific name of each model is shown in <ref type="table" target="#tab_2">Table 4</ref>. We also present the shapes of some training and testing 3D models in our dataset in <ref type="figure">Fig. 9</ref> and <ref type="figure">Fig. 10</ref>, respectively. As we can see, our collected datasets have a large variation in geometry shapes, containing 3D models with smooth surface regions (first row) and 3D models with sharp corners and edges (second row). There is also a large variation between training and testing 3D models, indicating a good generalization ability of our proposed method.  <ref type="figure">Figure 9</ref>. Examples 3D models in our training dataset. The first row shows 3D models with smooth surface regions, while the second row shows 3D models with sharp corners and edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camel</head><p>Elephant Elk Horse Kitten Moai Casting Chair Fandisk Icosahedron Quadric Sculpt <ref type="figure">Figure 10</ref>. Examples 3D models in our testing dataset. The first row shows 3D models with smooth surface regions, while the second row shows 3D models with sharp corners and edges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Related work: optimization-based methods. An early work by Alexa et al. [2] upsamples a point set by interpolating points at vertices of a Voronoi diagram in the local tangent space. Lipman et al. [24] present a locally optimal projection (LOP) operator for points resampling and surface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2.3) and reconstruct the 3D coordinates of the output point cloud via a series of fully connected layers in the coordinate reconstruction component (Sec. 2.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Example point distributions with corresponding normalized uniformity coefficients (NUC) computed with p = 0.2%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Fig. 2shows three different point distributions with their corresponding NUC values. As we 2 https://github.com/yulequan/PU-Net input Comparison with the EAR method<ref type="bibr" target="#b14">[15]</ref>. We color-code all point clouds to show the deviation from the ground truth mesh. For the EAR method, the radius of the Chair model is 0.1 and 0.182, while the radius of the Spider model is 0.106 and 0.155.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison on samples from our collected dataset (top row) and SHREC (bottom row). The colors on points (see color map) reveal the surface distance errors, where blue indicates low error and red indicates high error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Surface reconstruction results from the upsampled point clouds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Surface reconstruction results from noisy input points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Results on real-scanned point clouds (Screw nut &amp; Turtle). We color-code input patches and upsampling results to show the depth information. Blue points are closer to viewpoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison on our collected dataset.</figDesc><table><row><cell>Method</cell><cell>NUC with different p 0.2% 0.4% 0.6% 0.8% 1.0% 1.2%</cell><cell cols="3">Deviation (10 ?2 ) Time (s) mean std</cell></row><row><cell>Input</cell><cell>0.316 0.224 0.185 0.164 0.150 0.142</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointNet [29]</cell><cell>0.409 0.334 0.295 0.270 0.252 0.239</cell><cell>2.27</cell><cell>3.18</cell><cell>0.05</cell></row><row><cell>PointNet++ [30]</cell><cell>0.215 0.178 0.160 0.150 0.143 0.139</cell><cell>1.01</cell><cell>0.83</cell><cell>0.16</cell></row><row><cell cols="2">PointNet++(MSG) [30] 0.208 0.169 0.152 0.143 0.137 0.134</cell><cell>0.78</cell><cell>0.61</cell><cell>0.45</cell></row><row><cell>PU-Net (Ours)</cell><cell>0.174 0.138 0.122 0.115 0.112 0.110</cell><cell>0.63</cell><cell>0.53</cell><cell>0.15</cell></row><row><cell></cell><cell>Table 2. Quantitative comparison on SHREC15 dataset.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>NUC with different p 0.2% 0.4% 0.6% 0.8% 1.0% 1.2%</cell><cell cols="3">Deviation (10 ?2 ) Time (s) mean std</cell></row><row><cell>Input</cell><cell>0.315 0.222 0.184 0.165 0.153 0.146</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PointNet [29]</cell><cell>0.490 0.405 0.360 0.330 0.309 0.293</cell><cell>2.03</cell><cell>2.94</cell><cell>0.05</cell></row><row><cell>PointNet++ [30]</cell><cell>0.259 0.218 0.197 0.185 0.176 0.170</cell><cell>0.88</cell><cell>0.75</cell><cell>0.16</cell></row><row><cell cols="2">PointNet++(MSG) [30] 0.250 0.199 0.175 0.161 0.153 0.148</cell><cell>0.69</cell><cell>0.59</cell><cell>0.45</cell></row><row><cell>PU-Net (Ours)</cell><cell>0.219 0.173 0.154 0.144 0.140 0.137</cell><cell>0.52</cell><cell>0.45</cell><cell>0.15</cell></row><row><cell cols="2">baselines in total, and we train them only with the recon-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">struction loss. Please refer to the supplemental material for</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">details of the baseline network architectures. Although we</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">modify the PointNet, PointNet++, and PointNet++(MSG)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">architectures for the upsampling problem, for convenience,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">we still call the baselines by their original names.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Architecture design analysis on our collect dataset.</figDesc><table><row><cell>Methods</cell><cell>NUC with different p 0.4% 0.6% 0.8% 1.0%</cell><cell cols="2">Deviation (10 ?2 ) mean std</cell></row><row><cell cols="2">interp1+EMD 0.153 0.136 0.126 0.121</cell><cell>0.67</cell><cell>0.54</cell></row><row><cell cols="2">interp2+EMD 0.144 0.129 0.122 0.118</cell><cell>0.71</cell><cell>0.57</cell></row><row><cell>CD</cell><cell>0.185 0.167 0.154 0.147</cell><cell>0.87</cell><cell>0.69</cell></row><row><cell>EMD</cell><cell>0.140 0.126 0.119 0.116</cell><cell>0.68</cell><cell>0.58</cell></row><row><cell>Ours</cell><cell>0.138 0.122 0.115 0.112</cell><cell>0.63</cell><cell>0.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>The complete name list of the 3D models in our training and testing datasets.</figDesc><table><row><cell></cell><cell>Model Names</cell></row><row><cell></cell><cell>Armadillo, Boy1, Boy2, Bumpy torus, Bunny, Cad, Cylinder, Child1,</cell></row><row><cell></cell><cell>Child2, Chinese lion, Cone, Cup, Dino, Egea, Ellipsoid, Eros, Fish, Fo-</cell></row><row><cell>Training</cell><cell>cal octa, Gargoyle, Girl1, Girl2, Hand, Joint, Julius, Nicolo, Octa flower,</cell></row><row><cell></cell><cell>Pierrot, Pulley, Pyramid1, Pyramid2, Retinal, Rolling stage, Screwdriver,</cell></row><row><cell></cell><cell>Sharp sphere, Special cube, Star1, Turbine, Twirl, Vaselion</cell></row><row><cell></cell><cell>Camel, Casting, Chair, Cover rear, Cow, Duck, Eight, Elephant, Elk, Fan-</cell></row><row><cell>Testing</cell><cell>disk, Genus, Horse, Icosahedron, Kitten, Moai, Octahedron, Pig, Quadric,</cell></row><row><cell></cell><cell>Sculpt, Star2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The complete object list can be found in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank anonymous reviewers for the comments and suggestions. The work is supported in part by the National Basic Program of China, the 973 Program (Project No. 2015CB351706), the Research Grants Council of the Hong Kong Special Administrative Region (Project no. CUHK 14225616), the Shenzhen Science and Technology Program (No. JCYJ20170413162617606), and the CUHK strategic recruitment fund.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of Network Architectures</head><p>The details of our network architecture are listed as follows.</p><p>? In the hierarchical feature learning component, we use four levels to extract local features. Following the notations in PointNet++, we use (K, r, [l 1 , ..., l d ]) to represent a level with K local regions of ball radius r, and [l 1 , ..., l d ] the d fully connected layers with width l i (i = 1, ..., d). Therefore, the parameters we use are (N, 0.05, <ref type="bibr">[32, 32,</ref>  ? In the multi-level feature aggregation component, we use interpolation to restore the feature of each level and use a convolution to reduce the restored feature to 64 dimensions. Therefore,C = 259 in our experiments.</p><p>? In the feature expansion component, the output feature channel numbersC 1 andC 2 are 256 and 128, respectively.</p><p>? In the coordinate reconstruction component, we use two fully connected layers with 64 and 3 output channels, respectively.</p><p>The details of the baseline architectures are illustrated in <ref type="figure">Fig. 11, Fig. 12</ref> and <ref type="figure">Fig. 13</ref>. All the convolution layers and fully connected layers in the above networks are followed by the ReLU operator, except for the last coordinate regression layer.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visionair</surname></persName>
		</author>
		<title level="m">Online; accessed on 14</title>
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing and rendering point set surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. &amp; Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3D model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MeshLab: an open-source mesh processing tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cignoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Callieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dellepiane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ganovelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ranzuglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Italian Chapter Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape completion using 3D-Encoder-Predictor CNNs and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. &amp; Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A point set generation network for 3D object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AtlasNet: A papier-m?ch? approach to learning 3D surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>to appear. 2</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PCPNet: Learning local shape properties from raw point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Eurographics). to appear. 2</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>to appear. 2</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Consolidation of unorganized point clouds for surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Edge-aware point set resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symposium on Geometry Processing (SGP)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Escape from cells: deep Kd-Networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">PointCNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-rigid 3D shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elnaghy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>El-Sana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giachetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Limberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">U</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Nonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pevzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Unal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2015 Eurographics Workshop on 3D Object Retrieval</title>
		<meeting>of the 2015 Eurographics Workshop on 3D Object Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning efficient point cloud generation for dense 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf. on Artificial Intell. (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parameterization-free projection for geometry reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tal-Ezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV) workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Intell. Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum PointNets for 3D object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>to appear. 2</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SGPN: Similarity group proposal network for 3D point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>to appear. 2</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep points consolidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<title level="m">Aggregated residual transformations for deep neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">ShuffleNet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
