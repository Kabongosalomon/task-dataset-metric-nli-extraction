<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho</forename><forename type="middle">Kei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<email>aschwing@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Frame 0 (input) Frame 295 Frame 460 Frame 1285 Frame 2327</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present XMem, a video object segmentation architecture for long videos with unified feature memory stores inspired by the Atkinson-Shiffrin memory model. Prior work on video object segmentation typically only uses one type of feature memory. For videos longer than a minute, a single feature memory model tightly links memory consumption and accuracy. In contrast, following the Atkinson-Shiffrin model, we develop an architecture that incorporates multiple independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. Crucially, we develop a memory potentiation algorithm that routinely consolidates actively used working memory elements into the long-term memory, which avoids memory explosion and minimizes performance decay for long-term prediction. Combined with a new memory reading mechanism, XMem greatly exceeds state-of-the-art performance on long-video datasets while being on par with state-of-theart methods (that do not work on long videos) on short-video datasets. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video object segmentation (VOS) highlights specified target objects in a given video. Here, we focus on the semi-supervised setting where a first-frame annotation is provided by the user, and the method segments objects in all other frames as accurately as possible while preferably running in real-time, online, and while having a small memory footprint even when processing long videos.</p><p>As information has to be propagated from the given annotation to other video frames, most VOS methods employ a feature memory to store relevant deep-net representations of an object. Online learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b41">42]</ref> use the weights of a network as their feature memory. This requires training at test-time, which slows down prediction. Recurrent methods propagate information often from the most recent frames, either via a mask <ref type="bibr" target="#b38">[39]</ref> or via a hidden representation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">47]</ref>. These methods are prone to drifting and struggle with occlusions. Recent state-of-the-art VOS methods use attention <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b59">60]</ref> to link representations of past frames stored in the feature memory with features extracted from the newly observed query frame which needs to be segmented. Despite the high performance of these methods, they require a large amount of GPU memory to store past frame representations. In practice, they usually struggle to handle videos longer than a minute on consumer-grade hardware. Methods that are specifically designed for VOS in long videos exist <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27]</ref>. However, they often sacrifice segmentation quality. Specifically, these methods reduce the size of the representation during feature memory insertion by merging new features with those already stored in the feature memory. As high-resolution features are compressed right away, they produce less accurate segmentations. <ref type="figure" target="#fig_0">Figure 1</ref> shows the relation between GPU memory consumption and segmentation quality in short/long video datasets (details are given in Section 4.1).</p><p>We think this undesirable connection of performance and GPU memory consumption is a direct consequence of using a single feature memory type. To address this limitation we propose a unified memory architecture, dubbed XMem. Inspired by the Atkinson-Shiffrin memory model <ref type="bibr" target="#b0">[1]</ref> which hypothesizes that the human memory consists of three components, XMem maintains three independent yet deeply-connected feature memory stores: a rapidly updated sensory memory, a high-resolution working memory, and a compact thus sustained long-term memory. In XMem, the sensory memory corresponds to the hidden representation of a GRU <ref type="bibr" target="#b10">[11]</ref> which is updated every frame. It provides temporal smoothness but fails for long-term prediction due to representation drift. To complement, the working memory is agglomerated from a subset of historical frames and considers them equally <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9]</ref> without drifting over time. To control the size of the working memory, XMem routinely consolidates its representations into the long-term memory, inspired by the consolidation mechanism in the human memory <ref type="bibr" target="#b45">[46]</ref>. XMem stores long-term memory as a set of highly compact prototypes. For this, we develop a memory potentiation algorithm that aggregates richer information into these prototypes to prevent aliasing due to sub-sampling. To read from the working and long-term memory, we devise a space-time memory reading operation. The three feature memory stores combined permit handling long videos with high accuracy while keeping GPU memory usage low.</p><p>We find XMem to greatly exceed prior state-of-the-art results on the Longtime Video dataset <ref type="bibr" target="#b28">[29]</ref>. Importantly, XMem is also on par with current stateof-the-art (that cannot handle long videos) on short-video datasets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b56">57]</ref>. In summary:</p><p>-We devise XMem. Inspired by the Atkinson-Shiffrin memory model <ref type="bibr" target="#b0">[1]</ref>, we introduce memory stores with different temporal scales and equip them with a memory reading operation for high-quality video object segmentation on both long and short videos. -We develop a memory consolidation algorithm that selects representative prototypes from the working memory, and a memory potentiation algorithm that enriches these prototypes into a compact yet powerful representation for long-term memory storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>General VOS Methods. Most VOS methods employ a feature memory to store information given in the first frame and to segment any new frames. Online learning approaches either train or fine-tune their networks at test-time and are therefore typically slow in inference <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b31">32]</ref>. Recent improvements are more efficient <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2]</ref>, but they still require online adaptation which is sensitive to the input and has diminishing gains when more training data is available. In contrast, tracking-based approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b55">56]</ref> perform frame-to-frame propagation and are thus efficient at test-time. They however lack long-term context and often lose track after object occlusions. While some methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref> also include the first reference frame for global matching, the context is still limited and it becomes harder to match as the video progresses. To address the context limitation, recent state-of-the-art methods use more past frames as feature memory <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b15">16]</ref>. Particularly, Space-Time Memory (STM) <ref type="bibr" target="#b35">[36]</ref> is popular and has been extended by many follow-up works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b32">33]</ref>. Among these extensions, we use STCN <ref type="bibr" target="#b8">[9]</ref> as our working memory backbone as it is simple and effective. However, most variants cannot handle long videos due to the ever-expanding feature memory bank of STM. AOT <ref type="bibr" target="#b59">[60]</ref> is a recent work that extends the attention mechanism to transformers but does not solve the GPU memory explosion problem. Some methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref> employ a local feature memory window that fails to consider long-term context outside of this window. In contrast, XMem uses multiple memory stores to capture different temporal contexts while keeping the GPU memory usage strictly bounded due to our long-term memory and consolidation. Methods that Specialize in Handling Long Videos. Liang et al. <ref type="bibr" target="#b28">[29]</ref> propose AFB-URR which selectively uses exponential moving averages to merge a given memory element with existing ones if they are close, or to add it as a new element otherwise. A least-frequently-used-based mechanism is employed to remove unused features when the feature memory reaches a predefined limit. Li et al. <ref type="bibr" target="#b26">[27]</ref> propose the global context module. It averages all past memory </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2.</head><p>Overview of XMem. The memory reading operation extracts relevant features from all three memory stores and uses those features to produce a mask. To incorporate new memory, the sensory memory is updated every frame while the working memory is only updated every r-th frame. The working memory is consolidated into the longterm memory in a compact form when it is full, and the long-term memory will forget obsolete features over time.</p><p>into a single representation, thus having zero GPU memory increase over time. However, both of these methods eagerly compress new high-resolution feature memory into a compact representation, thus sacrificing segmentation accuracy. Our multi-store feature memory avoids eager compression and achieves much higher accuracy in both short-term and long-term predictions. <ref type="figure" target="#fig_3">Figure 2</ref> provides an overview of XMem. For readability, we consider a single target object. However, note that XMem is implemented to deal with multiple objects, which is straightforward. Given the image and target object mask at the first frame (top-left of <ref type="figure" target="#fig_3">Figure 2</ref>), XMem tracks the object and generates corresponding masks for subsequent query frames. For this, we first initialize the different feature memory stores using the inputs. For each subsequent query frame, we perform memory reading (Section 3.2) from long-term memory (Section 3.3), working memory (Section 3.4), and sensory memory (Section 3.5) respectively. The readout features are used to generate a segmentation mask. Then, we update each of the feature memory stores at different frequencies. We update the sensory memory every frame and insert features into the working memory at every r-th frame. When the working memory reaches a pre-defined maximum of T max frames, we consolidate features from the working memory into the long-term memory in a highly compact form. When the long-term memory is also full (which only happens after processing thousands of frames), we discard obsolete features to bound the maximum GPU memory usage. These feature memory stores work in conjunction to provide high-quality features with low GPU memory usage even for very long videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">XMem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><formula xml:id="formula_0">3 ? 0 ? 0 k ? ? ? v ? ? 0 ? 0 k ? v ? h ? ? Deep update ? ? ? Concatenation</formula><p>Value encoder <ref type="figure">Fig. 3</ref>. Process of memory reading and mask decoding of a single query frame. We extract query q from the image and perform attention-based memory reading from the working/long-term memory to obtain features F . Together with the sensory memory, it is fed into the decoder to generate a mask. For every r-th frame, we store new features into the working memory and perform a deep update to the sensory memory.</p><p>XMem consists of three end-to-end trainable convolutional networks as shown in <ref type="figure">Figure 3</ref>: a query encoder that extracts query-specific image features, a decoder that takes the output of the memory reading step to generate an object mask, and a value encoder that combines the image with the object mask to extract new memory features. See Section 3.6 for details of these networks. In the following, we will first describe the memory reading operation before discussing each feature memory store in detail. <ref type="figure">Figure 3</ref> illustrates the process of memory reading and mask generation for a single frame. The mask is computed via the decoder which uses as input the short-term sensory memory h t?1 ? R C h ?H?W and a feature F ? R C v ?H?W representing information stored in both the long-term and the working memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Reading</head><p>The feature F representing information stored in both the long-term and the working memory is computed via the readout operation F = vW(k, q).</p><p>(1)</p><p>Here, k ? R C k ?N and v ? R C v ?N are C k -and C v -dimensional keys and values for a total of N memory elements which are stored in both the long-term and working memory. Moreover, W(k, q) is an affinity matrix of size N ? HW , representing a readout operation that is controlled by the key k and a query q ? R C k ?HW obtained from the query frame through the query encoder. The readout operation maps every query element to a distribution over all N memory elements and correspondingly aggregates their values v. The affinity matrix W(k, q) is obtained by applying a softmax on the memory dimension (rows) of a similarity matrix S(k, q) which contains the pairwise similarity between every key element and every query element. For computing the similarity matrix we note that the L2 similarity proposed in STCN <ref type="bibr" target="#b8">[9]</ref> is more stable than the dot product <ref type="bibr" target="#b35">[36]</ref>, but it is less expressive, e.g., it cannot encode the confidence level of a memory element. To overcome this, we propose a new similarity function (anisotropic L2 ) by introducing two new scaling terms that break the symmetry between key and query. <ref type="figure" target="#fig_1">Figure 4</ref> visualizes their effects. Concretely, the key is associated with a shrinkage term s ? [1, ?) N and the query is associated with a selection term e ? [0, 1] C k ?HW . Then, the similarity between the i-th key element and the j-th query element is computed via</p><formula xml:id="formula_1">S(k, q) ij = ?s i C k c e cj (k ci ? q cj ) 2 ,<label>(2)</label></formula><p>which equates to the original L2 similarity <ref type="bibr" target="#b8">[9]</ref> if s i = e cj = 1 for all i, j, and c. The shrinkage term s directly scales the similarity and explicitly encodes confidence -a high shrinkage represents low confidence and leads to a more local influence. Note that even low-confidence keys can have a high contribution if the query happens to coincide with it -thus avoiding the memory domination problem of the dot product, as discussed in <ref type="bibr" target="#b8">[9]</ref>. Differently, the selection term e controls the relative importance of each channel in the key space such that attention is given to the more discriminative channels. The selection term e is generated together with the query q by the query encoder. The shrinkage term s is collected together with the key k and the value v from the working and the long-term memory. <ref type="bibr" target="#b1">2</ref> The collection is simply implemented as a concatenation in the last dimension: k = k w ? k lt and v = v w ? v lt , where superscripts 'w' and 'lt' denote working and long-term memory respectively. The working memory consists of key k w ? R C k ?T HW and value v w ? R C v ?T HW , where T is the number of working memory frames. The longterm memory similarly consists of keys k lt ? R C k ?L and values v lt ? R C v ?L ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature extraction Prototype selection Potentiation</head><p>Add to long-term memory <ref type="figure">Fig. 5</ref>. Memory consolidation procedure. Given an image, we extract features as memory keys (image stride exaggerated). We visualize these features with colors. For memory consolidation, we first select prototype keys (stars) from the candidates (all grids). Then, we invoke potentiation which non-locally aggregates values from all the candidates to generate more representative prototype values (golden outline). The resultant prototype keys and values are added to the long-term memory. Only one frame is shown here -in practice multiple frames are used in a single consolidation.</p><p>where L is the number of long-term memory prototypes. Thus, the total number of elements in the working/long-term memory is N = T HW + L.</p><p>Next, we discuss the feature memory stores in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long-Term Memory</head><p>Motivation. A long-term memory is crucial for handling long videos. With the goal of storing a set of compact (consume little GPU memory) yet representative (lead to high segmentation quality) memory features, we design a memory consolidation procedure that selects prototypes from the working memory and enriches them with a memory potentiation algorithm, as illustrated in <ref type="figure">Figure 5</ref>. We perform memory consolidation when the working memory reaches a predefined size T max . The first frame (with user-provided ground-truth) and the most recent T min ? 1 memory frames will be kept in the working memory as a high-resolution buffer while the remainder (T max ? T min frames) are candidates for being converted into long-term memory representations. We refer to the keys and values of these candidates as k c ? k w and v c ? v w respectively. In the following, we describe the prototype selection process that picks a compact set of prototype keys k p ? k c , and the memory potentiation algorithm that generates enriched prototype values v p associated with these prototype keys. Finally, these prototype keys and values are appended to the long-term memory k lt and v lt .</p><p>Prototype Selection. In this step, we sample a small representative subset k p ? k c from the candidates as prototypes. It is essential to pick only a small number of prototypes, as their amount is directly proportional to the size of the resultant long-term memory. Inspired by human memory which moves frequently accessed or studied patterns to a long-term store, we pick candidates with high usage. Concretely, we pick the top-P frequently used candidates as prototypes. "Usage" of a memory element is defined by its cumulative total affinity (probability mass) in the affinity matrix W (Eq. (1)), and normalized by the duration that each candidate is in the working memory. Note that the duration for each candidate is at least r ? (T min ? 1), leading to stable usage statistics. We obtain the keys of these prototypes as k p ? R C k ?P .</p><p>Memory Potentiation. Note that, so far, our sampling of prototype keys k p from the candidate keys k c is both sparse and discrete. If we were to sample the prototypes values v p in the same manner, the resultant prototypes would inevitably under-represent other candidates and would be prone to aliasing. The common technique to prevent aliasing is to apply an anti-aliasing (e.g., Gaussian) filter <ref type="bibr" target="#b14">[15]</ref>. Similarly motivated, we perform filtering and aggregate more information into every sampled prototype. While standard filtering can be easily performed on the image plane (2D) or the spatial-temporal volume (3D), it leads to blurry features -especially near object boundaries. To alleviate, we instead construct the neighbourhood for the filtering in the high dimensional (C k ) key space, such that the highly expressive adjacency information given by the keys k p and k c is utilized. As these keys have to be computed and stored for memory reading anyway, it is also economical in terms of run-time and memory consumption.</p><p>Concretely, for each prototype, we aggregate values from all the value candidates v c via a weighted average. The weights are computed using a softmax over the key-similarity. For this, we conveniently re-use Eq. (2). By substituting the memory key k with the candidate key k c , and the query q with the prototype keys k p , we obtain the similarity matrix S(k c , k p ). As before, we use a softmax to obtain the affinity matrix W(k c , k p ) (where every prototype corresponds to a distribution over candidates). Then, we compute the prototype values v p via v p = v c W(k c , k p ).</p><p>(</p><p>Finally, k p and v p are appended to the long-term memory k lt and v lt respectively -concluding the memory consolidation process. Note, similar prototypical approximations have been used in transformers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b37">38]</ref>. Differently, our approach uses a novel prototype selection scheme suitable for video object segmentation.</p><p>Removing Obsolete Features. Although the long-term memory is extremely compact with a high (&gt; 6000%) compression ratio, memory can still overflow since we are continuously appending new features. Empirically, with a 6GB memory budget (e.g., a consumer-grade mid-end GPU), we can process up to 34,000 frames before running into any memory issues. To handle even longer videos, we introduce a least-frequently-used (LFU) eviction algorithm similar to <ref type="bibr" target="#b28">[29]</ref>. Unlike <ref type="bibr" target="#b28">[29]</ref>, our "usage" (as defined in Section 3.3, Prototype Selection) is defined by the cumulative affinity after top-k filtering <ref type="bibr" target="#b7">[8]</ref> which circumvents the introduction of an extra threshold hyperparameter. Long-term memory elements with the least usage will be evicted when a pre-defined memory limit is reached.</p><p>The long-term memory is key to enabling efficient and accurate segmentation of long videos. Next, we discuss the working memory, which is crucial for accurate short-term prediction. It acts as the basis for the long-term memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Working Memory</head><p>The working memory stores high-resolution features in a temporary buffer. It facilitates accurate matching in the temporal context of a few seconds. It also acts as a gateway into the long-term memory, as the importance of each memory element is estimated by their usage frequency in the working memory.</p><p>In our multi-store feature memory design, we find that a classical instantiation of the working memory is sufficient for good results. We largely employ a baseline STCN-style <ref type="bibr" target="#b8">[9]</ref> feature memory bank as our working memory, which we will briefly describe for completeness. We refer readers to <ref type="bibr" target="#b8">[9]</ref> for details. However, note that our memory reading step (Section 3.2) differs significantly. The working memory consists of keys</p><formula xml:id="formula_3">k w ? R C k ?T HW and values v w ? R C v ?T HW ,</formula><p>where T is the number of working memory frames. The key is encoded from the image and resides in the same embedding space as the query q while the value is encoded from both the image and the mask. Bottom-right of <ref type="figure">Figure 3</ref> illustrates the working memory update process. At every r-th frame, we 1) copy the query as a new key; and 2) generate a new value by feeding the image and the predicted mask into the value encoder. The new key and value are appended to the working memory and are later used in memory reading for subsequent frames. To avoid memory explosion, we limit the number of frames in the working memory T : T min ? T &lt; T max by consolidating extra frames into the long-term memory store as discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Sensory Memory</head><p>The sensory memory focuses on the short-term and retains low-level information such as object location which nicely complements the lack of temporal locality in the working/long-term memory. Similar to the working memory, we find a classical baseline to work well. Concretely, the sensory memory stores a hidden representation h t ? R C h ?H?W , initialized as the zero vector, and propagated by a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b10">[11]</ref> as illustrated in <ref type="figure">Figure 6</ref>. This sensory memory is updated every frame using multi-scale features of the decoder. At every r-th frame, whenever a new working memory frame is generated, we perform a deep update. Features from the value encoder are used to refresh the sensory memory with another GRU. This allows the sensory memory to 1) discard redundant information that has already been saved to the working memory, and 2) receive updates from a deep network (i.e., the value encoder) with minimal overhead as we are reusing existing features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skipconnections</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head><p>Here, we describe some key implementation details. To fully reproduce both training and inference, please see our open-source implementation (footnote 1).</p><p>Networks. Following common practice <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>, we adopt ResNets <ref type="bibr" target="#b16">[17]</ref> as the feature extractor, removing the classification head and the last convolutional stage. This results in features with stride 16. The query encoder is based on a ResNet-50 and the value encoder is based on a ResNet-18, following <ref type="bibr" target="#b8">[9]</ref>. To generate the query q, the shrinkage term s, and the selection term e, we apply separate 3 ? 3 convolutional projections to the query encoder feature output. Note that both the query and the shrinkage term are used for the current query frame, while the selection term is copied to the memory (along the copy path in <ref type="figure">Figure 3</ref>) for later use if and only if we are inserting new working memory. We set C k = 64, C v = 512 following <ref type="bibr" target="#b8">[9]</ref>, and C h = 64. To control the range of the shrinkage factor to be in [1, ?), we apply (?) 2 + 1, and to control the range of the selection factor to be in [0, 1], we apply a sigmoid.</p><p>The decoder concatenates hidden representation h t?1 and readout feature F. It then iteratively upsamples by 2? at a time until stride 4 while fusing skip-connections from the query encoder at every level, following STM <ref type="bibr" target="#b35">[36]</ref>. The stride 4 feature map is projected to a single channel logit via a 3 ? 3 convolution, and is bilinearly upsampled to the input resolution. In the multi-object scenario, we use soft-aggregation <ref type="bibr" target="#b35">[36]</ref> to fuse the final logits from different objects. Note that the bulk of the computation (i.e., query encoder, affinity W) can be shared between different objects as they are only conditioned on the image <ref type="bibr" target="#b8">[9]</ref>.</p><p>Training. Following <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>, we first pretrain our network on synthetic sequences of length three generated by deforming static images. We adopt the open-source implementation of STCN <ref type="bibr" target="#b8">[9]</ref> without modification, which trains on <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b6">7]</ref>. Next, we perform the main training on YouTubeVOS <ref type="bibr" target="#b56">[57]</ref> and DAVIS <ref type="bibr" target="#b40">[41]</ref> with curriculum sampling <ref type="bibr" target="#b35">[36]</ref>. We note that the default sequence length of three is insufficient to train the sensory memory as it would be heavily dependent on the initial state. Thus, we instead sample sequences of length eight. To reduce training time and for regularization, a maximum of three (instead of all) past frames are randomly selected to be the working memory for any query in training time. The entire training process takes around 35 hours on two RTX A6000 GPUs. Deep updates are performed with a probability of 0.2, which is 1/r as we use r = 5 by default following <ref type="bibr" target="#b8">[9]</ref>. Optionally, we also pretrain on BL30K <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4]</ref> which gives a further boost in accuracy. We label any method that uses BL30K with an asterisk ( * ).</p><p>We use bootstrapped cross entropy loss and dice loss with equal weighting following <ref type="bibr" target="#b59">[60]</ref>. For optimization, we use AdamW <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> with a learning rate of 1e-5 and a weight decay of 0.05, for 150K iterations with batch size 16 in static image pretraining, and for 110K iterations with batch size 8 in main training. We drop the learning rate by a factor of 10 after the first 80K iterations. For a fair comparison, we also retrain the STCN <ref type="bibr" target="#b8">[9]</ref> baseline with the above setting. There is no significant difference in performance for STCN (see appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Unless otherwise specified, we use T min = 5, T max = 10, and P = 128, resulting in a compression ratio of 6328% from working memory to long-term memory. We set the maximum number of long-term memory elements to be 10,000 which means XMem never consumes more than 1.4GB of GPU memory, possibly enabling applications even on mobile devices. We use top-k filtering <ref type="bibr" target="#b7">[8]</ref> with k = 30. 480p videos are used by default. To evaluate we use standard metrics (higher is better) <ref type="bibr" target="#b39">[40]</ref>: Jaccard index J , contour accuracy F, and their average J &amp;F. For YouTubeVOS <ref type="bibr" target="#b56">[57]</ref>, J and F are computed for "seen" and "unseen" classes separately, denoted by subscripts S and U respectively. G is averaged J &amp;F for both seen and unseen classes. For AOT <ref type="bibr" target="#b59">[60]</ref>, we compare with their R50 variant which has the same ResNet backbone as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Long-Time Video Dataset</head><p>To evaluate long-term performance, we test models on the Long-time Video dataset <ref type="bibr" target="#b28">[29]</ref> which contains three videos with more than 7,000 frames in total. We also synthetically extend it to even longer variants by playing the video back and forth. n? denotes a variant that has n times the number of frames. For comparison, we select state-of-the-art methods with available implementation as we need to re-run their models. Most SOTA methods cannot handle long videos natively. We first measure their GPU memory increase per frame by averaging the memory consumption difference between the 100-th and 200-th frame in 480p. <ref type="bibr" target="#b2">3</ref>  <ref type="figure" target="#fig_0">Figure 1</ref> (left) shows our findings, assuming 24FPS. For methods with prohibitive memory usage on long videos, we limit their feature memory insertion frequency accordingly, using 50 memory frames in STM as a baseline following <ref type="bibr" target="#b28">[29]</ref>. Our method uses less memory than this baseline. We note that a low memory insertion frequency leads to high variances in performance, thus we run these experiments with 5 evenly-spaced offsets to the memory insertion routine and show "mean ? standard deviation" if applicable. In this dataset, we use r = 10. We do not find BL30K <ref type="bibr" target="#b7">[8]</ref> pretraining to help here. <ref type="table" target="#tab_3">Table 1</ref> tabulates the quantitative results, and <ref type="figure" target="#fig_0">Figure 1</ref> (right) plots the short-term performance against the long-term performance. Methods that use a temporally local feature window (CFBI(+) <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b60">61]</ref>, JOINT <ref type="bibr" target="#b32">[33]</ref>) have a constant memory cost but fail when they lose track of the context. Methods with a fastgrowing memory bank (e.g., STM <ref type="bibr" target="#b35">[36]</ref>, AOT <ref type="bibr" target="#b59">[60]</ref>, STCN <ref type="bibr" target="#b8">[9]</ref>) are forced to use a low feature memory insertion frequency and do not scale well to long videos. <ref type="figure">Figure 7</ref> shows the scaling behavior of STCN vs. XMem in more detail.</p><p>AFB-URR <ref type="bibr" target="#b28">[29]</ref> is designed to handle long videos and scales well with no degradation -but due to eager feature compression it has relatively low performance in the short term compared to other methods. In contrast, XMem not only holds up well in scaling to longer videos but also performs well in the shortterm as shown in the next section. We provide qualitative comparisons in the appendix.  <ref type="table" target="#tab_5">Table 2</ref> and <ref type="table" target="#tab_6">Table 3</ref>   <ref type="bibr" target="#b56">[57]</ref> 2019 validation can be found in the appendix. The test set for YouTubeVOS is closed at the time of writing. We use r = 5 for these datasets. Following standard practice <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b8">9]</ref>, we report single/multi-object FPS on DAVIS 2016/2017 validation. We additionally report FPS on YouTubeVOS 2018 validation which has longer videos on average. We measure FPS on a V100 GPU. For a fair comparison, we re-time prior works that report FPS on a slower GPU if possible and label this with a ?. We note that some methods (not ours) are faster on a 2080Ti than on a V100. In these cases, we always give competing methods the benefit. Our speed-up solely comes from the use of long-term memory -a compact feature memory representation is faster to read from.  <ref type="figure">Fig. 7</ref>. Least-square fits of performance over video length for XMem and STCN <ref type="bibr" target="#b8">[9]</ref> on variants of the Long-time Video dataset <ref type="bibr" target="#b28">[29]</ref> from 1? to 10?. In longer videos, STCN decays due to missing context while ours stabilizes as we gain sufficient context.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Short Video Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>We perform ablation studies on validation sets of YouTubeVOS 2018 <ref type="bibr" target="#b56">[57]</ref> (Y 18 ), DAVIS 2017 <ref type="bibr" target="#b40">[41]</ref> (D 17 ), and Long-time Video (n?) <ref type="bibr" target="#b28">[29]</ref> (L n? ). We report the most representative metric (G for YouTubeVOS, J &amp;F for DAVIS/Long-time Video). FPS is measured on DAVIS 2017 validation unless otherwise specified. We highlight our final configuration with cyan . Memory Stores. <ref type="table" target="#tab_7">Table 4</ref> tabulates the performance of XMem without any one of the memory stores. If the working memory is removed, long-term memory cannot function and it becomes "sensory memory only" with a constant memory cost. If the long-term memory is removed, all the memory frames are stored in the working memory. Although it has a slightly better performance due to its higher resolution feature, it cannot handle long videos and is slower.</p><p>Memory Reading. <ref type="table" target="#tab_8">Table 5</ref> shows the importance of the two scaling terms in the anisotropic L2 similarity. Interestingly, the selection term e alone does not help. We hypothesize that the selection term allows attention on a different subset of memory elements for every query, thus increasing the relative importance of each memory element. The shrinkage term s allows element-level modulation of confidence, thus avoiding too much emphasis on less confident elements. There is a synergy between the two terms, and our final model benefits from both. Long-term Memory Strategies. <ref type="table" target="#tab_9">Table 6</ref> compares different prototype selection strategies and shows the importance of potentiation. We run all algorithms 5 times with evenly-spaced memory insertion offsets and show standard deviations. We choose the usage-based selection scheme with P = 128 for a balance between performance and memory compression. <ref type="table" target="#tab_10">Table 7</ref> compares additional strategies used by prior works, employed on our model. Eager compression is inspired by AFB-URR <ref type="bibr" target="#b28">[29]</ref>. We set T min = 1 and T max = 2. Note, since we cannot   compute usage statistics in this setting, we use random prototype selection with the same compression ratio. Sparse insertion follows our treatment to methods with a growing memory bank <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b8">9]</ref>. We set the maximum number of memory frames to be 50 following <ref type="bibr" target="#b28">[29]</ref>. Local window follows <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>, where we simply discard the oldest memory frame when the memory bank reaches its capacity. We always keep the first reference frame and set the memory bank capacity to be 50. Our memory consolidation algorithm is the most effective among these.</p><p>Deep Update. <ref type="table" target="#tab_11">Table 8</ref> shows different configurations of the deep update. Employing deep update every r-th frame results in a performance boost, with no noticeable speed drop (recall that we have to use the value encoder every rth frame for our working memory anyway). However, using deep updates more often requires extra invocations of the value encoder and leads to a slowdown.</p><p>Pretraining. There are prior works that do not use static image pretraining <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. We provide our results without pretraining in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Limitations</head><p>Our method sometimes fails when the target object moves too quickly or has severe motion blur as even the fastest updating sensory memory cannot catch up. See the appendix for examples. We think a sensory memory with a large receptive field that is more powerful than our baseline instantiation could help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present XMem -to our best knowledge the first multi-store feature memory model used for video object segmentation. XMem achieves excellent performance with minimal GPU memory usage for both long and short videos. We believe XMem is a good step toward accessible VOS on mobile devices, and we hope to draw attention to the more widely-applicable long-term VOS task. Acknowledgment. Work supported in part by NSF under Grants 1718221, 2008387, 2045586, 2106825, MRI 1725729, and NIFA award 2020-67021-32799. <ref type="figure" target="#fig_0">Fig. S1</ref>. Visualization of memory consolidation. The first row shows the candidate frames to be converted into long-term memory. Each of the following rows show a prototype position (indicated by a yellow cross), and the corresponding aggregation weights (visualized as a red overlay). Frames that contain a prototype are framed in red. The consolidation process aggregates information from semantically meaningful regions (top-to-bottom): the swan's beak, part of the vegetation, part of the riverbank, the transition between vegetation and river bank, and part of the water surface. <ref type="figure" target="#fig_3">Fig. S2</ref>. Visualization of memory consolidation. The first row shows the candidate frames to be converted into long-term memory. Each of the following rows show a prototype position (indicated by a yellow cross), and the corresponding aggregation weights (visualized as a red overlay). Frames that contain a prototype are framed in red. The consolidation process aggregates information from semantically meaningful regions (top-to-bottom): torso, legs, arms, trees, and part of the wall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Qualitative Results</head><p>Here, we compare qualitatively to JOINT <ref type="bibr" target="#b32">[33]</ref>, AFB-URR <ref type="bibr" target="#b28">[29]</ref>, and STCN <ref type="bibr" target="#b8">[9]</ref> using several long videos and using the same setting as in the paper. We show results on the dressage sequence (10,767 frames) which is part of the Long-time Video (3?) dataset <ref type="bibr" target="#b28">[29]</ref>, and two additional in-the-wild videos. breakdance contains a single foreground object with large and fast motion, and has 18,187 frames. cans is very challenging, contains five different objects, two of which (Dr. Pepper and Coca-Cola cans) are similar. The two cans are completely occluded for more than 2,000 frames, and our method can successfully capture them when they reappear. <ref type="figure">Figure S3</ref>, S4, and S5 compare results on these videos respectively. We show one potential application where an image layer is inserted between the foreground and the background using the predicted object mask on a snippet of the breakdance sequence.  <ref type="figure">Fig. S3</ref>. Results on the dressage sequence. JOINT <ref type="bibr" target="#b32">[33]</ref> uses a temporally local feature window and loses track over time. AFB-URR <ref type="bibr" target="#b28">[29]</ref> is stable but produces overall less accurate segmentations. STCN <ref type="bibr" target="#b8">[9]</ref> uses a low memory insertion frequency to avoid memory explosion, and thus misses fast changes (2nd and 4th column). Ours is sometimes better than the provided ground-truth (last column, the horse's front legs).</p><p>Images JOINT AFB-URR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STCN Ours</head><p>Frame 0 (input) Frame 2342 Frame 4637 Frame 6644 Frame 18052 <ref type="figure" target="#fig_1">Fig. S4</ref>. Results on the breakdance sequence. We manually annotated the first frame as input. Similarly to the dressage sequence ( <ref type="figure">Figure S3</ref>), JOINT <ref type="bibr" target="#b32">[33]</ref> loses track over time, AFB-URR <ref type="bibr" target="#b28">[29]</ref> is overall less accurate, STCN <ref type="bibr" target="#b8">[9]</ref> struggles with fast motion, and our method performs well on this sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images JOINT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFB-URR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STCN Ours</head><p>Frame 0 (input) Frame 680</p><p>Frame 1062 Frame 1793 Frame 3980 <ref type="figure">Fig. S5</ref>. Results on the cans sequence. We manually annotated the first frame as input. The Dr. Pepper can is labeled with red, and the Coca-Cola can is labeled with green. The two cans are completely occluded with a towel after frame 1,793, and reappear about 2,000 frames later. The color tone change is due to the camera's auto white balance. JOINT <ref type="bibr" target="#b32">[33]</ref> misses the Coca-Cola can after occlusion. It still captures the Dr. Pepper can as the available first reference frame helps. AFB-URR <ref type="bibr" target="#b28">[29]</ref> mixes up the two cans early on, and fails to capture them after they reappear. This is due to its eager feature compression and thus lower modeling capability. STCN <ref type="bibr" target="#b8">[9]</ref> uses a low memory insertion frequency to avoid memory explosion which causes it to be less accurate when changes happen. Our method is the most accurate overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Failure Cases</head><p>As mentioned in the limitation section of the main text, our method struggles with very fast moving objects. This is because even the fastest-updating sensory memory fails to track such objects, and the working memory fails to model objects with large motion blur. <ref type="figure">Figure S6</ref> visualizes some failure cases. <ref type="figure">Fig. S6</ref>. Failure cases. We point to objects of interest with an arrow. First row: multiple birds with similar appearances are flying. We fail to discriminate between some birds that are close to each other. Second row: a frisbee is being thrown. We cannot catch up as it is moving quickly with a large motion blur. Third row: two flags are being waved quickly. We fail to segment the whole left flag due to fast motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Long-Term Memory Size and FPS Scaling</head><p>By default, we use a maximum long-term memory size of 10,000 which consumes a small amount of GPU memory and is reasonably capable -it can store information from around 3,900 frames after memory consolidation (r = 10). In practice, users might opt for a different upper limit of the long-term memory (LT max ), in consideration of any memory constraints, speed, and the complexity of the video. Here, we test the performance of different LT max settings on the Long-time Video (3?) dataset <ref type="bibr" target="#b28">[29]</ref> and show the results in <ref type="table" target="#tab_3">Table S1</ref>. There is significant memory saving and speed-up when LT max is decreased. While a smaller LT max seems to be sufficient for this dataset, we expect using a higher LT max can benefit more challenging videos with long-term occlusions.</p><p>We also plot the single-object FPS (1/time required to process a new frame) against the total number of processed frames for STCN <ref type="bibr" target="#b8">[9]</ref> and different LT max settings of XMem in <ref type="figure">Figure S7</ref>. We use a high-capacity 32GB V100 GPU in this experiment such that STCN can be run without out-of-memory errors. FPSs for XMem plateaue after reaching LT max .  <ref type="figure">Fig. S7</ref>. FPS scaling of STCN <ref type="bibr" target="#b8">[9]</ref> vs. variants of XMem. STCN starts off faster due to its simpler construction but slows down drastically as its memory bank expands. As STCN soon becomes too slow for practical use, we estimate its FPS by fitting a linear function to its processing time (i.e., inverse linear to FPS). This linear function is illustrated with a dashed line. XMem maintains a relatively stable and fast FPS throughout, thanks to our memory consolidation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Re-training STCN</head><p>We have changed the training schedule (see Section 3.6, Implementation Details) and adjusted parts of the network (including removing some convolutional layers and adding a feature fusion block <ref type="bibr" target="#b8">[9]</ref> to the decoder for incorporating the sensory memory). For a fair comparison, we re-train the STCN <ref type="bibr" target="#b8">[9]</ref> baseline under our setting. This is equivalent to removing the sensory memory, long-term memory, and both scaling terms. BL30K <ref type="bibr" target="#b7">[8]</ref> is not used. <ref type="table" target="#tab_5">Table S2</ref> tabulates the results on the YouTubeVOS 2018 validation set <ref type="bibr" target="#b56">[57]</ref> and the DAVIS 2017 validation set <ref type="bibr" target="#b40">[41]</ref>. The re-trained method achieves a 1.2 higher J &amp;F on DAVIS and a 1.0 higher G on YouTubeVOS. On average, the change is insignificant, i.e., our training schedule is not a sufficient condition for improved results.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Results with Different Training Datasets</head><p>Following prior works <ref type="bibr" target="#b35">[36]</ref>, we first pretrain our network on static images. As in the implementation of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we use a mix of single object datasets <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b6">7]</ref>. We compare with prior works that do not use pretraining in <ref type="table" target="#tab_7">Table S4</ref>. We additionally present detailed results of our method when it is trained on 1) DAVIS 2017 <ref type="bibr" target="#b39">[40]</ref> only, 2) YouTubeVOS 2019 <ref type="bibr" target="#b56">[57]</ref> only, and 3) a mix of both, in the followings tables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Multi-scale Evaluation</head><p>Multi-scale evaluation is a general trick used in segmentation tasks to boost accuracy by combining results from augmented inputs. Common augmentations include scale-change or vertical mirroring. Here, we show XMem's results with multi-scale evaluation as an attempt to achieve the best performance with a single model without retraining or using a better backbone. For these results, we use P = 512 for a relaxed compression. Vertical mirroring is used. Different augmentations are processed independently and the output probability maps are simply averaged.</p><p>On DAVIS, we note that a single large scale (720p) is better than merging multiple smaller scales. We use r = 3 for better results which has also been noted in STCN <ref type="bibr" target="#b8">[9]</ref>. In the test-dev set, we additionally include results with r = 5 (i.e., multi-temporal-scale) in the merge. Table S10 tabulates our results. On YouTubeVOS, we adopt multiple scales: {480, 528, 576, 624}. Unlike on DAVIS, we find larger scales to be unhelpful -which might be due to the overall less accurate annotation of YouTubeVOS. We do not adopt multiple temporal scales here. Table S11 tabulates our results. I Implementation of the Anisotropic L2 Similarity <ref type="bibr">STCN [9]</ref> decomposes the L2 similarity into a sequence of tensor operations for a memory-and compute-efficient implementation. For the proposed similarity function to be practical, a similar decomposition is required. Here, we derive and outline our implementation. Recall the definition of the anisotropic L2 similarity:</p><p>We are given key k ? R C k ?N , value v ? R C v ?N , and query q ? R C k ?HW . The key is associated with a shrinkage term s ? [1, ?) N and the query is associated with a selection term e ? [0, 1] C k ?HW . Then, the similarity between the i-th key element and the j-th query element is computed via</p><formula xml:id="formula_4">S(k, q) ij = ?s i C k c e cj (k ci ? q cj ) 2 ,<label>(S1)</label></formula><p>which equates to the original L2 similarity <ref type="bibr" target="#b8">[9]</ref> if s i = e cj = 1 for all i, j, and c. We use ":" to denote all the elements in a dimension and "@" to denote a singleton dimension to be broadcasted. <ref type="bibr" target="#b3">4</ref>  </p><p>This gives a fully vectorized implementation consisting of only element-wise operations and matrix multiplications with broadcasting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Do state-of-the-art VOS algorithms scale well? Left: Memory scaling with respect to short-term segmentation quality. Right: Segmentation quality scaling from standard short videos (y-axis) to long videos (x-axis) -the dashed line indicates a 1:1 performance ratio. Error bars show standard deviations in memory sampling if applicable. See Section 4.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of similarity functions in 2D with the background color showing the influence of each memory element (RGB). L2 similarity (a)<ref type="bibr" target="#b8">[9]</ref> considers all memory elements uniformly. The shrinkage term (b) allows encoding element-level confidence (visualized by the size of dots) that accounts for the area of influence and sharpness of the mixing weights. The selection term allows query-specific interpretation of the memory -(c) and (d) show its effect with two different queries that focus on the vertical and horizontal dimension respectively. (b) can be seen as a case where the selection term is isotropic. When combined, we can model more complex similarity relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 ==</head><label>2</label><figDesc>denotes the Hadamard (element-wise) product. 1 is an all-ones row vector with length C k .S(k, q) ij = ?s i C k c e cj (k ci ? q cj ) ?s i (k :i k :i ) T e :j ? 2k T:i (e :j q :j ) + 1(e :j q :j q :j ) ? S(k, q) = s :@ ?(k k) T e + 2k T (e q) ? 1(e q q) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons on the Long-time Video dataset [29]. ?3.9 59.7 ?8.3 60.0 ?7.5 57.0 ?1.6 56.6 ?1.5 57.3 ?1.8 ?3.5 64.5 ?4.2 69.6 ?3.9 57.7 ?0.2 55.7 ?0.3 59.7 ?0.2 ?1.8 79.9 ?1.2 83.0 ?1.5 73.4 ?3.3 72.6 ?3.1 74.3 ?1.3 79.9 ?0.9 81.3 ?1.0 75.3 ?13.0 74.3 ?13.0 76.3 ?13.1 -5.3 MiVOS * [8] 81.1 ?3.2 80.2 ?2.0 82.0 ?3.1 78.5 ?4.5 78.0 ?3.7 79.0 ?0.7 83.2 ?3.2 85.4 ?3.3 81.2 ?2.5 79.6 ?3.0 82.8 ?0.7 85.4 ?1.1 89.2 ?1.1 84.6 ?1.9 83.3 ?1.7 85.9</figDesc><table><row><cell></cell><cell cols="3">Long-time Video (1?)</cell><cell cols="3">Long-time Video (3?)</cell><cell>?1??3?</cell></row><row><cell>Method</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>J &amp;F</cell></row><row><cell>CFBI+ [61]</cell><cell>50.9</cell><cell>47.9</cell><cell>53.8</cell><cell>55.3</cell><cell>54.0</cell><cell>56.5</cell><cell>4.4</cell></row><row><cell>RMNet [54]</cell><cell cols="7">59.8 -2.8</cell></row><row><cell>JOINT [33]</cell><cell cols="7">67.1 -9.4</cell></row><row><cell>CFBI [59]</cell><cell>53.5</cell><cell>50.9</cell><cell>56.1</cell><cell>58.9</cell><cell>57.7</cell><cell>60.1</cell><cell>5.4</cell></row><row><cell>HMMN [44]</cell><cell cols="6">81.5 ?3.5</cell><cell>-8.1</cell></row><row><cell>STM [36]</cell><cell cols="6">80.6 ?5.4</cell><cell>-2.6</cell></row><row><cell>AOT [60]</cell><cell cols="6">84.3 ?2.1</cell><cell>-3.1</cell></row><row><cell cols="2">AFB-URR [29] 83.7</cell><cell>82.9</cell><cell>84.5</cell><cell>83.8</cell><cell>82.9</cell><cell>84.6</cell><cell>0.1</cell></row><row><cell>STCN [9]</cell><cell cols="6">87.3 ?2.2</cell><cell>-2.7</cell></row><row><cell cols="7">XMem (Ours) 89.8?0.2 88.0?0.2 91.6?0.2 90.0?0.4 88.2?0.3 91.8?0.4</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons on three commonly used short-term datasets.</figDesc><table><row><cell></cell><cell></cell><cell>YT-VOS 2018 val [57]</cell><cell></cell><cell cols="2">DAVIS 2017 val [41]</cell><cell cols="3">DAVIS 2016 val [40]</cell></row><row><cell>Method</cell><cell>G</cell><cell cols="2">Js Fs Ju Fu FPS</cell><cell>J &amp;F J</cell><cell>F FPS</cell><cell cols="2">J &amp;F J</cell><cell>F FPS</cell></row><row><cell>STM [36]</cell><cell cols="2">79.4 79.7 84.2 72.8 80.9</cell><cell>-</cell><cell cols="2">81.8 79.2 84.3 11.1 ?</cell><cell cols="3">89.3 88.7 89.9 14.0 ?</cell></row><row><cell cols="3">AFB-URR [29] 79.6 78.8 83.1 74.1 82.6</cell><cell>-</cell><cell cols="2">76.9 74.4 79.3 6.8 ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CFBI [59]</cell><cell cols="3">81.4 81.1 85.8 75.3 83.4 3.4</cell><cell cols="2">81.9 79.1 84.6 5.9</cell><cell cols="3">89.4 88.3 90.5 6.2</cell></row><row><cell>RMNet [54]</cell><cell cols="2">81.5 82.1 85.7 75.7 82.4</cell><cell>-</cell><cell cols="2">83.5 81.0 86.0 4.4 ?</cell><cell cols="3">88.8 88.9 88.7 11.9</cell></row><row><cell>HMMN [44]</cell><cell cols="2">82.6 82.1 87.0 76.8 84.6</cell><cell>-</cell><cell cols="2">84.7 81.9 87.5 9.3 ?</cell><cell cols="3">90.8 89.6 92.0 13.0 ?</cell></row><row><cell>MiVOS  *  [8]</cell><cell cols="2">82.6 81.1 85.6 77.7 86.2</cell><cell>-</cell><cell cols="2">84.5 81.7 87.4 11.2</cell><cell cols="3">91.0 89.6 92.4 16.9</cell></row><row><cell>STCN [9]</cell><cell cols="3">83.0 81.9 86.5 77.9 85.7 13.2 ?</cell><cell cols="2">85.4 82.2 88.6 20.2 ?</cell><cell cols="3">91.6 90.8 92.5 26.9 ?</cell></row><row><cell>JOINT [33]</cell><cell cols="2">83.1 81.5 85.9 78.7 86.5</cell><cell>-</cell><cell cols="2">83.5 80.8 86.2 6.8 ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>STCN  *  [9]</cell><cell cols="3">84.3 83.2 87.9 79.0 87.3 13.2 ?</cell><cell cols="2">85.3 82.0 88.6 20.2 ?</cell><cell cols="3">91.7 90.4 93.0 26.9 ?</cell></row><row><cell>AOT [60]</cell><cell cols="3">85.5 84.5 89.5 79.6 88.2 6.4</cell><cell cols="2">84.9 82.3 87.5 18.0</cell><cell cols="3">91.1 90.1 92.1 18.0</cell></row><row><cell cols="4">XMem (Ours) 85.7 84.6 89.3 80.2 88.7 22.6</cell><cell cols="2">86.2 82.9 89.5 22.6</cell><cell cols="3">91.5 90.4 92.7 29.6</cell></row><row><cell cols="4">XMem  *  (Ours) 86.1 85.1 89.8 80.3 89.2 22.6</cell><cell cols="2">87.7 84.0 91.4 22.6</cell><cell cols="3">92.0 90.7 93.2 29.6</cell></row></table><note>* de- notes BL30K [8] pretraining. Bold and underline denote the best and the second-best re- spectively in each column. ? denotes FPS re-timed on our hardware. On YouTubeVOS, we re-run AOT with all input frames (improving its performance) for a fair comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Results on DAVIS 2017 test-dev.?: uses 600p videos.</figDesc><table><row><cell></cell><cell cols="2">DAVIS 2017 td</cell></row><row><cell>Method</cell><cell>J &amp;F J</cell><cell>F</cell></row><row><cell>STM ? [36]</cell><cell cols="2">72.2 69.3 75.2</cell></row><row><cell>RMNet [54]</cell><cell cols="2">75.0 71.9 78.1</cell></row><row><cell>STCN [9]</cell><cell cols="2">76.1 73.1 80.0</cell></row><row><cell>CFBI+ ? [61]</cell><cell cols="2">78.0 74.4 81.6</cell></row><row><cell>HMMN [44]</cell><cell cols="2">78.6 74.7 82.5</cell></row><row><cell>MiVOS  *  [8]</cell><cell cols="2">78.6 74.9 82.2</cell></row><row><cell>AOT [60]</cell><cell cols="2">79.6 75.9 83.3</cell></row><row><cell>STCN  *  [9]</cell><cell cols="2">79.9 76.3 83.5</cell></row><row><cell>XMem (Ours)</cell><cell cols="2">81.0 77.4 84.5</cell></row><row><cell cols="3">XMem  *  (Ours) 81.2 77.6 84.7</cell></row><row><cell cols="3">XMem  *   ? (Ours) 82.5 79.1 85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Ablation on our memory stores. Standard deviations for L1? are omitted.</figDesc><table><row><cell>Setting</cell><cell cols="3">Y18 D17 L1? FPSD17 FPSY18</cell></row><row><cell>All memory stores</cell><cell cols="2">85.7 86.2 89.8 22.6</cell><cell>22.6</cell></row><row><cell>No sensory memory</cell><cell cols="2">84.4 85.1 87.9 23.1</cell><cell>23.1</cell></row><row><cell cols="3">No working memory 72.7 77.6 38.7 31.8</cell><cell>28.1</cell></row><row><cell cols="2">No long-term memory 85.9 86.3 n/a</cell><cell>17.6</cell><cell>10.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Ablation on the two scaling terms in memory reading.</figDesc><table><row><cell>Setting</cell><cell>Y18 D17</cell></row><row><cell>With both terms</cell><cell>85.7 86.2</cell></row><row><cell cols="2">With shrinkage s only 85.1 85.6</cell></row><row><cell cols="2">With selection e only 84.8 84.8</cell></row><row><cell>With neither</cell><cell>85.0 85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Comparisons between different memory consolidation methods.</figDesc><table><row><cell></cell><cell></cell><cell>Compress</cell></row><row><cell>Setting</cell><cell>L3?</cell><cell>ratio</cell></row><row><cell>Random</cell><cell cols="2">P = 64 89.5 ?0.8 12625%</cell></row><row><cell cols="3">K-means centroid P = 64 89.5 ?0.5 12625%</cell></row><row><cell>Usage-based</cell><cell cols="2">P = 64 89.6 ?0.4 12625%</cell></row><row><cell>Random</cell><cell>P = 128 89.7 ?0.7</cell><cell>6328%</cell></row><row><cell cols="3">K-means centroid P = 128 82.4 ?10.3 6328%</cell></row><row><cell>Usage-based</cell><cell>P = 128 90.0 ?0.4</cell><cell>6328%</cell></row><row><cell>Random</cell><cell>P = 256 89.8 ?0.7</cell><cell>3164%</cell></row><row><cell cols="3">K-means centroid P = 256 74.5 ?17.0 3164%</cell></row><row><cell>Usage-based</cell><cell>P = 256 90.1 ?0.4</cell><cell>3164%</cell></row><row><cell>No potentiation</cell><cell>87.9 ?0.2</cell><cell></cell></row><row><cell>With potentiation</cell><cell>90.0 ?0.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Comparisons between different strategies for handling long videos.</figDesc><table><row><cell>Setting</cell><cell>L1?</cell><cell>L3?</cell><cell>?1??3?</cell></row><row><cell>Consolidation</cell><cell cols="2">89.8 ?0.2 90.0 ?0.4</cell><cell>0.2</cell></row><row><cell cols="3">Eager compression 87.8 ?0.3 87.3 ?1.3</cell><cell>-0.5</cell></row><row><cell>Sparse insertion</cell><cell cols="2">89.8 ?0.4 87.3 ?1.0</cell><cell>-2.5</cell></row><row><cell>Local window</cell><cell cols="2">86.2 ?1.5 85.5 ?0.9</cell><cell>-0.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Ablation on the deep update frequency of sensory memory.</figDesc><table><row><cell>Setting</cell><cell>Y18 D17 FPS</cell></row><row><cell cols="2">Every r-th frame 85.7 86.2 22.6</cell></row><row><cell cols="2">Every single frame 85.5 86.1 18.5</cell></row><row><cell>No deep update</cell><cell>85.3 85.4 22.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S1 .</head><label>S1</label><figDesc>Performance of XMem with different upper limits of the long-term memory LTmax on the Long-time Video (3?) dataset<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table><row><cell>LTmax</cell><cell></cell><cell>J &amp;F</cell><cell cols="3">Max. GPU memory</cell><cell cols="2">FPS</cell></row><row><cell>500</cell><cell cols="2">87.2?4.7</cell><cell></cell><cell cols="2">1168 MB</cell><cell cols="2">35.3</cell></row><row><cell>1,000</cell><cell cols="2">89.5?0.3</cell><cell></cell><cell cols="2">1186 MB</cell><cell cols="2">34.1</cell></row><row><cell>2,500</cell><cell cols="2">89.8?0.2</cell><cell></cell><cell cols="2">1243 MB</cell><cell cols="2">31.1</cell></row><row><cell>5,000</cell><cell cols="2">89.9?0.2</cell><cell></cell><cell cols="2">1332 MB</cell><cell cols="2">27.5</cell></row><row><cell>10,000</cell><cell cols="2">90.0?0.4</cell><cell></cell><cell cols="2">1515 MB</cell><cell cols="2">23.4</cell></row><row><cell>20,000</cell><cell cols="2">90.0?0.4</cell><cell></cell><cell cols="2">1632 MB</cell><cell cols="2">21.1</cell></row><row><cell>30,000</cell><cell cols="2">90.0?0.4</cell><cell></cell><cell cols="2">1632 MB</cell><cell cols="2">20.9</cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FPS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">XMem (LTmax = 1000) XMem (LTmax = 2500)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">XMem (LTmax = 5000)</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">XMem (LTmax = 10000)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">XMem (LTmax = 20000)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>STCN</cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1,000</cell><cell>2,000</cell><cell>3,000</cell><cell>4,000</cell><cell>5,000</cell><cell>6,000</cell><cell>7,000</cell></row><row><cell></cell><cell></cell><cell cols="4">Number of processed frames</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S2 .</head><label>S2</label><figDesc>We compare the performance of STCN<ref type="bibr" target="#b8">[9]</ref> to the re-trained version with our training setup. On average, there is no significant difference. The performance of XMem is provided as a reference.</figDesc><table><row><cell>Method</cell><cell>YouTubeVOS 2018 val G</cell><cell>DAVIS 2017 val J &amp;F</cell></row><row><cell>STCN [9] (original)</cell><cell>83.0</cell><cell>85.4</cell></row><row><cell>STCN [9] (re-trained)</cell><cell>84.0</cell><cell>84.2</cell></row><row><cell>XMem (Ours)</cell><cell>85.7</cell><cell>86.2</cell></row><row><cell cols="3">F Results on YouTubeVOS 2019 validation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S3</head><label>S3</label><figDesc>tabulates our results on the YouTubeVOS<ref type="bibr" target="#b56">[57]</ref> 2019 validation set. We compare the measured FPS on the 2018 version. The FPS on these two versions are highly correlated as their average number of objects and video length are similar.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table S3 .</head><label>S3</label><figDesc>Quantitative comparisons on YouTubeVOS 2019 validation. XMem * (Ours) 85.8 84.8 89.2 80.3 88.8 22.6</figDesc><table><row><cell></cell><cell></cell><cell cols="4">YouTubeVOS 2019 val [57]</cell></row><row><cell>Method</cell><cell>G</cell><cell>Js</cell><cell>Fs</cell><cell>Ju</cell><cell cols="2">Fu FPSY18</cell></row><row><cell>CFBI [59]</cell><cell cols="5">81.0 80.6 85.1 75.2 83.0</cell><cell>3.4</cell></row><row><cell>SST [14]</cell><cell cols="2">81.8 80.9</cell><cell>-</cell><cell>76.7</cell><cell>-</cell><cell>-</cell></row><row><cell>MiVOS  *  [8]</cell><cell cols="5">82.4 80.6 84.7 78.1 86.4</cell><cell>-</cell></row><row><cell>HMMN [44]</cell><cell cols="5">82.5 81.7 86.1 77.3 85.0</cell><cell>-</cell></row><row><cell>CFBI+ [61]</cell><cell cols="5">82.6 81.7 86.2 77.1 85.2</cell><cell>4.0</cell></row><row><cell>STCN [9]</cell><cell cols="5">82.7 81.1 85.4 78.2 85.9</cell><cell>13.2</cell></row><row><cell>JOINT [33]</cell><cell cols="5">82.8 80.8 84.8 79.0 86.6</cell><cell>-</cell></row><row><cell>STCN  *  [9]</cell><cell cols="5">84.2 82.6 87.0 79.4 87.7</cell><cell>13.2</cell></row><row><cell>AOT [60]</cell><cell cols="5">85.3 83.9 88.8 79.9 88.5</cell><cell>6.4</cell></row><row><cell cols="6">XMem (Ours) 85.5 84.3 88.6 80.3 88.6</cell><cell>22.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table S4 .Table S5 .Table S6 .Table S7 .Table S8 .Table S9 .</head><label>S4S5S6S7S8S9</label><figDesc>Comparisons with methods without static image pretraining. Performance of XMem on DAVIS 2016 with different training data. Performance of XMem on DAVIS 2017 validation with different training data. Performance of XMem on DAVIS 2017 test-dev with different training data. Performance of XMem on YouTubeVOS 2018 validation with different training data. Performance of XMem on YouTubeVOS 2019 validation with different training data.</figDesc><table><row><cell>Method Training data</cell><cell cols="3">Y18 Y19 D16 D17 D 17td FPSD17 G Js Fs Ju Fu</cell></row><row><cell cols="4">LWL [2] SST [14] YouTubeVOS only DAVIS+YouTubeVOS only 81.5 81.0 81.7 81.8 CFBI+ [61] 82.0 82.6 89.9 82.9 78.0 5.6 -81.6 ---82.5 -84.3 83.6 88.0 78.5 87.1 -84.2 83.8 88.3 78.1 86.7 JOINT [33] 83.1 82.8 -83.5 -Static+DAVIS+YouTubeVOS 85.5 84.3 88.6 80.3 88.6 6.8 Ours ? 84.3 84.2 90.8 84.5 79.8 20.2 Static+BL30K+DAVIS+YouTubeVOS 85.8 84.8 89.2 80.3 88.8</cell></row><row><cell>Training data</cell><cell></cell><cell></cell><cell>J &amp;F J</cell><cell>F</cell></row><row><cell>DAVIS only</cell><cell></cell><cell></cell><cell>87.8 86.7 88.9</cell></row><row><cell cols="2">DAVIS+YouTubeVOS only</cell><cell></cell><cell>90.8 89.6 91.9</cell></row><row><cell cols="2">Static+DAVIS+YouTubeVOS</cell><cell></cell><cell>91.5 90.4 92.7</cell></row><row><cell cols="4">Static+BL30K+DAVIS+YouTubeVOS 92.0 90.7 93.2</cell></row><row><cell>Training data</cell><cell></cell><cell></cell><cell>J &amp;F J</cell><cell>F</cell></row><row><cell>DAVIS only</cell><cell></cell><cell></cell><cell>76.7 74.1 79.3</cell></row><row><cell cols="2">DAVIS+YouTubeVOS only</cell><cell></cell><cell>84.5 81.4 87.6</cell></row><row><cell cols="2">Static+DAVIS+YouTubeVOS</cell><cell></cell><cell>86.2 82.9 89.5</cell></row><row><cell cols="4">Static+BL30K+DAVIS+YouTubeVOS 87.7 84.0 91.4</cell></row><row><cell>Training data</cell><cell></cell><cell></cell><cell>J &amp;F J</cell><cell>F</cell></row><row><cell>DAVIS only</cell><cell></cell><cell></cell><cell>64.8 61.4 68.1</cell></row><row><cell cols="2">DAVIS+YouTubeVOS only</cell><cell></cell><cell>79.8 76.3 83.4</cell></row><row><cell cols="2">Static+DAVIS+YouTubeVOS</cell><cell></cell><cell>81.0 77.4 84.5</cell></row><row><cell cols="4">Static+BL30K+DAVIS+YouTubeVOS 81.2 77.6 84.7</cell></row><row><cell>Training data</cell><cell></cell><cell>G</cell><cell>Js Fs Ju Fu</cell></row><row><cell>YouTubeVOS only</cell><cell></cell><cell cols="2">84.4 83.7 88.5 78.2 87.2</cell></row><row><cell cols="2">DAVIS+YouTubeVOS only</cell><cell cols="2">84.3 83.9 88.8 77.7 86.7</cell></row><row><cell cols="2">Static+DAVIS+YouTubeVOS</cell><cell cols="2">85.7 84.6 89.3 80.2 88.7</cell></row><row><cell cols="4">Static+BL30K+DAVIS+YouTubeVOS 86.1 85.1 89.8 80.3 89.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table S10 .</head><label>S10</label><figDesc>XMem with/without multi-scale evaluation on DAVIS. ?: 600p evaluation. XMem (MS) 88.2 85.4 91.0 92.7 92.0 93.5 83.1 79.7 86.4 XMem * (MS) 89.5 86.3 92.6 93.3 92.2 94.4 83.7 80.5 87.0</figDesc><table><row><cell></cell><cell cols="8">DAVIS 2017 val DAVIS 2016 val DAVIS 2017 test-dev</cell></row><row><cell>Method</cell><cell cols="2">J &amp;F J</cell><cell cols="3">F J &amp;F J</cell><cell cols="2">F J &amp;F J</cell><cell>F</cell></row><row><cell>XMem</cell><cell cols="7">86.2 82.9 89.5 91.5 90.4 92.7 81.0 77.4</cell><cell>84.5</cell></row><row><cell>XMem  *</cell><cell cols="7">87.7 84.0 91.4 92.0 90.7 93.2 81.2 77.6</cell><cell>84.7</cell></row><row><cell>XMem  *   ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.5 79.1</cell><cell>85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table S11 .</head><label>S11</label><figDesc>XMem with/without multi-scale evaluation on YouTubeVOS. 84.6 89.3 80.2 88.7 85.5 84.3 88.6 80.3 88.6 XMem * 86.1 85.1 89.8 80.3 89.2 85.8 84.8 89.2 80.3 88.8 XMem (MS) 86.7 85.3 89.9 81.7 89.9 86.4 84.9 89.2 81.8 89.8 XMem * (MS) 86.9 85.6 90.3 81.7 90.2 86.8 85.5 89.8 81.8 89.9</figDesc><table><row><cell></cell><cell></cell><cell cols="4">YouTubeVOS 2018 val</cell><cell></cell><cell cols="4">YouTubeVOS 2019 val</cell></row><row><cell>Method</cell><cell>G</cell><cell>Js</cell><cell>Fs</cell><cell>Ju</cell><cell>Fu</cell><cell>G</cell><cell>Js</cell><cell>Fs</cell><cell>Ju</cell><cell>Fu</cell></row><row><cell>XMem</cell><cell>85.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at hkchengrex.github.io/XMem arXiv:2207.07115v2 [cs.CV] 18 Jul 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For brevity, we omit the handling of these two scaling terms in memory updates for the rest of the paper. They are updated in the same way as the value.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We make sure to exclude any caching or input buffering overhead.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Boardcasting as in numpy.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The appendix is structured as follows:</p><p>-We first provide a more detailed analysis of the memory consolidation process (Sec. A). -We then provide qualitative results, comparing the proposed XMem to baselines (Sec. B). -We demonstrate failure cases (Sec. C).</p><p>-We compare different limits on the size of the long-term memory and illustrate the processing rate over the number of processed frames (Sec. D). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Visualizing Memory Consolidation</head><p>Here, we visualize the memory consolidation process (Section 3.3) by showing the candidate frames, some of the selected prototypes, and the corresponding aggregation weights (columns of W(k c , k p ), each mapping to a distribution over all the candidates). Recall that W(k c , k p ) is used to aggregate candidate values v c into prototype values v p . <ref type="figure">Figure S1</ref> and <ref type="figure">Figure S2</ref> show two examples. As illustrated in <ref type="figure">Figure 5</ref> of the main paper we observe semantically meaningful regions to be grouped.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human memory: A proposed system and its control processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Shiffrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="89" to="195" />
			<date type="published" when="1968" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">State-aware tracker for realtime video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">arXiv</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Winkelbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Olefir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elbadrawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Katam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01911</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Blenderproc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Capsulevos: Semi-supervised video object segmentation using capsule routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computer vision: A modern approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Prentice hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Video object segmentation using global and instance embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning position and target consistency for memory-based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Motion-guided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast video object segmentation with temporal aggregation network and dynamic template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Video object segmentation with dynamic memory networks and adaptive object alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Video object segmentation with adaptive feature bank and uncertain-region refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Luc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Joint inductive and transductive learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Make one-shot video object segmentation efficient again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning dynamic network using a reuse gate function in semi-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The 2017 davis challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning fast and robust target models for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Hierarchical memory matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Wixted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Morris</surname></persName>
		</author>
		<title level="m">Cold Spring Harbor perspectives in biology. Cold Spring Harbor Lab</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Memory consolidation</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Swiftnet: Real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Efficient regional memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Spatiotemporal cnn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Reliable propagation-correction modulation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Associating objects with transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Collaborative video object segmentation by multi-scale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Fast video object segmentation via dynamic targeting network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A transductive approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
