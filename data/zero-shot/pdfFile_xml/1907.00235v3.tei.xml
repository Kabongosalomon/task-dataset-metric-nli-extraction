<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
							<email>shiyangli@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
							<email>x_jin@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xuan</surname></persName>
							<email>yxuan@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<email>wenhuchen@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
							<email>yuxiangw@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
							<email>xyan@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer [1]. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dotproduct self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only O(L(log L) 2 ) memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and realworld datasets show that it compares favorably to the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series forecasting plays an important role in daily life to help people manage resources and make decisions. For example, in retail industry, probabilistic forecasting of product demand and supply based on historical data can help people do inventory planning to maximize the profit. Although still widely used, traditional time series forecasting models, such as State Space Models (SSMs) <ref type="bibr" target="#b1">[2]</ref> and Autoregressive (AR) models, are designed to fit each time series independently. Besides, they also require practitioners' expertise in manually selecting trend, seasonality and other components. To sum up, these two major weaknesses have greatly hindered their applications in the modern large-scale time series forecasting tasks.</p><p>To tackle the aforementioned challenges, deep neural networks <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> have been proposed as an alternative solution, where Recurrent Neural Network (RNN) <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr">[9]</ref> has been employed to model time series in an autoregressive fashion. However, RNNs are notoriously difficult to train <ref type="bibr">[10]</ref> because of gradient vanishing and exploding problem. Despite the emergence of various variants, including LSTM <ref type="bibr">[11]</ref> and GRU <ref type="bibr" target="#b11">[12]</ref>, the issues still remain unresolved. As an example, <ref type="bibr" target="#b12">[13]</ref> shows that language models using LSTM have an effective context size of about 200 tokens on average but are only able to sharply distinguish 50 tokens nearby, indicating that even LSTM struggles to capture long-term dependencies. On the other hand, real-world forecasting applications often have both long-and short-term repeating patterns <ref type="bibr" target="#b6">[7]</ref>. For example, the hourly occupancy rate of a freeway in traffic data has both daily and hourly patterns. In such cases, how to model long-term dependencies becomes the critical step in achieving promising performances.</p><p>Recently, Transformer <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> has been proposed as a brand new architecture which leverages attention mechanism to process a sequence of data. Unlike the RNN-based methods, Transformer allows the model to access any part of the history regardless of distance, making it potentially more suitable for grasping the recurring patterns with long-term dependencies. However, canonical dot-product self-attention matches queries against keys insensitive to local context, which may make the model prone to anomalies and bring underlying optimization issues. More importantly, space complexity of canonical Transformer grows quadratically with the input length L, which causes memory bottleneck on directly modeling long time series with fine granularity. We specifically delve into these two issues and investigate the applications of Transformer to time series forecasting. Our contributions are three fold:</p><p>? We successfully apply Transformer architecture to time series forecasting and perform extensive experiments on both synthetic and real datasets to validate Transformer's potential value in better handling long-term dependencies than RNN-based models. ? We propose convolutional self-attention by employing causal convolutions to produce queries and keys in the self-attention layer. Query-key matching aware of local context, e.g. shapes, can help the model achieve lower training loss and further improve its forecasting accuracy. ? We propose LogSparse Transformer, with only O(L(log L) 2 ) space complexity to break the memory bottleneck, not only making fine-grained long time series modeling feasible but also producing comparable or even better results with much less memory usage, compared to canonical Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Due to the wide applications of forecasting, various methods have been proposed to solve the problem. One of the most prominent models is ARIMA <ref type="bibr" target="#b14">[15]</ref>. Its statistical properties as well as the wellknown Box-Jenkins methodology <ref type="bibr" target="#b15">[16]</ref> in the model selection procedure make it the first attempt for practitioners. However, its linear assumption and limited scalability make it unsuitable for large-scale forecasting tasks. Further, information across similar time series cannot be shared since each time series is fitted individually. In contrast, <ref type="bibr" target="#b16">[17]</ref> models related time series data as a matrix and deal with forecasting as a matrix factorization problem. <ref type="bibr" target="#b17">[18]</ref> proposes hierarchical Bayesian methods to learn across multiple related count time series from the perspective of graph model.</p><p>Deep neural networks have been proposed to capture shared information across related time series for accurate forecasting. <ref type="bibr" target="#b2">[3]</ref> fuses traditional AR models with RNNs by modeling a probabilistic distribution in an encoder-decoder fashion. Instead, <ref type="bibr" target="#b18">[19]</ref> uses an RNN as an encoder and Multi-layer Perceptrons (MLPs) as a decoder to solve the so-called error accumulation issue and conduct multiahead forecasting in parallel. <ref type="bibr" target="#b5">[6]</ref> uses a global RNN to directly output the parameters of a linear SSM at each step for each time series, aiming to approximate nonlinear dynamics with locally linear segments. In contrast, <ref type="bibr">[9]</ref> deals with noise using a local Gaussian process for each time series while using a global RNN to model the shared patterns. <ref type="bibr" target="#b19">[20]</ref> tries to combine the advantages of AR models and SSMs, and maintain a complex latent process to conduct multi-step forecasting in parallel.</p><p>The well-known self-attention based Transformer <ref type="bibr" target="#b0">[1]</ref> has recently been proposed for sequence modeling and has achieved great success. Several recent works apply it to translation, speech, music and image generation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>. However, scaling attention to extremely long sequences is computationally prohibitive since the space complexity of self-attention grows quadratically with sequence length <ref type="bibr" target="#b20">[21]</ref>. This becomes a serious issue in forecasting time series with fine granularity and strong long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Problem definition Suppose we have a collection of N related univariate time series {z i,1:t0 } N i=1 , where z i,1:t0 [z i,1 , z i,2 , ? ? ? , z i,t0 ] and z i,t ? R denotes the value of time series i at time t 1 . We are going to predict the next ? time steps for all time series, i.e. {z i,t0+1:t0+? } N i=1 . Besides, let {x i,1:t0+? } N i=1 be a set of associated time-based covariate vectors with dimension d that are assumed to be known over the entire time period, e.g. day-of-the-week and hour-of-the-day. We aim to model the following conditional distribution</p><formula xml:id="formula_0">p(z i,t0+1:t0+? |z i,1:t0 , x i,1:t0+? ; ?) = t0+? t=t0+1 p(z i,t |z i,1:t?1 , x i,1:t ; ?).</formula><p>We reduce the problem to learning a one-step-ahead prediction model p(z t |z 1:t?1 , x 1:t ; ?) 2 , where ? denotes the learnable parameters shared by all time series in the collection. To fully utilize both the observations and covariates, we concatenate them to obtain an augmented matrix as follows:</p><formula xml:id="formula_1">y t [z t?1 ? x t ] ? R d+1 , Y t = [y 1 , ? ? ? , y t ] T ? R t?(d+1) , where [? ? ?] represents concatenation. An appropriate model z t ? f (Y t )</formula><p>is then explored to predict the distribution of z t given Y t .</p><p>Transformer We instantiate f with Transformer 3 by taking advantage of the multi-head selfattention mechanism, since self-attention enables Transformer to capture both long-and short-term dependencies, and different attention heads learn to focus on different aspects of temporal patterns. These advantages make Transformer a good candidate for time series forecasting. We briefly introduce its architecture here and refer readers to <ref type="bibr" target="#b0">[1]</ref> for more details.</p><p>In the self-attention layer, a multi-head self-attention sublayer simultaneously transforms</p><formula xml:id="formula_2">Y 4 into H distinct query matrices Q h = YW Q h , key matrices K h = YW K h , and value matrices V h = YW V h respectively, with h = 1, ? ? ? , H. Here W Q h , W K h ? R (d+1)?d k and W V h ? R (d+1)</formula><p>?dv are learnable parameters. After these linear projections, the scaled dot-product attention computes a sequence of vector outputs:</p><formula xml:id="formula_3">O h = Attention(Q h , K h , V h ) = softmax Q h K T h ? d k ? M V h .</formula><p>Note that a mask matrix M is applied to filter out rightward attention by setting all upper triangular elements to ??, in order to avoid future information leakage. Afterwards, O 1 , O 2 , ? ? ? , O H are concatenated and linearly projected again. Upon the attention output, a position-wise feedforward sublayer with two layers of fully-connected network and a ReLU activation in the middle is stacked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enhancing the locality of Transformer</head><p>Patterns in time series may evolve with time significantly due to various events, e.g. holidays and extreme weather, so whether an observed point is an anomaly, change point or part of the patterns is highly dependent on its surrounding context. However, in the self-attention layers of canonical Transformer, the similarities between queries and keys are computed based on their point-wise values without fully leveraging local context like shape, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and (b). Query-key matching agnostic of local context may confuse the self-attention module in terms of whether the observed value is an anomaly, change point or part of patterns, and bring underlying optimization issues.</p><p>We propose convolutional self-attention to ease the issue. The architectural view of proposed convolutional self-attention is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c) and (d). Rather than using convolution of <ref type="bibr" target="#b0">1</ref> Here time index t is relative, i.e. the same t in different time series may represent different actual time point. <ref type="bibr" target="#b1">2</ref> Since the model is applicable to all time series, we omit the subscript i for simplicity and clarity. <ref type="bibr" target="#b2">3</ref> By referring to Transformer, we only consider the autoregressive Transformer-decoder in the following. <ref type="bibr" target="#b3">4</ref> At each time step the same model is applied, so we simplify the formulation with some abuse of notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked Multi-Head Attention</head><p>Masked Multi-Head Attention  kernel size 1 with stride 1 (matrix multiplication), we employ causal convolution of kernel size k with stride 1 to transform inputs (with proper paddings) into queries and keys. Note that causal convolutions ensure that the current position never has access to future information. By employing causal convolution, generated queries and keys can be more aware of local context and hence, compute their similarities by their local context information, e.g. local shapes, instead of point-wise values, which can be helpful for accurate forecasting. Note that when k = 1, the convolutional self-attention will degrade to canonical self-attention, thus it can be seen as a generalization.</p><formula xml:id="formula_4">Q V K Conv, 1 Conv, 1 Conv, 1 Conv, k Conv, 1 Conv, k Q V K (a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Breaking the memory bottleneck of Transformer</head><p>To motivate our approach, we first perform a qualitative assessment of the learned attention patterns with a canonical Transformer on traffic-f dataset. The traffic-f dataset contains occupancy rates of 963 car lanes of San Francisco bay area recorded every 20 minutes <ref type="bibr" target="#b5">[6]</ref>. We trained a 10-layer canonical Transformer on traffic-f dataset with full attention and visualized the learned attention patterns. One example is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Layer 2 clearly exhibited global patterns, however, layer 6 and 10, only exhibited pattern-dependent sparsity, suggesting that some form of sparsity could be introduced without significantly affecting performance. More importantly, for a sequence with length L, computing attention scores between every pair of cells will cause O(L 2 ) memory usage, making modeling long time series with fine granularity and strong long-term dependencies prohibitive.</p><p>We propose LogSparse Transformer, which only needs to calculate O(log L) dot products for each cell in each layer. Further, we only need to stack up to O(log L) layers and the model will be able to access every cell's information. Hence, the total cost of memory usage is only O(L(log L) 2 ). We define I k l as the set of indices of the cells that cell l can attend to during the computation from k th (a). Full Self Attention (b). LogSparse Self Attention (d). Restart Attention + LogSparse Self Attention (c). Local Attention + LogSparse Self Attention LogSparse Attention Range LogSparse Attention Range LogSparse Attention Range Local Attention Range</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self</head><p>LogSparse Attention Range Self Self Self <ref type="figure">Figure 3</ref>: Illustration of different attention mechanism between adjacent layers in Transformer. layer to (k + 1) th layer. In the standard self-attention of Transformer, I k l = {j : j ? l}, allowing every cell to attend to all its past cells and itself as shown in <ref type="figure">Figure 3</ref>(a). However, such an algorithm suffers from the quadratic space complexity growth along with the input length. To alleviate such an issue, we propose to select a subset of the indices I k l ? {j : j ? l} so that |I k l | does not grow too fast along with l. An effective way of choosing indices is |I k l | ? log L. Notice that cell l is a weighted combination of cells indexed by I k l in kth self-attention layer and can pass the information of cells indexed by I k l to its followings in the next layer. Let S k l be the set which contains indices of all the cells whose information has passed to cell l up to k th layer. To ensure that every cell receives the information from all its previous cells and itself, the number of stacked layers k l should satisfy that Sk l l = {j : j ? l} for l = 1, ? ? ? , L. That is, ?l and j ? l, there is a directed path P jl = (j, p 1 , p 2 , ? ? ? , l) withk l edges, where j ? I 1 p1 , p 1 ? I 2 p2 , ? ? ? , pk l ?1 ? Ik l l . We propose LogSparse self-attention by allowing each cell only to attend to its previous cells with an exponential step size and itself. That is, ?k and l, I k l = {l ? 2 log 2 l , l ? 2 log 2 l ?1 , l ? 2 log 2 l ?2 , ..., l ? 2 0 , l}, where ? denotes the floor operation, as shown in <ref type="figure">Figure 3</ref>(b). <ref type="bibr" target="#b4">5</ref> Theorem 1. ?l and j ? l, there is at least one path from cell j to cell l if we stack log 2 l + 1 layers. Moreover, for j &lt; l, the number of feasible unique paths from cell j to cell l increases at a rate of O( log 2 (l ? j) !).</p><p>The proof, deferred to Appendix A.1, uses a constructive argument. Theorem 1 implies that despite an exponential decrease in the memory usage (from O(L 2 ) to O(L log 2 L)) in each layer, the information could still flow from any cell to any other cell provided that we go slightly "deeper" -take the number of layers to be log 2 L + 1. Note that this implies an overall memory usage of O(L(log 2 L) 2 ) and addresses the notorious scalability bottleneck of Transformer under GPU memory constraint <ref type="bibr" target="#b0">[1]</ref>. Moreover, as two cells become further apart, the number of paths increases at a rate of super-exponential in log 2 (l ? j), which indicates a rich information flow for modeling delicate long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Attention</head><p>We can allow each cell to densely attend to cells in its left window of size O(log 2 L) so that more local information, e.g. trend, can be leveraged for current step forecasting. Beyond the neighbor cells, we can resume our LogSparse attention strategy as shown in <ref type="figure">Figure 3</ref> Restart Attention Further, one can divide the whole input with length L into subsequences and set each subsequence length L sub ? L. For each of them, we apply the LogSparse attention strategy. One example is shown in <ref type="figure">Figure 3</ref> Employing local attention and restart attention won't change the complexity of our sparse attention strategy but will create more paths and decrease the required number of edges in the path. Note that one can combine local attention and restart attention together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic datasets</head><p>To demonstrate Transformer's capability to capture long-term dependencies, we conduct experiments on synthetic data. Specifically, we generate a piece-wise sinusoidal signals</p><formula xml:id="formula_5">f (x) = ? ? ? ? ? A 1 sin(?x/6) + 72 + N x x ? [0, 12), A 2 sin(?x/6) + 72 + N x x ? [12, 24), A 3 sin(?x/6) + 72 + N x x ? [24, t 0 ), A 4 sin(?x/12) + 72 + N x x ? [t 0 , t 0 + 24),</formula><p>where x is an integer, A 1 , A 2 , A 3 are randomly generated by uniform distribution on [0, 60], A 4 = max(A 1 , A 2 ) and N x ? N (0, 1). Following the forecasting setting in Section 3, we aim to predict the last 24 steps given the previous t 0 data points. Intuitively, larger t 0 makes forecasting more difficult since the model is required to understand and remember the relation between A 1 and A 2 to make correct predictions after t 0 ? 24 steps of irrelevant signals. Hence, we create 8 different datasets by varying the value of t 0 within {24, 48, 72, 96, 120, 144, 168, 192}. For each dataset, we generate 4.5K, 0.5K and 1K time series instances for training, validation and test set, respectively. An example time series with t 0 = 96 is shown in <ref type="figure" target="#fig_4">Figure 4</ref>(a).</p><p>In this experiment, we use a 3-layer canonical Transformer with standard self-attention. For comparison, we employ DeepAR <ref type="bibr" target="#b2">[3]</ref>, an autoregressive model based on a 3-layer LSTM, as our baseline. Besides, to examine if larger capacity could improve performance of DeepAR, we also gradually increase its hidden size h as {20, 40, 80, 140, 200}. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref>, we evaluate both methods using ?-quantile loss R ? with ? ? (0, 1),</p><formula xml:id="formula_6">R ? (x,x) = 2 i,t D ? (x (i) t ,x (i) t ) i,t |x (i) t | , D ? (x,x) = (? ? I {x?x} )(x ?x),</formula><p>wherex is the empirical ?-quantile of the predictive distribution and I {x?x} is an indicator function.  When t 0 = 24, both of them perform very well. But, as t 0 increases, especially when t 0 ? 96, the performance of DeepAR drops significantly while Transformer keeps its accuracy, suggesting that Transformer can capture fairly longterm dependencies when LSTM fails to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real-world datasets</head><p>We further evaluate our model on several realworld datasets. The electricity-f (fine) dataset consists of electricity consumption of 370 customers recorded every 15 minutes and the electricity-c (coarse) dataset is the aggregated electricity-f by every 4 points, producing hourly electricity consumption. Similarly, the traffic-f (fine) dataset contains occupancy rates of 963 freeway in San Francisco recorded every 20 minutes and the traffic-c (coarse) contains hourly occupancy rates by averaging every 3 points in traffic-f. The solar dataset 6 contains the solar power production records from January to August in 2006, which is sampled every hour from 137 PV plants in Alabama. The wind 7 dataset contains daily <ref type="table">Table 1</ref>: Results summary (R 0.5 /R 0.9 -loss) of all methods. e-c and t-c represent electricity-c and traffic-c, respectively. In the 1st and 3rd row, we perform rolling-day prediction of 7 days while in the 2nd and 4th row, we directly forecast 7 days ahead. TRMF outputs points predictions, so we only report R 0.5 . denotes results from <ref type="bibr" target="#b5">[6]</ref>.  Long-term and short-term forecasting We first show the effectiveness of canonical Transformer equipped with convolutional self-attention in long-term and short-term forecasting in electricity-c and traffic-c dataset. These two datasets exhibit both hourly and daily seasonal patterns. However, traffic-c demonstrates much greater difference between the patterns of weekdays and weekends compared to electricity-c. Hence, accurate forecasting in traffic-c dataset requires the model to capture both long-and short-term dependencies very well. As baselines, we use classical forecasting methods auto.arima, ets implemented in R's forecast package and the recent matrix factorization method TRMF <ref type="bibr" target="#b16">[17]</ref>, a RNN-based autoregressive model DeepAR and a RNN-based state space model DeepState <ref type="bibr" target="#b5">[6]</ref>. For short-term forecasting, we evaluate rolling-day forecasts for seven days ( i.e., prediction horizon is one day and forecasts start time is shifted by one day after evaluating the prediction for the current day <ref type="bibr" target="#b5">[6]</ref>). For long-term forecasting, we directly forecast 7 days ahead. As shown in <ref type="table">Table 1</ref>, our models with convolutional self-attention get betters results in both long-term and short-term forecasting, especially in traffic-c dataset compared to strong baselines, partly due to the long-term dependency modeling ability of Transformer as shown in our synthetic data.</p><p>Convolutional self-attention In this experiment, we conduct ablation study of our proposed convolutional self-attention. We explore different kernel size k ? {1, 2, 3, 6, 9} on the full attention model and fix all other settings. We still use rolling-day prediction for seven days on electricity-c and traffic-c datasets. The results of different kernel sizes on both datasets are shown in <ref type="table">Table 2</ref>. On electricity-c dataset, models with kernel size k ? {2, 3, 6, 9} obtain slightly better results in term of R 0.5 than canonical Transformer but overall these results are comparable and all of them perform very well. We argue it is because electricity-c dataset is less challenging and covariate vectors have already provided models with rich information for accurate forecasting. Hence, being aware of larger local context may not help a lot in such cases. However, on much more challenging traffic-c dataset, the model with larger kernel size k can make more accurate forecasting than models with smaller ones with as large as 9% relative improvement. These consistent gains can be the results of more accurate query-key matching by being aware of more local context. Further, to verify if incorporating more local context into query-key matching can ease the training, we plot the <ref type="table">Table 2</ref>: Average R 0.5 /R 0.9 -loss of different kernel sizes for rolling-day prediction of 7 days. training loss of kernel size k ? {1, 3, 9} in electricity-c and traffic-c datasets. We found that Transformer with convolutional self-attention also converged faster and to lower training errors, as shown in <ref type="figure" target="#fig_6">Figure 5</ref>, proving that being aware of local context can ease the training process.</p><p>Sparse attention Further, we compare our proposed LogSparse Transformer to the full attention counterpart on fine-grained datasets, electricity-f and traffic-f. Note that time series in these two datasets have much longer periods and are noisier comparing to electricity-c and traffic-c. We first compare them under the same memory budget. For electricity-f dataset, we choose L e1 = 768 with subsequence length L e1 /8 and local attention length log 2 (L e1 /8) in each subsequence for our sparse attention model and L e2 = 293 in the full attention counterpart. For traffic-f dataset, we select L t1 = 576 with subsequence length L t1 /8 and local attention length log 2 (L t1 /8) in each subsequence for our sparse attention model, and L t2 = 254 in the full attention counterpart. The calculation of memory usage and other details can be found in Appendix A.4. We conduct experiments on aforementioned sparse and full attention models with/without convolutional self-attention on both datasets. By following such settings, we summarize our results in <ref type="table">Table 3</ref> (Upper part). No matter equipped with convolutional self-attention or not, our sparse attention models achieve comparable results on electricity-f but much better results on traffic-f compared to its full attention counterparts. Such performance gain on traffic-f could be the result of the dateset's stronger long-term dependencies and our sparse model's better capability of capturing these dependencies, which, under the same memory budget, the full attention model cannot match. In addition, both sparse and full attention models benefit from convolutional self-attention on challenging traffic-f, proving its effectiveness.</p><p>To explore how well our sparse attention model performs compared to full attention model with the same input length, we set L e2 = L e1 = 768 and L t2 = L t1 = 576 on electricity-f and traffic-f, respectively. The results of their comparisons are summarized in <ref type="table">Table 3 (Lower part)</ref>. As one expects, full attention Transformers can outperform our sparse attention counterparts no matter they are equipped with convolutional self-attention or not in most cases. However, on traffic-f dataset with strong long-term dependencies, our sparse Transformer with convolutional self-attention can get better results than the canonical one and, more interestingly, even slightly outperform its full attention counterpart in term of R 0.5 , meaning that our sparse model with convolutional self-attention can capture long-term dependencies fairly well. In addition, full attention models under length constraint consistently obtain gains from convolutional self-attention on both electricity-f and traffic-f datasets, showing its effectiveness again. <ref type="table">Table 3</ref>: Average R 0.5 /R 0.9 -loss comparisons between sparse attention and full attention models with/without convolutional self-attention by rolling-day prediction of 7 days. "Full" means models are trained with full attention while "Sparse" means they are trained with our sparse attention strategy. "+ Conv" means models are equipped with convolutional self-attention with kernel size k = 6. Further Exploration In our last experiment, we evaluate how our methods perform on datasets with various granularities compared to our baselines. All datasets except M4-Hourly are evaluated by rolling window 7 times since the test set of M4-Hourly has been provided. The results are shown in <ref type="table" target="#tab_2">Table 4</ref>. These results further show that our method achieves the best performance overall. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose to apply Transformer in time series forecasting. Our experiments on both synthetic data and real datasets suggest that Transformer can capture long-term dependencies while LSTM may suffer. We also showed, on real-world datasets, that the proposed convolutional self-attention further improves Transformer' performance and achieves state-of-the-art in different settings in comparison with recent RNN-based methods, a matrix factorization method, as well as classic statistical approaches. In addition, with the same memory budget, our sparse attention models can achieve better results on data with long-term dependencies. Exploring better sparsity strategy in self-attention and extending our method to better fit small datasets are our future research directions.</p><p>training on these two datasets are very unstable with Adam. Rather, we found that BERTAdam <ref type="bibr" target="#b37">[38]</ref> 8 , a variant of Adam with warmup and learning rate annealing, can stabilize the training process on these two datasets.</p><p>For electricity-c and traffic-c, we take 500K training windows while for electricity-f and traffic-f, we select 125K and 200K training windows, respectively. For wind, M4-Hourly and solar, we choose 10K, 50K and 50K training windows, respectively. The window selection strategy is described above. For our Transformer models, all of them use H = 8 heads and the dimension of position embedding and time series ID embedding are all 20. All of our models have 3 layers except experiments on electricity-f and traffic-f, where our models use 6 and 10 layers, respectively. The data before the forecast start time is used as the training set and split into two partitions. For each experiment on real-world datasets, we train our model on the first partition of the training set containing 90% of the data 5 times with different random seeds and we pick the one that has the minimal negative log-likelihood on the remaining 10%. The results on test set corresponding to minimal negative log-likelihood on the remaining 10% are reported. All models are trained on GTX 1080 Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation</head><p>Following the experimental settings in <ref type="bibr" target="#b5">[6]</ref>, one week data from 9/1/2014 00:00 (included) <ref type="bibr">9</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Implementation of sparse attention and its memory cost</head><p>During the implementation of our sparse attention, ?l ? |I k L |, one can allow such cell l to densely attend all its past cells and itself without increasing space usage as query-key matching are parallelly computed in reality and maximum number of cells that a cell can attend is reached by cell L.</p><p>Our current implementation of LogSparse attention is via a mask matrix and its relative memory usage is calculated ideally from the attention matrix, which is the memory bottleneck of Transformer.</p><p>For electricity-f dataset, we choose Le 1 = 768 with subsequence length L e 1 sub = Le 1 /8 = 96 and local attention length L e 1 loc = log 2 (L e 1 sub ) = 7 in each subsequence for our sparse attention model, and Le 2 = 293 in its full attention counterpart. We stack the same layers on both sparse attention and full attention models. Hence, we can make sure that their whole memory usage is comparable if their memory usage is comparable in every layer. In sparse attention equipped with local attention, every cell attends to 2 * L e 1 loc = 14 cells in each subsequence at most, causing a cell attend to 2 * L e 1 loc * Le 1 /L e 1 sub = 14 * 8 = 112 cells at most in total. Therefore, we get the memory usage of sparse attention in each layer is Le 1 * 2 * L e 1 loc * Le 1 /L e 1 sub = 768 * 112 = L 2 e 2 ? 293. Following such setting, the memory usage of the sparse attention model is comparable to that of the full attention model. For traffic-f dataset, one can follow the same procedure to check the memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Visualization of attention matrix</head><p>Here we show an example of learned attention patterns in the masked attention matrix of a head within canonical Transformer's last layer on traffic-c dataset. <ref type="figure" target="#fig_8">Figure 6 (a)</ref> is a time series window containing 8 days in traffic-c . The time series obviously demonstrates both hourly and daily patterns. From its corresponding masked attention matrix, as shown in <ref type="figure" target="#fig_8">Figure 6</ref> (b), we can see that for points in weekdays, they heavily attend to previous cells (including itself) at the same time in weekdays while points on weekends tend to only attend to previous cells (including itself) at the same time on weekends. Hence, the model automatically learned both hourly and daily seasonality, which is the key to accurate forecasting. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The comparison between canonical and our convolutional self-attention layers. "Conv, 1" and "Conv, k" mean convolution of kernel size {1, k} with stride 1, respectively. Canonical self-attention as used in Transformer is shown in (b), may wrongly match point-wise inputs as shown in (a). Convolutional self-attention is shown in (d), which uses convolutional layers of kernel size k with stride 1 to transform inputs (with proper paddings) into queries/keys. Such locality awareness can correctly match the most relevant features based on shape matching in (c). score in layer 2 attn score in layer 6 attn score in layer 10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Learned attention patterns from a 10-layer canonical Transformer trained on traffic-f dataset with full attention. The green dashed line indicates the start time of forecasting and the gray dashed line on its left side is the conditional history. Blue, cyan and red lines correspond to attention patterns in layer 2, 6 and 10, respectively, for a head when predicting the value at the time corresponding to the green dashed line. a) Layer 2 tends to learn shared patterns in every day. b) Layer 6 focuses more on weekend patterns. c) Layer 10 further squeezes most of its attention on only several cells in weekends, causing most of the others to receive little attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>(a) An example time series with t 0 = 96. Black line is the conditional history while red dashed line is the target. (b) Performance comparison between DeepAR and canonical Transformer along with the growth of t 0 . The larger t 0 is, the longer dependencies the models need to capture for accurate forecasting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 (</head><label>4</label><figDesc>b) presents the performance of DeepAR and Transformer on the synthetic datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Training curve comparison (with proper smoothing) among kernel size k ? {1, 3, 9} in traffic-c (left) and electricity-c (right) dataset. Being aware of larger local context size, the model can achieve lower training error and converge faster. estimates of 28 countries' energy potential from 1986 to 2015 as a percentage of a power plant's maximum output. The M4-Hourly contains 414 hourly time series from M4 competition [24].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>1d 0.060/0.030 0.058/0.030 0.057/0.031 0.057/0.031 0.059/0.034 traffic-c 1d 0.134/0.089 0.124/0.085 0.123/0.083 0.123/0.083 0.122/0.081</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>(a): An example time series window in traffic-c dataset. (b): Corresponding learned attention patterns in the masked attention matrix of a head within the last layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>R 0.5 /R 0.9 -loss of datasets with various granularities. The subscript of each dataset presents the forecasting horizon (days). TRMF is not applicable for M4-Hourly 2d and we leave it blank. For other datasets, TRMF outputs points predictions, so we only report R 0.5 . denotes results from[10].</figDesc><table><row><cell></cell><cell cols="2">electricity-f 1d traffic-f 1d</cell><cell>solar 1d</cell><cell>M4-Hourly 2d</cell><cell>wind 30d</cell></row><row><cell>TRMF</cell><cell>0.094/-</cell><cell>0.213/-</cell><cell>0.241/-</cell><cell>-/-</cell><cell>0.311/-</cell></row><row><cell>DeepAR</cell><cell>0.082/0.063</cell><cell>0.230/0.150</cell><cell cols="2">0.222/0.093 0.090 /0.030</cell><cell>0.286/0.116</cell></row><row><cell>Ours</cell><cell>0.074/0.042</cell><cell>0.139/0.090</cell><cell>0.210 /0.082</cell><cell>0.067 /0.025</cell><cell>0.284/0.108</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>For solar, we leave the last 7 days in August as test set. For wind, last 210 days in year 2015 are left as test set. For M4-Hourly, its training and test set are already provided. After training on previous settings, we evaluate our models on aforementioned test intervals and report standard quantile loss (R0.5 and R0.9) on the test sets.</figDesc><table><row><cell>on electricity-c</cell></row><row><cell>and 6/15/2008 17:00 (included) 10 on traffic-c is left as test sets. For electricity-f and traffic-f</cell></row><row><cell>datasets, one week data from 8/31/2014 00:15 (included) and 6/15/2008 17:00 (included) 11 is left as test sets,</cell></row><row><cell>respectively.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Applying other bases is trivial so we don't discuss other bases here for simplicity and clarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.nrel.gov/grid/solar-power-data.html 7 https://www.kaggle.com/sohier/30-years-of-european-wind-generation</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Materials</head><p>A.1 Proof of <ref type="bibr">Theorem 1</ref> Proof. According to the attention strategy in LogSparse Transformer, in each layer, cell l could attend to the cells with indicies in I k l = {l ? 2 log 2 l , l ? 2 log 2 l ?1 , l ? 2 log 2 l ?2 , ? ? ? , l ? 2 0 , l}. To ensure that every cell receives the information from all its previous cells and itself, the number of stacked layersk l should satisfy that Sk l l = {j : j ? l} for l = 1, ? ? ? , L. That is, ? l and j ? l, there is a directed path P jl = (j, p1, p2, ? ? ? , l) withk l edges, where j ? I 1 p 1 , p1 ? I 2 p 2 , ? ? ? , pk l ?1 ? Ik l l . We prove the theorem by constructing a path from cell j to cell l, with length (number of edges) no larger than log 2 l + 1. Case j = l is trivial, we only need to consider j &lt; l here. Consider the binary representation of l ? j, l ? j =</p><p>and mp is the p th element of {m sub }. A feasible path from j to l is P jl = {j, j + 2 m 0 , j + 2 m 0 + 2 m 1 , ? ? ? , l}. The length of this path is |{m sub }|, which is no larger than log 2 (l ? j) + 1. So</p><p>Furthermore, by reordering {m sub }, we can generate multiple different paths from cell j to cell l. The number of feasible paths increases at a rate of O( log 2 (l ? j) !) along with l. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training</head><p>, where T is the length of all available observations and M is the number of different time series. The dataset statistics is shown as <ref type="table">Table 5</ref>. Following <ref type="bibr" target="#b2">[3]</ref>, we create training instances by selecting windows with fixed history length t0 and forecasting horizon ? but varying the start point of forecasting from each of the original long time series. As a follow-up of <ref type="bibr" target="#b2">[3]</ref>, we sample training windows through weight sampling strategy in <ref type="bibr" target="#b2">[3]</ref>. Note that during selecting training windows, data in the test set can never be accessed. As a result, we get a training dataset with N sliding windows {zi,1:t 0 +? , xi,1:t 0 +? } N i=1 . For positional encoding in Transformer, we use learnable position embedding. For covariates, following <ref type="bibr" target="#b2">[3]</ref>, we use all or part of year, month, day-of-the-week, hour-of-the-day, minute-of-the-hour, age and time-series-ID according to the granularities of datasets. age is the distance to the first observation in that time series <ref type="bibr" target="#b2">[3]</ref>. Each of them except time series ID has only one dimension and is normalized to have zero mean and unit variance (if applicable). For time-series-ID, it has the same dimension as position embedding through ID embedding matrix so that they can be summed up (with broadcasting). The summation is then concatnated with aforementioned other covariates as the input of 1st layer in Transformer.</p><p>DeepAR <ref type="bibr" target="#b2">[3]</ref> uses an encoder-decoder fashion, where the encoder is the same as the decoder and the final hidden state of the encoder is used to initialize the hidden state of the decoder. Such an architecture can be seen as a decoder-only network as the encoder and decoder are the same, where the objective is to predict the distribution of next point according to current input and last hidden state. Inspired by this observation, we use Transformer decoder-only mode <ref type="bibr" target="#b35">[36]</ref> to model time series. Similar to <ref type="bibr" target="#b36">[37]</ref>, a fully-connected layer on the top of Transformer is stacked, which outputs the parameters of the probability distribution after scaling for the next time point with appropriate transformations. For example, for parameters requiring positivity, a softplus activation is applied. We use the same scale handling technique as in <ref type="bibr" target="#b2">[3]</ref> to scale our input and output of our models. We refer readers to <ref type="bibr" target="#b2">[3]</ref> for more details of scale handling. In our experiments, we use Gaussian likelihood since our training datasets are real-valued data. Note that one can also use other likelihood models, e.g. negative-binomial likelihood for positive count data. In synthetic datasets, we only count log-likelihood from t0 + 1 to t0 + ? . On real-world datasets, we not only count log-likelihood from t0 + 1 to t0 + ? , but also include the log-likelihood from 1 to t0, similar to training in <ref type="bibr" target="#b2">[3]</ref> and pre-training in <ref type="bibr" target="#b36">[37]</ref>.</p><p>During training, we use vanilla Adam optimizer <ref type="bibr" target="#b27">[28]</ref> with early stopping except experiments on electricity-f and traffic-f to maximize the log-likelihood of each training instance. Our preliminary study show that</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Time series analysis by state space methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siem Jan Koopman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04110</idno>
		<title level="m">Probabilistic forecasting with autoregressive recurrent networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep state space models for time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><forename type="middle">W</forename><surname>Syama Sundar Rangapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7785" to="7794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00073</idno>
		<title level="m">Long-term forecasting using tensor-train rnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Danielle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyang</forename><surname>Maddix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00098</idno>
		<title level="m">Deep factors with gaussian processes for forecasting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sharp nearby, fuzzy far away: How neural language models use context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04623</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Some recent advances in forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gwilym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gwilym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greta</forename><forename type="middle">M</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ljung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for highdimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Effective bayesian modeling of groups of related count time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Chapados</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3738</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A multi-horizon quantile recurrent forecaster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofeng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kari</forename><surname>Torkkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Madeka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11053</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-step deep autoregressive forecasting with latent states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://roseyu.com/time-series-workshop/submissions/2019/timeseries-ICML19_paper_19.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An improved relative self-attention mechanism for transformer with application to music generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A time-restricted self-attention layer for asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5874" to="5878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The m4 competition: Results, findings, conclusion and way forward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Makridakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Spiliotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Assimakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="802" to="808" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrushikesh</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brando</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="519" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep vs. shallow networks: An approximation theory perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hrushikesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="829" to="848" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>SSW</publisher>
			<biblScope unit="volume">125</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Borovykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Bohte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelis W</forename><surname>Oosterlee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04691</idno>
		<title level="m">Conditional time series forecasting with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09224</idno>
		<title level="m">Gpu kernels for block-sparse weights</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?aglar</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09025</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Recurrent batch normalization</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning: A generic approach for extreme condition traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Demiryurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="777" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Forecasting with artificial neural networks:: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael Y</forename><surname>Eddy Patuwo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="62" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Time-series extreme event forecasting with neural networks at uber</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawek</forename><surname>Smyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Value in 00:00 is the aggregation of original value in 00:15</title>
		<idno>00:30, 00:45 and 01:00</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Value in 17:00 is the mean of original value in 17:00</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Value in 17:00 is the mean of original value in 17</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
