<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Point Cloud Pre-training via Occlusion Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Point Cloud Pre-training via Occlusion Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a simple pre-training approach for point clouds. It works in three steps: 1. Mask all points occluded in a camera view; 2. Learn an encoder-decoder model to reconstruct the occluded points; 3. Use the encoder weights as initialisation for downstream point cloud tasks. We find that even when we pre-train on a single dataset (ModelNet40), this method improves accuracy across different datasets and encoders, on a wide range of downstream tasks. Specifically, we show that our method outperforms previous pre-training methods in object classification, and both part-based and semantic segmentation tasks. We study the pre-trained features and find that they lead to wide downstream minima, have high transformation invariance, and have activations that are highly correlated with part labels. Code and data are available at: https://github.com/hansen7/OcCo</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There has been a flurry of exciting new point cloud models for object detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b64">64]</ref> and segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b65">65]</ref>. These methods rely on large scale point cloud datasets that are labelled. Unfortunately, labelling point clouds is challenging for a number of reasons: (1) Point clouds can be sparse, occluded, and at low resolutions, making the identity of points ambiguous; <ref type="bibr" target="#b1">(2)</ref> Datasets that are not sparse can easily reach hundreds of millions of points (e.g., small dense point clouds for object classification <ref type="bibr" target="#b63">[63]</ref> and large vast point clouds for reconstruction <ref type="bibr" target="#b66">[66]</ref>); (3) Labelling individual points or drawing 3D bounding boxes are both more time-consuming and errorprone than labelling 2D images <ref type="bibr" target="#b49">[50]</ref>. These challenges have impeded the deployment of point cloud models into new real world settings where labelled data is scarce.</p><p>However, current 3D sensing modalities (i.e., 3D scanners, stereo cameras, lidars) have enabled the creation of large unlabelled repositories of point cloud data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b40">41]</ref>. This has inspired a recent line of work on unsupervised pretraining methods to learn point cloud model initialisation. Initial work used latent generative models such as generative adversarial networks (GANs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b53">54]</ref> and autoen- Jigsaw <ref type="bibr" target="#b40">[41]</ref> cTree <ref type="bibr" target="#b42">[43]</ref> OcCo (ours) <ref type="figure">Figure 1</ref>: The relative improvement over random initialisation of multiple pre-training methods: Jigsaw <ref type="bibr" target="#b41">[42]</ref>, cTree <ref type="bibr" target="#b43">[44]</ref>, and OcCo (ours) for various downstream tasks.</p><p>coders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">59]</ref>. These have been recently outperformed by self-supervised objectives <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b61">61]</ref>. Inspired by this recent line of work, we propose Occlusion Completion (OcCo), an unsupervised pre-training method that consists of: (a) a mechanism to generate masked point clouds via view-point occlusions, and (b) a completion task to reconstruct the occluded point cloud. The idea of occlusion+completion is grounded in three observations: <ref type="bibr" target="#b0">(1)</ref> A pre-trained model that is accurate at completing occluded point clouds needs to understand spatial and semantic properties of these point clouds. (2) 3D scene completion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref> has been shown to be a useful auxiliary task for learning representations for visual localisation <ref type="bibr" target="#b42">[43]</ref>. (3) Mask-based completion tasks have become the de facto standard for learning pre-trained representations in natural language processing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref> and are widely used in pre-training for images <ref type="bibr" target="#b34">[35]</ref> and graphs <ref type="bibr" target="#b23">[24]</ref>.</p><p>We demonstrate that pre-training on a single object-level dataset (ModelNet40) can improve the performance of a range of downstream tasks, even on completely different datasets. Specifically we find that OcCo has the following properties compared to other initialisation techniques: 1) Improved sample efficiency in few-shot learning experiments; 2) Improved generalisation in object classification, object part segmentation, and semantic segmentation; 3) Wider local minima found after fine-tuning; 4) More seman-  <ref type="figure">Figure 2</ref>: Overview of OcCo. 1. Take any point cloud dataset and generate occluded objects for each input by (a) randomly sampling a camera view-point, and (b) removing points hidden from that view-point (for all experiments we use the same occluded dataset generated from ModelNet40); 2. Train an encoder-decoder model to complete the occluded point clouds (the encoder can be any model that learns representations of point clouds, the decoder can be any completion model); 3. Use the learned encoder weights as initialisation for any downstream task (e.g., few-shot learning, object classification, part/semantic segmentation). We show that OcCo outperforms a variety of pre-training methods across multiple models and tasks.</p><p>tically meaningful representations as described via network dissection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>; 5) Better clustering quality under jittering, translation, and rotation transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised pre-training is gaining popularity due to its success in many problem settings, such as natural language understanding <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32]</ref>, object detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, graph learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, and visual localisation <ref type="bibr" target="#b42">[43]</ref>. Currently, the two most common unsupervised pre-training methods for point clouds are based on (i) generative modelling, and (ii) self-supervised learning. Work in generative modelling includes models based on generative adversarial networks (GANs) <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14]</ref>, autoencoders <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">59]</ref>, normalizing flows <ref type="bibr" target="#b58">[58]</ref>, and approximate convex decomposition <ref type="bibr" target="#b11">[12]</ref>.</p><p>However, generative models for unsupervised pretraining on point clouds have recently been outperformed by self-supervised approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">56]</ref>. These approaches work by learning to predict key geometric properties of point clouds that are invariant across datasets. Specifically, <ref type="bibr" target="#b41">[42]</ref> propose a pre-training procedure based on rearranging permuted point clouds. It works by splitting a point cloud into k 3 voxels, randomly permuting the voxels, and then training a model to predict the original voxel location of each point. The idea is that the pre-trained model implicitly learns about the geometric structure of point clouds by learning this rearrangement. However, there are two key issues with this objective: 1. The voxel representation is not permutation invariant. Thus, the model could learn very different representations if point clouds are rotated or translated; 2. Point clouds generated from real objects and scenes will have very different structure from randomly permuted clouds, so it is unclear why pre-trained weights that are accurate at rearrangement will be good initialisation for object classification or segmentation models. Another work <ref type="bibr" target="#b43">[44]</ref> uses cover trees <ref type="bibr" target="#b5">[6]</ref> to hierarchically partition points for few-shot learning. They then train a model to classify each point to their assigned partitions. However, because cover trees are designed for fast nearest neighbour search, they may arbitrarily partition semantically-contiguous regions of point clouds (e.g., airplane wings, car tires) into different regions of the hierarchy, and so ignore key point cloud geometry. A third work, PointContrast <ref type="bibr" target="#b56">[56]</ref>, uses contrastive learning to pre-train weights for point clouds of scenes. Their method uses known point-wise correspondences between different views of a complete 3D scene. These point-wise correspondences require post-processing the data by registering different depth maps into a single 3D scene. Thus, their method can only be applied to static scenes that have been registered, limiting the applicability of the approach: we leave a comparison between OcCo and PointContrast to future work. In what follows we will show that unsupervised pre-training based on a simple self-supervised objective: completing occluded point clouds, produces weights that outperform <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b43">[44]</ref> on downstream tasks.</p><p>Completing 3D shapes to learn model initialisations is not new, <ref type="bibr" target="#b42">[43]</ref> used scene completion <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref> as a pretraining task to initialise 3D voxel descriptors for visual localisation. To do so, they generated partial voxelised scenes based on depth images and trained a variational autoencoder for completion. Differently, our focus is to describe a technique to learn an initialisation for point cloud models. Our aim is for this pre-trained initialisation to improve a variety of downstream tasks including few-shot learning, object classification, and segmentation, on a variety of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Occlusion Completion</head><p>The overall idea of our approach is shown in <ref type="figure">Figure 2</ref>. Our observation is that by occluding point clouds based on different view-points then learning a model to complete them, the weights of the completion model can be used as initialisation for downstream tasks (e.g., classification, segmentation). This approach not only improves accuracy in few-shot learning settings but also the final generalisation accuracy in fully-supervised tasks.</p><p>Throughout we define point clouds P as sets of points in 3D Euclidean space, P = {p 1 , p 2 , ..., p n }, where each point p i is a vector of both coordinates (x i , y i , z i ) and other features (e.g. colour and normal). We begin by describing the components that make up our occlusion mapping o(?). Then we detail how to learn a completion model c(?), giving pseudo-code and the architectural details in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating Occlusions</head><p>We define a randomised occlusion mapping o : P ? P (where P is the space of all point clouds) from a full point cloud P to an occluded point cloudP. This mapping con-structsP by removing points from P that cannot be seen from a particular view-point. This is accomplished in three steps: (1) A projection of the complete point cloud (in a world reference frame) into the coordinates of a camera reference frame (which specifies the view-point); (2) Identification of the points that are occluded in the camera viewpoint; (3) A projection of the points back from the camera reference frame to the world reference frame.</p><p>Viewing the point cloud from a camera. A camera defines a projection from a 3D world reference frame into a distinctive 3D camera reference frame. It does so by specifying a camera model and a camera view-point from which the projection occurs. While any camera model can be used, for illustration consider the simplest camera model: the pinhole camera. View-point projection for the pinhole camera is given by a simple linear equation:</p><formula xml:id="formula_0">? ? x cam y cam z cam ? ? = ? ? f ? w/2 0 f h/2 0 0 1 ? ? intrinsic [ K ] ? ? r 1 r 2 r 3 t 1 r 4 r 5 r 6 t 2 r 7 r 8 r 9 t 3 ? ? rotation | translation [ R | t ] ? ? ? ? x y z 1 ? ? ? ? (1)</formula><p>In the above, (x, y, z) are the original point cloud coordinates (in a world reference), the camera viewpoint is de-scribed by the concatenation of a rotation matrix (r entries) with a translation vector (t entries) describing the camera view-point, and the final matrix is the camera intrinsics (f specifies the camera focal length, ? is the skewness between the x and y axes in the camera, and w, h are the width and height of the camera image). Given these, the final coordinates (x cam , y cam , z cam ) are the positions of the point in the camera reference frame. We will refer to the intrinsic matrix as K and the rotation/translation matrix as [R|t].</p><p>Determining occluded points. We can think of the point (x cam , y cam , z cam ) in multiple ways: (a) a 3D point in the camera reference frame; (b) a 2D pixel with coordinates (f x cam /z cam , f y cam /z cam ) with a depth of z cam . In this way, some 2D points resulting from the projection may be occluded by others if they have the same pixel coordinates, but appear at a farther depth. To determine which points are occluded, we first use Delaunay triangulation to reconstruct a polygon mesh, then we remove the points which belong to the hidden surfaces that are determined via z-buffering <ref type="bibr" target="#b46">[47]</ref>.</p><p>Mapping back from camera frame to world frame.</p><p>Once occluded points are removed, we re-project the point cloud to the original world reference frame, via the inverse transformation of eq. (1). Thus, the randomised occlusion mapping o(?) is constructed as follows. Fix an initial point cloud P. Given a camera intrinsics matrix K, sample rota-</p><formula xml:id="formula_1">tion/translation matrices [[R 1 |t 1 ], . . . , [R V |t V ]], where V is the number of views. For each view v ? [V ]</formula><p>, project P into the camera frame of that view-point using eq. (1), find occluded points and remove them, then map all other points back to the world reference using its inverse. This yields the final occluded point cloudP v for each view-point v ? [V ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Completion Task</head><p>Given an occluded point cloudP produced by o(?), the goal of the completion task is to learn a completion mapping c : P ? P fromP to a completed point cloud P. A completion mapping is accurate w.r.t. loss (?, ?) if EP ?o(P) (c(P), P) ? 0. The structure of the completion model c(?) is an "encoder-decoder" network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">60]</ref>. The encoder maps an occluded point cloud to a vector, and the decoder completes the point cloud. After pre-training, the encoder weights can be used as initialisation for downstream tasks. In the appendix we give pseudocode for OcCo. We describe details of the completion model architecture in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present the setup of pre-training (Section 4.1) and downstream fine-tuning (Section 4.2). Then, the results of few-shot learning, object classification, part and semantic segmentation are shown in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">OcCo Pre-Training Setup</head><p>Dataset. For all experiments, we use ModelNet40 <ref type="bibr" target="#b54">[55]</ref> as the pre-training dataset. ModelNet40 includes 12,311 synthesised CAD objects from 40 categories, and the dataset is divided into 9,843/2,468 objects for training and testing, respectively. We construct a pre-training dataset using the training set. Occluded point clouds are generated with camera intrinsic parameters {f =1000, ?=0, ?=1600, h=1200}. For each point cloud, we randomly select 10 viewpoints, where the yaw, pitch and roll angles are uniformly chosen between 0 and 2?, and the translation is set as zero.</p><p>Architecture. As described above, our pre-training completion model c(?) is an encoder-decoder model. To showcase that our pre-training method is agnostic to architectures, we choose three different encoders, including Point-Net <ref type="bibr" target="#b36">[37]</ref>, PCN <ref type="bibr" target="#b60">[60]</ref> and DGCNN <ref type="bibr" target="#b52">[53]</ref>. These encoders map an occluded point cloud into a 1024-dimensional vector. We adapt the folding-based decoder from <ref type="bibr" target="#b60">[60]</ref> to complete an occluded point cloud in two steps. The decoder first outputs a coarse shape consisting of 1024 points,P coarse , then warps a 4?4 2D grid around each point inP coarse to reconstruct a fine shape,P f ine , which consists of 16384 points. We use the Chamfer Distance (CD) as a closeness measure between predictionP and ground-truth P:</p><formula xml:id="formula_2">CD(P, P) = 1 |P| x?P min x?P ||x ? x|| 2 + 1 |P| x?P min x?P ||x ?x|| 2 .<label>(2)</label></formula><p>The loss of the completion model is a weighted sum of the Chamfer distances on the coarse and fine shapes:</p><formula xml:id="formula_3">:= CD(P coarse , P coarse ) + ?CD(P f ine , P f ine ). (3)</formula><p>Hyperparameters. We use the Adam <ref type="bibr" target="#b24">[25]</ref> optimiser with no weight decay (L2 regularisation). The learning rate is set to 1e-4 initially and is decayed by 0.7 every 10 epochs. We pre-train the models for 50 epochs. The batch size is 32, and the momentum of batch normalisation is 0.9. The coefficient ? in eq. (3) is set as 0.01 for the first 10000 training iterations, then increased to 0.1, 0.5 and 1.0 after 10000, 20000 and 50000 training steps, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-Tuning Setup</head><p>Few-shot learning. Few-shot learning (FSL) aims to train accurate models with very limited data. A typical setting of FSL is "K-way N -shot". During training, K classes  <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53]</ref>. We follow the same setting as cTree, where we pre-train the models in a "K-way N -shot" configuration on ModelNet40, before evaluating on Model-Net40 and ScanObjectNN.</p><p>Object classification. Given an object represented by a set of points, object classification predicts the class that the object belongs to. We use three benchmarks: Model-Net40 <ref type="bibr" target="#b54">[55]</ref>, ScanNet10 <ref type="bibr" target="#b38">[39]</ref> and ScanObjectNN <ref type="bibr" target="#b48">[49]</ref>, the dataset statistics are summarised in <ref type="table" target="#tab_0">Table 1</ref>. The latter two are more challenging since they consist of occluded objects from the real-world indoor scans. We use the same settings as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b52">53]</ref> for fine-tuning. Specifically, for PCN and Point-Net, we use the Adam optimizer with an initial learning rate of 1e-3, and the learning rate is decayed by 0.7 every 20 epochs with the minimum value 1e-5. For DGCNN, we use the SGD optimizer with momentum 0.9 and weight decay 1e-4. The learning rate starts from 0.1 and then decays using cosine annealing <ref type="bibr" target="#b30">[31]</ref> with the minimum value 1e-3. We use dropout <ref type="bibr" target="#b45">[46]</ref> in the fully connected layers before the softmax output layer. The dropout rate is set to 0.7 for PointNet and PCN and is set to 0.5 for DGCNN. For all three models, we train them for 200 epochs with batch size 32. We report the test results based on three runs in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Part segmentation. Part segmentation is a challenging fine-grained 3D recognition task. The mission is to predict the part category label (e.g., chair leg, cup handle) of each point for a given object. To evaluate the effectiveness of OcCo pre-training, we use ShapeNetPart <ref type="bibr" target="#b2">[3]</ref> benchmark, which contains 16,881 objects from 16 categories and has 50 parts in total. Each object is represented by 2048 points. For PCN and PointNet, we use the Adam optimizer with an initial learning rate of 1e-3, and the learning rate is decayed by 0.5 every 20 epochs with the minimum value 1e-5. For DGCNN, we use an SGD optimizer with momentum 0.9 and weight decay 1e-4. The learning rate starts from 0.1 and then decays using cosine annealing <ref type="bibr" target="#b30">[31]</ref> with the minimum value 1e-3. We train the models for 250 epochs with batch size 16. We use the same post-processing during testing as <ref type="bibr" target="#b36">[37]</ref> and report the results over three runs in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Semantic segmentation. Semantic segmentation predicts the semantic object category of each point under an indoor/outdoor scene. We use S3DIS benchmark <ref type="bibr" target="#b2">[3]</ref> for indoor scene segmentation and SensatUrban benchmark <ref type="bibr" target="#b20">[21]</ref> for outdoor scene segmentation. S3DIS contains 3D scans collected via Matterport scanners in 6 different places, encompassing 271 rooms and 13 semantic classes. While SensatUrban consists of over three billion annotated points, covering large areas in a total of 7.6 km 2 from three UK cities (Birmingham, Cambridge, and York). Each point in SensatUrban is labelled as one of 13 semantic classes. We use the same pre-processing, post-processing and training settings as <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b52">53]</ref>. Each point is described by a 9dimensional vector (coordinates, RGBs and normalised location). We train all the models for 100 epochs with batch size 24. We report the results based on three runs in <ref type="table" target="#tab_4">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-Tuning Results</head><p>Few-shot learning. We report the experimental results on few-shot learning in <ref type="table" target="#tab_1">Table 2</ref>. We colour the best results with blue for each encoder and bold the overall best score for each dataset. We use the same colouring scheme in all subsequent results. We find that OcCo outperforms both few-shot baselines Jigsaw <ref type="bibr" target="#b41">[42]</ref> and cTree <ref type="bibr" target="#b43">[44]</ref> in-domain (Mod-elNet40) and cross-domain (ScanObjectNN). We believe this is due to the fact that the occlusions OcCo generates will be due to the geometric structure of the object, whereas the voxel permutations of <ref type="bibr" target="#b41">[42]</ref> and the cover tree partitioning of <ref type="bibr" target="#b43">[44]</ref> may destroy aspects of this structure.</p><p>Object classification. <ref type="table" target="#tab_2">Table 3</ref> compares OcCo with random and Jigsaw <ref type="bibr" target="#b41">[42]</ref> initialisation on object classification. <ref type="bibr" target="#b0">1</ref> We show that OcCo-initialised models outperform these baselines on all datasets. OcCo performs well not only on the in-domain dataset (ModelNet), but also on cross-domain datasets (ScanNet and ScanObjectNN). The improvements are consistent across the three encoders. In the following section we will provide one explanation: the local minima found after fine-tuning an OcCo-based initialisation appear to be wider than those found using other initialisations.</p><p>Object part segmentation. <ref type="table" target="#tab_3">Table 4</ref> compares OcCoinitialisation with random and Jigsaw <ref type="bibr" target="#b41">[42]</ref> initialisation on object part segmentation. We observe that OcCo-initialised models outperform the others in terms of overall accuracy and mean class IoU. These results are consistent across various encoders. We further analyse why OcCo helps the encoders better recognise the object parts with feature visualisation and concept detection in Section 5.</p><p>Semantic segmentation. We compare random, Jigsaw and OcCo initialisation on both indoor and outdoor semantic segmentation tasks. For S3DIS, we evaluate the trained models using 6-fold cross-validation following <ref type="bibr" target="#b2">[3]</ref>, and report the scores in <ref type="table" target="#tab_4">Table 5</ref>. It is clear that OcCo-initialised models outperform random and Jigsaw-initialised ones. For SensatUrban, we report the scores in <ref type="table" target="#tab_5">Table 6</ref>. We observe that OcCo outperforms random initialisation and Jigsaw initialisation for semantic categories that are included in the pre-training dataset, such as cars. For classes that are not included in ModelNet40, OcCo is competitive with the other methods. This makes sense as the geometries of these objects are likely not well understood by the learned initialisations. Ultimately, we find it encouraging that OcCo which learns representations at the object-level can still improve generalisation on segmentation on outdoor scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis</head><p>In this section, we first show that OcCo pre-training leads to a fine-tuned model that converges to a local minimum that is flatter than other initialisations. Then we evaluate the learned representations from OcCo with feature visualisation, semantic concept detection and unsupervised mutual    Visualisation of optimisation landscape. We follow the same procedure of <ref type="bibr" target="#b27">[28]</ref> to visualise the loss landscapes of random, Jigsaw and OcCo initialised PointNet in <ref type="figure" target="#fig_3">Figure 4</ref>. All three models are fine-tuned on ScanObjectNN with the training settings described in Section 4.2. For visualisation, we use two random vectors, ? and ?, to perturb the finetuned parameters ? * and obtain corresponding loss values.</p><p>The 2D plot f (?, ?) is defined as:</p><formula xml:id="formula_4">f (?, ?) = L (? * + ?? + ??)<label>(4)</label></formula><p>where each filter in ? and ? is normalised w.r.t the corresponding filter in ? * . ? and ? have the same ranges of [?1, 1]. We observe that the model with OcCo pre-training can converge to a flatter local minimum, which is known to have better generalisation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Visualisation of learned features. We use feature visualisation to explore what a pre-trained model has learned about point cloud objects before fine-tuning. In <ref type="figure" target="#fig_2">Figure 3</ref>, we visualise the features/embeddings of the objects from the test split of ModelNet40. We colour the points according to their channel activations. The larger the activation value is, the darker the colour will be. We observe that   the pre-trained encoder can learn low-level geometric primitives, e.g., planes, cylinders and cones, in the early stage. While it later recognises more complex shapes like wings, leaves and upper bodies. We further use t-SNE to visualise the object embeddings on ShapeNet10. We notice that distinguishable clusters are formed after pre-training. Thus, it seems that OcCo can learn features that are useful to distinguish different parts of an object or a scene. These features will be beneficial to downstream tasks, e.g., object classification and scene segmentation.</p><p>Unsupervised mutual information probe. We hypothesise that a pre-trained model without fine-tuning can learn label information in an unsupervised fashion, i.e., zeroshot learning on cross-domain datasets. To validate, we utilise OcCo-PointNet to extract global features for objects from ShapeNet10 and ScanObjectNN. Then, we cluster the extracted embeddings with an unsupervised clustering method, K-means (where K is set to the number of object categories). To evaluate the clustering quality, we calculate the adjusted mutual information (AMI) <ref type="bibr" target="#b32">[33]</ref> between the generated and the ground-truth clusters. AMI reaches 1 if two clusters are identical, while it has an expected value of 0 for a random categorical cluster assignment. Besides, we also study whether the OcCo-PointNet is robust to input transformations. In particular, we consider three transformations, including rotation, translation and jittering. We apply these transformations to an input point cloud before using PointNet for feature/embedding extraction. We compare OcCo with Jigsaw and two hand-crafted point cloud global descriptors: viewpoint feature histogram (VFH) <ref type="bibr" target="#b39">[40]</ref> and M2DP <ref type="bibr" target="#b16">[17]</ref> in <ref type="table" target="#tab_6">Table 7</ref>. We observe that pretraining methods, e.g., Jigsaw and OcCo , can learn more discriminative feature representations than hand-crafted descriptors, while the representations learned from OcCo pretrained encoder are more predictive than Jigsaw based method. These results demonstrate that OcCo is effective for unsupervised feature learning.</p><p>Detection of semantic concepts. We adapt network dissection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> to study whether OcCo pre-trained models can learn semantic concepts in an unsupervised fashion without fine-tuning. Specifically, for each object, we first create an activation mask M k based on the feature map from the kth channel in the network. We assign the i-th entry of M k as 1 if the activation of the i-th point in that feature map is among the top 20%, otherwise the i-th entry is assigned to 0. The concept mask C n marks the points as 1 if they belong to the n-th semantic concept (e.g., chair legs) in the ground truth annotations. Given a set of point clouds D P , we calculate the mean intersection of union (mIoU) scores based on these binary masks:</p><formula xml:id="formula_5">mIoU (k,n) = E P?D P |M k (P) ? C n (P)| |M k (P) ? C n (P)|<label>(5)</label></formula><p>where | ? | is the set cardinality. mIoU (k,n) can be interpreted as how well channel k detects the concept n. In <ref type="figure">Figure 5</ref>, we plot the number of detected concepts (i.e., mIoU (k,n) &gt; 0.5). We conclude that OcCo outperforms Jigsaw in terms of the total number of detected concepts. We visualise some masks from OcCo-PointNet in <ref type="figure">Figure 6</ref>. We observe that OcCo pre-training can capture rich concept information. These results demonstrate that pre-training with OcCo can unsupervisedly learn semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this work, we have demonstrated that Occlusion Completion (OcCo) can learn representations for point clouds that are accurate in few-shot learning, in object classification, and in part and semantic segmentation tasks, as compared to prior work. We performed multiple analyses to explain why this occurs, including a visualisation of the loss landscape, visualisation of learned features, tests of transformation invariance, and quantifying how well the initialisations can learn semantic concepts. In the future, it would be interesting to design a completion model that is explicitly aware of the occlusion procedure. This model would may converge even quicker and require fewer parameters, as this could act as a stronger inductive bias during learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>Completion pre-training Previous point completion models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref> all use an "encoder-decoder" architecture. The encoder maps a partial point cloud to a vector of a fixed dimension, and the decoder reconstructs the full shape.</p><p>In the OcCo experiments, we exclude the last few MLPs of PointNet and DGCNN, and use the remaining architecture as the encoder to map a partial point cloud into a 1024-dimensional vector. We adapt the folding-based decoder design from PCN, which is a two-stage point cloud generator that generates a coarse and a fine-grained output point cloud (Y coarse , Y f ine ) for each input feature. We sketch the network structures of PCN encoder and output layers for downstream tasks in <ref type="figure" target="#fig_5">Figure 7</ref>. We removed all the batch normalisation in the folding-based decoder since we find they bring negative effects in the completion process in terms of loss and convergence rate, this has been reported in image generations <ref type="bibr" target="#b33">[34]</ref>. Also, we find L2 normalisation in the Adam optimiser is undesirable for completion training but brings improvements for the downstream fine-tuning tasks.</p><formula xml:id="formula_6">n ? 3</formula><p>Input Points  We compare the occluded datasets based on ModelNet40 and ShapeNet8 for the OcCo pre-training. We construct the ModelNet Occluded using the methods described in Section 3 and for ShapeNet Occluded we directly use the data provided in the PCN, whose generation method are similar but not exactly the same with ours. Basic statistics of these two datasets are reported in <ref type="table" target="#tab_8">Table 8</ref>. By visualising the objects from the both datasets in <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref>, we show that our generated occluded shapes are more naturalistic and closer to real collected data. We believe this realism will be beneficial for the pre-training. We then test our hypothesis by pre-training models on one of the dataset, and fine tune them on the other. We report these results in <ref type="table" target="#tab_9">Table 9</ref>. Clearly we see that the OcCo models pre-trained on ShapeNet Occluded do not perform as well as the ones pretrained on ModelNet Occluded in most cases. Therefore we choose our generated ModelNet Occluded rather than ShapeNet Occlude <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref> used in for the pre-training.  Re-Implementation details of "Jigsaw" pre-training methods We describe how we reproduce the 'Jigsaw' pre-training methods from <ref type="bibr" target="#b41">[42]</ref>. Following their description, we first separate the objects/chopped indoor scenes into 3 3 = 27 small cubes and assign each point a label indicting which small cube it belongs to. We then shuffle all the small cubes, and train a model to make a prediction for each point. We reformulate this task as a 27-class semantic segmentation, for the details on the data generation and model training, please refer to our released code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablations</head><p>As pointed out by the reviewers, we agree adding more runs will help. To help judge significance, we have ran 10 runs for three settings 2 and computed p-values via t-tests (unpaired, unequal variances) between OcCo and baselines (i.e., Jigsaw or random). We observe that all p-values are below the conventional significance threshold ? = 0.05 (the family-wise error rate is also, using Holm-Bonferroni).</p><p>As suggested by the reviewers, we ran ablations varying the number of object views and categories in <ref type="table" target="#tab_0">Tables 11 and 12</ref>. We use Setting (1) from <ref type="table" target="#tab_0">Table 10</ref> as it is the fastest to run ( * indicates few-shot result in main paper).    <ref type="bibr" target="#b41">[42]</ref> have already proven their methods are better than the prior, here we only systematically compare with theirs. We report the results <ref type="bibr" target="#b2">3</ref> in <ref type="table" target="#tab_0">Table 13</ref>, we can see that all OcCo models achieve superior results compared to the randomly-initialised counterparts, demonstrating that OcCo pre-training helps the generalisation both in-domain and cross-domain. Few-shot learning We use the same setting and train/test split as cTree <ref type="bibr" target="#b43">[44]</ref>, and report the mean and standard deviation across on 10 runs. The top half of the table reports results for eight randomly initialised point cloud models, while the bottom-half reports results on two models across three pre-training methods. We bold the best results (and those whose standard deviation overlaps the mean of the best result). It is worth mentioning cTree <ref type="bibr" target="#b43">[44]</ref> pre-trained the encoders on both datasets before fine tuning, while we only pre-trained once on ModelNet40. The results show that models pre-trained with OcCo either outperform or have standard deviations that overlap with the best method in 7 out of 8 settings. Detailed results of the part segmentation Here in <ref type="table" target="#tab_0">Table 15</ref> we report the detailed scores on each individual shape category from ShapeNetPart, we bold the best scores for each class respectively. We show that for all three encoders, OcCoinitialisation has achieved better results over two thirds of these 15 object classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Visualisation from Completion Pre-Training</head><p>In this section, we show some qualitative results of OcCo pre-training by visualising the input, coarse output, fine output and ground truth at different training epochs and encoders. In <ref type="figure">Figure.</ref> 10, <ref type="figure" target="#fig_8">Figure. 11</ref> and <ref type="figure">Figure.</ref> 12, we notice that the trained completion models are able to complete even difficult occluded shapes such as plants and planes. In <ref type="figure">Figure.</ref> 13 we plot some failure examples of completed shapes, possibly due to their complicated fine structures, while it is worth mentioning that the completed model can still completed these objects under the same category.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualisation on the learned features by OcCo-PointNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Loss landscape visualisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>la n e (4 ) B a g (2 ) C a p (2 ) C a r (4 ) C h a ir (4 ) E a rp h o n e (3 ) G u it a r (3 ) K n if e (2 ) L a m p (4 ) L a p to p (2 ) M o to rb ik e (6 ) M u g (2 ) P is to l (3 ) R o c k e t (3 ) S k a te b o a rd (Number of detected object parts in the 'Feat1' (above), 'Feat2' (middle) and 'Feat3' (below) module of Jigsaw and OcCo-initialised PointNet feature encoder. Digits in the brackets are the number of parts under that object category. 41 st Unit in Feat1, Part: 12, mIoU = 0.606 1018 th Unit in Feat3, Part: 44, mIoU = 0. 619 63 rd Unit in Feat2, Part: 17, mIoU = 0. 584 Visualisation of detected concepts. Parts marked by blue and green are the binary masks based on the feature activations (M k ) and the ground truth labels (C n ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Encoder and Output Layers of PCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Examples from ShapeNet Occluded which fail to depict the underlying object shapes Examples of our generated self-occluded objects from ModelNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>OcCo pre-training with PCN encoder on occluded ModelNet40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>OcCo pre-training with PointNet encoder on occluded ModelNet40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>OcCo pre-training with DGCNN encoder on occluded ModelNet40.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Failure completed examples during OcCo pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of classification datasets , and each category contains N samples. The trained models are then evaluated on the objects from the test split. We compare OcCo with Jigsaw<ref type="bibr" target="#b41">[42]</ref>, and cTree<ref type="bibr" target="#b43">[44]</ref> since it outperforms previous unsupervised methods<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b59">59]</ref> as well as supervised variants</figDesc><table><row><cell>Name</cell><cell>Type</cell><cell cols="2"># Class # Training/Testing</cell></row><row><cell>ModelNet</cell><cell>synthesised</cell><cell>40</cell><cell>9,843 / 2,468</cell></row><row><cell>ScanNet</cell><cell>real scanned</cell><cell>10</cell><cell>6,110 / 1,769</cell></row><row><cell cols="2">ScanObjectNN real scanned</cell><cell>15</cell><cell>2,304 / 576</cell></row><row><cell cols="2">are randomly selected</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Few-shot learning results. We report mean and standard error over 10 runs and bold the best results. Rand 52.0?3.8 57.8?4.9 46.6?4.3 35.2?4.8 PointNet, Jigsaw 66.5?2.5 69.2?2.4 56.9?2.5 66.5?1.4 PointNet, cTree 63.2?3.4 68.9?3.0 49.2?1.9 50.1?1.6 PointNet, OcCo 89.7?1.9 92.4?1.6 83.9?1.8 89.7?1.5 DGCNN, Rand 31.6?2.8 40.8?4.6 19.9?2.1 16.9?1.5 DGCNN, Jigsaw 34.3?1.3 42.2?3.5 26.0?2.4 29.9?2.6 DGCNN, cTree 60.0?2.8 65.7?2.6 48.5?1.8 53.0?1.3 DGCNN, OcCo 90.6?2.8 92.5?1.9 82.9?1.3 86.5?2.2 ScanObjectNN PointNet, Rand 57.6?2.5 61.4?2.4 41.3?1.3 43.8?1.9 PointNet, Jigsaw 58.6?1.9 67.6?2.1 53.6?1.7 48.1?1.9 PointNet, cTree 59.6?2.3 61.4?1.4 53.0?1.9 50.9?2.1 PointNet, OcCo 70.4?3.3 72.2?3.0 54.8?1.3 61.8?1.2 DGCNN, Rand 62.0?5.6 67.8?5.1 37.8?4.3 41.8?2.4 DGCNN, Jigsaw 65.2?3.8 72.2?2.7 45.6?3.1 48.2?2.8 DGCNN, cTree 68.4?3.4 71.6?2.9 42.4?2.7 43.0?3.0 DGCNN, OcCo 72.4?1.4 77.2?1.4 57.0?1.3 61.6?1.2</figDesc><table><row><cell>Baseline</cell><cell>5-way 10-shot 20-shot 10-shot 20-shot 10-way</cell></row><row><cell></cell><cell>ModelNet40</cell></row><row><cell>PointNet,</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Overal accuracy on 3D object classification benchmarks. We reported the mean and standard error over three runs. 2?0.1 89.6?0.1 90.1?0.1 89.3?0.1 89.6?0.2 90.3?0.2 92.5?0.4 92.3?0.3 93.0?0.2 ScanNet 76.9?0.2 77.2?0.2 78.0?0.2 77.0?0.3 77.9?0.3 78.2?0.3 76.1?0.7 77.8?0.5 78.5?0.3 ScanObjectNN 73.5?0.5 76.5?0.4 80.0?0.2 78.3?0.3 78.2?0.1 80.4?0.2 82.4?0.4 82.7?0.8 83.9?0.4</figDesc><table><row><cell>Dataset</cell><cell>Random</cell><cell>PointNet Jigsaw</cell><cell>OcCo</cell><cell>Random</cell><cell>PCN Jigsaw</cell><cell>OcCo</cell><cell>Random</cell><cell>DGCNN Jigsaw</cell><cell>OcCo</cell></row><row><cell>ModelNet</cell><cell>89.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Overall accuracy and intersection of union (mIoU) on ShapeNetPart. We reported the mean and ste over three runs. 2?2.4 82.2?2.8 83.4?1.9 81.3?2.6 81.2?2.9 82.3?2.4 84.4?1.2 84.3?1.2 85.0?1.0</figDesc><table><row><cell></cell><cell></cell><cell>PointNet</cell><cell></cell><cell></cell><cell>PCN</cell><cell></cell><cell></cell><cell>DGCNN</cell></row><row><cell></cell><cell>Random</cell><cell>Jigsaw</cell><cell>OcCo</cell><cell>Random</cell><cell>Jigsaw</cell><cell>OcCo</cell><cell>Random</cell><cell>Jigsaw</cell><cell>OcCo</cell></row><row><cell>OA (%)</cell><cell cols="9">92.8?0.9 93.1?0.5 93.4?0.7 92.3?1.0 92.6?0.9 93.0?0.9 92.2?0.9 92.7?0.9 94.4?0.7</cell></row><row><cell cols="2">mIoU (%) 82.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Overall accuracy (OA) and mean intersection of union (mIoU) on the S3DIS across six folds over three runs. 0?1.4 52.6?1.9 54.9?1.0 51.1?2.4 52.2?1.9 53.4?2.1 54.9?2.1 55.6?1.4 58.0?1.7</figDesc><table><row><cell></cell><cell></cell><cell>PointNet</cell><cell></cell><cell></cell><cell>PCN</cell><cell></cell><cell></cell><cell>DGCNN</cell></row><row><cell></cell><cell>Rand</cell><cell>Jigsaw</cell><cell>OcCo</cell><cell>Rand</cell><cell>Jigsaw</cell><cell>OcCo</cell><cell>Rand</cell><cell>Jigsaw</cell><cell>OcCo</cell></row><row><cell>OA (%)</cell><cell cols="9">78.2?0.7 80.1?1.2 82.0?1.0 82.9?0.9 83.7?0.7 85.1?0.5 83.7?0.7 84.1?0.7 84.6?0.5</cell></row><row><cell cols="2">mIoU (%) 47.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Overall point accuracy (OA), mean class accuracy (mAcc) and mean class intersection of union (mIoU) on Sensat-Urban. We reported the mean and standard error over three runs. We use the same preprocess procedures as PointNet.<ref type="bibr" target="#b52">53</ref>.33 45.10 80.05 93.98 87.05 23.05 19.52 41.80 3.38 43.47 24.20 63.43 26.86 0.00 79.53 PointNet-Jigsaw 87.38 56.97 47.90 83.36 94.72 88.48 22.87 30.19 47.43 15.62 44.49 22.91 64.14 30.33 0.00 77.88 PointNet-OcCo 87.87 56.14 48.50 83.76 94.81 89.24 23.29 33.38 48.04 15.84 45.38 24.99 65.00 27.13 0.00 79.58 PCN 86.79 57.66 47.91 82.61 94.82 89.04 26.66 21.96 34.96 28.39 43.32 27.13 62.97 30.87 0.00 80.06 PCN-Jigsaw 87.32 57.01 48.44 83.20 94.79 89.25 25.89 19.69 40.90 28.52 43.46 24.78 63.08 31.74 0.00 84.42 PCN-OcCo 86.90 58.15 48.54 81.64 94.37 88.21 25.43 31.54 39.39 22.02 45.47 27.60 65.33 32.07 0.00 77.99 DGCNN 87.54 60.27 51.96 83.12 95.43 89.58 31.84 35.49 45.11 38.57 45.66 32.97 64.88 30.48 0.00 82.34 DGCNN-Jigsaw 88.65 60.80 53.01 83.95 95.92 89.85 30.05 43.59 46.40 35.28 49.60 31.46 69.41 34.38 0.00 80.55 DGCNN-OcCo 88.67 61.35 53.31 83.64 95.75 89.96 29.22 41.47 46.89 40.64 49.72 33.57 70.11 32.35 0.00 79.74 information. The analysis demonstrates that OcCo can learn rich and discriminative point cloud features.</figDesc><table><row><cell></cell><cell>OA(%)</cell><cell>mAcc(%)</cell><cell>mIoU(%)</cell><cell>ground</cell><cell>veg</cell><cell>building</cell><cell>wall</cell><cell>bridge</cell><cell>parking</cell><cell>rail</cell><cell>traffic</cell><cell>street</cell><cell>car</cell><cell>footpath</cell><cell>bike</cell><cell>water</cell></row><row><cell>PointNet</cell><cell>86.29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Adjusted mutual information (AMI) under transformations. We report the mean and standard error over 10 random initialisation. Under the 'transformation' column, 'J', 'T', 'R' represent jittering, translation and rotation, respectively.</figDesc><table><row><cell cols="3">Transformation</cell><cell></cell><cell cols="2">ShapeNet10</cell><cell></cell><cell></cell><cell cols="2">ScanObjectNN</cell><cell></cell></row><row><cell>J</cell><cell>T</cell><cell>R</cell><cell>VFH</cell><cell>M2DP</cell><cell>Jigsaw</cell><cell>OcCo</cell><cell>VFH</cell><cell>M2DP</cell><cell>Jigsaw</cell><cell>OcCo</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.12?0.01</cell><cell>0.22?0.03</cell><cell>0.33?0.04</cell><cell>0.51?0.03</cell><cell>0.05?0.02</cell><cell>0.18?0.02</cell><cell>0.29?0.02</cell><cell>0.44?0.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.12?0.02</cell><cell>0.19?0.02</cell><cell>0.32?0.02</cell><cell>0.45?0.02</cell><cell>0.06?0.02</cell><cell>0.17?0.02</cell><cell>0.27?0.02</cell><cell>0.42?0.04</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.13?0.03</cell><cell>0.21?0.02</cell><cell>0.29?0.07</cell><cell>0.38?0.04</cell><cell>0.04?0.02</cell><cell>0.18?0.03</cell><cell>0.24?0.04</cell><cell>0.39?0.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.07?0.03</cell><cell>0.20?0.04</cell><cell>0.28?0.03</cell><cell>0.35?0.05</cell><cell>0.04?0.01</cell><cell>0.16?0.03</cell><cell>0.18?0.09</cell><cell>0.34?0.06</cell></row><row><cell></cell><cell>Random</cell><cell></cell><cell>Jigsaw</cell><cell></cell><cell>OcCo</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Statistics of occluded datasets for OcCo pre-training</figDesc><table><row><cell>Name</cell><cell cols="4"># of Class # of Object # of Views # of Points/Object</cell></row><row><cell>ShapeNet Occluded (PCN and follow-ups)</cell><cell>8</cell><cell>30974</cell><cell>8</cell><cell>1045</cell></row><row><cell>ModelNet Occluded (OcCo )</cell><cell>40</cell><cell>12304</cell><cell>10</cell><cell>20085</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Performance of OcCo pre-trained models with different pre-trained datasets</figDesc><table><row><cell cols="2">OcCo Settings</cell><cell cols="2">Classification Accuracy</cell></row><row><cell cols="4">Encoder Pre-Trained Dataset ModelNet Occ. ShapeNet Occ.</cell></row><row><cell>PointNet</cell><cell>ShapeNet Occ. ModelNet Occ.</cell><cell>81.0 85.6</cell><cell>94.1 95.0</cell></row><row><cell>PCN</cell><cell>ShapeNet Occ. ModelNet Occ.</cell><cell>81.6 85.1</cell><cell>94.4 95.1</cell></row><row><cell>DGCNN</cell><cell>ShapeNet Occ. ModelNet Occ.</cell><cell>86.7 89.1</cell><cell>94.5 95.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>P-values for unpaired (unequal variance) t-tests between OcCo and baselines (across 10 runs). Setting: (1) Few-Shot (10-way 10-shot), ScanObjectNN, DGCNN; (2) Classification, ScanNet, PCN; (3) Segmentation, SensatUrban, PointNet.</figDesc><table><row><cell cols="3">Setting OcCo vs. Rand OcCo vs. Jigsaw</cell></row><row><cell>(1)</cell><cell>10 ?7</cell><cell>10 ?7</cell></row><row><cell>(2)</cell><cell>0.02</cell><cell>0.05</cell></row><row><cell>(3)</cell><cell>0.006</cell><cell>0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Ablation: number of views (5 runs), * =main paper result.</figDesc><table><row><cell># of Views</cell><cell>1</cell><cell>5</cell><cell>10*</cell><cell>20</cell></row><row><cell>PointNet</cell><cell cols="4">44.7?1.8 53.6?1.2 54.9?1.2 54.8?1.0</cell></row><row><cell>DGCNN</cell><cell cols="4">42.7?2.1 56.9?1.4 56.8?1.5 57.0?1.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12</head><label>12</label><figDesc>Linear SVMs We follow the similar procedures from<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b59">59]</ref>, to train a linear Support Vector Machine (SVM) to examine the generalisation of OcCo encoders that are pre-trained on the occluded objects from ModelNet40. For all six classification datasets, we fit a linear SVM on the output 1024-dimensional embeddings of the train split and evaluate it on the test split. Since</figDesc><table><row><cell cols="4">: Ablation: number of object categories (5 runs)</cell></row><row><cell># of Categories</cell><cell>1</cell><cell>10</cell><cell>40*</cell></row><row><cell>PointNet</cell><cell cols="3">41.1?1.2 52.2?1.5 54.9?1.2</cell></row><row><cell>DGCNN</cell><cell cols="3">37.9?3.8 44.8?2.9 56.8?1.5</cell></row><row><cell>C. More results</cell><cell></cell><cell></cell><cell></cell></row><row><cell>3D object classification with</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>linear SVM on the output embeddings from random, Jigsaw and OcCo initialised encoders</figDesc><table><row><cell>Dataset</cell><cell cols="9">PointNet Rand Jigsaw OcCo Rand Jigsaw OcCo Rand Jigsaw OcCo PCN DGCNN</cell></row><row><cell>ShapeNet10</cell><cell>91.3</cell><cell>91.1</cell><cell>93.9</cell><cell>88.5</cell><cell>91.8</cell><cell>94.6</cell><cell>90.6</cell><cell>91.5</cell><cell>94.5</cell></row><row><cell>ModelNet40</cell><cell>70.6</cell><cell>87.5</cell><cell>88.7</cell><cell>60.9</cell><cell>73.1</cell><cell>88.0</cell><cell>66.0</cell><cell>84.9</cell><cell>89.2</cell></row><row><cell>ShapeNet Oc</cell><cell>79.1</cell><cell>86.1</cell><cell>91.1</cell><cell>72.0</cell><cell>87.9</cell><cell>90.5</cell><cell>78.3</cell><cell>87.8</cell><cell>91.6</cell></row><row><cell>ModelNet Oc</cell><cell>65.2</cell><cell>70.3</cell><cell>80.2</cell><cell>55.3</cell><cell>65.6</cell><cell>83.3</cell><cell>60.3</cell><cell>72.8</cell><cell>82.2</cell></row><row><cell>ScanNet10</cell><cell>64.8</cell><cell>64.1</cell><cell>67.7</cell><cell>62.3</cell><cell>66.3</cell><cell>75.5</cell><cell>61.2</cell><cell>69.4</cell><cell>71.2</cell></row><row><cell cols="2">ScanObjectNN 45.9</cell><cell>55.2</cell><cell>69.5</cell><cell>39.9</cell><cell>49.7</cell><cell>72.3</cell><cell>43.2</cell><cell>59.5</cell><cell>78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>More results on few-shot learning. 8?10.7 65.8?9.9 40.3?6.5 48.4?5.6 54.2?4.6 58.8?5.8 36.0?6.2 45.3?7.9 FoldingNet, Rand 33.4 ?13.1 35.8?18.2 18.6?6.5 15.4?6.8 58.9?5.6 71.2?6.0 42.6?3.4 63.5?3.9 Latent-GAN, Rand 41.6?16.9 46.2?19.7 32.9?9.2 25.5?9.9 64.5?6.6 79.8?3.4 50.5?3.0 62.5?5.1 PointCapsNet, Rand 42.3?17.4 53.0?18.7 38.0?14.3 27.2?14.9 59.4?6.3 70.5?4.8 44.1?2.0 60.3?4.9 PointNet++, Rand 38.5?16.0 42.4?14.2 23.1?7.0 18.8?5.4 79.9?6.8 85.0?5.3 55.4?2.2 63.4?2.8 PointCNN, Rand 65.4?8.9 68.6?7.0 46.6?4.8 50.0?7.2 75.8?7.7 83.4?4.4 56.3?2.4 73.1?4.1 PointNet, Rand 52.0?12.2 57.8?15.5 46.6?13.5 35.2?15.3 74.2?7.3 82.2?5.1 51.4?1.3 58.3?2.6 PointNet, cTree 63.2?10.7 68.9?9.4 49.2?6.1 50.1?5.0 76.5?6.3 83.7?4.0 55.5?2.3 64.0?2.4 PointNet, OcCo 89.7?6.1 92.4?4.9 83.9?5.6 89.7?4.6 77.7?8.0 84.9?4.9 60.9?3.7 65.5?5.5 DGCNN, Rand 31.6 ?9.0 40.8?14.6 19.9?6.5 16.9?4.8 58.3?6.6 76.7?7.5 48.1?8.2 76.1?3.6 DGCNN, cTree 60.0?8.9 65.7?8.4 48.5?5.6 53.0?4.1 86.2?4.4 90.9?2.5 66.2?2.8 81.5?2.3 DGCNN, OcCo 90.6?2.8 92.5?6.0 82.9?4.1 86.5?7.1 79.9?6.7 86.4?4.7 63.3?2.7 77.6?3.9</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ModelNet40</cell><cell></cell><cell>Sydney10</cell></row><row><cell>Baseline</cell><cell cols="2">5-way</cell><cell cols="2">10-way</cell><cell>5-way</cell><cell>10-way</cell></row><row><cell></cell><cell>10-shot</cell><cell>20-shot</cell><cell>10-shot</cell><cell>20-shot</cell><cell>10-shot 20-shot 10-shot 20-shot</cell></row><row><cell>3D-GAN, Rand</cell><cell>55.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Detailed Results on Part Segmentation Task on ShapeNetPart</figDesc><table><row><cell>Shapes</cell><cell cols="9">PointNet Rand* Jigsaw OcCo Rand Jigsaw OcCo Rand* Jigsaw* OcCo PCN DGCNN</cell></row><row><cell>mean (point)</cell><cell>83.7</cell><cell>83.8</cell><cell>84.4</cell><cell>82.8</cell><cell>82.8</cell><cell>83.7</cell><cell>85.1</cell><cell>85.3</cell><cell>85.5</cell></row><row><cell>Aero</cell><cell>83.4</cell><cell>83.0</cell><cell>82.9</cell><cell>81.5</cell><cell>82.1</cell><cell>82.4</cell><cell>84.2</cell><cell>84.1</cell><cell>84.4</cell></row><row><cell>Bag</cell><cell>78.7</cell><cell>79.5</cell><cell>77.2</cell><cell>72.3</cell><cell>74.2</cell><cell>79.4</cell><cell>83.7</cell><cell>84.0</cell><cell>77.5</cell></row><row><cell>Cap</cell><cell>82.5</cell><cell>82.4</cell><cell>81.7</cell><cell>85.5</cell><cell>67.8</cell><cell>86.3</cell><cell>84.4</cell><cell>85.8</cell><cell>83.4</cell></row><row><cell>Car</cell><cell>74.9</cell><cell>76.2</cell><cell>75.6</cell><cell>71.8</cell><cell>71.3</cell><cell>73.9</cell><cell>77.1</cell><cell>77.0</cell><cell>77.9</cell></row><row><cell>Chair</cell><cell>89.6</cell><cell>90.0</cell><cell>90.0</cell><cell>88.6</cell><cell>88.6</cell><cell>90.0</cell><cell>90.9</cell><cell>90.9</cell><cell>91.0</cell></row><row><cell>Earphone</cell><cell>73.0</cell><cell>69.7</cell><cell>74.8</cell><cell>69.2</cell><cell>69.1</cell><cell>68.8</cell><cell>78.5</cell><cell>80.0</cell><cell>75.2</cell></row><row><cell>Guitar</cell><cell>91.5</cell><cell>91.1</cell><cell>90.7</cell><cell>90.0</cell><cell>89.9</cell><cell>90.7</cell><cell>91.5</cell><cell>91.5</cell><cell>91.6</cell></row><row><cell>Knife</cell><cell>85.9</cell><cell>86.3</cell><cell>88.0</cell><cell>84.0</cell><cell>83.8</cell><cell>85.9</cell><cell>87.3</cell><cell>87.0</cell><cell>88.2</cell></row><row><cell>Lamp</cell><cell>80.8</cell><cell>80.7</cell><cell>81.3</cell><cell>78.5</cell><cell>78.8</cell><cell>80.4</cell><cell>82.9</cell><cell>83.2</cell><cell>83.5</cell></row><row><cell>Laptop</cell><cell>95.3</cell><cell>95.3</cell><cell>95.4</cell><cell>95.3</cell><cell>95.1</cell><cell>95.6</cell><cell>96.0</cell><cell>95.8</cell><cell>96.1</cell></row><row><cell>Motor</cell><cell>65.2</cell><cell>63.7</cell><cell>65.7</cell><cell>64.1</cell><cell>64.7</cell><cell>64.2</cell><cell>67.8</cell><cell>71.6</cell><cell>65.5</cell></row><row><cell>Mug</cell><cell>93.0</cell><cell>92.3</cell><cell>91.6</cell><cell>90.3</cell><cell>90.8</cell><cell>92.6</cell><cell>93.3</cell><cell>94.0</cell><cell>94.4</cell></row><row><cell>Pistol</cell><cell>81.2</cell><cell>80.8</cell><cell>81.0</cell><cell>81.0</cell><cell>81.5</cell><cell>81.5</cell><cell>82.6</cell><cell>82.6</cell><cell>79.6</cell></row><row><cell>Rocket</cell><cell>57.9</cell><cell>56.9</cell><cell>58.2</cell><cell>51.8</cell><cell>51.4</cell><cell>53.8</cell><cell>59.7</cell><cell>60.0</cell><cell>58.0</cell></row><row><cell>Skateboard</cell><cell>72.8</cell><cell>75.9</cell><cell>74.2</cell><cell>72.5</cell><cell>71.0</cell><cell>73.2</cell><cell>75.5</cell><cell>77.9</cell><cell>76.2</cell></row><row><cell>Table</cell><cell>80.6</cell><cell>80.8</cell><cell>81.8</cell><cell>81.4</cell><cell>81.2</cell><cell>81.2</cell><cell>82.0</cell><cell>81.8</cell><cell>82.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note we intentionally did not compare with cTree<ref type="bibr" target="#b43">[44]</ref> as it is specifically designed for few-shot learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We chose settings that have low FLOPs across tasks and encoders.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In our implementation, we also provide an alternative to use grid search to find the optimal set of parameters for SVM with a Radial Basis Function (RBF) kernel. In this setting, all the OcCo pre-trained models have outperformed the random and Jigsaw initialised ones by a large margin as well.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We would like to thank Qingyong Hu, Shengyu Huang, Matthias Niessner, Kilian Q. Weinberger, and Trevor Darrell for valuable discussions and feedbacks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint supervised and self-supervised learning for 3d realworld challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Alliegro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07392,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the role of individual units in a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cover trees for nearest neighbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning (ICML)</title>
		<meeting>the 23rd international conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Entropy-sgd: Biasing gradient descent into wide valleys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Baldassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">T</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Zecchina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">A simple framework for contrastive learning of visual representations. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sg-nn: Sparse generative neural networks for self-supervised scene completion of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Diller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shape completion using 3d-encoder-predictor cnns and shape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Ruizhongtai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Label-efficient learning on point clouds using approximate convex decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopal</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NET: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semantic3d</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, volume IV-1-W1</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">View inter-prediction gan: Unsupervised representation learning for 3d shapes by learning global shape memories to support local view predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised multi-task feature learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Haley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">M2dp: A novel 3d point cloud descriptor and its application in loop closure detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Sepp Hochreiter and J?rgen Schmidhuber. Flat minima. Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revealnet: Seeing behind objects in rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exploring data-efficient 3d scene understanding with contrastive scene contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards semantic segmentation of urban-scale 3d point clouds: A dataset, benchmarks and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheikh</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03137</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pointcnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SGDR: stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: is a correction for chance necessary?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Vinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The North American Chapter of the Association for Computational Linguistics: Human Language Technologies, (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointdan: A multi-scale 3d domain adaption network for point cloud representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast 3d recognition and pose using the viewpoint feature histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-supervised few-shot learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Schnelle kurven-und fl?chendarstellung auf grafischen sichtger?ten</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Stra?er</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Topnet: Structural point cloud decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><forename type="middle">Angelina</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Latte: accelerating lidar point cloud annotation via sensor fusion, one-click annotation, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cascaded refinement network for point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pillarbased object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A deep representation for volumetric shapes</title>
	</analytic>
	<monogr>
		<title level="m">the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pretraining for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning object bounding boxes for 3d instance segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of 3d features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">3d point capsule networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Complete residential urban area reconstruction from dense aerial lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="118" to="125" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Cylindrical and asymmetrical 3d convolution networks for lidar segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10033,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dublincity: Annotated lidar point cloud and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susana</forename><surname>Sm Zolanvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Ruano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eduardo Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Rahbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
