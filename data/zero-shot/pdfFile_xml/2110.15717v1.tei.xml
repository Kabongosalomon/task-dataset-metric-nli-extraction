<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIDSNet: A Lightweight on-device Intent Detection model using Deep Siamese Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Agarwal</surname></persName>
							<email>vibhav.a@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute Bangalore</orgName>
								<address>
									<postCode>560037</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Deepak Shivnikar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute Bangalore</orgName>
								<address>
									<postCode>560037</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Ghosh</surname></persName>
							<email>sourav.ghosh@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute Bangalore</orgName>
								<address>
									<postCode>560037</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Arora</surname></persName>
							<email>him.arora@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute Bangalore</orgName>
								<address>
									<postCode>560037</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashwant</forename><surname>Saini</surname></persName>
							<email>yash.saini@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute Bangalore</orgName>
								<address>
									<postCode>560037</postCode>
									<region>Karnataka</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LIDSNet: A Lightweight on-device Intent Detection model using Deep Siamese Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-intent detection</term>
					<term>natural language understand- ing</term>
					<term>Siamese Networks</term>
					<term>mobile device</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intent detection is a crucial task in any Natural Language Understanding (NLU) system and forms the foundation of a task-oriented dialogue system. To build high-quality realworld conversational solutions for edge devices, there is a need for deploying intent detection model on device. This necessitates a light-weight, fast, and accurate model that can perform efficiently in a resource-constrained environment. To this end, we propose LIDSNet, a novel lightweight on-device intent detection model, which accurately predicts the message intent by utilizing a Deep Siamese Network for learning better sentence representations. We use character-level features to enrich the sentence-level representations and empirically demonstrate the advantage of transfer learning by utilizing pre-trained embeddings. Furthermore, to investigate the efficacy of the modules in our architecture, we conduct an ablation study and arrive at our optimal model. Experimental results prove that LIDSNet achieves state-of-theart competitive accuracy of 98.00% and 95.97% on SNIPS [1] and ATIS [2] public datasets respectively, with under 0.59M parameters. We further benchmark LIDSNet against fine-tuned BERTs and show that our model is at least 41x lighter and 30x faster during inference than MobileBERT [3] on Samsung Galaxy S20 device, justifying its efficiency on resource-constrained edge devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Identifying message intents from natural language utterances is a crucial task for conversational systems. In applications ranging from natural language response generation to offering intelligent suggestions, understanding the primary intent of the context communication is critical. Traditionally, research on intent detection has focused on this task with the assumption that training and inference would be performed on a well-equipped server or cloud infrastructure. This has led to the unsuitability of existing machine learning approaches for real-world dialog systems in low-resource edge devices due to their high latency and reliance on huge pre-trained models. Lately, there has been an increased academic and commercial interest in supporting AI solutions that can work directly on a user's device on local data <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. On-device AI models have the potential to support intent detection in real-time at low latency and also helps in enhancing the privacy of sensitive user data like smartphone messages.</p><p>Recently, Siamese Networks is being popularly used for few-shot learning and similarity tasks. Siamese Networks with Triplet Loss (SN-TL) brings representations of relevant inputs closer in latent space. Huang et al. <ref type="bibr" target="#b5">[6]</ref> have demonstrated the effectiveness of using SN-TL in the vision domain. Reimers <ref type="figure">Fig. 1</ref>: On-device Intent Understanding from conversational messages: A critical problem for downstream tasks like response generation and intelligent suggestion and Gurevych <ref type="bibr" target="#b6">[7]</ref> modifies pre-trained BERT with SN-TL to derive contextual sentence embeddings. Dhaliwal et al. <ref type="bibr" target="#b7">[8]</ref> uses a similar approach to bring representations of domain specific synonyms closer and later classify them. This motivates us to leverage Siamese Networks with Triplet Loss for intent detection task. In the context of intent detection, this can bring sentence representations of utterances that belong to the same intent class, closer in latent space. The knowledge gained from this task can be further fine-tuned for classification task. This enables us to take advantage of transfer learning.</p><p>In the current work, we propose a novel approach for intent classification with two phase training. In the first phase, we train an encoder using a Siamese Network to learn sentence representations. This functions as a feature extractor that takes into account both character and word level features. This is further fine-tuned for intent classification task in phase II training. Our primary contributions are summarized as follows:</p><p>? We present a lightweight framework, LIDSNet, to accurately predict message intent as shown in <ref type="figure">Fig. 1</ref>  Experiments show that LIDSNet achieves an accuracy of 98.00% and 95.97% on SNIPS and ATIS datasets respectively. Compared to different versions of BERT, LIDSNet attains 9x-87x speedup in inference time and 24x-186x reduction in the number of model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Early research on intent detection include Maximum Entropy Markov Models (MEMM) <ref type="bibr" target="#b8">[9]</ref>. This task has also been approached using Support Vector Machines (SVM) <ref type="bibr" target="#b9">[10]</ref>. Lafferty et al. <ref type="bibr" target="#b10">[11]</ref> shows that CRF based methods to build probabilistic models for segmentation and labelling sequence data perform better than MEMMs. Following this, Purohit et al. <ref type="bibr" target="#b11">[12]</ref> demonstrates the effectiveness of using knowledgeguided patterns in short-text intent classification. While such Shallow Learning techniques show decent results on data adhering to a command-driven pattern, their performance in natural language, filled with colloquials and lingos, is lacking.</p><p>The emergence of deep learning effectively alleviates the constraints of statistical methods and achieves state-of-the-art (SOTA) results from natural language processing to computer vision. The problem statements explored using deep learning techniques include intent detection and the related problem of slot filling, which aims to extract the values of certain types of attributes for a given entity from the input <ref type="bibr" target="#b12">[13]</ref>.</p><p>The choice of embeddings plays an important role in the design decision of most intent detection approaches. Instead of using token-level word embeddings, many researchers approach NLU problems using sentence embeddings <ref type="bibr" target="#b6">[7]</ref>. However, most popular pre-trained models are domain-specific and do not translate well to other domains. This encourages us to explore techniques that maximize the intra-class sentence similarity scores and minimize the inter-class ones. Utilizing deep neural network with a distance metric to learn the feature embedding has been successfully applied to many tasks, such as face recognition <ref type="bibr" target="#b13">[14]</ref> and speech recognition <ref type="bibr" target="#b14">[15]</ref>.</p><p>For mobile devices, intent detection forms the backbone of Natural Language Understanding (NLU) modules, which can either be used in single-domain or multi-domain conversations <ref type="bibr" target="#b15">[16]</ref>. We intend to perform multi-domain intent detection for on-device dialog systems, which drives our choice of datasets for experiments. Desai et al. <ref type="bibr" target="#b16">[17]</ref> propose lightweight convolutional representations for on-device task-oriented systems, related to intent classification and other NLP tasks. But they do not benchmark against other pre-trained language models and solely evaluate on a manually curated dataset. In contrast, we compare the efficiency of our proposed model against strong baselines -including BERT <ref type="bibr" target="#b17">[18]</ref> and MobileBERT <ref type="bibr" target="#b2">[3]</ref> on the SNIPS <ref type="bibr" target="#b0">[1]</ref> and ATIS <ref type="bibr" target="#b1">[2]</ref> datasets.</p><p>Thus, existing work has focused on simple-but-lowaccuracy statistical models or high-accuracy-but-heavy deep learning models. In contrast, we propose a lightweight DNN model that performs at SOTA-competitive accuracy with much less latency and memory footprint than SOTA on edge devices. Furthermore, instead of using pre-trained sentence level features, we employ a two-phase training approach, wherein we train a sentence encoder using a Deep Siamese Network for our multi-domain intent detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>In this section, we describe LIDSNet architecture and its two phase training approach. It consists of a sentence encoder that acts as a feature extractor and utilizes character and word features. In the phase I training, the sentence encoder is trained using Siamese Network with Triplet Loss and is utilized to detect the intent of the utterance in training phase II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Representation Techniques</head><p>Combining character and word level input representations has shown great success in NLP domain. This is because word representation is suitable for relation classification but it does not perform well on short, informal, and conversational texts, whereas character representation effectively handles misspelt and Out-of-Vocabulary (OOV) words. To leverage the best of both representations, our proposed architecture employs a combination of both.</p><p>1) Character level features: We model morphology by incorporating character level representations of words. This boosts the accuracy of neural models by learning rich semantic and orthographic features. The char-level CNN technique utilized in our model encodes the input character, c i , into e ci . These encoded vectors are then passed through a 1D convolution layer followed by max-pooling. Our model has two such CNN blocks with different kernel sizes to capture multiple character level n-gram features, which are then concatenated as defined in <ref type="formula">(1)</ref>.</p><p>2) Fine-tuned Word Vectors: We hypothesize that using knowledge from pre-trained word embeddings can enable an improved understanding of semantics and inter-word relationships over random weights initialization. We are utilizing language semantic knowledge acquired from the pre-trained embeddings and then fine-tuning it for our task.</p><p>We dynamically fine-tune word embedding, e wj , for each word, w j , in the training vocabulary. This word embedding is then concatenated with the character level representation of corresponding word to form an output embedding, o wj : o wj = concat e wj , CNN 1 (e c1 , ..., e ci , ..., e cn ), CNN 2 (e c1 , ..., e ci , ..., e cn ) (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Architecture</head><p>Siamese Network <ref type="bibr" target="#b18">[19]</ref> with Triplet Loss <ref type="bibr" target="#b13">[14]</ref> consists of three identical neural sub-networks with shared weights. Our proposed model consists of a sentence encoder that is trained in two phases. We use two blocks of CNN with different filter  <ref type="figure" target="#fig_0">Fig. 2</ref>. This helps in obtaining representations of rare words <ref type="bibr" target="#b19">[20]</ref> and modeling sub-word structures. We apply transfer learning by using a subset of pre-trained word embeddings that capture semantics. These embeddings are concatenated with character level features of the corresponding words. The resultant word level features are then fed to a BiLSTM layer to obtain sentence level representations. In training phase II, we pass these through a Feed Forward Neural Network (FFNN), which acts as a classifier to compute the probability distribution over the defined intents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Phase I</head><p>In the first phase, we train the Siamese Network with the Triplet Loss function. We use standard train splits of the entire dataset and do not include any additional samples for model training. Each input batch sample consists of three text sequences. Two of these, which we refer to as A (Anchor) and P (Positive), are from the same intent class, while the third sequence, N (Negative), belongs to a different class. For each intent class, we generate all possible &lt; A, P &gt; combinations and sample 50k pairs from these. For each &lt; A, P &gt; pair, we randomly select an N -sequence.</p><p>Let the shared parameters that need to be optimized be W . Our objective is to tune the weights of the sentence encoder in such a way that the sentence representations for A and P should be closer in vector space than A and N . Mathematically, our Triplet Loss is computed as: The Triplet Loss tries to maximize the similarity between sentence representations of A and P , while minimizing the similarity between those of A and N . With this training phase, our aim is to tune the weights of the shared neural network in a way that it is able to segregate and create a distinction between the sentence embeddings of positive (same intent) and negative (different intent) training sequences. After training, the same sentence encoder is used as a feature extractor for the Classifier network in training phase II.</p><formula xml:id="formula_0">L siamese (W, (A, P, N )) = m i=1 max 0, ? ? s ( ? ? v A , ? ? v P ) + s ( ? ? v A , ? ? v N )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Phase II</head><p>This training phase focuses on identifying the most probable intent class for the input sequence. The sentence representations from phase I are passed through two dense layers with Softmax activation function to emit probability scores for each intent. We use categorical cross-entropy loss function, L classifier , defined as ? c i=1 y i ? log y i , where, c is the output size, y i represents the actual label of input i, and y i represents the prediction of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP A. Datasets</head><p>The experiments presented in this paper are carried out on our Custom dataset (for conversational texts) along with SNIPS <ref type="bibr" target="#b0">[1]</ref> and ATIS <ref type="bibr" target="#b1">[2]</ref> public datasets.</p><p>1) Custom Dataset: For the task of user intent understanding for conversational texts, we curate our own dataset using CLINIC150 <ref type="bibr" target="#b20">[21]</ref> and HWU64 <ref type="bibr" target="#b21">[22]</ref> datasets. We map the relevant intent labels from these public datasets to our intent classes as shown in <ref type="table" target="#tab_1">Table I</ref>. Using this approach, we extract 4148 samples mapping to our six intents. This dataset is then split into 90% training set and 10% validation set. The intent-wise distribution of this data is illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. For performance evaluation, we prepare an unseen test set of 298 samples by crowdsourcing conversational texts.</p><p>2) Public Datasets: To evaluate the efficiency of our proposed model, we also evaluate LIDSNet on two real-world datasets, widely used to benchmark intent detection models. The first are the custom-intent engines collected by SNIPS <ref type="bibr" target="#b0">[1]</ref>, and the second is ATIS <ref type="bibr" target="#b1">[2]</ref> dataset, containing audio recordings </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In sentence encoder, we use Conv1D of filter sizes 2 and 3. The filter count is set to 16. We use a subset of the 50dim GloVe <ref type="bibr" target="#b23">[24]</ref> embeddings corresponding to the training set vocabulary. We apply regular and recurrent dropouts with value 0.2. The batch size is set to 512 and 32 for phases I and II respectively. A margin of 0.2 and 24 hidden units in LSTM give the best results as discussed in subsection V-A. A constant learning rate of 0.001 is used with Adam <ref type="bibr" target="#b24">[25]</ref> optimizer for training the model in both phases. We use same set of hyperparameters to build model for all three datasets. We choose accuracy as the evaluation metric as it is commonly used by the existing models. We build all our models using TensorFlow framework. Furthermore, we convert them to TensorFlow Lite format for deploying these models on mobile and edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION RESULTS</head><p>We perform an ablation study of the different aspects of LIDSNet in order to show the impact of every component  of its architecture. Furthermore, we analyze the impact of varying hyperparameters on performance. We also benchmark our model against SOTA intent detection models and compare its computational efficiency with baseline BERT-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study</head><p>We investigate the effect of different architectural and methodical choices, the results of which are presented in <ref type="table" target="#tab_1">Table III</ref>. We train and evaluate our final model against other model variants. These models are named as iP_embeddingType, where i = 1 represents that model is trained only for classification task (bypassing phase 1), and i = 2 implies that model is trained for both phases.  <ref type="bibr" target="#b19">[20]</ref> and model is trained for both phases. 6) 2P_GloVe (LIDSNet): Embeddings weights are initialized from GloVe and model is trained for both phases. We observe that LIDSNet achieves absolute accuracy improvement of 1.29% and 0.56% on SNIPS and ATIS respectively when compared to 1P_GloVe classifier. This empirically proves that our two phase training methodology is useful. Moreover, by comparing the accuracy of 2P_Char and LIDSNet, we show the effectiveness of combining word and character level features in learning a better representation for classification. The impact of GloVe embeddings can be seen from the fact that LIDSNet achieves 1.14% and 0.45% improvement over 2P_random classifier on SNIPS and ATIS respectively. With LIDSNet, we obtain an accuracy improvement of 1.57% and 1.12% on SNIPS and ATIS respectively, compared to the base classifier. This improvement shows the combined effect of our methodical choices. The model sizes of LIDSNet, when trained on SNIPS, ATIS, and Custom datasets, are only 0.63 MB, 0.12 MB and 0.19 MB respectively. <ref type="figure">Fig. 4</ref> illustrates the effect of varying hyperparameters on model accuracy. The classification performance of LIDSNet is presented in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with SOTA</head><p>We compare our best model with other SOTA methods on SNIPS and ATIS datasets. <ref type="table" target="#tab_1">Table IV</ref> shows that LIDSNet  <ref type="bibr" target="#b22">[23]</ref> and Capsule-NLU <ref type="bibr" target="#b25">[26]</ref> respectively. However, our accuracy is slightly lower than some baselines for ATIS, but given the tiny model size of LIDSNet, it offers a compelling accuracy-memory trade-off for inference on resource constraint edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Computational Experiments</head><p>To understand how our LIDSNet model performs in the absence of significant computational resources, we benchmark its latency against fine-tuned BERTs. Model inferencing experiments are conducted on Samsung Galaxy S20 device (8 GB RAM, 2 GHz octa-core Exynos 990 processor).</p><p>Model parameters of LIDSNet on SNIPS and ATIS are 0.59M and 0.065M respectively. The higher number of parameters with SNIPS is due to its large vocabulary. <ref type="table" target="#tab_5">Table V</ref> shows that BERT BASE <ref type="bibr" target="#b17">[18]</ref> performs best with 98.26% and 97.16% accuracy on SNIPS and ATIS respectively. However, LIDSNet stands out on top with 0.53% parameters and 87x inference speedup in comparison to BERT BASE . Compared to 4-layer TinyBERT 4 <ref type="bibr" target="#b28">[29]</ref>, LIDSNet is 24x smaller and yet 9x faster. The results also show that our proposed model is 41x smaller, 30x faster than MobileBERT <ref type="bibr" target="#b2">[3]</ref>, and 111x smaller, 43x faster than DistilBERT <ref type="bibr" target="#b27">[28]</ref>. Our model has a maximum inference time of only 18 milliseconds as reported in <ref type="table" target="#tab_5">Table V</ref>. All above memory-latency comparative analysis is with respect to SNIPS training. Since these BERT-based models have an excess of tens of millions of parameters, they are impractical to be deployed on-device, where our model significantly outperforms all the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose a lightweight, fast, and accurate LIDSNet model for intent classification. We adopt a two phase training methodology and empirically demonstrate the advantage of transfer learning with fine-tuned vectors in improving performance. Our experimental analysis on SNIPS (98.00%, 0.63 MB), ATIS (95.97%, 0.12 MB), and custom dataset (93.62%, 0.19 MB) proves that LIDSNet achieves SOTA-competitive accuracy with the lowest memory footprint. Furthermore, we explore and analyze how LIDSNet clearly outperforms fine-tuned BERTs in terms of system-specific metrics like ROM and latency which is crucial for creating a commercial conversational AI solution. In the future, we plan to develop a joint model for intent detection and slot filling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Proposed LIDSNet Architecture sizes to encode multiple character level features as illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Class distribution in Custom Datasetwhere, ? is a hyperparameter to control the margin between positive and negative inputs, m is the total number of training samples, and s ( ? ? v x , ? ? v y ) is the cosine similarity between two sequence representations, ? ? v x and ? ? v y , given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 )Fig. 5 :</head><label>15</label><figDesc>1P_random (base classifier): Model is trained with randomly initialized word embeddings, bypassing phase I training. Comparison of LIDSNet with base classifier shows the combined effect of two phase training methodology and initialization of word vectors with pretrained embeddings in improving performance. 2) 1P_GloVe: Model is trained with GloVe embeddings initialization, bypassing phase I training. We use this to investigate the effectiveness of knowledge gained in phase I training at improving classification performance. 3) 2P_Char: Only character-level embeddings are utilized in sentence encoder and is trained for both phases. This helps highlight the importance of semantic and syntactic knowledge gained through word embeddings. 4) 2P_random: Random word embedding initialization and model is trained for both phases. This variant helps us investigate the improvement due to knowledge transfer of pre-trained word embeddings. Confusion Matrices summarizing the Classification Performance of LIDSNet on SNIPS and Custom test data 5) 2P_fastText: Embedding weights are initialized from fastText</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We benchmark LIDSNet against various variants of BERT in subsection V-C, and empirically demonstrate that LIDSNet outperforms all the baselines and SOTA models in terms of accuracy-size trade-off.</figDesc><table /><note>by incorporating a Siamese Network with Triplet Loss.? We demonstrate the efficacy of using Siamese Network in learning better sentence representations by conducting an ablation study, discussed in subsection V-A.? We present promising benchmarking results with LIDSNet against off-the-shelf SOTA models, achieving high accuracy with the lowest memory footprint, as detailed in subsection V-B.?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Custom Dataset Intents</figDesc><table><row><cell>Intents from CLINIC150</cell><cell>+</cell><cell>Intents from HWU64</cell><cell>=</cell><cell>Unified intents</cell></row><row><cell>shopping list,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>shopping list update, todo list,</cell><cell></cell><cell>lists createoradd, lists query</cell><cell></cell><cell>Notes</cell></row><row><cell>todo list update</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>calendar,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>meeting schedule, calendar update,</cell><cell></cell><cell>calendar query, calendar remove,</cell><cell></cell><cell>Calendar</cell></row><row><cell>schedule meeting</cell><cell></cell><cell>calendar set</cell><cell></cell><cell></cell></row><row><cell>reminder, reminder update</cell><cell></cell><cell>?</cell><cell></cell><cell>Reminder</cell></row><row><cell>make call, text</cell><cell></cell><cell>email addcontact, email querycontact</cell><cell></cell><cell>Contacts</cell></row><row><cell>directions,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>current location,</cell><cell></cell><cell>recommendation location</cell><cell></cell><cell>Location</cell></row><row><cell>share location</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>greeting,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>goodbye,</cell><cell></cell><cell>general praise</cell><cell></cell><cell>Greeting</cell></row><row><cell>thank you</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>of airline travel information. Table II presents statistics of all three datasets used in our experiments. We use the same train- validation-test distribution for pre-processed public datasets as Goo et al. [23].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell cols="2">SNIPS ATIS</cell><cell>Custom Dataset</cell></row><row><cell>Training Data</cell><cell>13084</cell><cell>4478</cell><cell>3734</cell></row><row><cell>Validation Data</cell><cell>700</cell><cell>500</cell><cell>414</cell></row><row><cell>Test Data</cell><cell>700</cell><cell>893</cell><cell>298</cell></row><row><cell>Vocabulary Size</cell><cell>11241</cell><cell>722</cell><cell>2088</cell></row><row><cell># of Intents</cell><cell>7</cell><cell>21</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Ablation Study</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell></row><row><cell>Model</cell><cell>SNIPS</cell><cell>ATIS</cell><cell>Custom Dataset</cell></row><row><cell>1P_random (base)</cell><cell>96.43</cell><cell>94.85</cell><cell>90.60</cell></row><row><cell>1P_GloVe</cell><cell>96.71</cell><cell>95.41</cell><cell>91.61</cell></row><row><cell>2P_Char</cell><cell>96.71</cell><cell>95.30</cell><cell>90.94</cell></row><row><cell>2P_random</cell><cell>96.86</cell><cell>95.52</cell><cell>91.95</cell></row><row><cell>2P_fastText</cell><cell>96.86</cell><cell>95.63</cell><cell>91.61</cell></row><row><cell>2P_GloVe (LIDSNet)</cell><cell>98.00</cell><cell>95.97</cell><cell>93.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with SOTA Models</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>beats most SOTA models in terms of accuracy even with</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the lowest memory footprint. For model size comparison,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>we re-implemented the models and obtained the results on</cell></row><row><cell></cell><cell></cell><cell></cell><cell>same datasets. In SNIPS dataset, we achieve the second</cell></row><row><cell></cell><cell></cell><cell></cell><cell>highest accuracy of 98.00% (next only to Stack-Propagation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>with BERT [13]) with the lowest model size of 0.63 MB.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>This can enable effective deployment of LIDSNet on edge</cell></row><row><cell></cell><cell></cell><cell></cell><cell>devices such as mobile phones. In ATIS dataset, we observe</cell></row><row><cell></cell><cell></cell><cell></cell><cell>absolute accuracy improvement of 1.87% and 0.97% over Slot-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gated BiLSTM with Attention</cell></row><row><cell>Model</cell><cell cols="2">Accuracy (%)</cell><cell>Model Size</cell></row><row><cell></cell><cell cols="2">SNIPS ATIS</cell><cell>(MB): SNIPS</cell></row><row><cell>Stack-Propagation + BERT [13]</cell><cell>99.00</cell><cell>97.50</cell><cell>&gt;1200.00</cell></row><row><cell>LIDSNet</cell><cell>98.00</cell><cell>95.97</cell><cell>0.63</cell></row><row><cell>Stack-Propagation [13]</cell><cell>98.00</cell><cell>96.90</cell><cell>3.32</cell></row><row><cell>Capsule-NLU [26]</cell><cell>97.70</cell><cell>95.00</cell><cell>643.27</cell></row><row><cell>SF-ID (BLSTM) network [27]</cell><cell>97.43</cell><cell>97.76</cell><cell>11.61</cell></row><row><cell>Slot-Gated BiLSTM with Attention [23]</cell><cell>97.00</cell><cell>94.10</cell><cell>11.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Benchmarking LIDSNet for Mobile Inference</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy (%)</cell><cell>Params</cell><cell>Latency</cell><cell>Speedup</cell></row><row><cell></cell><cell cols="2">SNIPS ATIS</cell><cell>(M)</cell><cell>(ms)</cell><cell></cell></row><row><cell>BERT BASE [18]</cell><cell>98.26</cell><cell>97.16</cell><cell>110</cell><cell>1580</cell><cell>1.0x</cell></row><row><cell>DistilBERT [28]</cell><cell>97.94</cell><cell>96.98</cell><cell>66</cell><cell>781</cell><cell>2.0x</cell></row><row><cell cols="2">MobileBERT [3] 97.71</cell><cell>96.30</cell><cell>24.6</cell><cell>545</cell><cell>2.9x</cell></row><row><cell>TinyBERT 4 [29]</cell><cell>97.43</cell><cell>95.97</cell><cell>14.5</cell><cell>162</cell><cell>9.8x</cell></row><row><cell>LIDSNet</cell><cell>98.00</cell><cell>95.97</cell><cell>0.59 (SNIPS) 0.065 (ATIS)</cell><cell>18</cell><cell>87.0x</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Primet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dureau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of spoken language systems: the ATIS domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley</title>
		<meeting><address><addrLine>Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MobileBERT: a compact task-agnostic BERT for resource-limited devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">edATLAS: An Efficient Disambiguation Algorithm for Texting in Languages with Abugida Scripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Gothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R K</forename><surname>Raja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 15th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="325" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EmpLite: A lightweight sequence labeling model for emphasis selection of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Challa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Harshavardhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kandur Raja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Joint NLP Modelling for Conversational AI @ ICON 2020</title>
		<meeting>the Workshop on Joint NLP Modelling for Conversational AI @ ICON 2020<address><addrLine>Patna, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
	<note>NLP Association of India (NLPAI)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning social image embedding with deep multimodal attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia 2017 -Thematic Workshops &apos;17</title>
		<meeting>the on Thematic Workshops of ACM Multimedia 2017 -Thematic Workshops &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERT-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="3982" to="3992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic creation of a domain specific thesaurus using siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Dhaliwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 15th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enriching the knowledge sources used in a maximum entropy part-of-speech tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanvoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. Hong Kong</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000-10" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep belief nets for natural language call-routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5680" to="5683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ser. ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ser. ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intent classification of short-text on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thirunarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="222" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A stack-propagation framework with token-level intent detection for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end text-independent speaker verification with triplet loss on short utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1487" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khaitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8689" to="8696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lightweight convolutional representations for on-device natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Siamese Neural Networks: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="73" to="94" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An evaluation dataset for intent classification and out-of-scope prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Peper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP 2019. Hong Kong, China: Association for Computational Linguistics</title>
		<meeting>EMNLP-IJCNLP 2019. Hong Kong, China: Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="1311" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Benchmarking natural language understanding services for building conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IWSDS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Slot-gated modeling for joint slot filling and intent prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2018</title>
		<meeting>NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="753" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Joint slot filling and intent detection via capsule neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A novel bi-directional interrelated model for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
