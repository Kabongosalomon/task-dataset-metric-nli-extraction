<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DOTA: A Large-scale Dataset for Object Detection in Aerial Images *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="201919-05-21">May 21, 2019 19 May 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
							<email>guisong.xia@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">EIS, Huazhong Univ. Sci. and Tech</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
							<email>jding@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
							<email>zzhu@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">EIS, Huazhong Univ. Sci. and Tech</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Depart</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jiebo.luo@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Depart</orgName>
								<orgName type="institution">Rochester University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
							<email>mihai.datcu@dlr.de</email>
							<affiliation key="aff4">
								<orgName type="department">German Aerospace Center (DLR)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
							<email>pelillo@dsi.unive.it</email>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">DAIS</orgName>
								<orgName type="institution" key="instit2">University of Venice</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab. LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DOTA: A Large-scale Dataset for Object Detection in Aerial Images *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="201919-05-21">May 21, 2019 19 May 2019</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of wellannotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000 ? 4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188, 282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging. * DOTA dataset is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection in Earth Vision refers to localizing objects of interest (e.g., vehicles, airplanes) on the earth's surface and predicting their categories. In contrast to conventional object detection datasets, where objects are generally oriented upward due to gravity, the object instances in aerial images often appear with arbitrary orientations, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, depending on the perspective of the Earth Vision platforms.</p><p>Extensive studies have been devoted to object detection in aerial images <ref type="bibr">[3, 14, 17-19, 21, 23, 30, 31, 37]</ref>, drawing upon recent advances in Computer Vision and accounting for the high demands of Earth Vision applications. Most of these methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref> attempt to transfer object detection algorithms developed for natural scenes to the aerial image domain. Recently, driven by the successes of deep learning-based algorithms for object detection, Earth Vision researchers have pursued approaches based on fine-tuning networks pre-trained on large-scale image datasets (e.g., ImageNet <ref type="bibr" target="#b5">[6]</ref> and MSCOCO <ref type="bibr" target="#b12">[13]</ref>) for detection in the aerial domain, see e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>While such fine-tuning based approaches are a reasonable avenue to explore, images such as <ref type="figure" target="#fig_0">Fig. 1</ref> reveals that the task of object detection in aerial images is distinguished from the conventional object detection task in the following respects:</p><p>-The scale variations of object instances in aerial images are huge. This is not only because of the spatial resolutions of sensors, but also due to the size variations inside the same object category.</p><p>-Many small object instances are crowded in aerial images, for example, the ships in a harbor and the vehicles in a parking lot, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Moreover, the frequencies of instances in aerial images are unbalanced, for example, some small-size (e.g. 1k ? 1k) images contain 1900 instances, while some large-size images (e.g. 4k ? 4k) may contain only a handfull of small instances.</p><p>-Objects in aerial images often appear in arbitrary orientations. There are also some instances with an extremely large aspect ratio, such as a bridge.</p><p>Besides these distinct difficulties, the studies of object detection in Earth Vision are also challenged by the well-known dataset bias problem <ref type="bibr" target="#b27">[28]</ref>, i.e.the degree of generalizability across datasets is often low. In order to alleviate such biases, the dataset should be annotated to reflect the demands of real world applications.</p><p>Therefore, it is not surprising that the object detectors learned from natural images are not suitable for aerial images. However, existing annotated datasets for object detection in aerial images, such as UCAS-AOD <ref type="bibr" target="#b38">[39]</ref> and NWPU VHR-10 <ref type="bibr" target="#b1">[2]</ref>, tend to use images in ideal conditions (clear backgrounds and without densely distributed instances), which cannot adequately reflect the problem complexity.</p><p>To advance the object detection research in Earth Vision, this paper introduces a large-scale Dataset for Object deTection in Aerial images (DOTA). We collect 2806 aerial images from different sensors and platforms with crowdsourcing. Each image is of the size about 4000 ? 4000 pixels and contains objects of different scales, orientations and shapes. These DOTA images are annotated by experts in aerial image interpretation, with respect to 15 common object categories. The fully annotated DOTA dataset contains 188,282 instances, each of which is labeled by an oriented bounding box, instead of an axis-aligned one, as is typically used for object annotation in natural scenes. The main contributions of this work are: -To our knowledge, DOTA is the largest annotated object dataset with a wide variety of categories in Earth Vision. It can be used to develop and evaluate object detectors in aerial images. We will continue to update DOTA, to grow in size and scope and to reflect evolving real world conditions.</p><formula xml:id="formula_0">(b) (d) (c) (a)</formula><p>-We also benchmark state-of-the-art object detection algorithms on DOTA, which can be used as the baseline for future algorithm development.</p><p>In addition to advancing object detection studies in Earth Vision, DOTA will also pose interesting algorithmic questions to conventional object detection in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivations</head><p>Datasets have played an important role in data-driven research in recent years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. Large datasets like MSCOCO <ref type="bibr" target="#b12">[13]</ref> are instrumental in promoting object detection and image captioning research. When it comes to the classification task and scene recognition task, the same is true for ImageNet <ref type="bibr" target="#b5">[6]</ref> and Places <ref type="bibr" target="#b37">[38]</ref>, respectively.</p><p>However, in aerial object detection, a dataset resembling MSCOCO and ImageNet both in terms of image number and detailed annotations has been missing, which becomes one of the main obstacles to the research in Earth Vision, especially for developing deep learning-based algorithms. Aerial object detection is extremely helpful for vehicle counting, remote object tracking and unmanned driving. Therefore, a large-scale and challenging aerial object detection benchmark, being as close as possible to real-world applications, is imperative for promoting research in this field.</p><p>We argue that a good aerial image dataset should possess four properties, namely, 1) a large number of images, 2) many instances per categories, 3) properly oriented object annotation, and 4) many different classes of objects, which make it approach to real-world applications. However, existing aerial image datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b38">39]</ref> share in common several shortcomings: insufficient data and classes, lack of detailed annotations, as well as low image resolution. Moreover, their complexity is inadequate to be considered as a reflection of the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Annotation way #main categories #Instances #Images Image width NWPU VHR-10 <ref type="bibr" target="#b1">[2]</ref> horizontal BB 10 3651 800 ?1000 SZTAKI-INRIA <ref type="bibr" target="#b0">[1]</ref> oriented BB 1 665 9 ?800 TAS <ref type="bibr" target="#b8">[9]</ref> horizontal BB 1 1319 30 792 COWC <ref type="bibr" target="#b19">[20]</ref> one dot 1 32716 53 2000?19,000 VEDAI <ref type="bibr" target="#b23">[24]</ref> oriented BB 3 2950 1268 512, 1024 UCAS-AOD <ref type="bibr" target="#b38">[39]</ref> oriented BB 2 14,596 1510 ?1000 HRSC2016 <ref type="bibr" target="#b16">[17]</ref> oriented BB 1 2976 1061 ?1100 3K Vehicle Detection <ref type="bibr" target="#b14">[15]</ref> oriented  Datasets like TAS <ref type="bibr" target="#b8">[9]</ref>, VEDAI <ref type="bibr" target="#b23">[24]</ref>, COWC <ref type="bibr" target="#b19">[20]</ref> and DLR 3K Munich Vehicle <ref type="bibr" target="#b14">[15]</ref> only focus on vehicles. UCAS-AOD <ref type="bibr" target="#b38">[39]</ref> contains vehicles and planes while HRSC2016 <ref type="bibr" target="#b16">[17]</ref> only contains ships even though fine-grained category information are given. All these datasets are short in the number of classes, which restricts their applicabilities to complicated scenes. In contrast, NWPU VHR-10 [2] is composed of ten different classes of objects while its total number of instances is only around 3000. Detailed comparisons of these existing datasets are shown in Tab. 1. Compared to these aerial datasets, as we shall see in Section 4, DOTA is challenging for its tremendous object instances, arbitrary but well-distributed orientations, various categories and complicated aerial scenes. Moreover, scenes in DOTA is in coincidence with natural scenes, so DOTA is more helpful for real-world applications.</p><p>When it comes to general objects datasets, ImageNet and MSCOCO are favored by researchers due to the large number of images, many categories and detailed annotations. ImageNet has the largest number of images among all object detection datasets. However, the average number of instances per image is far smaller than MSCOCO and our DOTA, plus the limitations of its clean backgrounds and carefully selected scenes. Images in DOTA contain an extremely large number of object instances, some of which have more than 1,000 instances. PASCAL VOC Dataset <ref type="bibr" target="#b6">[7]</ref> is similar to ImageNet in instances per image and scenes but the inadequate number of images makes it unsuitable to handle most detection needs. Our DOTA resembles MSCOCO in terms of the instance numbers and scene types, but DOTA's categories are not as many as MSCOCO because objects which can be seen clearly in aerial images are quite limited.</p><p>Besides, what makes DOTA unique among the above mentioned large-scale general object detection benchmarks is that the objects in DOTA are annotated with properly oriented bounding boxes (OBB for short). OBB can better enclose the objects and differentiate crowded objects from each other. The benefits of annotating objects in aerial images with OBB are further described in Section 3. We draw a comparison among DOTA, PASCAL VOC, ImageNet and MSCOCO to show the differences in <ref type="table">Tab</ref>  3 Annotation of DOTA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Images collection</head><p>As mentioned in <ref type="bibr" target="#b4">[5]</ref>, in aerial images, the resolution and variety of sensors being used are factors to produce dataset biases. To eliminate the biases, images in our dataset are collected from multiple sensors and platforms (e.g. Google Earth) with multiple resolutions. To increase the diversity of data, we collect images shot in multiple cities carefully chosen by experts in aerial image interpretation. We record the exact geographical coordinates of the location and capture time of each image to ensure there are no duplicate images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Category selection</head><p>Fifteen categories are chosen and annotated in our DOTA dataset, including plane, ship, storage tank, baseball diamond, tennis court, basketball court, ground track field, harbor, bridge, large vehicle, small vehicle, helicopter, roundabout, soccer ball field and basketball court. The categories are selected by experts in aerial image interpretation according to whether a kind of objects is common and its value for real-world applications. The first 10 categories are common in the existing datasets, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39]</ref>, We keep them all except that we further split vehicle into large ones and small ones because there is obvious difference between these two sub-categories in aerial images. Others are added mainly from the values in real applications. For example, we select helicopter considering that moving objects are of significant importance in aerial images. Roundabout is chosen because it plays an important role in roadway analysis.</p><p>It is worth discussing whether to take "stuff" categories into account. There are usually no clear definitions for the "stuff" categories (e.g. harbor, airport, parking lot), as is shown in the SUN dataset <ref type="bibr" target="#b31">[32]</ref>. However, the context information provided by them may be helpful for detection. We only adopt the harbor category because its border is relatively easy to define and there are abundant harbor instances in our image sources. The final extended category is soccer field.</p><p>In <ref type="figure" target="#fig_1">Fig.2</ref>, we compare the categories of DOTA with NWPU VHR-10 <ref type="bibr" target="#b1">[2]</ref>, which has the largest number of categories in previous aerial object detection datasets. Note that DOTA surpass NWPU VHR-10 not only in category numbers, but also the number of instances per category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation method</head><p>We consider different ways of annotating. In computer vision, many visual concepts such as region descriptions, objects, attributes, and relationships, are annotated with bounding boxes, as shown in <ref type="bibr" target="#b11">[12]</ref>. A common description of bounding boxes is (x c , y c , w, h), where (x c , y c ) is the center location, w, h are the width and height of the bounding box, respectively.</p><p>Objects without many orientations can be adequately annotated with this method. However, bounding boxes labeled in this way cannot accurately or compactly outline oriented instances such as text and objects in aerial images. In an extreme but actually common condition as shown in <ref type="figure" target="#fig_2">Fig. 3</ref> (c) and (d), the overlap between two bounding boxes is so large that state-of-the-art object detection methods cannot differentiate them. In order to remedy this, we need to find an annotation method suitable for oriented objects.</p><p>An option for annotating oriented objects is ?-based oriented bounding box which is adopted in some text detection benchmarks <ref type="bibr" target="#b34">[35]</ref>, namely (x c , y c , w, h, ?), where ? denotes the angle from the horizontal direction of the standard bounding box. A flaw of this method is the inability to compactly enclose oriented objects with large deformation among different parts. Considering the complicated scenes and various orientations of objects in aerial images, we need to abandon this method and choose a more flexible and easy-to-understand way. An alternative is arbitrary quadrilateral bounding boxes, which can be denoted as {(x i , y i ), i = 1, 2, 3, 4}, where (x i , y i ) denotes the positions of the oriented bounding boxes' vertices in the image. The vertices are arranged in a clockwise order. This way is widely adopted in oriented scene text detection benchmarks <ref type="bibr" target="#b10">[11]</ref>. We draw inspiration from these researches and use arbitrary quadrilateral bounding boxes to annotate objects.</p><p>To make a more detailed annotation, as illustrated <ref type="figure" target="#fig_2">Fig. 3</ref>, we emphasize the importance of the first point (x 1 , y 1 ), which normally implies the "head" of the object. For helicopter, large vehicle, small vehicle, harbor, baseball diamond, ship and plane, we carefully denote their first point to enrich potential usages. While for soccer-ball field, swimming pool, bridge, ground track field, basketball court and tennis court, there are no visual clues to decide the first point, so we normally choose the top-left point as the starting point.</p><p>Some samples of annotated patches (not the whole original image) in our dataset are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p><p>It is worth noticing that, Papadopoulos et al. <ref type="bibr" target="#b21">[22]</ref> have explored an alternative annotation method and verify its efficiency and robustness. We assume that the annotations would be more precise and robust with more elaborately designed annotation methods, and alternative annotation protocols would facilitate more efficient crowd-sourced image annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset splits</head><p>In order to ensure that the training data and test data distributions approximately match, we randomly select half of the original images as the training set, 1/6 as validation set, and 1/3 as the testing set.</p><p>We will publicly provide all the original images with ground truth for training set and validation set, but not for the testing set. For testing, we are currently building an evaluation server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Properties of DOTA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image size</head><p>Aerial images are usually very large in size compared to those in natural images dataset. The original size of images in our dataset ranges from about 800 ? 800 to about 4000 ? 4000 while most images in regular datasets (e.g. PASCAL-VOC and MSCOCO) are no more than 1000 ? 1000. We make annotations on the original full image without partitioning it into pieces to avoid the cases where a single instance is partitioned into different pieces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Various orientations of instances</head><p>As shown in <ref type="figure" target="#fig_0">Fig.1 (f)</ref>, our dataset achieves a good balance in the instances of different directions, which is significantly helpful for learning a robust detector. Moreover, our dataset is closer to real scenes because it is common to see objects in all kinds of orientations in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Spatial resolution information</head><p>We also provide the spatial resolution for each image in our dataset, which implies the actual size of an instance and plays a significant role in aerial object detection. The importance of spatial resolution for detection task are two folds. First, it allows the model to be more adaptive and robust for varieties  of objects of the same category. It's known that objects appear smaller when seen from a distance.</p><p>The same object with different sizes will trouble the model and hurt classification. However, a model can pay more attention to the shape with resolution information provided instead of objects' size. Second, it's better for fine-grained classification. For example, it will be simple to distinguish a small boat from a large warship. Spatial resolution can also be used to filter mislabeled outliers in our dataset because intra-class varieties of actual sizes for most categories are limited. Outliers can be found by selecting the objects whose size is far different from those of the same category in a small range of spatial resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Various pixel size of categories</head><p>Following the convention in <ref type="bibr" target="#b32">[33]</ref>, we refer to the height of a horizontal bounding box, which we call pixel size for short, as a measurement for instance size. We divide all the instances in our dataset into three splits according to their height of horizontal bounding box: small for range from 10 to 50, middle for range from 50 to 300, and large for range above 300. Tab. 3 illustrates the percentages of three instance splits in different datasets. It is clear that the PASCAL VOC dataset, NWPU VHR-10 dataset and DLR 3K Munich Vehicle dataset are dominated by middle instances, middle instances and small instances, respectively. However, we achieve a good balance between small instances and middle instances, which is more similar to real-world scenes and thus, helpful to better capture different size of objects in practical applications.</p><p>It's worth noting that pixel size varies in different categories. For example, a vehicle may be as small as 30, however, a bridge can be as large as 1200, which is 40 times larger than a vehicle. The huge differences among instances from different categories make the detection task more challenging because models have to be flexible enough to handle extremely tiny and huge objects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Various aspect ratio of instances</head><p>Aspect ratio is an essential factor for anchor-based models, such as Faster RCNN <ref type="bibr" target="#b25">[26]</ref> and YOLOv2 <ref type="bibr" target="#b24">[25]</ref>. We count two kinds of aspect ratio for all the instances in our dataset to provide a reference for better model design: 1) Aspect ratio of minimally circumscribed horizontal rectangle bounding box, 2) Aspect ratio of original quadrangle bounding box. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates these two types of distribution of aspect ratio for instances in our dataset. We can see that instances varies greatly in aspect ratio. Moreover, there are a large number of instances with a large aspect ratio in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Various instance density of images</head><p>It is common for aerial images to contain thousands of instances, which is different from natural images. For example, images in ImageNet <ref type="bibr" target="#b5">[6]</ref> contain on the average 2 categories and 2 instances, while MSCOCO contains 3.5 categories and 7.7 instances, respectively. Our dataset is much richer in instances per image, which can be up to 2000. <ref type="figure" target="#fig_4">Fig. 5</ref> illustrates the number of instances in our DOTA dataset.</p><p>With so many instances in a single image, it is unavoidable to see areas densely crowded with instances. For COCO, instances are not annotated one by one because occlusion makes it difficult to distinguish an instance from its neighboring instances. In these cases, the group of instances is marked as one segment with attribute named "crowd". However, this is not the case for aerial images because there are rarely occlusion due to the perspective from the above. Therefore, we can annotate all the instances in a dense area one by one. <ref type="figure" target="#fig_3">Fig. 4</ref> shows examples of densely packed instances. Detecting objects in these cases poses an enormous challenge for the current detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluations</head><p>We evaluate the state of the art object detection methods on DOTA. For horizontal object detection, we carefully select Faster R-CNN 1 <ref type="bibr" target="#b25">[26]</ref>, R-FCN 2 <ref type="bibr" target="#b3">[4]</ref>, YOLOv2 3 <ref type="bibr" target="#b24">[25]</ref> and SSD 2 <ref type="bibr" target="#b15">[16]</ref> as our benchmark testing algorithms for their excellent performance on general object detection. For oriented object detection, we modify the original Faster R-CNN algorithm such that it can predict properly oriented bounding boxes denoted as {(x i , y i ), i = 1, 2, 3, 4}.</p><p>Note that, the backbone networks are ResNet-101 <ref type="bibr" target="#b7">[8]</ref> for R-FCN and Faster R-CNN, Incep-tionV2 <ref type="bibr" target="#b9">[10]</ref> for SSD and customized GoogLeNet <ref type="bibr" target="#b26">[27]</ref> for YOLOv2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks</head><p>To comprehensively evaluate the state of the art deep learning based detection methods on DOTA, we propose two tasks, namely detection on horizontal bounding boxes (HBB for short) and detection on oriented bounding boxes (OBB for short). To be more specific, we evaluate those methods on two different kinds of ground truths, HBB or OBB, no matter how those methods were trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation prototypes</head><p>Images in DOTA are so large that they cannot be directly sent to CNN-based detectors. Therefore, we crop a series of 1024 ? 1024 patches from the original images with a stride set to 512. Note that some complete objects may be cut into two parts during the cropping process. For convenience, we denote the area of the original object as A o , and the area of divided parts P i , (i = 1, 2) as a i , (i = 1, 2). Then we compute the parts areas over the original object area:</p><formula xml:id="formula_1">U i = a i A o .</formula><p>Finally, we label the part P i with U i &lt; 0.7 as difficult and for the other one, we keep it the same as the original annotation. For the vertices of the newly generated parts, we need to ensure they can be described as an oriented bounding box with 4 vertices in the clockwise order with a fitting method.</p><p>In the testing phase, first we send the cropped image patches to obtain temporary results and then we combine the results together to restore the detecting results on the original image. Finally, we   use non-maximum suppression (NMS) on these results based on the predicted classes. We keep the threshold of NMS as 0.3 for the HBB experiments and 0.1 for the oriented experiments. In this way, we indirectly train and test CNN-based models on DOTA. For evaluation metrics, we adopt the same mAP calculation as for PASCAL VOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines with horizontal bounding boxes</head><p>Ground truths for HBB experiments are generated by calculating the axis-aligned bounding boxes over original annotated bounding boxes. To make it fair, we keep all the experiments' settings and hyper parameters the same as depicted in corresponding papers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. The experimental results of HBB prediction are shown in Tab. 4. Note that results of SSD is much lower than other models. We suspect it should be attributed to the random crop operation in SSD's data augmentation strategies, which is quite useful in general object detection while degrades in aerial object detection for tremendous small training instances. The results further indicate the huge differences between aerial and general objects with respect to instance sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Baselines with oriented bounding boxes</head><p>Prediction of OBB is difficult because the state of the art detection methods are not designed for oriented objects. Therefore, we choose Faster R-CNN as the base framework for its accuracy and efficiency and then modify it to predict oriented bounding boxes. RoIs (Region of Interests) generated by RPN (Region Proposal Network) are rectangles which can be written as R = (x min , y min , x max , y max ), for a more detailed interpretation, R = {(x i , y i ), i = 1, 2, 3, 4}, where x 1 = x 4 = x min , x 2 = x 3 = x max , y 1 = y 2 = y min , y 3 = y 4 = y max . In R-CNN procedure, each RoI is attached to a ground truth oriented bounding box written as G = {(g xi , g yi ), i = 1, 2, 3, 4}. Then R-CNN's output target T = {(t xi , t yi ), i = 1, 2, 3, 4} is calculated by following equations,</p><formula xml:id="formula_2">t xi = (g xi ? x i )/w,<label>(1)</label></formula><formula xml:id="formula_3">t yi = (g yi ? y i )/h,<label>(2)</label></formula><p>where i = 1, 2, 3, 4, w = x max ? x min , and h = y max ? y min . Other settings and hyper parameters are kept the same as depicted in Faster R-CNN <ref type="bibr" target="#b25">[26]</ref>. The numerical results are shown in Tab. 5. To make a comparison to our implemented Faster R-CNN for OBB, we evaluate YOLOv2, R-FCN, SSD and Faster R-CNN trained on HBB with the OBB ground truth. As shown in Tab.5, the results of those methods trained on HBB are much lower than Faster R-CNN trained on OBB, indicating that for oriented object detection in aerial scenes, those methods should be adjusted accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experimental analysis</head><p>When analyzing the results exhibited in <ref type="table">Table.</ref> 4, performances in categories like small vehicle, large vehicle and ship are far from satisfactory, which attributes to their small size and densely crowded locations in aerial images. As a contrast, large and discrete objects, like planes, swimming pools and tennis courts, the performances are rather fair.</p><p>In <ref type="figure" target="#fig_6">Fig. 6</ref>, we compare the results between object detection experiments of HBB and OBB. For densely packed and oriented objects shown in <ref type="figure" target="#fig_6">Fig. 6 (a)</ref> and (b), location precision of objects in HBB experiments are much lower than OBB experiments and many results are suppressed through postprogress operations. So OBB regression is the correct way for oriented object detection that can be really integrated to real applications. In <ref type="figure" target="#fig_6">Fig. 6 (c)</ref>, large aspect ratio objects annotated in OBB style like (harbor, bridge) are hard for current detectors to regress. But in HBB style, those objects usually have normal aspect ratios and as a consequence, results seem to be fairly good as shown in <ref type="figure" target="#fig_6">Fig. 6 (d)</ref>. However in extremely dense scenes, e.g in <ref type="figure" target="#fig_6">Fig. 6</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Cross-dataset validations</head><p>The cross dataset generalization <ref type="bibr" target="#b27">[28]</ref> is an evaluation for the generalization ability of a dataset. We choose the UCAS-AOD dataset <ref type="bibr" target="#b38">[39]</ref> to do cross-dataset generalization for its comparatively large number of data comparing to other aerial object detection datasets. For there are no official data splits for UCAS-AOD, we randomly select 1110 for training and 400 for testing. We choose YOLOv2 as the testing detector for all experiments described below and HBB-style annotations for all ground truths. Input image size is changed to 960 ? 544 around the original image sizes in UCAS-AOD while other setting kept unchanged. Results are shown in Tab. 6. The performance difference across two datasets is 35.8 for YOLOv2-A and 15.6 for YOLOv2-D models, respectively. It suggests that DOTA hugely covers UCAS-AOD and furthermore has more patterns and properties that are not shared in UCAS-AOD. And both models get a low results on DOTA which reflects that DOTA is much more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We build a large-scale dataset for oriented objects detection in aerial images which is much larger than any existing datasets in this field. In contrast to general object detection benchmarks, we  annotate a huge number of well-distributed oriented objects with oriented bounding boxes. We assume this dataset is challenging but very similar to natural aerial scenes, which are more appropriate for practical applications. We also establish a benchmark for object detection in aerial images and show the feasibility to produce oriented bounding boxes by modifying a mainstream detection algorithm. Detecting densely packed small instances and extremely large instances with arbitrary orientations in a large picture would be particularly meaningful and challenging. We believe DOTA will not only promote the development of object detection algorithms in Earth Vision, but also pose interesting algorithmic questions to general object detection in computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example taken from DOTA. (a) Typical image in DOTA consisting of many instances across multiple categories. (b) Illustration of the variety in instance orientation and size. (c),(d) Illustration of sparse instances and crowded instances, respectively. Here we show four out of fifteen of the possible categories in DOTA. Examples shown in (b),(c),(d) are cropped from source image (a). The histograms (e),(f) exhibit the distribution of instances with respect to size and orientation in DOTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between DOTA and NWPU VHR-10 in categories and responding quantity of instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of adopted annotation method. The yellow point represents the starting point, which refers to: (a) top left corner of a plane, (b) the center of sector-shaped baseball diamond, (c) top left corner of a large vehicle. (d) is a failure case of the horizontal rectangle annotation, which brings high overlap compared to (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Samples of annotated images in DOTA. We show three samples per each category, except six for large-vehicle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Statistics of instances in DOTA. AR denotes the aspect ratio. (a) The AR of horizontal bounding box. (b) The AR of oriented bounding box. (c) Histogram of number of annotated instances per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(e) and (f), results of HBB and OBB are all not satisfying which implies the defects of current detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visualization results of testing on DOTA using well-trained Faster R-CNN. TOP and Bottom respectively illustrate the results for HBB and OBB in cases of orientation, large aspect ratio, and density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison among DOTA and object detection datasets in aerial images. BB is short for bounding box. One-dot refers to annotations with only the center coordinates of an instance provided. Fine-grained categories are not taken into account. For example, DOTA consist of 15 different categories but only 14 main categories, because small vehicle and large vehicle are both sub-categories of vehicle.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. 2.</figDesc><table><row><cell>Dataset</cell><cell>Category</cell><cell>Image quantity</cell><cell>BBox quantity</cell><cell>Avg. BBox quantity</cell></row><row><cell>PASCAL VOC (07++12)</cell><cell>20</cell><cell>21,503</cell><cell>62,199</cell><cell>2.89</cell></row><row><cell>MSCOCO (2014 trainval)</cell><cell>80</cell><cell>123,287</cell><cell>886,266</cell><cell>7.19</cell></row><row><cell>ImageNet (2017train)</cell><cell>200</cell><cell>349,319</cell><cell>478,806</cell><cell>1.37</cell></row><row><cell>DOTA</cell><cell>15</cell><cell>2,806</cell><cell>188,282</cell><cell>67.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison among DOTA and other general object detection datasets. BBox is short for bounding boxes, Avg. BBox quantity indicates average bounding box quantity per image. Note that for the average number of instances per image, DOTA surpasses other datasets hugely.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of instance size distribution of some datasets in aerial images and natural images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Numerical results (AP) of baseline models evaluated with HBB ground truths. Swimming pool, and HC-Helicopter. FR-H means Faster R-CNN [26] trained on Horizontal bounding boxes. FR-O means Faster R-CNN [26] trained on Oriented bounding boxes.</figDesc><table><row><cell>The short</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Numerical results (AP) of baseline models evaluated with OBB ground truths. The short names are defined the same as depicted in Tab. 4. Note that only FR-O [26] is trained with OBB.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results of cross-dataset generalization. Top: Detection performance evaluated on UCAS-AOD. Bottom: Detection performance evaluated on DOTA. YOLOv2-A and YOLOv2-D are trained with UCAS-AOD and DOTA, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/msracver/Deformable-ConvNets 2 https://github.com/tensorflow/models/tree/master/research/object_detection 3 https://github.com/pjreddie/darknet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head><p>We thank Fan Hu, Pu Jin, Xinyi Tong, Xuan Hu, Zhipeng Dong, Liang Wu, Jun Tang, Linyan Cui, Duoyou Zhou, Tengteng Huang, and all the others who involved in the annotations of DOTA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building development monitoring in multitemporal remotely sensed image pairs with stochastic birth-death dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zerubia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rifd-cnn: Rotation-invariant and fisher discriminative convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2884" to="2893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-FCN: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Earth observation image semantic bias: A collaborative user annotation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>De Oca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nistor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
		<title level="m">ICDAR 2015 competition on robust reading</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. ICDAR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rotation-invariant object detection in remote sensing images based on radial-gradient angle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci.Remote Sensing Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="750" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast multiclass vehicle detection on aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>M?ttyus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sensing Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1938" to="1942" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ship rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sensing Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1074" to="1078" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate object localization in remote sensing images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2486" to="2498" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting cars in uav images with a catalog-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moranduzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6356" to="6367" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated detection of arbitrarily shaped buildings in complex environments from monocular VHR optical satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">?</forename><surname>Ok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Senaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Y?ksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. and Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3-2</biblScope>
			<biblScope unit="page" from="1701" to="1717" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>abs/1708.02750</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical and contextual model for aerial image parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Porway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="283" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vehicle detection in aerial imagery: A small target detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Razakarivony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Vis. Commun. Image R</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="187" to="203" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building detection in very high resolution multispectral data with deep learning features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1873" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Affine invariant description and large-margin dimensionality reduction for target detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sensing Lett</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature extraction by rotation-invariant matrix representation for object detection in aerial image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci.Remote Sensing Lett</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Introduction to a large-scale general purpose ground truth database: Methodology, annotation tool and benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMMCVPR 2007</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="169" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building a large scale dataset for image emotion recognition: The fine print and the benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="308" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised learning based on coupled convolutional neural networks for aircraft detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5553" to="5563" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
