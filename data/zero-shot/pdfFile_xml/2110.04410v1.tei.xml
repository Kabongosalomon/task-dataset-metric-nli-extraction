<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TITANET: NEURAL MODEL FOR SPEAKER REPRESENTATION WITH 1D DEPTH-WISE SEPARABLE CONVOLUTIONS AND GLOBAL CONTEXT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithin</forename><forename type="middle">Rao</forename><surname>Koluguri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boris Ginsburg NVIDIA</orgName>
								<address>
									<settlement>Taejin Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TITANET: NEURAL MODEL FOR SPEAKER REPRESENTATION WITH 1D DEPTH-WISE SEPARABLE CONVOLUTIONS AND GLOBAL CONTEXT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speaker verification</term>
					<term>speaker embedding</term>
					<term>t-vectors</term>
					<term>context</term>
					<term>diarization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose TitaNet, a novel neural network architecture for extracting speaker representations. We employ 1D depth-wise separable convolutions with Squeezeand-Excitation (SE) layers with global context followed by channel attention based statistics pooling layer to map variable-length utterances to a fixed-length embedding (tvector). TitaNet is a scalable architecture and achieves stateof-the-art performance on speaker verification task with an equal error rate (EER) of 0.68% on the VoxCeleb1 trial file and also on speaker diarization tasks with diarization error rate (DER) of 1.73% on AMI-MixHeadset, 1.99% on AMI-Lapel and 1.11% on CH109. Furthermore, we investigate various sizes of TitaNet and present a light TitaNet-S model with only 6M parameters that achieve near state-of-the-art results in diarization tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speaker recognition is a broad research area that solves two major tasks based on characteristics of voices: speaker identification and speaker verification. Speaker identification is about identifying a person and speaker verification is about verifying whether the speaker is who they claim to be. Speaker diarization is a task of partitioning audio recordings into speaker-homogeneous segments belonging to each individual speaker. Typically, speaker recognition and diarization systems operate on unconstrained speech utterances, which are converted to a vector of fixed length, called speaker embeddings. These speaker embeddings represent the identity of each speaker and are used for speaker recognition and speaker diarization tasks.</p><p>In recent years, deep neural networks (DNNs) have been actively employed for speaker embedding extractors since d-vector <ref type="bibr" target="#b0">[1]</ref> was proposed. Subsequently, x-vector <ref type="bibr" target="#b1">[2]</ref> was widely used because of the superior performance achieved by employing statistical pooling and time delay neural network (TDNN). Other architectures such as ResNet-based convolutional neural networks (CNNs) <ref type="bibr" target="#b2">[3]</ref> and CNNs with cross convolutional layers <ref type="bibr" target="#b3">[4]</ref> were employed for capturing the traits of speech. In addition, to cope with the variable-length inputs, Transformer <ref type="bibr" target="#b4">[5]</ref>, CNN-LSTM <ref type="bibr" target="#b5">[6]</ref> and a slew of variants of TDNN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> were applied for DNN-based speaker embedding extractors.</p><p>In this work, we develop a speaker embedding extractor model that shows superior performance on both speaker verification and diarization tasks. We adopt the architecture from ContextNet <ref type="bibr" target="#b9">[10]</ref>, a state-of-the-art automatic speech recognition (ASR) model, which combines local features from 1D depth-wise separable convolutions and global context from Squeeze and Excitation layers.</p><p>We train text-independent speaker recognition models where the identity of the speaker is based on how speech is spoken, not necessarily on what is being said. The following are the main contributions of this paper: ? We propose use of 1D separable depth-wise convolutions compared to full 1D convolutions.</p><p>? We bring in global context to speaker embedding models by introducing global average pooling after the squeeze and excitation module. This contrasts with the embedding extractors based on TDNN, such as x-vector <ref type="bibr" target="#b1">[2]</ref> or most recently ECAPA-TDNN <ref type="bibr" target="#b10">[11]</ref>.</p><p>? TitaNet-M is half the size of comparable speaker embedding extractors like ECAPA-TDNN or Conformer-based baselines and achieves superior performance in speaker diarization.</p><p>? We train our networks end-to-end using angular softmax margin loss and use cosine similarity as a backend for speaker representations. Such approach leads us to avoid the burden of training external models like probabilistic linear discriminant analysis (PLDA) <ref type="bibr" target="#b11">[12]</ref> and agglomerative hierarchical clustering (AHC) <ref type="bibr" target="#b12">[13]</ref> as in well-known speaker verification systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>, or speaker diarization systems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. We introduce three models with different sizes. The architecture is easily scalable in both depth and width by design, and we show that the scaling in width is very effective in reducing the model size with a small change in performance. In the experimental section, we demonstrate the performance of the model on a VoxCeleb1 cleaned test speaker verification trial file. In addition, we evaluate diarization performance on popular evaluation datasets like AMI (Lapel), AMI (Mix-Headset) <ref type="bibr" target="#b14">[15]</ref>, NIST-SRE-2000 <ref type="bibr" target="#b15">[16]</ref> and CH109 <ref type="bibr" target="#b16">[17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoder</head><p>The model is based on the ContextNet ASR architecture <ref type="bibr" target="#b9">[10]</ref> comprising of an encoder and decoder structure. We use the encoder of the ContextNet model as a top-level feature extractor, and feed the output to the attentive pooling layer. This layer computes attention features across channel dimensions to capture the time-independent utterance-level speaker representations.</p><p>TitaNet is a 1D time depth-wise channel separable convolutional model with ContextNet-like architecture combined with channel attention pooling. <ref type="figure" target="#fig_0">Fig. 1</ref> describes the ContextNet-B?R?C model encoder and attention pooling decoder, where B is the number of blocks, R is the number of repeated sub-blocks per block, and C is the number of filters in the convolution layers of each block. The encoder starts with a prologue block B 0 , followed by mega blocks B 1 . . . B N ?1 and ends with epilogue block B N . Prologue and epilogue blocks differ from mega blocks, they both have the same convolution module (Conv), batchnorm and relu layers and have fixed kernel sizes of 3 in prologue and 1 in epilogue for all the network architectures we propose. They do not contain residual connections and dropout layers. Each mega block begins with time-channel separable convolutional <ref type="bibr" target="#b17">[18]</ref> layer with stride 1 and dilation 1, followed by batchnorm, relu and dropout.</p><p>Each time-channel separable convolution module is made up of two parts: a depth-wise convolutional layer and a pointwise convolutional layer. Depth-wise convolutions apply a single filter per input channel (input depth). Pointwise convolutions are 1?1 convolutions, used to create a linear combination of the outputs of the depth-wise layer. These layers are repeated R times, which can be modified to vary the depth of the network. These repeated layers are residually connected with Squeeze and Excitation layers with global average pooling for context inclusion. By using global context, the SE layer squeezes a sequence of local feature vectors into a single global context vector, broadcasts this context back to each local feature vector, and merges the two via multiplications. The width of the network can be increased or decreased by varying output channel filter sizes of each mega block. For Ti-taNet models, width and depth are changed by varying these filter sizes, C and the number of repeated layers, R respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Decoder and Embeddings</head><p>The top level acoustic features obtained from the output of encoder are used to compute intermediate features that are passed to the decoder for getting utterance level speaker embeddings. The intermediate time-independent features are computed using an attentive statistics pooling layer <ref type="bibr" target="#b8">[9]</ref>, where the channel attention features E are computed across timechannels to get a time-independent feature representation S of size B?3072.</p><p>The intermediate features S are passed through the Decoder consisting of two linear layers, one of output size 192 and another for a linear transformation from 192 to the final number of classes N , to compute the probability that the current segment belongs to a speaker from the training set. In this fashion, the network extracts fixed-length representation from variable length speech segments. We extract t-vectors before the final logits linear layer of fixed size 192.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Loss function</head><p>The TitaNet model was trained end-to-end with additive angular margin (AAM) loss <ref type="bibr" target="#b18">[19]</ref>. The AAM helps to optimize the cosine distance between speaker embeddings. For all the verification and diarization experiments presented in this paper, we use cosine similarity as the back-end:</p><formula xml:id="formula_0">L = ? 1 N N i=1</formula><p>log e s(cos(?y i +m)) e s(cos(?y i +m)) + n j=1,j =yi e s cos ?j</p><p>where m is margin, s is scale and ? j is the angle between the final linear layer weight W j and incoming feature x i . Here m and s are predefined hyper parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We designed three TitaNet models: TitaNet-S with 256 channels, TitaNet-M with 512 channels, and TitaNet-L with 1024 TitaNet-S has 6.4M , TitaNet-M -13.4M, and TitaNet-L -25.3M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Training Data</head><p>We use following datasets to datasets to train TitaNet: Vox-Celeb1 and VoxCeleb2 dev <ref type="bibr" target="#b19">[20]</ref>, NIST SRE portion of datasets from 2004-2008 (LDC2009E100), Switchboard-Cellular1 and Switchboard-Cellular2 <ref type="bibr" target="#b20">[21]</ref>, Fisher <ref type="bibr" target="#b21">[22]</ref>, and Librispeech <ref type="bibr" target="#b22">[23]</ref> (see <ref type="table" target="#tab_0">Table 1</ref>). Combined, these datasets consist of about 4.8M utterances from 16.6K speakers. We augment the training data with RIR <ref type="bibr" target="#b23">[24]</ref> impulse corpora, speed pertubation with 0.95x &amp; 1.05x speeds and also spec augment <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Evaluation Data</head><p>We use the VoxCeleb1 cleaned test trial file to evaluate EER for speaker verification. We use the following three datasets for the evaluating speaker diarization system: ? NIST-SRE-2000 <ref type="bibr" target="#b15">[16]</ref>: all sessions from LDC2001S97. ? AMI Corpus <ref type="bibr" target="#b14">[15]</ref>: Lapel and MixHeadset audio subsets from partition set <ref type="bibr" target="#b25">[26]</ref>. ? CH109 <ref type="bibr" target="#b16">[17]</ref>: we use a subset of CALLHOME American English speech (CHAES), which contains only two speakers. There are 109 sessions in this subset. The remaining 11 sessions in CHAES are used as a dev set for CH109 and NIST-SRE-2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiment Setup</head><p>Every speaker recognition experiment consists of common data pre-processing steps for training, development, and evaluation steps. During the pre-processing, we do not use a speech activity detector (SAD) to avoid dependence on an additional model. Instead, we split speech segments longer than 3 sec into random chunks of 1.5, 2, and 3 sec. We compute acoustic features for every 25 ms frame window shifted over 10 ms. The acoustic features are 80-dimensional mel spectrograms computed using a 512 FFT and a Hann window. Next, Mel-spectrogram features are normalized over the frequency axis. Every utterance fed to the encoder has size Both the EER for verification and the DER -for diarization are done using cosine similarity (CS) back-end. In our diarization experiments, the evaluation datasets are divided into two groups: telephonic and non-telephonic speech. Based on experiments with the dev sets, we found window size of 1.5 sec with a shift of 0.75 sec works best for telephonic speech. For non-telephonic speech, the best settings were 3 sec and 1.75 sec for window and shift respectively. In the evaluation datasets used in this paper, AMI Lapel and MixHeadset fall under non-telephonic speech, and the rest of them are in the telephonic speech group. Unlike the previous studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>, we do not use external dev data to tune the clustering parameters by relying on an autotuning approach <ref type="bibr" target="#b27">[28]</ref>. Similar to the previous systems <ref type="bibr" target="#b8">[9]</ref>, we use collar 0.25 sec and ignore overlap speech regions for speaker error rate calculation. All TitaNet models in <ref type="table" target="#tab_1">Table 2</ref> are trained for 250 epochs with SGD optimizer, with initial learning rate (LR) 0.08 using cosine annealing LR scheduler on 4 nodes with 8 V100 GPUs per node.  <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>TitaNet comparison with other models for speaker diarization with oracle SAD known speakers number, DER(%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Speaker Verification</head><p>In the speaker verification experiments, we train the model on the datasets shown in the <ref type="table" target="#tab_0">Table 1</ref>. We train these systems initially as a speaker identification model with 10 percent of audio files of each speaker set aside as validation data from training sets. With this setup, we trained TitaNet models endto-end using additive margin angular loss. <ref type="table" target="#tab_1">Table 2</ref> shows the performance of TitaNet models on the VoxCeleb cleaned trial file. We observed a high-degree of sensitivity on validation curves with slight variations in the margin (m) and scale (s) for angular loss. With m as 30 and s as 0.2 TitaNet-L showed state of the art performance with EER of 0.68% on VoxCeleb1 cleaned test trial file outperforming previously reported results in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>As it can be noticed from <ref type="table" target="#tab_1">Table 2</ref>, Titanet models are easily scalable to achieve very competitive performances even with relatively few parameters around 6M. These models show a direct relationship on accuracy in contrast to the number of parameters. We show the Detection Error Tradeoff (DET) curves to compare TitaNet-L model with other previously stated CNN based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Speaker Diarization</head><p>We employ our proposed speaker embedding extractor models for speaker diarization tasks. Cosine similarity is used for measuring the distance between speaker embeddings and Normalized Maximum Eigengap Spectral Clustering (NME-SC) <ref type="bibr" target="#b27">[28]</ref> on the extracted embeddings to obtain the clustering result. We show the performance of each TitaNet model on popular evaluation datasets as shown in <ref type="table">Tables 3 and 4</ref>. The diarization experiments are based on oracle SAD to evaluate the SAD-independent performance. In <ref type="table">Table 3</ref>, we show the results for known number of speakers case and in <ref type="table">Table 4</ref>, we present the results for unknown number of speakers for which the speaker count is estimated using NME-SC clustering algorithm.</p><p>TitaNet models outperform the previous state-of-the-art models on the AMI-Lapel, AMI-MixHeadset and CH109 evaluation datasets. It is worth noting that the performance of the small and medium TitaNet models show minor differences even if we reduce their model parameters by 2x and 4x respectively, compared to the largest model. In di-  <ref type="table">Table 4</ref>. TitaNet comparison with other models for speaker diarization with oracle SAD estimated number of speakers, DER(%).</p><p>arization systems we believe there is no major performance improvement using larger TitaNet models when compared to verification. We hypothesize that this is related to the fact that separability in the embedding space does not require a higher level of precision since the clustering process only involves relatively few speakers. The TitaNet models show very good improvement on all datasets, except on the NIST-SRE-2000 evaluation set. But note that the clustering approaches in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref> involves additional training for Hidden Markov Model or PLDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this paper, we present TitaNet, a new speaker representation learning model that utilizes the global context of squeeze-and-excitation layers combined with channel attention pooling for extracting fixed length speaker embeddings. The model employs 1D depth-wise separable convolutions for speaker embedding models that showed state-of-the-art performance in ASR tasks. The TitaNet-M model, which is half the size of previous state-of-the-art systems outperforms them in speaker diarization tasks while achieving competitive numbers on verification tasks. The TitaNet-L model significantly outperforms existing models in speaker verification and diarization tasks. The models' implementation and pre-trained checkpoints are made available through NVIDIA NeMo toolkit <ref type="bibr" target="#b30">[31]</ref>. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ACKNOWLEDGMENTS</head><p>We would like to thank NVIDIA AI Applications team for the help and valuable feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>TitaNet Encoder and Decoder Architecture 2. MODEL ARCHITECTURE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>DET curve for VoxCeleb1 cleaned trial comparing with previous studies of T ?80, where T is the number of frames in a given speech utterance file. The accuracy of speaker verification systems is measured using EER and minimum normalized detection cost (MinDCF) with P target = 10 ?2 and C F A = C M iss = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of each dataset used for training TitaNet channels. All models have the same number of repeating layers R as 3, and the same kernel filter sizes as 3,7,11 and 15.</figDesc><table><row><cell>Dataset</cell><cell># of Speakers</cell><cell>Duration (in Hrs)</cell><cell># Utterances (in K)</cell></row><row><cell>VoxCeleb1</cell><cell>1211</cell><cell>227</cell><cell>332</cell></row><row><cell>VoxCeleb2</cell><cell>5994</cell><cell>1895</cell><cell>2274</cell></row><row><cell>SRE</cell><cell>3787</cell><cell>503</cell><cell>944</cell></row><row><cell>Fisher</cell><cell>951</cell><cell>162</cell><cell>278</cell></row><row><cell>Switchboard</cell><cell>2400</cell><cell>247</cell><cell>425</cell></row><row><cell>LibriSpeech</cell><cell>2338</cell><cell>336</cell><cell>634</cell></row><row><cell>Total</cell><cell>16681</cell><cell>3373</cell><cell>4890</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>(Backend)</cell><cell># Params (M)</cell><cell cols="2">VoxCeleb1 EER (%) MinDCF</cell></row><row><cell>x-vector (PLDA) [2]</cell><cell>9</cell><cell>2.97</cell><cell>0.323</cell></row><row><cell>ECAPA (CS) [9]</cell><cell>22.3</cell><cell>0.69</cell><cell>0.082</cell></row><row><cell>Conformer (CS)[27]</cell><cell>26.4</cell><cell>2.43</cell><cell>0.264</cell></row><row><cell>TitaNet-S (CS)</cell><cell>6.4</cell><cell>1.15</cell><cell>0.131</cell></row><row><cell>TitaNet-M (CS)</cell><cell>13.4</cell><cell>0.81</cell><cell>0.106</cell></row><row><cell>TitaNet-L (CS)</cell><cell>25.3</cell><cell>0.68</cell><cell>0.087</cell></row></table><note>. TitaNet comparison with other models for speaker verification task. All models has been evaluated with Cosine Similarity (CS) backend except x-vector which used PLDA.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/NVIDIA/NeMo</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep neural networks for small footprint text-dependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ensemble additive margin softmax for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An improved deep embedding learning method for short duration speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Self-attention encoding and pooling for speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>India</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A complete end-to-end speaker verification system using deep neural networks: From raw signals to verification result</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">State-of-theart speaker recognition for telephone and video speech: The JHU-MIT submission for NIST SRE18.,&quot; in Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Orthogonal training for textindependent speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ECAPA-TDNN embedding for speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thienpondt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desplanques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01466</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03191</idno>
		<title level="m">Con-textNet: Improving convolutional neural networks for automatic speech recognition with global context</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Desplanques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thienpondt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07143</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian speaker verification with heavytailed priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Odyssey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Diarization is hard: Some experiences and lessons learned for the JHU team in the inaugural DIHARD challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian HMM clustering of x-vector sequences (VBx) in speaker diarization: theory, implementation and analysis on standard tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Landini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Profant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">101254</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The AMI meeting corpus: A preannouncement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carletta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on machine learning for multimodal interaction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The NIST speaker recognition evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<pubPlace>Odyssey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CallHome american english speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zipperlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">QuartzNet: Deep automatic speech recognition with 1D timechannel separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beliaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ArcFace: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Switchboard-1 release 2 LDC97S62</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Holliman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fisher english training speech part 1 transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Linguistic Data Consortium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LibriSpeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study on data augmentation of reverberant speech for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">pyannote.audio: neural building blocks for speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bredin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Korshunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lavechin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fustes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Titeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Gill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Auto-tuning spectral clustering for speaker diarization using normalized maximum eigengap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="38" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-learning with latent space clustering in generative adversarial network for speaker diarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeinali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silnova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mat?jka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Plchot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12592</idno>
		<title level="m">BUT system description to voxceleb speaker recognition challenge 2019</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Nemo: a toolkit for building ai applications using neural modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beliaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cook</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09577</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
