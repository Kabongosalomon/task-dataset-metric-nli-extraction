<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparsifying Transformer Models with Trainable Representation Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Pietruszka</surname></persName>
							<email>michal.pietruszka@applica.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">Jagiellonian University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Borchmann</surname></persName>
							<email>lukasz.borchmann@applica.ai</email>
							<affiliation key="aff1">
								<orgName type="institution">Poznan University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Garncarek</surname></persName>
							<email>lukasz.garncarek@applica.ai</email>
						</author>
						<title level="a" type="main">Sparsifying Transformer Models with Trainable Representation Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator. Our experiments on a challenging long document summarization task show that even our simple baseline performs comparably to the current SOTA, and with trainable pooling we can retain its top quality, while being 1.8? faster during training, 4.5? faster during inference and up to 13? more computationally efficient in the decoder. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The introduction of Transformer architecture led to an immense improvement in the performance of Natural Language Processing systems <ref type="bibr">(Vaswani et al., 2017;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr">Devlin et al., 2019)</ref>. Nevertheless, the underlying attention mechanism is marked by the original sin of quadratic memory complexity w.r.t. the input sequence length. It results from the attention matrix reflecting inter-connections between every two representations in the input sequence.</p><p>Previous approaches either reduce the full connectivity of its elements to its non-empty subset or approximate the self-attention matrix <ref type="bibr">(Dai et al., 2019;</ref><ref type="bibr" target="#b0">Beltagy et al., 2020;</ref><ref type="bibr">Kitaev et al., 2020;</ref><ref type="bibr">Tay et al., 2020;</ref><ref type="bibr">Zaheer et al., 2020a;</ref><ref type="bibr">Wang et al., 2020;</ref><ref type="bibr">Shen et al., 2021;</ref><ref type="bibr">Choromanski et al., 2021;</ref><ref type="bibr">Roy et al., 2020)</ref>. In particular, in these models, each word at every layer attends to at least one other word.</p><p>In contrast, we disregard attention for a given representation completely in the case of non-informative ones <ref type="figure">(Figure 1 and 2)</ref>.</p><p>In particular, we optimize the attention complexity by learning to select encoded representations for the given task and promoting only the chosen ones to the next layer of the model. This mechanism will be referred to as representation pooling. Consequently, a significantly 1 Code publicly available at https://github.com/ applicaai/pyramidions along with trained models.  <ref type="figure">Figure 1</ref>: An illustration of sparse attention matrices assuming a three-layer encoder and decoder (separated by the dashed line). The blue color reflects the memory consumption of self-attention (encoder) and cross-attention (decoder). (A) The complete input consumed at once. (B) Memory reduced with blockwise attention and (C) pooling applied after the encoder. (D) Gradual reduction of memory by pooling after every layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla</head><p>Blockwise Pooling <ref type="figure">Figure 2</ref>: Toy illustration of inter-connections constituting the attention matrices in various approaches to attention. White dots denote disregarded representations that are not attended to and removed from further processing as they obtained low scores.</p><p>lower memory consumption and an improved processing time are achieved. As the selection operation has to be trainable, we provide a suitable high-performance continuous relaxation of top-k, robust for every k value and input sequence length. We demonstrate this idea's applicability by performing on par to state-of-the-art on the challenging problem of long document summarization. Simultaneously, the proposed end-to-end model is a significant theoretical improvement over the previous systems, which are based on independently trained extractive and abstractive models.</p><p>Contribution. The specific contributions of this paper are the following: (1) We propose a method to sparsify Transformer architecture in a novel, previously unrecognized way, achieving sublinear time and memory complexity. Our model learns to select the subset of best representations depending on the advantage they give on a downstream task. (2) Additionally, we demonstrate an improvement of the decoder's cross-attention complexity. It is beneficial for both train/inference time and memory consumption. <ref type="formula">(3)</ref> We demonstrate an elegant way to train extractive-abstractive models in an endto-end manner with only a cross-entropy loss function. (4) We present a Successive Halving Top-k operator that outperforms previous approaches in terms of approximation quality and speed. We provide a detailed analysis of its differential properties and prove that it is trainable in an end-to-end manner, making it applicable within our neural networks. (5) We achieve state-of-the-art performance level in long document's summarization and show that previous models can be outperformed by a straightforward baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Word-vector elimination. It has been previously shown that the progressive elimination of word vectors occurring layer after layer can improve inference time of transformer-based language models used in a text classification scenario <ref type="bibr">(Goyal et al., 2020)</ref>. We extend this notion to tasks demanding text generation in a way that, contrary to previous work, is trainable and optimized concerning a downstream task. A similar approach has been taken in the Funnel Transformer proposed concurrently to our work <ref type="bibr">(Dai et al., 2020)</ref>. We directly compare to both methods' adaptations (see Section 5), and consider our work to surpass it in two aspects: 1) results were improved due to a better pooling mechanism than mean/max; 2) training was accelerated, which we attribute to the significant reduction of the decoder's complexity.</p><p>Sparse attention. Several authors proposed to limit attention connectivity, e.g., by dividing input into smaller 'blocks' <ref type="bibr">(Child et al., 2019;</ref><ref type="bibr" target="#b0">Beltagy et al., 2020;</ref><ref type="bibr">Rae and Razavi, 2020)</ref>. Blockwise attention is an optional element of our architectures, used in addition to trainable pooling.</p><p>Summarization. In terms of the type of summarization task we target, our representation pooling mechanism can be considered an end-to-end extractiveabstractive model. This is a conceptual breakthrough compared to recently proposed two-stage hybrids that extract and paraphrase in two independent steps, using separately trained modules <ref type="bibr">(Subramanian et al., 2019;</ref><ref type="bibr">Hsu et al., 2018;</ref><ref type="bibr">Gehrmann et al., 2018;</ref><ref type="bibr">Chen and Bansal, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Novel Approach of Representation Pooling</head><p>It is suspected that when humans engage in information search, they use various cognitive processes depending on the relevance level of constituent text fragments <ref type="bibr">(Gwizdka et al., 2017)</ref>. The method we propose is inspired by this search for relevant fragments, which is an important aspect of human cognition when engaged in reading to do actions <ref type="bibr">(Mosenthal, 1996;</ref><ref type="bibr">Mosenthal and Kirsch, 1992)</ref>. We intend to mimic relevance judgments and hypothesize that it is possible to answer problems involving natural language with only selected passages of the input text.</p><p>These passages may be of substantially shorter length than the original text. One may compare this to a person reading the paper and highlighting in such a way that it is possible to provide a summary using only the highlighted parts.</p><p>The end-to-end mechanism we introduce performs such highlighting by scoring the representations and passes only the selected ones to the next layer of the neural network ( <ref type="figure">Figure 3</ref>). The role of the selection is to reduce data resolution in a roughly similar way to how pooling works in CNNs, where the feature map is downsampled and only the most informative activations are retained. When pooling in a trainable manner at the bottleneck of the encoder-decoder, it impacts the encoding process because the additional, orthogonal, informational bottleneck forces the model to compress more context into one representation vector of constantlength, leveraging the already provided capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture Outline</head><p>Let n denote the number of input tokens that are projected onto d dimensions, resulting in a matrix of embedding representations E ? R n?d . We want to assign scores v i to embedding vectors E i , in such a way that v i measures the usefulness of E i for further layers and the training objective.</p><p>Typically, this can be achieved by defining a scoring function S : R d ? R (which we allow to depend on additional parameters, thus making it trainable) that assigns a usefulness score to every embedding vector, and putting v i = S(E i ).</p><p>Next, we use our soft top-k operator ? : R n?d ? R n ? R k?d to reduce the number of embeddings from  <ref type="figure">Figure 3</ref>: Transpooler architecture with pooling after one encoder layer. Each representation is scored, and then only those with the highest scores are passed to the decoder. Encoding can be performed on the full length input or in blocks of fixed length.</p><p>n to k, based on their usefulness scores. The k vectors produced by ? form the input for the next network layer. The path of residual connections starts on a reduced number of tokens.</p><p>Flavors. We consider two architectures in this work: with single or multiple pooling layers ( <ref type="figure">Figure 1</ref>). Specifically, the latter is a generalization of the former to any given number of pooling layers. We use the term Transpooler when a single pooling layer is placed after the encoder. This setup directly limits the amount of information passed to the decoder through the network's bottleneck. However, pooling can be applied between any subsequent layers, such that multiple operations of this type will be used in the network and gradually introduce the bottleneck along the encoding process. As a result, the same model bottleneck size can be achieved as when using Transpooler. Moreover, the decision to pool earlier has the advantage of attaining more substantial memory complexity reduction. This model will be referred to as the Pyramidion.</p><p>Blockwise attention. When propagating through layers, we use blockwise attention and split input into nonoverlapping chunks in such a way that the full quadratic attention is computed for each chunk. The score is then determined for each representation vector, and after selecting with the top-k operator, chosen representations are passed to the next layer. We assure our top-k operator selects representations without permuting their order, keeping them in line with their original position.</p><p>Scoring functions. Multiple scoring methods can be proposed. The most straightforward is to use a linear scoring function as used in conventional token classification, S(e) = e T w + b, where w ? R d and b ? R are trainable parameters. We found it to work best with our pooling method. In the Appendix A we perform ablations on different scoring functions. <ref type="table">Table 1</ref>: Time complexity of attention in the Transformer models. Improvements over the vanilla Transformer are in bold, whereas an underline indicates this paper's contributions. l -number of layers, n -input length, d -hidden state;s size, t -target length, hnumber of hashes LSH, r -rank of the factorization matrix, k -length of selected token's representation, can effective number of layers that is smaller than l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Self-attention Cross-attention <ref type="table">Table 1</ref> presents the complexity of attention in our models, and compares it to different architectures. The vanilla encoder depends on the number of layers l, the number of tokens in the input n and the number of tokens each attends to n. Likewise, the decoder's cross-attention depends on l, n and the target length t.</p><formula xml:id="formula_1">Vanilla l ? n ? n ? d l ? t ? n ? d Sparse l ? m ? n ? d l ? t ? n ? d Linformer l ? n ? r ? d - LSH l ? mh ? n ? d - Efficient l ? n ? d ? d - PoWER c ? n ? n ? d - Transpooler l ? m ? n ? d l ? t ? k ? d Pyramidion c ? m ? n ? d l ? t ? k ? d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complexity Analysis</head><p>The m denotes the effective number of tokens one can attend to, resulting from the attention's block size, allowed window size or the clustering of key-values. The number of parallel LSH hashes is denoted by h. The rank of the factorization matrix is r, which can be a constant that is independent of n.</p><p>Similarly, the number of best task-specific representations k, selected after encoding, is independent of n. c is an effective number of layers in a hierarchically decreasing encoder of the Pyramidion. The Pyramidion's c can be as low as ?. Blockwise sparse attention improved the vanilla Transformer's complexity by limiting the number of tokens each attends to from n (input length) to m (block size) as seen in <ref type="table">Table 1</ref>. As we keep the encoding of blockwise attention, the m improvement also applies to our self-attention.</p><p>For the Pyramidion model, we narrow down the size of the representation on the output of each chosen layer, leading to the exponential reduction of memory consumption as the encoding proceeds. For example, when pooling after every layer is considered, the total memory complexity across l layers would be p i=0 2 ?i mnd = (2 ? k/n)mnd where p denotes the number of passes p = log 2 (n/k), assuming k ? n and n, k ? {2 i | i ? Z + }. Hence, the effective complexity of all layers is lower than ?mnd, which means it is lower than ? times the complexity of the full-size first layer.</p><p>For the decoder cross-attention, the number of input representations that t target tokens can attend to is limited by k, thus decreasing the memory complexity of cross attention from O(tn) to O(tk). Optimization over quadratic sentence-length complexity is even more powerful and needed on the decoder side, as O(tn) complexity hurts performance of real-world applications based on auto-regressive decoding.</p><p>The blockwise attention itself reduces encoder complexity proportionally to the number of chunks. We further reduce the decoder layer's complexity in Transpooler models by a factor of n/k, thanks to representation pooling. The Pyramidion we propose offers an additional improvement on the encoder side, where time and memory consumption are reduced in each of the consecutive layers compared to the Transformer featuring blockwise attention. In other words, when b denotes the number of blocks, l stands for the number of layers, and the sequence length is halved in each layer, we reduce memory from b + b + ... + b = lb to b+b/2+b/4+...+b/(2 l ) ? 2b. Because the beneficial impact of pooling accumulates, we are able to improve complexity from one that is linearly dependent on l to one that is constant, independent of l. In the further DeepPyramidion's experiments, we will proceed with a higher reduction factor, where the length of a sequence is cut in four.</p><p>As a result, the Pyramidion achieves an effective selfattention time and space complexity linear of n and logarithmic of l. For comparison, other sparse models such as, e.g., Linformer depend linearly on n and linearly on l. The analysis of <ref type="figure" target="#fig_1">Figure 4</ref> found evidence that our method scales well with an increasing number of layers. In the evaluation (see Section 5), we demonstrate that our model achieves a 2.5? computation reduction in the encoder's self-attention and a 16? reduction in the decoder's cross-attention comparing to blockwise baseline, while both models are close to SOTA results on the task of long-document summarization. All things considered, we introduce Pyramidion with sublinear complexity that achieves remarkable results. The advantage of our approach is that it complements all other proposed sparsification techniques, thus paving a new interesting avenue of potential research. It can be effortlessly applied in-between layers and simultaneously with other improvements since representation pooling addresses a different aspect of the attention's complexity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Suitable Top-k Operator</head><p>The choice of the selection operator is challenging, as it has to be trainable to instantiate a pooler. In case of the hard top-k operator, back-propagation through the scores is impossible and prevents training the scoring function. It could be seen as an extreme case of the vanishing gradient problem. In this section we introduce a mechanism not prone to this issue, while the Appendix B is dedicated to a theoretical analysis of its differential properties, from a geometrical point of view.</p><p>The crux of our approach is the Successive Halving Top-k selection mechanism that finds k convex combinations of vector representations E i , dominated by those achieving the highest scores v i (pseudocode available in the Appendix B.1). <ref type="bibr">2</ref> The general idea is to perform a tournament soft selection, where candidate vectors are compared in pairs (i, j), until only k remained. After each tournament's round new E and v are computed as convex combinations of these pairs with weights based on their respective scores. Each new vector is calculated as:</p><formula xml:id="formula_2">E i = w i E i + w j E j ,</formula><p>where the w i , w j are the result of a peaked softmax over the scores v i , v j . Analogously, we use v i = w i v i +w j v j as the new-round's scores. Weights are calculated using a PeakedSoftmax function <ref type="bibr">(Goyal et al., 2017)</ref>, increasing the pairwise difference in scores between v i and v j . One round halves the number of elements in E and v. We perform it iteratively unless the size of E and v matches the chosen value of k.</p><p>To improve convergence towards selecting the real top-k, it is desired to permute v and E first. In our algorithm, we sort the vectors E i in descending order of their scores v i and then put them into the tournament in pairs of the form (i, n + 1 ? i). This method of pairing guarantees that the weights w i depend monotonically on the scores v i , which is the main motivation for using it. Extended benchmarks for time and accuracy are covered in details in Appendix B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The main focus of the experiments was to understand how to employ the Successive Halving Top-k operator within neural networks to build models that have better training and inference time and are expressive enough to achieve results comparable to state-of-the-art models. The first experiment was specifically designed to compare to other sparse Transformers and Vanilla baselines.</p><p>Choice of tasks. We demonstrate the benefit of pooling on the arXiv and PubMed summarization datasets <ref type="bibr">(Cohan et al., 2018)</ref> available under Apache License 2.0 license. Both tasks demand text generation and have the highest average input sequence length (6k and 3k words on average for arXiv and PubMed respectively). Assuming an embedding of dimensionality 768, it is important to note that for inputs shorter than approx. 4k tokens, more multiplications happen in the Transformer's FFN layers and projection layers than in the attention layers. Hence, the validation of the sparsification mechanism should be proved by showing that it works for longer inputs.</p><p>Time benchmarks. The average time of processing a batch of documents is reported to evaluate the computational improvements experimentally. Decoding experiments were synthetic with a forced fixed length of 512 output tokens to discount for the lower processing time of models predicting an earlier sequence end. We recorded time in seconds on batches of size 64 and 8 for training and generation, respectively. Details regarding the hyperparameters and test environment are reported in Appendix C.</p><p>Ablations on input and decoder lengths. <ref type="table" target="#tab_1">Table 2</ref> presents evaluation metrics and time benchmarks depending on encoder and decoder lengths, as well as used sparsification mechanisms. At this stage, we use shallow 4-layer models to perform ablation studies and estimate each approach's strengths and weaknesses. We observe that all sparse models deliver on the promise of accelerating training time over Vanilla Transformers for longer sequences in this setup. Methods requiring the elimination of word vectors scale well with the sequence length but incur additional pooling costs, which may be notable for shorter sequences. Nevertheless, inference time was significantly reduced only when methods eliminating word vectors were employed. The introduction of blockwise attention and pooling does not decrease scores while lowering the computational cost. The detailed training procedure for all models is provided in Appendix C.</p><p>Scaling deeper. In preliminary experiments it was estimated that the fastest-to-train model that performs comparably to the Vanilla Transformer is the Blockwise Transformer. Here, we scale it to 6-layers in each encoder and decoder and provide an interesting baseline for our model, since Transpooler's backbone is blockwise attention. We undertook the empirical analysis of scaling Transpooler to many layers in Appendix C.2 and found that in order to balance performance and speed, it is crucial to delay the first pooling and not to perform it directly on the first layer's output. It was also revealed that appending more layers at the end of the encoder (after pooling) results in a negligible increase in time while considerably improving scores. Both changes to the block size and reduction of the bottleneck harmed the performance. Thus, the data supports the premise that the 6-layers encoder should consume 8k tokens on the input and output representations of lengths 8k, 8k, 2k, 512, 512, 512 after each successive layer. We refer to this model as DeepPyramidion (note that pooling happens twice in the encoder). The decoder also has six layers, making our model directly comparable to the deeper Blockwise Transformer. We confront DeepPyramidion with the Blockwise baseline by training models from scratch on arXiv and PubMed datasets separately and report results in comparison to the state-of-the-art summarization models <ref type="table" target="#tab_2">(Table 3)</ref>.</p><p>Results. The evaluation of the data presented in Table 3 leads to the unexpected conclusion that our Blockwise Transformer baseline, despite its simplicity, is sufficient to outperform deeper, denser, and additionally pretrained models that were recently reported as stateof-the-art. We demonstrate that DeepPyramidion retains or improves the performance of the competitive baseline we produced. The training time speedup by 1.8? supports the notion that our model scales better to long sequences, assuming deeper models. This result stands in line with evidence in <ref type="figure" target="#fig_1">Figure 4</ref>. While our baseline Blockwise model reduces the computational demand of self-attention in encoder by a factor of 16? when comparing to Vanilla Transformer, it does not improve the decoder's computational complexity. It is interesting to highlight that DeepPyramidion further lowers the cost of self-attention by 2.5? and improves 16? over Blockwise's cross-attention in the decoder, and leads to overall 13? improvement in the number of multiplication operations in the decoder. Time benchmarks show a 4.5? improvement in the generation times for our method, proving how vital the improvement in the decoder's cross-attention complexity is for inference time.</p><p>DeepPyramidion achieves a ROUGE-2 score indistinguishable from SOTA on arXiv and performs competitively on PubMeb. At the same time, an entire DeepPyramidion costs five times less than a single Transformer layer consuming 8k tokens. However, when comparing our results to those of older studies, it must be pointed out that our models were trained from scratch only on the targeted dataset, whereas prior works often base on already pretrained models such as BART or RoBERTa and leverage unsupervised training on additional datasets. On the contrary, a longer input sequence was consumed by both Blockwise and DeepPyramidion, which we speculate, is the reason for their strong performance. 3 Impact of longer inputs. The results achieved in our paper are comparable to other, much heavier, and more costly models due to two main reasons, that will be briefly discussed below.</p><p>Firstly, to perform well on a long document summarization task, there is a need to strike the right balance not only between the depth and width of the network but also it is required for design optimization to take into account the length of the input. All previous work seem to underperform when considering all three factors, as they were designed and optimized for shorter tasks and generally have more parameters, denser computations, or even a hard limit on the range of positional encoding. The authors were thus bounded by the maximal sequence length of 512 or 1024 tokens. One can argue that within this prefix (corresponding to the first 2 ? 3 pages), any data point from the arXiv/PubMed datasets (a scientific paper) usually provides enough information to write a meaningful summary, but also, important details will be missing to some degree. Hence, increasing the length of the input that can be consumed on GPUs, at the price of using a shallower network, with sparser computation, may be considered a better fit for the task. Secondly, we think that pretraining in the Pyramidion's case may be disregarded due to an interesting "length exploiting hypothesis". That is, while we consume longer sequences on the input, the network learns more efficiently, as more information is available, and thus, the training signal is stronger. This can be convincingly portrayed in the case of embedding layers, as during training they see many more words and sentences from the chosen dataset, and hence, can provide more meaningful representations to the further layers.</p><p>One can think that making the most of already available domain texts and consuming longer inputs is an advantageous approach to masked pretraining on outof-domain datasets. While the latter approach may aid 'general' language understanding, it has insufficient transferability potential to domain-specific document understanding (e.g., scientific or medical texts).</p><p>To sum up, the Pyramidion has improvements that allow consuming longer inputs cheaply, which turns out to be a more cost-effective strategy compared to other models. This aspect is crucial for achieving strong results on the presented datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations and Social Impact</head><p>At this stage of understanding, we believe that sparsification based on trainable pooling is unlikely to improve processing time for short sequences specific to some NLP tasks, e.g., sentence-level Neural Machine Translation. In addition, the score improvement may be attainable for tasks characterized by at least an order of magnitude shorter outputs than inputs, as it was previously shown on classification, or, as in the case of this work, on summarization.</p><p>However, the extent to which it is possible to replace full-attention in Transformer with the sparse attention we propose is unknown. However, we argue that the benefits are visible starting from the inputs of length 4k. As discussed earlier, 4k is a break-even point where more calculations are needed for attention than for FFNs and projecting layers. As such, we recommend applying sparsification methods on datasets featuring sequences of length over that value. While we focus on the long end of the possible inputs, one can continue our analysis, to find improvements that work for shortest sequences, such as, e.g., concentrating on employing lighter projection layers and FFNs or stacking more attention blocks.</p><p>Although our method is a hybrid extractiveabstractive, it does not provide interpretable explanations to which specific representations were selected as the pooling operates in the latent space. How to match the selected vectors to the vocabulary tokens remains an open question. Moreover, framing the trainable pooling for language modeling remains a challenge to address in future works, especially as in this task the Markov assumption may serve as a basis for competitive pooling heuristics.</p><p>We did not consider Relative Positional Encoding in our work as pooling mechanism is not trivially applicable with it and some generalization of our method may be needed. In that case, as it demands more experiments and proofs, we will leave the generalization of the pooling method for future work.</p><p>Regarding the social impact and environmental sustainability, we actively considered the Earth's wellbeing by contributing a technique for reducing the computational demand of recent Deep Learning models. Our near-state-of-the-art DeepPyramidion model costs us 3 days of training on 8 NVIDIA A100 GPUs. Shallow models featuring trainable pooling were finished in about 2 days each, given the same hardware. Blockwise baselines cost us about 3.5x the price of respective pooling methods. The most prolonged training of the 8k Vanilla Transformer lasted for about 2 weeks. The total cost of training the models covered in this paper is about 2 months on the mentioned hardware, plus an additional month for models and ablations described in the appendices.</p><p>We roughly estimate that it is between half and onefourth of the total computation spent, including false runs, unpublished work, and initial experiments. The dataset preparation took less than 10 hours on 1 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>We propose representation pooling as a method to reduce the complexity of Transformer encoder-decoder models. Specifically, we optimize self-attention com-plexity and address the decoder's cross-attention complexity optimization, which has so far not been widely acknowledged by the research community. Moreover, the DeepPyramidion we introduced establishes results comparable to state-of-the-art, outperforming not only other systems relying on progressive word-vector elimination but also deeper, denser, and additionally pretrained models.</p><p>We tackle the problem by introducing a novel method of applying successive halving to a model's input in a tournament style. It is a theoretical improvement over existing approaches in terms of both computational complexity and approximation quality. Trainable Top-k selection allows to train scorer for a task and outperforms other pooling methods.</p><p>From the summarization task's point of view, the proposed end-to-end model is a significant theoretical improvement over the previous systems, where the extractive model was trained independently of the abstractive one. In contrast, our mechanism does not require the introduction of an additional training objective or training stage.</p><p>Our approach can be easily applied to other problems from Natural Language Processing and Computer Vision. E.g., in a recent work later than ours, Multiscale Vision Transformers were proposed. These, similarly to our Pyramidion model, introduce the bottleneck gradually along the encoding process of videos and images, leading to better results, and complexity (Fan et al.  <ref type="formula" target="#formula_0">2017)</ref>). We are looking forward to seeing these opportunities exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>For easy reproduction of the results, we release our utilities, code and pretrained models on the MIT license for all researchers not affiliated or working for Russian state-controlled institutions and public companies. The reason to ostracize scientists under those affiliations is the violent invasion of their armed forces on Ukraine, recklessly intended to inflict pain, threaten world peace and civilians life with nonhuman aggression against a sovereign nation.</p><p>The authors would like to thank Zofia Prochoroff and Pawe? Morawiecki for the helpful discussions on the draft of the paper. Moreover, we thank the reviewers for their comments and suggestions that helped improve the paper.</p><p>The Smart Growth Operational Programme supported this research under project no. POIR.01.01.01-00-0877 /19-00 (A universal platform for robotic automation of processes requiring text comprehension, with a unique level of implementation and service automation). </p><formula xml:id="formula_3">e T w 1 + b 1 ) ? w 2 + b 2 , where w 1 , w 2 ? R d and b 1 , b 2 ? R.</formula><p>PoWER-like. A column-wise sum over attention matrices A = Attn(E) from the preceding layer can be used as the usefulness score, that is v i = n j=1 A i,j as proposed by <ref type="bibr">Goyal et al. (2020)</ref> for hard top-k selection.</p><p>Embedding-based. Scoring can be performed based on a specified dimension in encoded space, i.e. by using a coordinate projection S(e) = e j , where j is a fixed index. This is a special case of the linear scoring function with fixed non-trainable weights.</p><p>Random. The baseline sampling scores randomly from a uniform distribution.</p><p>Index-based. A modulo-distributed score, that is nonzero for every k-th token, such as:</p><formula xml:id="formula_4">v i = 1 when i ? 0 (mod k) 0 otherwise</formula><p>Mean/Max Pooling. Pooling baselines characterized by aggregating scores within each window either by taking the mean value or the max value. In this case 4 nearest tokens were aggregated, and the window also traverse with the stride of 4. Both the PoWER-like and embedding-based scoring functions utilize mechanisms already provided in the Transformer model and are easy to use. Similarly to the index-based baseline method and the random one, they do not introduce any additional parameters to the model. The last two do not rely on a pooling operation at all.</p><p>PoWER was proposed assuming that the model's attention already contains useful information about the most critical parts of the input sequence <ref type="bibr">(Goyal et al., 2018)</ref>. In principle, it is possible to use its scorer with soft top-k, but we intended to follow the original formulation where scoring was followed by the hard top-k operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Results</head><p>Results obtained with the same, 4-layer Transpooler but different scoring functions are presented in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>All of the methods outperform the random baseline. Across them, the linear scorer achieved the highest evaluation metric. The index-based method we propose performs well, even though it does not require training.</p><p>In particular, models employing such fixed selection achieve better results than those equipped with a PoWER-like scorer. This can be attributed to the relatively low reduction of length required in the presented experiment: a model with index-based selection presumably learned to compress groups of the four nearest token neighbors.</p><p>Nevertheless, only nonlinear baseline approaches turned out not to be significantly worse than the linear scorer. Assuming preference towards a simpler method, the rest of the experiments were conducted using only the linear scorer.  <ref type="formula" target="#formula_0">(2019)</ref> replaced a sampling of k elements from the collection of items with Gumbel trick. Nevertheless, all the mentioned top-k approaches remain too costly as they perform many iterations over a considered vector. Their time performance degrades due to k softmaxes over the entire input length of n. Xie et al. (2020) parametrized the top-k operator in terms of an optimal transport problem. Employing such an algorithm instead of softmax may induce numerous zero weights in the attention matrix. However, this does not reduce the computational complexity of attention, as full-matrix multiplication has to be performed anyway and we are not concerned with such a method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Limitations and Assumptions</head><p>The choice of the selection operator is challenging, as it has to be trainable to instantiate a pooler. Let us view the hard top-k operator from a more geometric perspective.</p><p>In our setting, we consider sequences of n vectors from some vector space X (token embeddings), accompanied by real-valued scores, which are the basis for choosing the best k among n vectors. Thus, formally, a top-k operator should be defined as ? : X n ?R n ? X k , assigning to a sequence of n vectors x i ? X and their scores v i ? R a sequence of k vectors y i ? X. For ? to deserve the name 'top-k operator', the output vectors y i should depend mostly on the k input vectors x i with the largest corresponding scores.</p><p>In case of the hard top-k operator T , the y i are simply the vectors x i with the largest scores, i.e.</p><formula xml:id="formula_5">T ((x i ), (v i )) = (x i1 , x i2 , . . . , x i k ),<label>(2)</label></formula><p>where the indices i * are chosen so that</p><formula xml:id="formula_6">v i1 ? v i2 ? ? ? ? ? v i k ? v j for all j ? {i 1 , . . . , i k }.</formula><p>In other words, T can be described as a composition of sorting the sequence (x i ) according to descending scores v i , and projecting onto X k by discarding all but the first k elements.</p><p>To discuss the properties of T , let us denote by S n the set of all permutations of n indices {1, 2, . . . , n}. For every sequence (x 1 , x 2 , . . . , x n ) of length n there exists a permutation ? ? S n , such that (x ?(1) , x ?(2) , . . . , x ?(n) ) is sorted in descending order. We will refer to ? as the sorting permutation of the sequence (x i ). It is unique, provided that the elements x i are all distinct. Otherwise, the sequence x is invariant under permuting the indices of elements which are equal, and every two sorting permutations differ by such a factor.</p><p>For a permutation ? ? S n , define R ? ? R n as the set of all vectors v ? R n for which ? is a sorting permutation. The regions R ? cover R n and have disjoint interiors, containing vectors with pairwise distinct coordinates. The restriction of T to each region X n ? R ? is independent of v ? R ? , and it reduces to a linear operator:</p><p>T ((x i ), (v i )) = (x ?(1) , . . . , x ?(k) ).</p><p>( <ref type="formula">3)</ref> It follows that T is differentiable in the interior of each region X n ?R ? , and its non-differentiability points are constrained to the boundaries of the differentiability regions, i.e. the set X n ? D, where D = {x ? R n :</p><formula xml:id="formula_7">x i = x j for some i = j}.</formula><p>In particular, since D is a union of hyperplanes of codimension 1 in R n , the non-differentiability set of T has measure 0. Just as in the simpler case of the ReLU activation function, the non-differentiability of the hard top-k operator is not a serious problem-which is a possible misconception here.</p><p>The real problem is that although the gradient of T exists (almost everywhere), it is not particularly useful, since</p><formula xml:id="formula_8">?T ?v i = 0,<label>(4)</label></formula><p>because in each region X n ? R ? the operator T is independent of v i . This makes back-propagation through the scores impossible, and prevents training the scoring function. It could be seen as an extreme case of the vanishing gradient problem. In the next section, we introduce a mechanism not prone to this issue. return E 7: end procedure 8: for i ? 1, n do 21: Another key requirement for a robust top-k algorithm is to accurately approximate hard selection. Meanwhile, iteration-based algorithm disperses the probability mass over all items, resulting in a poor approximation of topk. This inefficiency of softmax over long vectors can be overcome by multiplying them by a large constant; however, this leads to numerical instability. Moreover, they tend to perform worse when employed as a neural network layer due to the long chain of backpropagation's dependencies.</p><formula xml:id="formula_9">9: procedure SORT(E, v) 10: v ? (v 1 , v 2 , ..), where v i ? v i+1 and v i ? v 11: E ? (E 1 , E 2 , ..), where v i ? v i+1 and v i ? v 12</formula><formula xml:id="formula_10">w ? PEAKEDSOFTMAX(v i , v 2n?i+1 ) 22: E i ? E i ? w 0 + E 2n?i+1 ? w 1 23: v i ? v i ? w 0 + v 2n?i+1 ? w</formula><p>In contrast, we always perform softmax over a pair of values, guaranteeing that there will be a candidate with a ? 0.5 probability assigned. After each pass, the best scoring k vectors with a small noise are obtained. It is a result of interpolating with the lower-scoring element from each pair.</p><p>As stated in the paper, we ensure that strong candidates have weakly-scoring opponents, strengthening their presence in the tournament's next round. The fundamental requirement of this trick is to sort inputs, resulting in an additional cost of O(n log(n)). However, in the case of modern CPUs, this cost is practically negligible. Yet, the sorting step can be omitted, leading to a slightly degraded top-k approximation. During the process, a vector with considerable noise may be produced for elements with indexes closer to the n/2. Nevertheless, some noise itself is desired, as it allows gradients to propagate to elements out of the top-k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Differential Properties</head><p>Recall the description of hard top-k from Section B.1. The main advantage introduced by soft top-k operator of Successive Halving, is providing reasonable gradients with respect to the scores v i . This allows to create a trainable pooling mechanism reducing the number of output embeddings. At the same time, it does not improve differentiability-which is another possible misconception we wanted to dispel.</p><p>In our proposed approach we assume that both n and k are powers of 2. The soft top-k operator is then defined through a composition of log 2 (n/k) halving operators H n : X n ? R n ? X n/2 ? R n/2 , reducing the number of vectors and their scores by half (see Appendix B).</p><p>The halving operator itself is the composition of sorting the vectors together with their scores, and a transformation C : X n ? R n ? X n/2 ? R n/2 producing n/2 convex combinations of the form</p><formula xml:id="formula_11">y i = w i x i + (1 ? w i )x n+1?i ,<label>(5)</label></formula><p>where the weights are the softmax of the pair of scores</p><formula xml:id="formula_12">(v i , v n+1?i ), i.e. w i = e vi e vi + e vn+1?i .<label>(6)</label></formula><p>Similarly as in the case of the hard top-k operator, the non-differentiability of H n arises from sorting. The convex combinations however smooth out some of the non-differentiabilities.</p><p>Let ? ? S n be the transposition of i and n + 1 ? i. The transformation C is then invariant under ? , which transposes both the weights (w i , 1 ? w i ), and vectors (x i , x n+1?i ). Hence, C is invariant under the subgroup G ? S n generated by such transpositions. As a consequence, on the set X n ? ??G? R ? the operator H is given by <ref type="figure">(1)</ref> , . . . , x ?(n) ), (v ?(1) , . . . , v ?(n) )), <ref type="formula">(7)</ref> and since C is differentiable, so is the restriction of H to this region.</p><formula xml:id="formula_13">H n ((x i ), (v i )) = = C((x ?</formula><p>In summary, while in the case of the hard top-k operator there are n! differentiability regions corresponding to sorting permutations, for the halving operator the differentiability regions are their unions corresponding to the cosets of G in S n . Since the generating transpositions of G are disjointly supported, it is isomorphic to Z n/2 2 , and therefore there are 2 ?n/2 n! differentiability regions.</p><p>The Successive Halving top-k operator is the composition of multiple halving operators, each introducing new non-differentiabilities, and the final projection onto X k . The arising non-differentiability set is still of measure 0, which is covered in detail in Appendix B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Differential Properties of Complete</head><p>Successive Halving Top-k Operator</p><p>We have shown that hard top-k operator makes backpropagation through the scores impossible, and prevents training the scoring function (Section B.1), whereas topn 2 halving is not prone to this problem (Section B.3).</p><p>We discuss the properties of full-featured Successive Halving bellow.</p><p>We have previously covered the case of H n . But the succesive halving top-k operator ? : X n ? R n ? X k is the composition</p><formula xml:id="formula_14">? = pr X k ?H 2k ? H 4k ? ? ? ? ? H n/2 ? H n (8)</formula><p>of multiple halving operators, each introducing new nondifferentiabilities, and the projection pr X k : X k ?R k ? X k . The non-differentiability set of ? is contained in the preimages of non-differentiability sets of the H i with respect to the preceding factors in the composition.</p><p>In such a situation it is generally not obvious that the resulting non-differentiability set is still of measure 0. To remedy this, let us first make some general observations about differentiability sets of mappings between manifolds.</p><p>For a mapping F : M ? N of smooth manifolds, denote by Z F the set of all points p ? M such that either F is not smooth in any neighborhood of p, or the rank of the derivative of F at p is not maximal. Observe that if the closure Z F of Z F ? M has measure 0, then the preimage F ?1 [E] of any set E ? N of measure 0 is itself of measure 0. Indeed, we may decompose such preimage as</p><formula xml:id="formula_15">F ?1 [E] = (F ?1 [E] ? Z F )? ? (F ?1 [E] ? (M \ Z F )),<label>(9)</label></formula><p>where the first component has measure zero (being a subset of Z F ), while the second component can be covered by a countable family of open sets on which F is differentiable, its derivative has maximal rank, and the constant rank theorem applies. Thus, locally on each set U of this cover, F is conjugate to a projection R m ? R n , and F | ?1 U [E] has measure 0. In the end, F ?1 [E] is decomposed into a countable union of zero-measure sets, so it has measure 0.</p><p>It follows that if G : N ? P is another mapping such that Z G has measure 0 in N , then Z G?F also has measure 0, since</p><formula xml:id="formula_16">Z G?F ? Z F ? F | ?1 M \Z F [Z G ] = = Z F ? F | ?1 M \Z F [Z G ]. (10)</formula><p>Above, F | ?1 M \Z F commutes with the closure operator because the restriction F | M \Z F is continuous. This result extends by induction to compositions of any number of mappings.</p><p>In order to show that ? defined as the composition <ref type="formula">(8)</ref> is almost everywhere differentiable, it therefore suffices to prove that Z ? has measure 0, which in turn amounts to showing that Z Hi has measure zero for any halving transformation H i . Recall that the halving transformation is the composition of the corresponding sorting operator and convex combination operator C defined in <ref type="formula" target="#formula_11">(5)</ref> and <ref type="formula" target="#formula_12">(6)</ref>.</p><p>For the sorting operator, the non-differentiability set is a union of a finite number of hyperplanes, hence a closed set of measure zero, and outside this set the derivative has maximal rank. The operator C on the other hand is smooth, and it remains to verify the rank of its derivative. Denote ((y i ), (u i )) = C((x i ), (v i )), and observe that ?u i /?x j = 0. Therefore it is enough to show that the matrices of partial derivatives (?y i /?x j ) ij and (?u i /?v j ) ij have linearly independent columns. For j ? {i, 2m + 1 ? i} we have</p><formula xml:id="formula_17">?y i ?x j = e vj e vi + e v2m+1?i &gt; 0,<label>(11)</label></formula><p>and ?y i /?x j = 0 for all other j. Since the sets {i, 2m + 1 ? i} are pairwise disjoint, the columns are linearly independent.</p><p>In case of ?u i /?v j the reasoning is similar. They are again nonzero only for j ? {i, 2m + 1 ? i}, for which</p><formula xml:id="formula_18">?u i ?v j = = e vj e v2m+1?j (v j ? v 2m+1?j ) + e vj + e v2m+1?j (e vj + e v2m+1?j ) 2 ,<label>(12)</label></formula><p>and this is strictly positive for at least one j ? {i, 2m + 1 ? i}. It follows that the columns are non-zero and have non-zero entries in different rows, so again they are linearly independent.</p><p>We have therefore shown that the Jacobian matrix of C has linearly independent columns, or in other words, its derivative is surjective at every point, which is what we needed to complete the proof that the nondifferentiability set of ? can be covered by a locally finite family of codimension 1 submanifolds, thus being of measure 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Performance</head><p>In <ref type="figure" target="#fig_4">Figure 5</ref> and 6 we show that our approach is highly similar to real top-k for any given k, and is significantly faster than alternative solutions, such as, e.g., iterative top-k selection.</p><p>We assessed the performance of the Successive Halving Top-k as compared to <ref type="bibr">Goyal et al. (2018)</ref> experimentally, on randomly sampled matrices E such that E ij ? U[?1, 1] and scores v i ? U[0, 1]. The selected k top-scoring vectors were compared to the real top-k selection using normalized Chamfer Cosine Similarity (nCCS) as given:</p><formula xml:id="formula_19">nCCS = 1 k k i=1</formula><p>max j? <ref type="bibr">[1,k]</ref> (cos(y i ,? j ))</p><p>Additionally, we measured an average time for processing a batch of size 16 on the NVIDIA A100 GPU, and addressed the question of how both algorithms differ in terms of speed ( <ref type="figure" target="#fig_4">Figure 5</ref>) and quality ( <ref type="figure" target="#fig_5">Figure 6</ref>), depending on k and n choices. One can notice that the higher the choice of k, the faster our algorithm is, and the slower is the iterative baseline of <ref type="bibr">Goyal et al. (2018)</ref> as predicted by their complexities. Our solution's qualitative robustness is proven by achieving higher similarity to real top-k for any given k. The score degrades as the number of rounds in the tournament increases, as each round introduces additional noise. To assess the importance of the sorting step, we removed it from the algorithm and compared with the proposed top-k. The results suggests that sorting is efficient and fast, as it is introduces average time overhead of 7.3%, while allowing error to be reduced by 45.2% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Summarization Experiments</head><p>This appendix covers other ablation studies and details of previously-reported experiments.  Shared setup. The models were trained using the Adam optimizer and cross-entropy loss, with hyperparameters specified in <ref type="table" target="#tab_7">Table 5</ref>. Validation was performed every three epochs on a validation set and the training stopped when no progress was observed taking the seven last scores into account. Presented scores are the best scores on a validation set. All of the considerations assumed the use of dot-product attention except for LSH and Efficient Transformers.</p><p>Vanilla. The exact setup of Vanilla Transformer is provided in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>Blockwise. We employed block attention with window size and stride equal to 512. We use block attention in the encoder, and the decoder features dense attention. The rest of the parameters follows shared setup. In the case of input chunking and use of blockwise attention, positions were calculated originating at the beginning of document. For simplicity, no positional embeddings were used on the decoder side. We argue, that embeddings passed down have already sufficient positional information from the encoder.</p><p>LSH. All of the previous considerations assumed the use of dot-product attention with memory and computational costs growing quadratically with the input size. Baselines relying on either efficient or LSH-based attention were conducted with two heads of local window attention that has been shown to improve models with long-range sparsity (Rae and Razavi, 2020). Without local attention, their results were several points lower. We assumed an LSH bucket size of 64 and four parallel hashes. Bucket size follows the authors' recommendations, whereas the number of hashes is a reasonable trade-off between memory complexity and approximation quality <ref type="bibr">(Kitaev et al., 2020)</ref>. Although one may obtain slightly better scores with eight hashes, it would result in higher memory consumption than in the case of full attention baselines for all of the considered sequence lengths. The rest of the parameters follow the Blockwise baseline. Funnel Transformer. The training setup of Funnel follows the original work, with the specific strided mean pooling and upsampling before passing to the decoder. For example, in Funnel 8k ? 512 (pooling from 8k to 512), 16 consecutive tokens were averaged after the first encoder layer. The decoder size is 8k, and the residual connections start from the first's layer output (taken just before pooling).</p><p>PoWER-BERT. As it comes to the PoWER-based models, we finetune Vanilla transformers with a progressive elimination of word vectors on the encoder side, following the approach of <ref type="bibr">Goyal et al. (2020)</ref>. We do not optimize the number of eliminated embeddings but assume the fixed reduction, similarly to our Pyramidion models. Additionally, <ref type="table" target="#tab_1">Table 2</ref> reports results with a progressive elimination of word vectors on the encoder side, adapted from PoWER- <ref type="bibr">BERT (Goyal et al., 2020)</ref>. Note that models are not trained from scratch in this approach, and we assumed blockwise attention to make it comparable with our models (see Appendix B). We started from appropriate checkpoints of a blockwise model and finetuned it for ten epochs. Here, we validated every one epoch. As training time, we provide times achieved during this finetuning. As presumed, a hard selection of word vectors offers an improved inference time for the cost of slightly decreased ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Number of Layers, Bottleneck Size</head><p>Deeper Pyramidion and Transpooler models with various pooling configurations were further examined in <ref type="table" target="#tab_9">Table 7</ref>. The training setup follows the previously described Transpooler setup. In the case of Pyramidion, we pool after the first or the second layer in the encoder. Scores of Pyramidion with pooling operation after the second and subsequent layers are significantly higher than #9, presumably because the representations after the first layer are not reliable enough to produce meaningful scores.</p><p>The Pyramidion with a three-layer encoder that reduces the input of 8k tokens gradually to 2k [#13] offers results 1.2 points better than the Vanilla model consuming input of the same length [#3]. Additionally, the complexity was reduced by a factor of 13 and 4 in the encoder and decoder, respectively, while achieving 3? training and 2.4? inference acceleration.</p><p>Finally, a series of Pyramidion experiments confirmed the applicability of gradual pooling with bottlenecks of 128, 512, and 2k sizes [#12, #11, #13]. It can be noticed that a reduction in the bottleneck's size leads to a decrease in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Effect of Block Size</head><p>We provide ablation experiments on block size effects in <ref type="table" target="#tab_10">Table 8</ref>. For simplicity, all of the previous experiments were conducted with an attention block size of 512 where applicable. Block consisting of 128 tokens lead to an improved encoder complexity and slightly lower computation time [#25, #28, #31]. It is not always achieved at the price of decreased ROUGE scores.</p><p>The scoring mechanism introduces some overhead during the training, which may be noticeable for shorter sequences. However, when it comes to the inference time we aimed at when proposing the method, it can be observed that a pooling operation positively impacts it. Pooling improves the inference time whether or not it is used in combination with blockwise attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Effect of Input Length</head><p>The importance of the longer input for the overall performance can be deduced by analyzing the performance of models #1-#8 in <ref type="table" target="#tab_1">Table 2</ref>, where we employed different input lengths for different models (Vanilla, Blockwise, and Pyramidion), and found out that a steady gain of 3.3 ? 3.6 R1 (and 2.1 ? 2.6 R2) points is observed for all of them when the input length is extended from 2k to 8k. Please note that while these results are provided in the ablation study that features a shallower network, the difference is significant and consistent. Hence, we did not repeat the experiment in the deeper setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Deep Model Setup</head><p>Training. <ref type="table" target="#tab_8">Table 6</ref> presents the shared setup of a Deep-Pyramidion and Blockwise, evaluated in the Section 5. We train until the validation score was not achieved for 7 consecutive validations.</p><p>Inference. We follow parameters for the generation of HAT-BART (Rohde et al., 2021): a beam width of 2, length penalty of 1, and minimum and maximum generation lengths of 72 and 966, respectively. We validated on the validation set every three epochs and chose the best performing model to generate outputs on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Hardware and Software Used</head><p>All experiments and benchmarks were performed on a DGX-A100 server equipped with eight NVIDIA Tesla A100 GPUs. We based our experiments using fairseq (Ott et al., 2019) v0.9.0, Python 3.6.10, PyTorch 1.6.0a0+9907a3e <ref type="bibr">(Paszke et al., 2019)</ref>, CUDA Version 11.0 and NVIDIA drivers 450.51.06. We trained in a full precision. <ref type="table" target="#tab_11">Table 9</ref> reports ROUGE scores for all of the evaluated models. In addition, we report 95% bootstrap confidence intervals of an estimate of the data here to mean scores.  The average time of processing a batch of documents is reported in <ref type="table">Table 10</ref>. We used batch of size 64 for training, and 8 for inference. Decoding experiments were synthetic. Specifically, we assumed a fixed length of either 256 or 512 tokens to decode to discount for lower processing time of models predicting the end of sequence token earlier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Detailed Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Training time for different model sizes of Vanilla Transformer, Blockwise, and Pyramidion 8k ? 512 with the input sequence length of 8192 tokens. Pooling is faster for models with 4 or more layers, achieving up to 3.8x speedup for 16-layer Transformer. Scores of a 2-layer version of these models do not differ significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, 2021). As it comes to Natural Language Processing, possible applications include Key Information Extraction, Machine Reading Comprehension, and Question Answering in scenarios where encoder-decoder models struggle or would struggle with input sequence length (see, e.g., Choi et al. (2017); Townsend et al. (2021); Kocisk? et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and DiscussionWe propose an O(n log 2 (n/k)) time-complexity algorithm for selecting k top-scoring representations from a vector of length n. An iterative approach of Goyal et al.(2018)with O(nk) complexity involves a higher cost for almost any k. The total number of exponentiation operations in the Successive Halving Top-k is bounded by 2n, as each round of the tournament halves the input size. Compared to kn in the case of the Goyal et al. (2018) algorithm, orders of magnitude savings in expensive exponentiation operations are obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Number of seconds required to process a batch of sequences (Y -axis). The lower the better. Results depending on n (X-axis) for various values of k, assuming k &lt; n. Depicted solution without sorting partially covers the data points of the solution with sorting(Our).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Approximation quality (Y -axis) in the nCCS metric. The higher the better. Results depending on n (X-axis) for various values of k, assuming k &lt; n.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Efficient Transformer. The training setup follows the original work. The Efficient Transformer does not have any specific parameters to determine, so all other training/validation choices agree with Blockwise baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Scores, complexity and benchmark depending on maximum encoder and decoder lengths, as well as used sparsification mechanism. All models features a 2-layer encoder and a 2-layer decoder, blocks of size 512. Results on arXiv summarization dataset(Cohan et al., 2018). Arrow ? denotes a pooling operation additional to the one between encoder and decoder. Note, that for the vanilla Transformer encoder lengths are equal to the decoder's length, whereas Transpoolers and Pyramidions lower the number of representations passed down to the decoder without the substantial quality decrese.</figDesc><table><row><cell>#</cell><cell>Architecture</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Lengths Encoder Decoder Training Inference R-1 R-2 Time ROUGE</cell></row><row><cell>1 2</cell><cell>Vanilla</cell><cell></cell><cell>? ?</cell><cell></cell><cell>512 2k</cell><cell>512 2k</cell><cell>0.13 0.60</cell><cell>4.23 28.1 5.77 38.2 14.0 8.3</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>8k</cell><cell>8k</cell><cell>4.46</cell><cell>13.27 41.8 16.1</cell></row><row><cell>4 5</cell><cell>Blockwise</cell><cell></cell><cell></cell><cell></cell><cell>2k 8k</cell><cell>2k 8k</cell><cell>0.31 0.85</cell><cell>5.28 38.6 14.1 11.49 41.9 16.7</cell></row><row><cell>6 7</cell><cell>Transpooler</cell><cell></cell><cell>? ?</cell><cell></cell><cell>2k 8k</cell><cell>512 512</cell><cell>0.54 1.44</cell><cell>4.24 39.1 14.6 4.28 41.8 16.4</cell></row><row><cell>8</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>8k</cell><cell>2k</cell><cell>1.26</cell><cell>5.51 42.7 16.7</cell></row><row><cell>9 10</cell><cell>LSH (Kitaev et al., 2020)</cell><cell cols="2">? ?</cell><cell></cell><cell>512 2k</cell><cell>512 2k</cell><cell>0.19 0.56</cell><cell>4.27 28.5 5.92 33.6 10.5 7.5</cell></row><row><cell>11</cell><cell></cell><cell cols="2">?</cell><cell></cell><cell>8k</cell><cell>8k</cell><cell>1.69</cell><cell>13.41 35.7 11.2</cell></row><row><cell>12 13</cell><cell cols="2">Efficient (Shen et al., 2021)</cell><cell cols="2">? ?</cell><cell>512 2k</cell><cell>512 2k</cell><cell>0.12 0.29</cell><cell>4.20 28.4 5.91 34.1 10.4 7.8</cell></row><row><cell>14</cell><cell></cell><cell></cell><cell cols="2">?</cell><cell>8k</cell><cell>8k</cell><cell>0.82</cell><cell>13.75 35.0 10.8</cell></row><row><cell>15 16</cell><cell>PoWER (Goyal et al., 2020)</cell><cell></cell><cell>? ?</cell><cell></cell><cell>2k ? 1k 8k ? 2k</cell><cell>512 512</cell><cell>1.04 1.87</cell><cell>4.28 35.3 12.7 5.33 36.9 14.1</cell></row><row><cell>17</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>8k ? 4k</cell><cell>2k</cell><cell>2.06</cell><cell>6.92 42.0 16.5</cell></row><row><cell>18 19</cell><cell>Funnel (Dai et al., 2020)</cell><cell></cell><cell>? ?</cell><cell cols="2">2k ? 512 8k ? 512</cell><cell>2k 8k</cell><cell>0.61 1.78</cell><cell>4.01 38.6 14.3 4.03 41.8 16.5</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>8k ? 2k</cell><cell>8k</cell><cell>1.53</cell><cell>5.25 42.0 16.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison to SOTA on long document summarization tasks. Our models have no pretraining whereas ? were initialized from BART, ? -from RoBERTa, * -from PEGASUS(Zhang et al., 2021; Rohde et al., 2021;  Zaheer et al., 2020b; Gidiotis and Tsoumakas, 2020).</figDesc><table><row><cell>Architecture</cell><cell>arXiv R-1</cell><cell>R-2</cell><cell cols="2">PubMed R-1 R-2</cell><cell>Params</cell><cell>Time Train. Infer.</cell></row><row><cell>PoolingFormer ?</cell><cell cols="2">48.47 20.23</cell><cell>-</cell><cell cols="2">-&gt;406M</cell><cell>-</cell><cell>-</cell></row><row><cell>HAT-BART ?</cell><cell cols="5">46.74 19.19 48.25 21.35 &gt;406M</cell><cell>-</cell><cell>-</cell></row><row><cell>BigBird-PEGASUS ?</cell><cell cols="4">46.63 19.02 46.32 20.65</cell><cell>568M</cell><cell>-</cell><cell>-</cell></row><row><cell>Dancer PEGASUS  *</cell><cell cols="4">45.01 17.60 46.34 19.97</cell><cell>568M</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Blockwise (our baseline) 46.85 19.39</cell><cell>-</cell><cell>-</cell><cell>124M</cell><cell>4.85 37.15</cell></row><row><cell>DeepPyramidion (our)</cell><cell cols="4">47.15 19.99 47.81 21.14</cell><cell>124M</cell><cell>2.71</cell><cell>8.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Multiple scoring methods can be proposed. The most straightforward is to use a linear scoring function used in conventional token classification, S(e) = e T w + b, where w ? R d and b ? R are trainable parameters.</figDesc><table><row><cell>A Scorers' Ablations</cell><cell></cell></row><row><cell>Linear. Nonlinear. A quite natural next step is to include non-linearity. We follow the specification of RoBERTa's</cell><cell>Guillaume Calmettes, Gordon B. Drummond, and Sarah L. Vowler. 2012. Making do with what we have: use your bootstraps. The Journal of Physiol-ogy, 590(15):3403-3406.</cell></row><row><cell>classification head (Liu et al., 2019), defined as S(e) =</cell><cell>Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-</cell></row><row><cell>tanh(</cell><cell>tive summarization with reinforce-selected sentence</cell></row><row><cell></cell><cell>rewriting.</cell></row><row><cell></cell><cell>Rewon Child, Scott Gray, Alec Radford, and Ilya</cell></row><row><cell></cell><cell>Sutskever. 2019. Generating long sequences with</cell></row><row><cell></cell><cell>sparse transformers.</cell></row><row><cell></cell><cell>Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia</cell></row><row><cell></cell><cell>Polosukhin, Alexandre Lacoste, and Jonathan Berant.</cell></row><row><cell></cell><cell>2017. Coarse-to-fine question answering for long</cell></row><row><cell></cell><cell>documents. In Proceedings of the 55th Annual Meet-</cell></row><row><cell></cell><cell>ing of the Association for Computational Linguistics</cell></row><row><cell></cell><cell>(Volume 1: Long Papers), pages 209-220, Vancouver,</cell></row><row><cell></cell><cell>Canada. Association for Computational Linguistics.</cell></row><row><cell></cell><cell>Krzysztof Choromanski, Valerii Likhosherstov, David</cell></row><row><cell></cell><cell>Dohan, Xingyou Song, Andreea Gane, Tamas Sar-</cell></row><row><cell></cell><cell>los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,</cell></row><row><cell></cell><cell>Lukasz Kaiser, David Belanger, Lucy Colwell, and</cell></row><row><cell></cell><cell>Adrian Weller. 2021. Rethinking attention with per-</cell></row><row><cell></cell><cell>formers.</cell></row><row><cell></cell><cell>Arman Cohan, Franck Dernoncourt, Doo Soon Kim,</cell></row><row><cell></cell><cell>Trung Bui, Seokhwan Kim, Walter Chang, and Nazli</cell></row><row><cell></cell><cell>Goharian. 2018. A discourse-aware attention model</cell></row><row><cell></cell><cell>for abstractive summarization of long documents. In</cell></row><row><cell></cell><cell>Proceedings of the 2018 Conference of the North</cell></row><row><cell></cell><cell>American Chapter of the Association for Computa-</cell></row><row><cell></cell><cell>tional Linguistics: Human Language Technologies,</cell></row><row><cell></cell><cell>Volume 2 (Short Papers), pages 615-621, New Or-</cell></row><row><cell></cell><cell>leans, Louisiana. Association for Computational Lin-</cell></row><row><cell></cell><cell>guistics.</cell></row><row><cell></cell><cell>Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le.</cell></row><row><cell></cell><cell>2020. Funnel-transformer: Filtering out sequential</cell></row><row><cell></cell><cell>redundancy for efficient language processing.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of different scorers, using the same 4-layer Transpooler model with reduction from 2048 to 512 representations. The difference of 0.4 is significant.(Calmettes et al., 2012).</figDesc><table><row><cell>Scorer</cell><cell cols="2">ROUGE-1 ROUGE-2</cell></row><row><cell>Linear</cell><cell>39.1</cell><cell>14.6</cell></row><row><cell>Nonlinear</cell><cell>38.9</cell><cell>14.6</cell></row><row><cell>Random</cell><cell>32.3</cell><cell>11.4</cell></row><row><cell>Index-based</cell><cell>38.2</cell><cell>13.9</cell></row><row><cell>Embedding-based</cell><cell>37.6</cell><cell>14.0</cell></row><row><cell>PoWER-like</cell><cell>36.9</cell><cell>13.6</cell></row><row><cell>Mean Pooling</cell><cell>38.1</cell><cell>13.9</cell></row><row><cell>Max Pooling</cell><cell>38.4</cell><cell>14.2</cell></row><row><cell cols="3">B Successive Halving Top-k Algorithm</cell></row><row><cell cols="3">Goyal et al. (2018) provides the most similar relaxation</cell></row><row><cell cols="3">for beam search, where they continuously relaxed the</cell></row><row><cell cols="3">top-k-argmax procedure by performing softmaxes iter-</cell></row><row><cell cols="3">atively k times and masking the previously extracted</cell></row><row><cell cols="3">values. Each beam can contribute to the newly selected</cell></row><row><cell cols="3">beam in every iteration, based on its distance to the max</cell></row><row><cell cols="3">value. By replacing one-hot coded vectors with their</cell></row><row><cell cols="3">expectations in a similar vein, Pl?tz and Roth (2018)</cell></row><row><cell cols="3">relaxed the KNN hard top-k selection rule. Xie and</cell></row><row><cell>Ermon</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters for shallow models used in the summarization experiments.</figDesc><table><row><cell>Hparam</cell><cell>Value</cell></row><row><cell>Encoder Layers</cell><cell>2</cell></row><row><cell>Decoder Layers</cell><cell>2</cell></row><row><cell>Vocab size</cell><cell>32k</cell></row><row><cell>Dropouts</cell><cell>.1</cell></row><row><cell>Activation</cell><cell>ReLU</cell></row><row><cell>Emb dim</cell><cell>512</cell></row><row><cell>FFN emb dim</cell><cell>2048</cell></row><row><cell cols="2">Encoder positional emb sinusoidal</cell></row><row><cell>Decoder positional emb</cell><cell>None</cell></row><row><cell>Batch size</cell><cell>256</cell></row><row><cell>Learning rate</cell><cell>5e-4</cell></row><row><cell>Learning rate decay</cell><cell>-</cell></row><row><cell>Shared emb</cell><cell>True</cell></row><row><cell>Weight decay</cell><cell>.1</cell></row><row><cell>Attention heads</cell><cell>8</cell></row><row><cell>Beam size</cell><cell>8</cell></row><row><cell>Total parameters</cell><cell>32M</cell></row><row><cell>C.1 Shallow Models Setup</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for DeepPyramidion and deep Blockwise baseline models used in the summarization experiments. Transpooler features linear scorer and successive halving algorithm. It uses Blockwise's setup of blockwise attention. Pooling is performed after the last encoder layer. The number of halving rounds depends on the proportion of maximal input sequence size and the desired bottleneck size. Transpoolers models were trained and validated with our soft top-k.</figDesc><table><row><cell>Hparam</cell><cell>Value</cell></row><row><cell>Encoder Layers</cell><cell>6</cell></row><row><cell>Decoder Layers</cell><cell>6</cell></row><row><cell>Vocab size</cell><cell>32k</cell></row><row><cell>Dropouts</cell><cell>.1</cell></row><row><cell>Activation</cell><cell>ReLU</cell></row><row><cell>Emb dim</cell><cell>768</cell></row><row><cell>FFN emb dim</cell><cell>3072</cell></row><row><cell cols="2">Encoder positional emb sinusoidal</cell></row><row><cell>Decoder positional emb</cell><cell>None</cell></row><row><cell>Batch size</cell><cell>256</cell></row><row><cell>Learning rate</cell><cell>5e-4</cell></row><row><cell>Learning rate decay</cell><cell>-</cell></row><row><cell>Shared emb</cell><cell>True</cell></row><row><cell>Weight decay</cell><cell>.1</cell></row><row><cell>Attention heads</cell><cell>8</cell></row><row><cell>Warmup steps</cell><cell>5k</cell></row><row><cell>Total Parameters</cell><cell>124M</cell></row><row><cell>Transpooler.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Scores and complexities of the Pyramidion and Transpooler with different encoder and decoder depths, as well as various lengths after pooling. The input of 8k representations pooled gradually to decoder length. Two-layer decoder and encoder of depth ranging from 2 to 4 layers. Arrow ? denotes an additional pooling between encoder layers.</figDesc><table><row><cell>#</cell><cell>Architecture</cell><cell></cell><cell>Encoder</cell><cell>Lengths</cell><cell cols="2">Time Decoder Training Inference R-1 R-2 ROUGE</cell></row><row><cell>21 22 23 24</cell><cell>Pyramidion</cell><cell>? ? ? ? ? ? ?</cell><cell cols="3">8k ? 2k 8k, 8k ? 2k 8k, 8k ? 2k ? 512 128 512 512 8k, 8k ? 4k 2k</cell><cell>1.07 1.55 1.78 1.47</cell><cell>4.18 31.1 11.5 4.26 41.2 16.5 3.74 37.3 14.3 5.49 43.0 17.2</cell></row><row><cell cols="2">25 Transpooler 26</cell><cell></cell><cell cols="2">8k, 8k 8k, 8k, 8k</cell><cell>2k 2k</cell><cell>1.26 1.74</cell><cell>5.51 42.7 16.7 5.54 43.1 17.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Scores depending on blockwise attention block size and sparsification mechanism with 2k and 8k encoder input length considered. Different models with a two-layer encoder and a two-layer decoder.</figDesc><table><row><cell>#</cell><cell>Pooling</cell><cell></cell><cell>Block size</cell><cell cols="4">Lengths Encoder Decoder Training Inference R-1 R-2 Time ROUGE</cell></row><row><cell>27 28</cell><cell>No pooling</cell><cell>? ?</cell><cell>128 512</cell><cell>2k 2k</cell><cell>2k 2k</cell><cell>0.25 0.31</cell><cell>5.11 39.1 14.4 5.28 38.6 14.1</cell></row><row><cell>29</cell><cell></cell><cell>?</cell><cell>(without)</cell><cell>2k</cell><cell>2k</cell><cell>0.60</cell><cell>5.77 38.2 14.0</cell></row><row><cell>30 31</cell><cell>Transpooler</cell><cell>? ?</cell><cell>128 512</cell><cell>2k 2k</cell><cell>512 512</cell><cell>0.49 0.54</cell><cell>3.99 38.2 14.1 4.24 39.1 14.6</cell></row><row><cell>32</cell><cell></cell><cell>?</cell><cell>(without)</cell><cell>2k</cell><cell>512</cell><cell>0.82</cell><cell>4.49 37.1 13.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Scores with 95% bootstrap confidence intervals of an estimate of the data (Calmettes et al., 2012).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Preliminary work regarding this method was previously presented in the form of a Student Abstract, seePietruszka  et al. (2020).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This view is supported by results of PoolingFormer that are concurrent to our work(Zhang et al., 2021). Despite that, at first sight, the methods seem similar and the authors present an interesting use of pooling in the attention, we argue that the mentioned model suffers from several weaknesses that are not present in our work. First of all, in the PoolingFormer model vectors are not removed from computations in further layers. Hence logarithmic complexity of the number of layers does not apply. PoolingFormer's approach suffers from having three orders of magnitude more calculations than when a global pooling based on scores of individual tokens is considered.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Longformer: The Long-Document Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>abs/2004.05150</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main"># ROUGE-1 (CI) ROUGE-2 (CI)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">We assumed a fixed length of 256 or 512 tokens to decode to discount for lower processing time of models predicting the end of sequence token earlier</title>
		<idno>Training Inference @ 256 Inference @ 512</idno>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>Mean time of processing and inference in seconds ? standard deviation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
