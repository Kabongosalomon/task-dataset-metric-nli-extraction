<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOWARDS NONLINEAR DISENTANGLEMENT IN NATURAL DATA WITH TEMPORAL SPARSE CODING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Klindt</surname></persName>
							<email>klindt.david@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schott</surname></persName>
							<email>lukas.schott@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
							<email>yash.sharma@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Ustyuzhaninov</surname></persName>
							<email>ivan.ustyuzhaninov@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
							<email>wieland.brendel@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
							<email>matthias.bethge@bethgelab.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><forename type="middle">M</forename><surname>Paiton</surname></persName>
							<email>dylan.paiton@bethgelab.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of T?bingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TOWARDS NONLINEAR DISENTANGLEMENT IN NATURAL DATA WITH TEMPORAL SPARSE CODING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Disentangling the underlying generative factors from data has so far been limited to carefully constructed scenarios. We propose a path towards natural data by first showing that the statistics of natural data provide enough structure to enable disentanglement, both theoretically and empirically. Specifically, we provide evidence that objects in natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. Leveraging this finding we provide a novel proof that relies on a sparse prior on temporally adjacent observations to recover the true latent variables up to permutations and sign flips, providing a stronger result than previous work. We show that equipping practical estimation methods with our prior often surpasses the current state-of-the-art on several established benchmark datasets without any impractical assumptions, such as knowledge of the number of changing generative factors. Furthermore, we contribute two new benchmarks, Natural Sprites and KITTI Masks, which integrate the measured natural dynamics to enable disentanglement evaluation with more realistic datasets. We test our theory on these benchmarks and demonstrate improved performance. We also identify non-obvious challenges for current methods in scaling to more natural domains. Taken together our work addresses key issues in disentanglement research for moving towards more natural settings. * ? Equal contribution. Code: https://github.com/bethgelab/slow_disentanglement Published as a conference paper at ICLR 2021 addressing this question is that generative factors of natural data have sparse transitions. To estimate these generative factors, we compute statistics on measured transitions of area and position for object masks from large-scale, natural, unstructured videos. Specifically, we extracted over 300,000 object segmentation mask transitions from YouTube-VOS <ref type="bibr" target="#b64">(Xu et al., 2018;</ref><ref type="bibr" target="#b65">Yang et al., 2019)</ref> and KITTI-MOTS (Voigtlaender et al., 2019; Geiger et al., 2012;<ref type="bibr" target="#b31">Milan et al., 2016)</ref> (discussed in detail in Appendix D). We fit generalized Laplace distributions to the collected data (Eq. 2), which we indicate with orange lines in <ref type="figure">Fig. 1</ref>. We see empirically that all marginal distributions of temporal transitions are highly sparse and that there exist complex dependencies between natural factors (e.g. motion typically affects both position and apparent size). In this study, we focus on the sparse marginals, which we believe constitutes an important advance that sets the stage for solving further issues and eventually applying the technology to real-world problems. With this information at hand, we are able to provide a stronger proof for capturing the underlying generative factors of the data up to permutations and sign flips that is not covered by previous work (Hyv?rinen and Morioka, 2016; Khemakhem et al., 2020a). Thus, we present the first work, to the best of our knowledge, which proposes a theoretically grounded solution that covers the statistics observed in real videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SlowVAE</head><p>Kitti (mean(?t) = 0.05s) 66.1 (4.5) PM-VAE (16) Kitti (mean(?t) = 0.05s) 63.1 (9.3) PM-VAE (10) Kitti (mean(?t) = 0.05s) 57.4 (8.5) PM-VAE <ref type="formula">(8)</ref> Kitti (mean(?t) = 0.05s) 59.0 (5.6) PM-VAE <ref type="formula">(4)</ref> Kitti (mean(?t) = 0.05s) 51.8 (9.2) PM-VAE <ref type="formula">(2)</ref> Kitti (mean(?t) = 0.05s) 50.3 (7.4) PM-VAE <ref type="formula">(1)</ref> Kitti (mean(?t) = 0.05s) 38.4 (6.8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SlowVAE</head><p>Kitti (mean(?t) = 0.15s) 79.6 (5.8) PM-VAE (16) Kitti (mean(?t) = 0.15s) 69.6 (5.9) PM-VAE (10) Kitti (mean(?t) = 0.15s) 78.2 (6.0) PM-VAE <ref type="formula">(8)</ref> Kitti (mean(?t) = 0.15s) 73.8 (10.0) PM-VAE <ref type="formula">(4)</ref> Kitti (mean(?t) = 0.15s) 67.9 (10.4) PM-VAE <ref type="formula">(2)</ref> Kitti (mean(?t) = 0.15s) 60.7 (8.8) PM-VAE <ref type="formula">(1)</ref> Kitti (mean(?t) = 0.15s) 60.9 (9.1)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Natural scene understanding can be achieved by decomposing the signal into its underlying factors of variation. An intuitive approach for this problem assumes that a visual representation of the world can be constructed via a generative process that receives factors as input and produces natural signals as output <ref type="bibr" target="#b3">(Bengio et al., 2013)</ref>. This analogy is justified by the fact that our world is composed of distinct entities that can vary independently, but with regularity imposed by physics. What makes the approach appealing is that it formalizes representation learning by directly comparing representations to underlying ground-truth states, as opposed to the indirect evaluation of benchmarking against heuristic downstream tasks (e.g. object recognition). However, the core issue with this approach is non-identifiability, which means a set of possible solutions may all appear equally valid to the model, while only one identifies the true generative factors.</p><p>Our work is motivated by the question of whether the statistics of natural data will allow for the formulation of an identifiable model. Our core observation that enables us to make progress in Our contributions are: With measurements from unstructured natural video annotations we provide evidence that natural generative factors undergo sparse changes across time. We provide a proof of identifiability that relies on the observed sparse innovations to identify nonlinearly mixed sources up to a permutation and sign-flips, which we then validate with practical estimation methods for empirical comparisons. We leverage the natural scene information to create novel datasets where the latent transitions between frames follow natural statistics. These datasets provide a benchmark to evaluate how well models can uncover the true latent generative factors in the presence of realistic dynamics. We demonstrate improved disentanglement over previous models on existing datasets and our contributed ones with quantitative metrics from both the disentanglement <ref type="bibr" target="#b22">(Locatello et al., 2018)</ref> and the nonlinear ICA community (Hyv?rinen and <ref type="bibr">Morioka, 2016)</ref>. We show via numerous visualization techniques that the learned representations for competing models have important differences, even when quantitative metrics suggest that they are performing equally well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK -DISENTANGLEMENT AND NONLINEAR ICA</head><p>Disentangled representation learning has its roots in blind source separation <ref type="bibr" target="#b12">(Cardoso, 1989;</ref><ref type="bibr">Jutten and Herault, 1991)</ref> and shares goals with fields such as inverse graphics <ref type="bibr">(Kulkarni et al., 2015;</ref><ref type="bibr" target="#b67">Yildirim et al., 2020;</ref><ref type="bibr" target="#b1">Barron and Malik, 2012)</ref> and developing models of invariant neural computation <ref type="bibr">(Hyv?rinen and Hoyer, 2000;</ref><ref type="bibr" target="#b62">Wiskott and Sejnowski, 2002;</ref><ref type="bibr" target="#b47">Sohl-Dickstein et al., 2010</ref>) (see <ref type="bibr" target="#b3">Bengio et al., 2013</ref>, for a review). A disentangled representation would be valuable for a wide variety of machine learning applications, including sample efficiency for downstream tasks <ref type="bibr" target="#b22">(Locatello et al., 2018;</ref><ref type="bibr">Gao et al., 2019)</ref>, fairness <ref type="bibr" target="#b23">(Locatello et al., 2019;</ref><ref type="bibr" target="#b17">Creager et al., 2019)</ref> and interpretability <ref type="bibr" target="#b3">(Bengio et al., 2013;</ref><ref type="bibr" target="#b29">Higgins et al., 2017;</ref><ref type="bibr" target="#b0">Adel et al., 2018)</ref>. Since there is no agreed upon definition of disentanglement in the literature, we adopt two common measurable criteria: i) each encoding element represents a single generative factor and ii) the values of generative factors are trivially decodable from the encoding <ref type="bibr" target="#b42">(Ridgeway and Mozer, 2018;</ref><ref type="bibr">Eastwood and Williams, 2018)</ref>.</p><p>Uncovering the underlying factors of variation has been a long-standing goal in independent component analysis (ICA) <ref type="bibr" target="#b16">(Comon, 1994;</ref><ref type="bibr" target="#b2">Bell and Sejnowski, 1995)</ref>, which provides an identifiable solution for disentangling data mixed via an invertible linear generator receiving at most one Gaussian factor as input. Recent unsupervised approaches for nonlinear generators have largely been based on Variational Autoencoders (VAEs) <ref type="bibr">(Kingma and Welling, 2013)</ref> and have assumed that the data is independent and identically distributed (i.i.d.) <ref type="bibr" target="#b22">(Locatello et al., 2018)</ref>, even though nonlinear methods that make this i.i.d. assumption have been proven to be non-identifiable <ref type="bibr">(Hyv?rinen and Pajunen, 1999;</ref><ref type="bibr" target="#b22">Locatello et al., 2018)</ref>. Nonetheless, the bottom-up approach of starting with a nonlinear generator that produces well-controlled data has led to considerable achievements in understanding nonlinear disentanglement in VAEs <ref type="bibr" target="#b29">(Higgins et al., 2017;</ref><ref type="bibr" target="#b8">Burgess et al., 2018;</ref><ref type="bibr" target="#b43">Rolinek et al., 2019;</ref><ref type="bibr" target="#b13">Chen et al., 2018)</ref>, consolidating ideas from neural computation and machine learning <ref type="bibr">(Khemakhem et al., 2020a)</ref>, and seeking a principled definition of disentanglement <ref type="bibr" target="#b41">(Ridgeway, 2016;</ref><ref type="bibr" target="#b8">Higgins et al., 2018;</ref><ref type="bibr">Eastwood and Williams, 2018)</ref>.</p><p>Recently, Hyv?rinen and colleagues (Hyv?rinen and <ref type="bibr">Morioka, 2016;</ref><ref type="bibr">Hyv?rinen et al., 2018)</ref> showed that a solution to identifiable nonlinear ICA can be found by assuming that generative factors are conditioned on an additional observed variable, such as past states or the time index itself. This contribution was generalized by <ref type="bibr">Khemakhem et al. (2020a)</ref> past the nonlinear ICA domain to any consistent parameter estimation method for deep latent-variable models, including the VAE framework. However, the theoretical assumptions underlying this branch of work do not account for the sparse transitions we observe in the statistics of natural scenes, which we discuss in further detail in appendix F.1.1. Another branch of work requires some form of supervision to demonstrate disentanglement <ref type="bibr" target="#b51">(Szab? et al., 2017;</ref><ref type="bibr" target="#b44">Shu et al., 2019;</ref>. We select two of the above approaches, that are both different in their formulation and state-of-the-art in their respective empirical settings, Hyv?rinen and <ref type="bibr">Morioka (2017)</ref> and , for our experiments below. The motivation of our method and dataset contributions is to address the limitations of previous approaches and to enable unsupervised disentanglement learning in more naturalistic scenarios. <ref type="bibr">1</ref> The fact that physical processes bind generative factors in temporally adjacent natural video segments has been thoroughly explored for learning in neural networks <ref type="bibr">(Hinton, 1990;</ref><ref type="bibr">F?ldi?k, 1991;</ref><ref type="bibr" target="#b32">Mitchison, 1991;</ref><ref type="bibr" target="#b62">Wiskott and Sejnowski, 2002;</ref><ref type="bibr" target="#b18">Denton and Birodkar, 2017)</ref>. We propose a method that uses time information in the form of an L 1 -sparse temporal prior, which is motivated by the natural scene measurements presented above as well as by previous work <ref type="bibr" target="#b45">(Simoncelli and Olshausen, 2001;</ref><ref type="bibr" target="#b35">Olshausen, 2003;</ref><ref type="bibr">Hyv?rinen et al., 2003;</ref><ref type="bibr" target="#b11">Cadieu and Olshausen, 2012)</ref>. Such a prior would intuitively allow for sharp changes in some latent factors, while most other factors remain unchanged between adjacent time-points. Almost all similar methods are variants of slow feature analysis (SFA, <ref type="bibr" target="#b62">Wiskott and Sejnowski, 2002)</ref>, which measure slowness in terms of the Euclidean (i.e. L 2 , or log Gaussian) distance between temporally adjacent encodings. Related to our approach, a probabilistic interpretation of SFA has been previously proposed <ref type="bibr" target="#b57">(Turner and Sahani, 2007)</ref>, as well as extensions to variational inference <ref type="bibr">(Grathwohl and Wilson, 2016)</ref>. <ref type="bibr">Additionally, Hashimoto (2003)</ref> suggested that a sparse (Cauchy) slowness prior improves correspondence to biological complex cells over the L 2 slowness prior in a two-layer model. However, to the best of our knowledge, an L 1 temporal prior has previously only been used in deep auto-encoder frameworks when applied to semi-supervised tasks <ref type="bibr" target="#b33">(Mobahi et al., 2009;</ref><ref type="bibr" target="#b68">Zou et al., 2012)</ref>, and was mentioned in <ref type="bibr" target="#b11">Cadieu and Olshausen (2012)</ref>, who used an L 2 prior, but claimed that an L 1 prior performed similarly on their task. Similar to <ref type="bibr">Hyv?rinen et al. (Hyv?rinen and Morioka, 2016;</ref><ref type="bibr">Hyv?rinen et al., 2018)</ref>, we only assume that the latent factors are temporally dependent, thus avoiding assuming knowledge of the number of factors where the two observations differ <ref type="bibr" target="#b44">(Shu et al., 2019;</ref>.</p><p>Most of the standard datasets for disentanglement (dSprites <ref type="bibr" target="#b29">(Matthey et al., 2017)</ref>, <ref type="bibr">Cars3D (Reed et al., 2015)</ref>, <ref type="bibr">SmallNORB (LeCun et al., 2004)</ref>, <ref type="bibr">Shapes3D (Kim and Mnih, 2018)</ref>, <ref type="bibr">MPI3D (Gondal et al., 2019)</ref>) have been compiled into a disentanglement library (DisLib) by <ref type="bibr" target="#b22">Locatello et al. (2018)</ref>. However, all of the DisLib datasets are limited in that the data generating process is independent and identically distributed (i.i.d.) and all generative factors are assumed to be discrete. In a follow-up study,  proposed combining pairs of images such that only k factors change, as this matches their modeling assumptions required to prove identifiability. Here, k ? U{1, D ? 1} and D denotes the number of ground-truth factors, which are then sampled uniformly. We additionally use the measurements from <ref type="figure" target="#fig_0">Fig. 1</ref> to construct datasets for evaluating disentanglement that have time transitions which directly correspond to natural dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THEORY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GENERATIVE MODEL</head><p>We have provided evidence to support the hypothesis that generative factors of natural videos have sparse temporal transitions (see <ref type="figure" target="#fig_0">Fig. 1</ref>). To model this process, we assume temporally adjacent input pairs (x t?1 , x t ) coming from a nonlinear generator that maps factors to images x = g(z), where generative factors are dependent over time:</p><p>p(z t , z t?1 ) = p(z t |z t?1 )p(z t?1 ).</p><p>(1) Assume the observed data (x t , x t?1 ) comes from the following generative process, where different latent factors are assumed to be independent (cf. Appendix F.2):</p><formula xml:id="formula_0">x = g(z), p(z t?1 ) = d i=1 p(z t?1,i ), p(z t |z t?1 ) = d i=1 ?? 2?(1/?) exp ?(?|z t,i ? z t?1,i | ? ),<label>(2)</label></formula><p>where ? is the distribution rate, p(z t?1 ) is a factorized Gaussian prior N (0, I) (as in <ref type="bibr">Kingma and Welling, 2013)</ref> and p(z t |z t?1 ) is a factorized generalized Laplace distribution <ref type="bibr" target="#b49">(Subbotin, 1923)</ref> with shape parameter ?, which determines the shape and especially the kurtosis of the function. 2 Intuitively, smaller ? implies larger kurtosis and sparser temporal transitions of the generative factors (special cases are Gaussian, ? = 2, and Laplacian, ? = 1). Critically, for our proof we assume ? &lt; 2 to ensure that temporal transitions are sparse. The novelty of our approach lies in our explicit modeling of sparse transitions that cover the statistics of natural data, which results in a stronger identifiability proof than previously achieved (see Appendix F.1.1 for a more detailed comparison with Hyv?rinen and <ref type="bibr">Morioka, 2017;</ref><ref type="bibr">Khemakhem et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">IDENTIFIABILITY PROOF</head><p>Theorem 1 For a ground-truth (g * , ? * , ? * ) and a learned (g, ?, ?) model as defined in Eq.</p><p>(2), if the functions g * and g are injective and differentiable almost everywhere, ? * = ?, ? * = ? &lt; 2 (i.e. there is no model misspecification) and the distributions of pairs of images generated from the priors z * ? p * (z) and z ? p(z) generated as (g * (z * t?1 ), g * (z * t )) and (g(z t?1 ), g(z t )), respectively, are matched almost everywhere, then g = g * ? ?, where ? is composed of a permutation and sign flips. The formal proof is provided in Appendix A.1. Similar to linear ICA, but in the temporal domain, we have to assume that the transitions of generative factors across time be non-Gaussian. Specifically, if the temporal changes of ground-truth factors are sparse, then the only generator consistent with the observations is the ground-truth one (up to a permutation and sign flips). The main idea behind the proof is to represent g as g * ? h and note that if h were not a permutation, then the distributions ((g * ? h)(z t?1 ), (g * ? h)(z t )) and (g * (z * t?1 ), g * (z * t )) would not match, due to the injectivity of g * . Whether or not these distributions are the same is equivalent to whether or not the distributions of pairs (z t?1 , z t ) and (h(z t?1 ), h(z t )) are the same. For these distributions to be the same, the function h must preserve the Gaussian marginal for the first time step as well as the joint distribution, implying that it must preserve both the vector lengths and distances in the latent space. As we argue in the extended proof, this can only be the case if h is a composition of permutations and sign flips.</p><p>Intuition <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates, by contradiction, why the model defined in Eq. (2) is identifiable. We consider temporal pairs of latents represented by connected points. A sparse transition prior encourages axis-alignment, as can be seen from the Laplace transition prior in the third image of <ref type="figure">Fig. 3</ref>.</p><p>This results in lines that are parallel with the axes in both the ground truth (left, blue, z * ) and learned model <ref type="bibr">(right, red, z)</ref>. In this example, z * 0 corresponds to horizontal position, while z * 1 corresponds to vertical position. The learned model must satisfy two criteria: (1) the latent factors should match the sparse prior (axis-aligned) and (2) the generated image pairs should match the ground-truth image pairs. If the learned latent factors were mismatched, for example by rotation, then the image pair distributions would not be matched. In this example, the ground truth model would produce image pairs with typically vertical or horizontal transitions, while the learned model pairs result in mostly diagonal transitions. Thus, the learned model cannot satisfy both criteria without aligning the latent axes with the ground-truth axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SLOW VARIATIONAL AUTOENCODER</head><p>In order to validate our proof, we must choose a probabilistic latent variable model for estimating the data density. We chose to build upon the framework of VAEs because of their efficiency in estimating a variational approximation to the ground truth posterior of a deep latent variable model <ref type="bibr">(Kingma and Welling, 2013)</ref>. We will refer to this model as SlowVAE. In Appendix B we note shortcomings of such an approach and test an alternative flow-based model.</p><p>The standard VAE objective assumes i.i.d. data and a standard normal prior with diagonal covariance on the learned latent representations z ? N (0, I). To extend this to sequences, we assume the same functional form for our model prior as in Eq. (1) and Eq. (2). The posterior of our model is independent across time steps. Specifically,</p><formula xml:id="formula_1">q(z t , z t?1 |x t , x t?1 ) = q(z t |x t ) q(z t?1 |x t?1 ), q(z|x) = d i=1 N (? i (x), ? 2 i (x)),<label>(3)</label></formula><p>where ? i (x) and ? 2 i (x) are the input-dependent mean and variance of our model's posterior. We visualize this combination of priors and posteriors in <ref type="figure">Fig. 3</ref>. For a given pair of inputs (x t , x t?1 ), the full evidence lower bound (ELBO, which we derive in Appendix A.2) can be written as</p><formula xml:id="formula_2">L(x t , x t?1 ) = E q(zt,zt?1|xt,xt?1) [log p(x t , x t?1 |z t , z t?1 )] ? D KL (q(z t?1 |x t?1 )|p(z t?1 )) ? ? E q(zt?1|xt?1) [D KL (q(z t |x t )|p(z t |z t?1 ))],<label>(4)</label></formula><p>where ? is a regularization term for the sparsity prior, analogous to ? in ?-VAEs (Higgins et al., 2017) (technically, Eq. 4 is only an ELBO with ? ? 1). The first term on the right-hand side is the log-likelihood (i.e. the negative reconstruction error, with p(x t , x t?1 |z t , z t?1 ) parameterized by the decoder of the VAE), the second term is the KL to a normal prior as in the standard VAE and the last term is an expectation of the KL between the posterior at time step t and the conditional prior p(z t |z t?1 ). The expectation in the last term is taken over samples from the posterior at the previous time step q(z t?1 |x t?1 ). We observed empirically that taking the mean, ?(x t?1 ), as a single sample produces good results, analogous to the log-likelihood that is typically evaluated at a single sample from the posterior (see <ref type="bibr" target="#b5">Blei et al. (2017)</ref> for context). <ref type="figure">Figure 3</ref>: SlowVAE illustration. The prior and posterior for a two-dimensional latent space. Left to right: Normal prior for t ? 1, posterior for t ? 1, conditional Laplace prior for t, and posterior for t. The blue cross in the right three plots indicates the mean of the posterior for t ? 1.</p><p>In practice, we need to choose ?, ?, and ?. For the latter two, we can perform a random search for hyperparameters, as we discuss below. For the former, any ? &lt; 2 would break the general rotation symmetry by having an optimum for axis-aligned representations, which theorem 1 includes as a requirement for identifiability. As can be seen in Figs. 1 and 11, ? ? 0.5 provides the best fit to the ground-truth marginals. However, we used ? = 1 as a parsimonious choice for SlowVAE, since the Laplace is a well-understood distribution that allows us to derive a simple closedform solution for the ELBO in Eq. 4, which we derive in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TOWARDS AN APPROXIMATE THEORY OF DISENTANGLEMENT</head><p>A number of our theoretical assumptions are violated in practice: After non-convex optimization, on a finite data sample, the distributions p(x t , x t?1 ) and p * (x t , x t?1 ) are probably not perfectly matched. In addition, the model assumptions on p(z t , z t?1 ) likely do not fully match the distribution of the ground truth factors. For example, the model may be misspecified such that ? = ? * or ? = ? * , or the chosen family of distributions may be incorrect altogether. In the following section we will present results on several datasets where the marginal distributions p(z t?1 ) are drawn from a Uniform (not Normal) distribution, and some of them are over unordered sets (categories) or bounded periodic spaces (rotation). Also, in practice the model latent space is usually chosen to have more dimensions than the ground truth generative model. On real data, factors of variation may be dependent <ref type="bibr" target="#b55">(Tr?uble et al., 2020;</ref><ref type="bibr" target="#b66">Yang et al., 2020)</ref>. We show this is the case on YouTube-VOS and KITTI-MOTS in Appendix G and we provide evidence that breaking these dependencies has no clear consequence on disentanglement in Appendix F.2. A more formal treatment of dependence is done by <ref type="bibr">Khemakhem et al. (2020b)</ref> who relax the independence assumption of ICA to Independently Modulated Components Analysis (IMCA) and introduce a family of conditional energy-based models that are identifiable up to simple transformations. Furthermore, the hypothesis class G of learnable functions in the VAE architecture may not contain the invertible ground truth generator g * / ? G, if it exists at all (e.g. occlusions may already lead to non-invertibility). Despite these violations, we consider it a strength of our method that the practical implementation still achieves improved disentanglement over previous approaches. However, we note understanding the impact of these violations as an important focus area for continued progress towards developing a practical yet theoretically supported method for disentanglement on natural scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATASETS WITH NATURAL TRANSITIONS</head><p>While the standard datasets compiled by DisLib are an important step towards real-world applications, they still assume the data is i.i.d.. As described in section 2,  proposed uniformly sampling the number of factors to be changed, k = Rnd, and changing said factors by uniformly sampling over the possible set of values. What we refer to as "UNI" is a dataset variant modeled after the described scheme  (further details in Appendix D). Considering our natural data analysis presented in <ref type="figure" target="#fig_0">Figure 1</ref>, such transitions are certainly unnatural. Given the current state of evaluation, we provide a set of incrementally more natural datasets which are otherwise comparable to existing work. We propose that said datasets should be included in the standard benchmark suite to provide a step towards disentanglement in natural data.</p><p>(1) Laplace Transitions (LAP) is a procedure for constructing image pairs from DisLib datasets by sampling from a sparse conditional distribution. For each ground-truth factor, the first value in the pair is chosen i.i.d. from the dataset and the second is chosen by weighting nearby factor values using Laplace distributed probabilities. LAP is a step towards natural data that closely resembles previous extensions of DisLib datasets to the time domain, but in a way that matches the marginal distribution of natural transitions (see Appendix D.2 for more details).</p><p>(2) Natural Sprites consists of pairs of rendered sprite images with generative factors sampled from real YouTube-VOS transitions. For a given image pair, the position and scale of the sprites are set using measured values from adjacent time points in YouTube-VOS. The sprite shapes and orientations are simple, like dSprites, and are fixed for a given pair. While fixing shape follows the natural transitions of objects, it is unclear how to accurately estimate object orientation from the masks, and thus we fixed the factor to avoid introducing artificial transitions. We additionally consider a version that is discretized to the same number of object states as dSprites, which i) allows us to use the standard DisLib evaluation metrics and ii) helps isolate the effect of including natural transitions from the effect of increasing data complexity (see Appendix D.4 for more details).</p><p>(3) KITTI Masks is composed of pedestrian segmentation masks from the autonomous driving vision benchmark KITTI-MOTS, thus with natural shapes and continuous natural transitions in all underlying factors. We consider adjacent frames which correspond to mean(?t) = 0.05s in physical time (we report the mean because of variable sampling rates in the original data); as well as frames with a larger temporal gap of mean(?t) = 0.15s, which corresponds to samples of pairs that are at most 5 frames apart. We show in Appendix G.3 that SlowVAE disentanglement performance increases and then plateaus as we continue to increase mean(?t).</p><p>In summary, we construct datasets with (1) imposed sparse transitions, (2) augmented with natural continuous generative factors using measurements from unstructured natural videos, as well as <ref type="formula" target="#formula_1">(3)</ref> data from unstructured natural videos themselves, but provided as segmentation masks to ensure visual complexity is manageable for current methods. For the provided datasets, the object categories  <ref type="table">Table 1</ref>: Mean and standard deviation (s.d.) metric scores across 10 random seeds. PCL is a scaled-up implementation of the method described by Hyv?rinen and <ref type="bibr">Morioka (2017)</ref>, leveraging the encoding architecture and training hyperparameters specified in appendix E. Ada-GVAE is the leading method proposed by . Bold indicates statistical significance above the next highest score (independent T-test, p &lt; 0.05). Red indicates statistical significance below the next lowest score. Results for additional datasets and models are in <ref type="table" target="#tab_1">Table 2</ref> and Appendix G.</p><p>never change across transitions -reflecting natural object permanence. Finally, as <ref type="formula" target="#formula_0">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref> use factor transitions measured from natural videos, they exhibit any natural statistical structure present for those factors, such as natural dependencies (further discussion is in Appendix F.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EMPIRICAL STUDIES</head><p>We evaluate models using the DisLib implementation for the following supervised metrics: Be-taVAE <ref type="bibr" target="#b29">(Higgins et al., 2017)</ref>; FactorVAE (Kim and Mnih, 2018); Mutual Information Gap (MIG; <ref type="bibr" target="#b13">Chen et al., 2018)</ref>; Disentanglement, Compactness, and Informativeness (DCI / Disentanglement; Eastwood and Williams, 2018); Modularity <ref type="bibr" target="#b42">(Ridgeway and Mozer, 2018)</ref>; and Separated Attribute Predictability (SAP; Kumar et al., 2018) (see Appendix C for metric details). None of the DisLib metrics support ground-truth labels with continuous variation, which is required for evaluation on the continuous Natural Sprites and KITTI Masks datasets. To reconcile this, we measure the Mean Correlation Coefficient (MCC), a standard metric in the ICA literature that is applicable to continuous variables. We report mean and standard deviation across 10 random seeds.</p><p>In order to select the conditional prior regularization and the prior rate in an unsupervised manner, we perform a random search over ? ? [1, 16] and ? ? [1, 10] and compute the recently proposed unsupervised disentanglement ranking (UDR) scores <ref type="bibr">(Duan et al., 2020)</ref>. We notice that the optimal values are close to ? = 10 and ? = 6 on most datasets, and thus use these values for all experiments. We leave finding optimal values for specific datasets to future work, but note that it is a strong advantage of our approach that it works well with the same model specification across 13 datasets (counting LAP and UNI for DisLib and optional discretization for Natural Sprites), addressing a concern posed in ( <ref type="bibr" target="#b22">Locatello et al., 2018)</ref>. Additional details on model selection and training can be found in Appendix E. Although we train on image pairs, our model does not need paired data points at test time. For all visualizations, we pick the models with the highest average score across the DisLib metrics.</p><p>To compare our model fairly against other methods that also take image pairs as inputs, we also present performance for Permutation-Contrastive Learning from nonlinear ICA (PCL, Hyv?rinen and <ref type="bibr">Morioka, 2017)</ref> and Ada-GVAE, the leading method in the study by . We scaled up the implementation of PCL for evaluation on our high-dimensional pixel inputs, and note this method does not have any hyperparameters. For Ada-GVAE, following the paper's recommendations, we select ? (per dataset) using the considered parameter set <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">16]</ref>, and use the reconstruction loss as the unsupervised model selection criterion . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">RESULTS ON DISLIB AND NEW BENCHMARKS</head><p>In <ref type="table">Table 1</ref> we demonstrate favorable performance compared to PCL and Ada-GVAE across all applicable metrics for discrete ground-truth variable datasets. The relative improvement on UNI is particularly surprising given the drastic mismatch between UNI and SlowVAE's assumptions. In Appendix G, we report results for the remaining DisLib datasets, where the observed dSprites results largely transfer. We also outperform PCL with a (flow-based) exact likelihood implementation of our slow transition prior in Appendix F.1.1. In Appendix F.3, we show that a model with an L 2 transition (? = 2) prior performs much worse, supporting our theoretical prediction.  On the KITTI Masks dataset, one source of variation in the data is the average temporal separation within pairs of images mean(?t). We present two settings (mean(?t) = 0.05s, mean(?t) = 0.15s) and observe a comparative increase in MCC for the latter <ref type="table" target="#tab_1">(Table 2)</ref>. Namely, the increase in performance for larger time gap is more pronounced with SlowVAE than the baselines, resulting in a statistically significant MCC gain. We provide details on the settings and ablate over the mean(?t) parameter in Appendix G.3, where we observe a positive trend between mean(?t) and MCC (reflecting <ref type="table" target="#tab_1">Table 2</ref> <ref type="bibr" target="#b36">, in Oord et al., 2018)</ref>. Finally, we also verify that the transition distributions remain sparse despite the increase in this parameter (Appendix G.3). In <ref type="figure" target="#fig_2">Fig. 4</ref>, we can see that SlowVAE has learned latent dimensions which have correspondence with the estimated ground truth factors of x/y-position and scale. <ref type="bibr" target="#b22">Locatello et al. (2018)</ref> showed that all i.i.d. models performed similarly across the DisLib datasets and metrics when testing was carefully controlled. However, in <ref type="figure" target="#fig_4">Fig. 5</ref> we observe that the different modeling assumptions result in differences in representation quality. To construct the visuals, we first compute the sorted correlation matrix between the latents (rows) and generative factors (columns), which we visualize as a correlation matrices. The matrices are sorted via linear sum assignment such that each ground-truth factor is non-greedily associated with the latent variable with highest correlation (Hyv?rinen and <ref type="bibr">Morioka, 2016)</ref>. Below the matrices are scatter plots that reveal the decodability of the assigned latent factors. In each scatter plot, the horizontal axis indicates the ground truth value, the vertical axis indicates the corresponding latent value, and the colors indicate object shape. The models displayed are those with the maximum average score across evaluated metrics.</p><p>The latent space visualizations use the known ground-truth factors to aid in understanding how each factor is encoded in a way that is more informative than exclusively visualizing latent traversals or embeddings of pairs of latent units <ref type="bibr" target="#b15">(Cheung et al., 2014;</ref><ref type="bibr" target="#b14">Chen et al., 2016;</ref><ref type="bibr" target="#b51">Szab? et al., 2017;</ref><ref type="bibr" target="#b26">Ma et al., 2018)</ref>. For example, in the third row, we observe that several models have a sinusoidal variation with frequencies ? ?, 2?, and 4?, which correspond to the three distinct rotational symmetries of the shapes: heart, ellipse and square. This directly impacts MCC performance (third row in the MCC matrix), which measures rank correlation between the matching latent factor (an angular variable) and the ground truth, which encodes the angles with monotonically increasing indices. Furthermore, the square has a four-fold rotational symmetry and repeats after 90 ? , but it is represented in a full 360 ? rotation in the DisLib ground truth encoding format, resulting in different ground truth labels for identical input images. A similar observation can be made with respect to the categorical factors, which are also represented as ordinal ground truth variables. For example, the PCL correlation score (top left element in the PCL MCC matrix) is quite high, while the corresponding shape correlation score for SlowVAE is quite low. However, if we consider the shape scatter plots, we clearly see that SlowVAE separates the three shapes more distinctively than PCL, only in an order that differs from the ground truth. One solution is to modify MCC to report the maximum correlation over all permutations of the ground truth assignments, although brute force methods for this would scale poorly with the number of categories. We also note that datasets where we see small performance differences among models (e.g., Cars3D) have significantly more discrete categories (e.g., 183) than the other datasets (3 ? 6). This could also explain why all models considered in <ref type="table">Table 1</ref> and 2 perform comparably on the Natural Sprites datasets, where unlike KITTI Masks the ground truth evaluation includes categorical and angular variables. We note that properly evaluating disentanglement is an ongoing area of research (Duan et al., 2020), with notable preliminary results in recent work <ref type="bibr" target="#b8">(Higgins et al., 2018;</ref><ref type="bibr" target="#b6">Bouchacourt et al., 2021;</ref><ref type="bibr" target="#b54">Tonnaer et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We provide evidence to support the hypothesis that natural scenes exhibit highly sparse marginal transition probabilities. Leveraging this finding, we contribute a novel nonlinear ICA framework that is provably identifiable up to permutations and sign-flips -a stronger result than has been achieved previously. With the SlowVAE model we provide a parsimonious implementation that is inspired by a long history of learning visual representations from temporal data <ref type="bibr" target="#b50">(Sutton, 1988;</ref><ref type="bibr">Hinton, 1990;</ref><ref type="bibr">F?ldi?k, 1991)</ref>. We apply this model to current metric-based disentanglement benchmarks to demonstrate that it outperforms existing approaches <ref type="bibr">Hyv?rinen and Morioka, 2017)</ref> on aggregate without any tuning of hyperparameters to individual datasets. We also provide novel video dataset benchmarks to guide disentanglement research towards more natural domains.</p><p>We observe that these datasets have complex dependencies that our theory will have to be extended to account for, although we demonstrate with empirical comparisons the efficacy of our approach. In addition to Natural Sprites and KITTI Masks, we suggest that YouTube-VOS will be valuable as a large-scale dataset that is unconstrained by object type and scenario for more advanced models. Variance in such categorical factors is problematic for evaluation due to the cited drawbacks of existing quantitative metrics, which should be addressed in tandem with scaling to natural data. Taken together, our dataset and model proposals set the stage for utilizing knowledge of natural scene statistics to advance unsupervised disentangled representation learning.</p><p>In our experiments we see that approximate identification as measured by the different disentanglement metrics increases despite violations of theoretical assumptions, which is in line with prior studies <ref type="bibr" target="#b44">(Shu et al., 2019;</ref><ref type="bibr">Khemakhem et al., 2020a;</ref>. Nevertheless, future work should address gaining a better understanding of the theoretical and empirical consequences of such model misspecifications, in order to make the theory of disentanglement more predictive about empirically found solutions. like to thank Johannes Ball?, Jon Shlens and Eero Simoncelli for early discussions related to the ideas developed in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BROADER IMPACT</head><p>Representation learning is at the heart of model building for cognition. Our specific contribution is focused on core methods for modeling natural videos and the datasets used are more simplistic than real-world examples. However, foundational research on unsupervised representation learning has potentially large impact on AI for advancing the power of self-learning systems.</p><p>The broader field of representation learning has a large number of focused research directions that span machine learning and computational neuroscience. As such, the application space for this work is vast. For example, applications in unsupervised analysis of complicated and unintuitive data, such as medical imaging and gene expression information, have great potential to solve fundamental problems in health sciences. A future iteration of our disentangling approach could be used to encode such complicated data into a lower-dimensional and more understandable space that might reveal important factors of variation to medical researchers. Another important and complex modeling space that could potentially be improved by this line of research is in environmental sciences and combating global climate change.</p><p>Nonetheless, we acknowledge that any machine learning method can be used for nefarious purposes, which can be mitigated via effective, scientifically informed communication, outreach, and policy direction. We unconditionally denounce the use of derivatives of our work for weaponized or wartime applications. Additionally, due to the lack of interpretability generally found in modern deep learning approaches, it is possible for practitioners to inadvertently introduce harmful biases or errors in machine learning applications. Although we certainly do not solve this problem, our focus on providing identifiable solutions to representation learning is likely beneficial for both interpretability and fairness in machine learning. </p><formula xml:id="formula_3">(z) Prior z ? p(z) Latent variables x = g(z)</formula><p>Generated images q(z|x) Variational posterior <ref type="table">Table 3</ref>: Glossary of terms. We use a * (i.e. g * ) when necessary to highlight that we are referring to the ground truth model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 PROOF OF IDENTIFIABILITY</head><p>To study disentanglement, we assume that the generative factors z ? R D are mapped to images x ? R N (usually D N , but see section B) by a nonlinear ground-truth generator g * : z ? x.</p><p>Theorem 1 Let (g * , ? * , ? * ) and (g, ?, ?) respectively be ground-truth and learned generative models as defined in Eq.</p><p>(2). If the following conditions are satisfied:</p><p>(i) The generators g * and g are defined everywhere in the latent space. Moreover, they are injective and differentiable almost everywhere,</p><formula xml:id="formula_4">(ii) There is no model misspecification i.e. ? = ? * and ? = ? * , so z ? p(z) = p * (z), (iii) Pairs of images are generated as (x * t?1 , x * t ) = (g * (z t?1 ), g * (z t )) and (x t?1 , x t ) = (g(z t?1 ), g(z t )), (iv) The distributions of (x * t?1 , x * t )</formula><p>and (x t?1 , x t ) are the same (i.e. the corresponding densities are equal almost everywhere: p * (x t?1 , x t ) = p(x t?1 , x t ), then g = g * ? ?, where ? is a composition of a permutation and sign flips.</p><p>Proof. Since x = g(z) can be written as x = (g * ? (g * ) ?1 ? g)(z), we can assume that g = g * ? h for some function h on the latent space.</p><p>We first show that the function h is a bijection on the latent space. It is injective, since both g and g * are injective. Because of continuity of h, if it were not surjective, there would be some neighborhood Uz ofz that would not have a pre-image under h. This would mean that images generated by g * from Uz would have zero density under the distribution of images generated by g (i.e. p(g * (Uz)) = 0). This density would be non-zero under the distribution of images directly generated by the ground-truth generator g * (i.e. p * (g * (Uz)) = 0), which contradicts the assumption that these distributions are equal. It follows that h is bijective.</p><p>In the next step, we show that the distribution of latent space pairs (h(z t?1 ), h(z t )) matches the latent space prior distribution (i.e. h preserves the prior distribution in the latent space). Indeed, using the assumption that the distributions of (g * (z t?1 ), g * (z t )) and ((g * ? h)(z t?1 ), (g * ? h)(z t )) are the same, we can write the following equality using the change of variables formula:</p><formula xml:id="formula_5">p * (x t?1 , x t ) = p((g * ) ?1 (x t?1 ), (g * ) ?1 (x t )) det d(g * ) ?1 d(x t?1 , x t ) = p h ((g * ) ?1 (x t?1 ), (g * ) ?1 (x t )) det d(g * ) ?1 d(x t?1 , x t ) = p(x t?1 , x t ),<label>(5)</label></formula><p>where p and p h are densities of (z t?1 , z t ) and (h(z t?1 ), h(z t )). Since the determinants above cancel, these densities are equal at the pre-image of any pair of images (x t?1 , x t ). Because g * is defined everywhere in the latent space, p and p h are equal for any pair of latent space points. Applying the change of variables formula again, we obtain the following equation:</p><formula xml:id="formula_6">p(z t?1 , z t ) = p(h ?1 (z t?1 ), h ?1 (z t )) det dh ?1 d(z t?1 , z t ) = p(h ?1 (z t?1 )) p(h ?1 (z t ) | h ?1 (z t?1 )) det dh ?1 (z t?1 ) dz t?1 det dh ?1 (z t ) dz t = p(z t?1 ) p(z t | z t?1 ).<label>(6)</label></formula><p>Note that the probability measure p is the same before and after the change of variables, since we showed that the prior distribution in the latent space must be invariant under the function h. The same condition for the marginal p(z t?1 ) is as follows:</p><formula xml:id="formula_7">p(z t?1 ) = p(h ?1 (z t?1 )) det dh ?1 (z t?1 ) dz t?1 .<label>(7)</label></formula><p>Solving for the determinant of the Jacobian in <ref type="formula" target="#formula_7">(7)</ref> and plugging it into <ref type="formula" target="#formula_6">(6)</ref>, we obtain</p><formula xml:id="formula_8">p(z t | z t?1 ) = p(h ?1 (z t ) | h ?1 (z t?1 )) p(z t ) p(h ?1 (z t ))</formula><p>.</p><p>Taking logs of both sides, we arrive at the following equation:</p><formula xml:id="formula_10">A(||z t ? z t?1 || ? ? ? ||h ?1 (z t ) ? h ?1 (z t?1 )|| ? ? ) = B(||z t || 2 2 ? ||h ?1 (z t )|| 2 2 ),<label>(9)</label></formula><p>where A and B are the constants appearing in the exponentials in p(z t?1 ) and p(z t | z t?1 ). The logs of normalization constants cancel out.</p><p>For any z t we can choose z t?1 = z t making the left hand side in (9) equal to zero. This implies that ||z t || 2 2 = ||h ?1 (z t )|| 2 2 for any z t , i.e. function h ?1 preserves the 2-norm. Moreover, the preservation of the 2-norm implies that p(z t?1 ) = p(h ?1 (z t?1 )) and therefore it follows from <ref type="formula" target="#formula_7">(7)</ref> that for any z</p><formula xml:id="formula_11">det dh ?1 (z) dz = 1.<label>(10)</label></formula><p>Thus, the left hand side of (9) can be re-written as</p><formula xml:id="formula_12">||z t ? z t?1 || ? ? ? ||h ?1 (z t ) ? h ?1 (z t?1 )|| ? ? = 0.<label>(11)</label></formula><p>This means that h ?1 preserves the ?-distances between points. Moreover, because h is bijective, the Mazur-Ulam theorem <ref type="bibr" target="#b30">(Mazur and Ulam, 1932)</ref> tells us that h must be an affine transform.</p><p>In the next step, to prove that h must be a permutation and sign flip, let us choose an arbitrary point z t?1 and z t = z t?1 + ? e k = (z 1,1 , . . . , z 1,k + ?, . . . , z 1,D ). Using (11) and performing a Taylor expansion around z t?1 , we obtain the following:</p><formula xml:id="formula_13">? ? = ||z t ? z t?1 || ? ? = ||h ?1 (z t?1 + ? e k ) ? h ?1 (z t?1 )|| ? ? = ? ? ?h ?1 1 (z t?1 ) ?z t?1,k , . . . , ?h ?1 D (z t?1 ) ?z t?1,k + O(? 2 ) ? ? .<label>(12)</label></formula><p>The higher-order terms O(? 2 ) are zero since h is affine, therefore dividing both sides of the above equation by ? ? we find that</p><formula xml:id="formula_14">?h ?1 1 (z t?1 ) ?z t?1,k , . . . , ?h ?1 D (z t?1 ) ?z t?1,k ? ? = 1.<label>(13)</label></formula><p>The vectors of k-th partial derivatives of components of h ?1 are columns of the Jacobian matrix</p><formula xml:id="formula_15">dh ?1 (z) dz</formula><p>. Using the fact that the determinant of that matrix is equal to one and applying Hadamard's inequality, we obtain that</p><formula xml:id="formula_16">det dh ?1 (z) dz = 1 ? D k=1 ?h ?1 1 (z t?1 ) ?z t?1,k , . . . , ?h ?1 D (z t?1 ) ?z t?1,k 2 .<label>(14)</label></formula><p>Since ? &lt; 2, for any vector v it holds that ||v|| 2 ? ||v|| ? , with equality only if at most one component of v is non-zero. This inequality implies that both <ref type="formula" target="#formula_1">(13)</ref> and <ref type="formula" target="#formula_2">(14)</ref> hold at the same time if and only if</p><formula xml:id="formula_17">?h ?1 1 (z t?1 ) ?z t?1,k , . . . , ?h ?1 D (z t?1 ) ?z t?1,k 2 = ?h ?1 1 (z t?1 ) ?z t?1,k , . . . , ?h ?1 D (z t?1 ) ?z t?1,k ? = 1,<label>(15)</label></formula><p>meaning that only one element of these vectors of k-th partial derivatives is non-zero, and it is equal to 1 or -1. Thus, the function h is a composition of a permutation and sign flips at every point. Potentially, this permutation might be input-dependent, but we argued above that h is affine, therefore the permutation must be the same for all points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 KULLBACK LEIBLER DIVERGENCE OF SLOW VARIATIONAL AUTOENCODER</head><p>The VAE learns a variational approximation to the true posterior by maximizing a lower bound on the log-likelihood of the empirical data distribution D</p><formula xml:id="formula_18">E xt?1,xt?D [log p(x t?1 , x t )] ? E xt?1,xt?D [E q(zt,zt?1|xt,xt?1) [log p(x t?1 , x t , z t?1 , z t ) ? log q(z t , z t?1 |x t , x t?1 )]].<label>(16)</label></formula><p>For this, we need to compute the Kullback-Leibler divergence (KL) between the posterior q(z t , z t?1 |x t , x t?1 ) and the prior p(z t , z t?1 ). Since all of these distributions are per design factorial, we will, for simplicity, derive the KL below for scalar variables (log-probabilities will simply have to be summed to obtain the full expression). Recall that the model prior and posterior factorize like</p><formula xml:id="formula_19">p(z t , z t?1 ) = p(z t |z t?1 ) p(z t?1 ) q(z t , z t?1 |x t , x t?1 ) = q(z t |x t ) q(z t?1 |x t?1 ).<label>(17)</label></formula><p>Then, given a pair of inputs (x t?1 , x t ), the KL can be written</p><formula xml:id="formula_20">D KL (q(z t , z t?1 |x t , x t?1 )|p(z t , z t?1 )) = E zt,zt?1?q(zt,zt?1|xt,xt?1) log q(z t |x t ) q(z t?1 |x t?1 ) p(z t |z t?1 ) p(z t?1 ) = E zt?1?q(zt?1|xt?1) log q(z t?1 |x t?1 ) p(z t?1 ) + E zt,zt?1?q(zt,zt?1|xt,xt?1) log q(z t |x t ) p(z t |z t?1 ) = D KL (q(z t?1 |x t?1 )|p(z t?1 )) ? H(q(z t |x t )) + E zt?1?q(zt?1|xt?1) [H(q(z t |x t ), p(z t |z t?1 ))]<label>(18)</label></formula><p>Where we use the fact that KL divergences decompose like D KL (X, Y ) = H(X, Y ) ? H(X) into (differential) cross-entropy H(X, Y ) and entropy H(X). The first term of the last line in <ref type="formula" target="#formula_9">(18)</ref> is the same KL divergence as in the standard VAE, namely between a Gaussian distribution q(z t?1 |x t?1 ) with some ?(x t?1 ) and ?(x t?1 ) and a standard Normal distribution p(z t?1 ). The solution of the KL is given by <ref type="bibr">, 2006)</ref>. The second term on the RHS, i.e. the entropy of a Gaussian is simply given by</p><formula xml:id="formula_21">D KL (q(z t?1 |x t?1 )|q(z t?1 )) = ? log ?(x t?1 ) + 1 2 (?(x t?1 ) 2 + ?(x t?1 ) 2 ? 1) (Bishop</formula><formula xml:id="formula_22">H(q(z t |x t )) = log(?(x t ) ? 2?e).</formula><p>To compute the last term on the RHS, let us recall the Laplace form of the conditional prior</p><formula xml:id="formula_23">p(z t |z t?1 ) = ? 2 exp ??|z t ? z t?1 |.<label>(19)</label></formula><p>Thus the cross-entropy becomes</p><formula xml:id="formula_24">H(q(z t |x t ), p(z t |z t?1 )) = ?E zt?q(zt|xt) [log p(z t |z t?1 )] = ? log ? 2 + ?E zt?q(zt|xt) [|z t ? z t?1 |].<label>(20)</label></formula><p>Now, if some random variable X ? N (?, ? 2 ), then Y = |X| follows a folded normal distribution, for which the mean is defined as</p><formula xml:id="formula_25">E[|x|] = ? 2 ? exp ? ? 2 2? 2 ? ? 1 ? 2 ? ? ? ,<label>(21)</label></formula><p>where ? is the cumulative distribution function of a standard normal distribution (mean zero and variance one). Thus, denoting ?(x t ) and ?(x t ) the mean and variance of q(z t |x t ), and defining ?(x t , z t?1 ) = ?(x t ) ? z t?1 , we can rewrite further  </p><formula xml:id="formula_26">H(q(z t |x t ), p(z t |z t?1 )) = ? log ? 2 + ? ?(x t ) 2 ? exp ? ?(x t , z t?1 ) 2 2?(x t ) 2 ? ?(x t , z t?1 ) 1 ? 2 ? ?(x t , z t?1 ) ?(x t ) .<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CHOOSING A LATENT VARIABLE MODEL</head><p>Our proposed method for disentanglement can be implemented in conjunction with different probabilistic latent variable models. In this section, we compare VAEs and normalizing flows as possible candidates.</p><p>Variational Autoencoders (VAEs) (Kingma and Welling, 2013) are a widely used probabilistic latent variable model. Despite their simple structure and empirical success, VAEs can converge to a pathological solution called posterior collapse <ref type="bibr" target="#b25">(Lucas et al., 2019;</ref><ref type="bibr" target="#b7">Bowman et al., 2016;</ref><ref type="bibr">He et al., 2019)</ref>. This solution results in the encoder's variational posterior approximation matching the prior, which is typically chosen to be a multivariate standard normal q(z|x) ? p(z) = N (0, I). This disconnects the encoder from the decoder, making them approximately independent, i.e. p(x|z) ? p(x). The failure mode is often observed when the decoder architecture is overly expressive, i.e. with autoregressive models, or when the likelihood p(x) is easy to estimate. Approaches that alleviate this problem rely on modifying the ELBO training objective <ref type="bibr" target="#b7">(Bowman et al., 2016;</ref><ref type="bibr">Kingma et al., 2016)</ref> or restricting the decoder structure <ref type="bibr" target="#b19">(Dieng et al., 2019;</ref><ref type="bibr" target="#b27">Maal?e et al., 2019)</ref>. However, these approaches come with various drawbacks, including optimization issues <ref type="bibr" target="#b25">(Lucas et al., 2019)</ref>.</p><p>Another approach to estimate latent variables are normalizing flows which describe a sequence of invertible mappings by iteratively applying the change of variables rule <ref type="bibr" target="#b21">(Dinh et al., 2017b)</ref>. Unlike VAEs, flow based latent variable models allow for a direct optimization of the likelihood <ref type="bibr" target="#b21">(Dinh et al., 2017b)</ref>. Most normalizing flow models rely on a fast and reliable calculation of the determinant of the Jacobian of the outputs with respect to the inputs, which constrains the architectural design and limits the capacity of the network <ref type="bibr" target="#b53">(Tabak et al., 2010;</ref><ref type="bibr" target="#b52">Tabak and Turner, 2013;</ref><ref type="bibr" target="#b21">Dinh et al., 2017b)</ref>. Thus, competitive flows require very deep architectures in practice <ref type="bibr">(Kingma and Dhariwal, 2018)</ref>. Furthermore, flows are not directly suited for a scenario where the observation space is higher dimensional than the generating latent factors, dim(z) &lt; dim(x), as the computation of the determinant requires a square Jacobian matrix. We tried setting dim(z) = dim(x) &gt; dim(z * ), but observed instability while optimizing the objective defined below.</p><p>It is straightforward to derive a flow-based objective based on the assumptions in Eq.</p><p>(2). We consider a normalizing flow with with K blocks f (</p><formula xml:id="formula_27">x) = f K ? ... ? f 1 : x ? z.</formula><p>The coupling blocks can refer to nonlinear mixing similar to <ref type="bibr">Kingma and Dhariwal (2018)</ref>, or in the linear case (K = 1) to an invertible de-mixing matrix. This leads to the following estimation of the likelihood</p><formula xml:id="formula_28">p(x t?1 , x t ) = p(f (x t?1 )) p(f (x t )|f (x t?1 )) K k=1 det ?f k ?z k?1,t?1 ?1 K k=1 det ?f k ?z k?1,t ?1 .<label>(23)</label></formula><p>Note that p(f (x t?1 )) is Gaussian and p(f (x t )|f (x t?1 )) is a Laplacian, similar to Eq. (2). During optimization we take the ? log of both sides and minimize w.r.t. the parameters of f . We refer to this estimator as SlowFlow. Our SlowFlow model is very similar to the flow described in <ref type="bibr" target="#b39">(Pineau et al., 2020)</ref>, who use a Gaussian transition prior and therefore would have weaker identifiability guarantees. Next, we compare SlowFlow and SlowVAE in the context of disentanglement.</p><p>To demonstrate the posterior collapse in VAEs, we generate data points (x t , x t?1 ) according to Eq. (2) with a two dimensional latent space dim(z * ) = 2. We consider a trivial linear mixing of</p><formula xml:id="formula_29">x * = W * z * = g * (z * ) with W * = diag(1, ?)<label>(24)</label></formula><p>and ? ? [0.1, 1]. As can be seen by looking at the ? and ? outputs of the encoder in <ref type="figure" target="#fig_6">Fig 6a,</ref> for ? &lt; 0.4, the encoder for the minor axis collapses to the prior. The decoder then tries to minimize the reconstruction loss by solely covering the first principal component of the data, which is also described in <ref type="bibr" target="#b43">Rolinek et al. (2019)</ref>. Despite the collapse and decrease in MCC, the SlowVAE loss from Eq. (4) still improves during training. On the other hand, a simple linear SlowFlow model f (x) = Wx, which directly optimizes the likelihood, recovers the latents consistently as seen by the MCC measure <ref type="figure" target="#fig_6">(Fig 6b)</ref>.</p><p>To show the strength of the VAE model we increase the complexity of the data-distribution by using a non-linear expanding decoder such that dim(x) dim(z * ). In <ref type="figure" target="#fig_7">Fig. 7</ref> we observe that increasing the input dimensionality is sufficient for SlowVAE to find the corresponding latents and achieve high MCC with low loss. VAEs prefer data dimensions to be greater than latent dimensions. Individual subplots are as described in <ref type="figure" target="#fig_6">Fig. 6</ref>. For all data in this experiment we used a 20-dimensional latent space, dim(z * ) = 20. Each row corresponds to the dimensionality of the x * , with values of 20, 200, and 2000. The first two dimensions of z * are plotted as well as the two dimensions of z with the highest corresponding mean correlation coefficient (MCC). The x * and x data are projected onto their first two principal component axes before plotting. A two-layer mixing matrix was used to transform data from Z gt to X gt . As one increases the data dimensionality, the SlowVAE network performs increasingly better in terms of MCC, although worse in terms of total training loss.</p><p>Each estimation method is practically useful in different experimental settings. In the case when the mixing operation is trivially defined (Eq. (24), or when the number of dimensions in z * match those in x * ), the VAE estimator tends to learn a pathological solution. On the other hand, the normalizing flow estimator does not scale well to high dimensional data due to the requirement of computing the network Jacobian. Additionally, the framework for constructing normalizing flow estimators assumes the latent dimensionality is equal to the data dimensionality to allow for an invertible transform. Together these results lead us to choose an estimator based on the nature of the problem. For our contributed datasets and the DisLib experiments we adopt the VAE framework. However, if one aims to perform simplified experiments such as those typically conducted in the nonlinear ICA literature, it will often make practical sense to switch to a flow-based estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DISENTANGLEMENT METRICS</head><p>Several recent studies have brought to light shortcomings in a number of proposed disentanglement metrics <ref type="bibr">(Kim and Mnih, 2018;</ref><ref type="bibr">Eastwood and Williams, 2018;</ref><ref type="bibr" target="#b13">Chen et al., 2018;</ref><ref type="bibr" target="#b8">Higgins et al., 2018;</ref><ref type="bibr" target="#b28">Mathieu et al., 2019)</ref>, many of which have been compiled in the DisLib benchmark. In addition to the concerns they raise, it is important to note that none of the supervised metrics implemented in DisLib allow for continuous ground-truth factors, which is necessary for evaluating with the Natural Sprites and KITTI Masks datasets, as factors such as position and scale are effectively continuous in reality. To rectify this issue without introducing novel metrics, we include the Mean Correlation Coefficient (MCC) in our evaluations, using the implementation of Hyv?rinen and Morioka (2016), which is described below.</p><p>We measure all metrics presented below between 10, 000 samples of latent factors z and the corresponding encoded means of our model ?(g * (z)). We increase this sample size to 100, 000 for Modularity and MIG to stabilize the entropy estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 MEAN CORRELATION COEFFICIENT</head><p>In addition to the DisLib metrics, we also compute the Mean Correlation Coefficient (MCC) in order to perform quantitative evaluation with continuous variables. Because of Theorem 1, perfect disentanglement in the noiseless case should always lead to a correlation coefficient of 1 or ?1, although note that we report 100 times the absolute value of the correlation coefficient. In our experiments, MCC is used without modification from the authors' open-sourced code <ref type="bibr" target="#b34">(Morioka, 2018)</ref>. The method first measures correlation between the ground-truth factors and the encoded latent variables. The initial correlation matrix is then used to match each latent unit with a preferred groundtruth factor. This is an assignment problem that can be solved in polynomial time via the Munkres algorithm, as described in the code release from <ref type="bibr" target="#b34">Morioka (2018)</ref>. After solving the assignment problem, the correlation coefficients are computed again for the vector of ground-truth factors and the resulting permuted vector of latent encodings, where the output is a matrix of correlation coefficients with D columns for each ground-truth factor and D rows for each latent variable. We use the (absolute value of the) Spearman coefficient as our correlation measure which assumes a monotonic relationship between the ground-truth factors and latent encodings but tolerates deviations from a strictly linear correspondence.</p><p>In the existing implementation for MCC, the ground truth factors, latent encodings, and mixed signal inputs are assumed to have the same dimensionality, i.e. D = D = N . However, in our case, the ground-truth generating factors are much lower dimensional than the signal, N D, and the latent encoding is higher dimensional than the ground-truth factors D &gt; D (see Appendix E for details). To resolve this discrepancy, we add D ? D standard Gaussian noise channels to the ground-truth factors. To compute the MCC score, we take the mean of the absolute value of the upper diagonal of the correlation matrix. The upper diagonal is the diagonal of the square matrix of D ground-truth factors by the top D most correlated latent dimensions after sorting. In this way, we obtain an MCC estimate which averages only over the D correlation coefficients of the D ground truth factors with their corresponding best matching latent factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 DISLIB METRICS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BetaVAE (Higgins et al., 2017)</head><p>The BetaVAE metric uses a biased estimator with tunable hyperparameters, although we follow the convention established in <ref type="bibr" target="#b22">(Locatello et al., 2018)</ref> of using the scikit-learn defaults. For a sample in a batch, a pair of images, (x 1 , x 2 ), is generated by fixing the value of one of the data generative factors while uniformly sampling the rest. The absolute value of the difference between the latent codes produced from the image pairs is then taken, z diff = |z 1 ? z 2 |. A logistic classifier is fit with batches of z diff variables and the corresponding index of the fixed ground-truth factor serves as the label. Once the classifier is trained, the metric itself is the mean classifier accuracy on a batch of held-out test data. The training minimizes the following loss:</p><formula xml:id="formula_30">L = 1 2 w T w + n i=1 log(exp(?y i (z T diff,i w + c)) + 1),<label>(25)</label></formula><p>where w and c are the learnable weight matrix and bias, respectively, and y is the index of the fixed ground-truth factor for the batch. The network is trained using the lbfgs optimizer <ref type="bibr" target="#b10">(Byrd et al., 1995)</ref>, which is implemented via the scikit-learn Python package <ref type="bibr" target="#b38">(Pedregosa et al., 2011)</ref> in the Disentanglement Library (DisLib, <ref type="bibr" target="#b22">Locatello et al., 2018)</ref>. In the original work, the authors argue that their metric improves over a correlation metric such as the mean correlation coefficient by additionally measuring interpretability. However, the linear operation of z T diff,i w + c can perform demixing, which means the measure gives no direct indication of identifiability and thus does not guarantee that the latent encodings are interpretable, especially in the case of dependent factors. Additionally, as noted by <ref type="bibr">Kim and Mnih (2018)</ref>, BetaVAE can report perfect accuracy when all but one of the ground-truth factors are disentangled, since the classifier can trivially attribute the remaining factor to the remaining latents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FactorVAE (Kim and Mnih, 2018)</head><p>For the FactorVAE metric, the variance of the latent encodings is computed for a large (10,000 in DisLib) batch of data where all factors could possibly be changing. Latent dimensions with variance below some threshold (0.05 in DisLib) are rejected and not considered further. Next, the encoding variance is computed again on a smaller batch (64 in DisLib) of data where one factor is fixed during sampling. The quotient of these two quantities (with the larger batch variance as the denominator) is then taken to obtain a normalized variance estimate per latent factor. Finally, a majority-vote classifier is trained to predict the index of the ground-truth factor with the latent unit that has the lowest normalized variance. The FactorVAE score is the classification accuracy for a batch of held-out data.</p><p>Mutual Information Gap <ref type="bibr" target="#b13">(Chen et al., 2018)</ref> The Mutual Information Gap (MIG) metric was introduced as an alternative to the classifier-based metrics. It provides a normalized measure of the mean difference in mutual information between each ground truth factor and the two latent codes that have the highest mutual information with the given ground truth factor. As it is implemented in DisLib, MIG measures entropy by discretizing the model's latent code using a histogram with 20 bins equally spaced between the representation minimum and maximum. It then computes the discrete mutual information between the groundtruth values and the discretized latents using the scikit-learn metrics.mutual_info_score function <ref type="bibr" target="#b38">(Pedregosa et al., 2011)</ref>. For the normalization it divides this difference by the entropy of the discretized ground truth factors.</p><p>Modularity <ref type="bibr" target="#b42">(Ridgeway and Mozer, 2018)</ref> Ridgeway and Mozer (2018) measure disentanglement in terms of three factors: modularity, compactness, and explicitness. For modularity, they first measure the mutual information between the discretized latents and ground-truth factors using the same histogram procedure that was used for the MIG, resulting in a matrix, M ? R D ?D with entries for each mutual information pair. Their measure of modularity is then</p><formula xml:id="formula_31">modularity = 1 D D i=1 ? 1 ? D j=1 M 2 i,j ? max(M 2 i ) max(M 2 i )(D ? 1) ,<label>(26)</label></formula><p>where max(M 2 i ) returns the maximum of the vector of squared mutual information measurements between ground truth i and each latent factor. Additionally, ? is a selection function that returns zero for any i where max(M 2 i ) = 0 and otherwise acts as the identity function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCI Disentanglement (Eastwood and Williams, 2018)</head><p>The DCI scores measure disentanglement, completeness, and informativeness, which have intuitive correspondence to the modularity, compactness, and explicitness of <ref type="bibr" target="#b42">(Ridgeway and Mozer, 2018)</ref>, respectively. To measure DCI Disentanglement, D regressors are trained to predict each ground truth factor state given the latent encoding. The DisLib implementation uses the ensemble.GradientBoostingClassifier function from scikit-learn with default parameters, which trains D gradient boosted logistic regression tree classifiers. Importance is assigned to each latent factor using the built-in feature_importance_ property of the classifier, which computes the normalized total reduction of the classifier criterion loss contributed by each latent. Disentanglement is then measured as</p><formula xml:id="formula_32">i=1 D(1 ? H(I i ))? i ,<label>(27)</label></formula><p>where H is the entropy computed with the stats.entropy function from scikit-learn, I ? R D?D is a matrix of the absolute value of the feature importance between each factor and each ground truth, and? is a normalized version of the matrix</p><formula xml:id="formula_33">I i = D j=1 I i,j D k=1 D j=1 I k,j<label>(28)</label></formula><p>SAP Score <ref type="bibr">(Kumar et al., 2018)</ref> To compute the SAP score, <ref type="bibr">Kumar et al. (2018)</ref> first train a linear support vector classifier with squared hinge loss and L 2 penalty to predict each ground truth factor from each latent variable. In DisLib this is implemented with the svm.LinearSVC function with default parameters from scikit-learn. They construct a score matrix S ? R D ?D , where each entry in the matrix is the batch-mean classifier accuracy for predicting each ground truth given each individual latent encoding. For each generative factor, they compute the difference between the top two most predictive latent dimensions, which are the two highest scores in a given column of S. The mean (across ground-truth factors) of these differences is the SAP score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D NATURAL DATASETS</head><p>We introduce several datasets to investigate disentanglement in more natural scenarios. Here, we provide an overview on the motivation and design of each dataset.</p><p>We have chosen to work with pairs of inputs as minimal sequences because we are interested in the first temporal derivative, more specifically in the sparsity of the transitions between pairs of images.</p><p>Other methods that look at the second temporal derivative, such as work from H?naff et al. <ref type="formula" target="#formula_0">(2019)</ref> on straightening, would require triplets as minimal sequences. Extending our approach beyond this minimal requirement would be simple in terms of the resulting ELBO (which would still factorise like in Eq. 4 because of the Markov property). The only additional complexity would be in the data and loss handling.</p><p>An issue with evaluating disentanglement on natural datasets is the fact that the existing disentanglement metrics require knowledge of the underlying generative process of the given data. Although we can observe that the world is composed of distinct entities that vary according to rules imposed by physics, we are unable to determine the appropriate "factors" that generate such scenes. To mitigate this problem, we compile object measurements by calculating the x and y coordinates of the center of mass as well as the area of object masks in natural video frames. We use these measurements to a) inform new disentanglement benchmarks with natural transitions that have similar complexity to existing benchmarks (Natural Sprites) and b) evaluate the ability of algorithms to decode intrinsic object properties (KITTI Masks). We additionally propose a simple extension to the existing DisLib datasets in the form of collecting images into pairs that exhibit sparse (i.e. Laplace) transition probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 UNIFORM TRANSITIONS (UNI)</head><p>The UNI extension is based on the description given by , where the number of changing factors is determined using draws from a uniform distribution. The key differences between our implementation and theirs is: (i) their code 3 randomly (with 50% probability) sets k = 1 even in the k = Rnd setting, and (ii) we ensure that exactly k factors change. Though we consider these discrepancies minor, we nonetheless label all results reported directly from  with "LOC", as opposed to "UNI", for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 LAPLACE TRANSITIONS (LAP)</head><p>For each of the datasets in DisLib, we collect pairs of images. For each ground-truth factor, the first value in the pair is chosen from a uniform distribution across all possible values in latent space, while the second is chosen by weighting nearby values in latent space using Laplace distributed probabilities (see Eq. 2). We reject samples that would push a factor outside of the preset range provided by the dataset. We call this the LAP DisLib extension. Although the sparse prior indicates that any individual factor is more likely to remain constant, the number of factors that change in a given transition is still typically greater than one. To show this in <ref type="figure" target="#fig_8">Fig. 8</ref>, we sampled 10,000 transitions from each DisLib dataset with LAP transitions and computed the number of factors that had changed within a pair. This extension of the DisLib datasets provides a bridge from i.i.d. data to natural data by explicitly modeling the observed sparse marginal transition distributions. When training models on the LAP dataset it is possible to reject samples without transitions (i.e. all factors remain constant) since the pair would not result in any temporal learning signal. However, it would arguably be more natural to leave these samples as they would more accurately reflect occurrences of stationary objects in real data. We report the rejection setting in the main text, but found no significant difference between the two settings (see Appendix G).</p><p>This dataset also introduces a hyper-parameter ? that controls the rate of the Laplace sampling distribution, while the location is set by the initial factor value. Effectively, when this rate is ? = 1 most of the factors change most of the time, whereas for a rate of ? = 10 most of the factors will not change most of the time. Note that this means ? (inversely) changes the scale, which results in larger or smaller movements, but does not affect the distribution itself. In other words, the sparsity is unchanged, as the sparsity is controlled by the shape ?. We fix ? = 1, which yields multiple changes, thus making this dataset fundamentally different both in spirit and in practice, from the UNI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 YOUTUBE-VOS</head><p>For the YouTube dataset, we download annotations from the 2019 version of the video instance segmentation (Youtube-VIS) dataset <ref type="bibr">(Yang et al., 2019) 4</ref> , which is built on top of the video object segmentation (Youtube-VOS) dataset <ref type="bibr" target="#b64">(Xu et al., 2018)</ref>. The dataset has multi-object annotations for every five frames in a 30fps video, which results in a 6fps sampling rate. The authors state that the temporal correlation between five consecutive frames is sufficiently strong that annotations can be omitted for intermediate frames to reduce the annotation efforts. Such a skip-frame annotation strategy enables scaling up the number of videos and objects annotated under the same budget, yielding 131,000 annotations for 2,883 videos, with 4,883 unique video object instances. Although we do not evaluate against YouTube-VOS in this study, we see it as the logical next step in transitioning to natural data. The large scale, lack of environmental constraints, and abundance of object types makes it the most challenging of the datasets considered herein.</p><p>The original image size of the YouTube-VOS dataset is 720 ? 1280. In order to preserve the statistics of the transitions, we choose not to directly downsample to 64 ? 64, but instead preserve the aspect ratio by downsampling to 64 ? 128. In order to minimize the bias yielded by the extraction method, noting the center bias typically present in human videos, we extract three overlapping, equally spaced 64 ? 64 pixel windows with a stride of 32. For each resulting 64 ? 64 ? T sequence, where T denotes the number of time steps in the sequence, we filter out all pairs where the given object instance is not present in adjacent frames, resulting in 234,652 pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 NATURAL SPRITES</head><p>The benchmark is available at https://zenodo.org/record/3948069.</p><p>Without a metric for disentanglement that can be applied to unknown data generating processes, we are limited to synthetic datasets with known ground-truth factors. Let us take dSprites <ref type="bibr" target="#b29">(Matthey et al., 2017)</ref> as an example. The dataset consists of all combinations of a set of latent factor values, namely,</p><p>? Color: white (1.0, 1.0, 1.0) (square, triangle, star_4, spoke_4) <ref type="bibr">(0,9,...,342,351)</ref>  Given the limited set of discrete values each factor can take on, all possible samples can be described by a tractable dataset, compiled and released to the public. But, in reality, all of these factors should be continuous: a spectrum of possible colors, shapes, scales, orientations, and positions exist. We address this by constructing a dataset that is augmented with natural and continuous ground truth factors, using the mask properties measured from the YouTube dataset described in Appendix D.3.</p><p>We can choose the complexity of the dataset by discretizing the 234,652 transition pairs of position and scale into an arbitrary number of bins. In this study, we discretize to match the number of possible object states as dSprites, which we present in <ref type="table" target="#tab_5">Table 4</ref>. This helps isolate the effect of including natural transitions from the effect of increasing data complexity. We produce a pair by fixing the color, shape, and orientation, but updating the position and scale with transitions sampled from the YouTube measurements. We motivate fixing shape and color by noting that this is consistent with object permanence in the real world. We decided to fix the orientation because we do not currently have a way to approximate it from object masks and we did not want to introduce artificial transition probabilities. To minimize the effect of extreme outliers, we filter out 10% of the data by removing frames if the mask area falls below the 5% or above the 95% quantiles, which reduces the number of pairs to 207,794. Finally, we use the Spriteworld  renderer to generate the images. Spriteworld allows us to render entirely new sprite objects at the precise position and scale as was measured from YouTube. For example, if one would want to apply YouTube-VOS transitions to <ref type="bibr">MPI3D (Gondal et al., 2019)</ref>, this option is unavailable without the associated renderer.</p><p>In relation to the Laplace transitions described in section D.2, this update i) produces pairs that correspond to transitions observed in real data, ii) allows for smooth transitions by defining the data generation process as opposed to being limited by the given collected dataset (e.g. dSprites), and iii) includes complex dependencies among factors that are present in natural data. We generate the data online, thus training the model to fit the underlying distribution as opposed to a sampled finite dataset.</p><p>However, as noted previously, all supervised metrics aggregated in DisLib are inapplicable to continuous factors, which is problematic as the generating distribution is effectively continuous with respect to a subset of the factors. Therefore, we limit our quantitative evaluation to MCC for continuous datasets. However, we are able to evaluate disentanglement with the standard metrics on the discretized version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 KITTI MOTS PEDESTRIAN MASKS (KITTI MASKS)</head><p>The benchmark is available at https://zenodo.org/record/3931823.</p><p>While Natural Sprites enables evaluation of disentanglement with natural transitions, we note that any disentanglement framework that requires knowledge of the underlying generative factors is unrealistic for real-world data. Measurements such as scale and position correspond to object properties that are ecologically relevant to the observer and can serve as suitable alternatives to the typical generative factors. We directly test this using our KITTI Masks dataset.</p><p>To create the dataset, we download annotations from the Multi-Object Tracking and Segmentation (MOTS) Evaluation Benchmark <ref type="bibr" target="#b59">(Voigtlaender et al., 2019;</ref><ref type="bibr">Geiger et al., 2012;</ref><ref type="bibr" target="#b31">Milan et al., 2016</ref>), x 20.6 y 41.4 ar 144</p><p>x 23.9 y 42.5 ar 169</p><p>x 27.5 y 44.1 ar 206 <ref type="figure">Figure 9</ref>: KITTI Masks. Each row corresponds to sequential frames from random sequences in the KITTI Mssks dataset. Above each image we denote measured object properties where x, y correspond the center of mass position and ar corresponds to the area.</p><p>which is split into KITTI MOTS and MOTSChallenge 5 . Both datasets contain sequences of pedestrians with their positions densely annotated in the time and pixel domains. For simplicity, we only consider the instance segmentation masks for pedestrians and do not use the raw data.</p><p>The resulting KITTI Masks dataset consists of 2,120 sequences of individual pedestrians with lengths between 2 and 710 frames each, resulting in a total of 84,626 individual frames. As we did with YouTube-VOS, we estimate ground truth factors by calculating the x and y coordinates of the center of mass of each pedestrian mask in each frame. We define the object size as the area of the mask, i.e. the total number of pixels. We consider the disentanglement performance for different mean time gaps between image pairs in table 2 and Appendix G.3. For samples and the corresponding ground truth factors see <ref type="figure">Fig. 9</ref>.</p><p>The original KITTI image sizes are 1080 ? 1920 or 480 ? 640 resolution for MOTSChallenge and between 370 and 374 pixels tall by 1224 and 1242 pixels wide for KITTI MOTS. The frame rates of the videos vary from 14 to 30 fps, which can be seen in <ref type="table" target="#tab_1">Table 2</ref> of <ref type="bibr" target="#b31">Milan et al. (2016)</ref>. We use nearest neighbor down-sampling for each frame such that the height was 64 pixels and the width is set to conserve the aspect ratio. After down-sampling, we use a horizontal sliding window approach to extract six equally spaced windows of size 64 ? 64 (with overlap) for each sequence in both datasets. This results in a 64 ? 64 ? T sequence, where T denotes the number of time steps in the sequence. Note that here we make reasonable assumptions on horizontal translation and scale invariance of the dataset. We justify the assumed scale invariance by observing that the data is collected from a camera mounted onto a car which has varying distance to pedestrians. To confirm the translation invariance, we performed an ablation study on the number of horizontal images. Instead of six horizontal, equally spaced sliding windows, we only use two which leads to differently placed windows. We do not observe significant changes in the reported data statistics (e.g. the kurtosis of the fit stays within ?10% of the previous value for ?x transitions). The values of ?y and ?area do not change significantly compared to <ref type="table" target="#tab_9">Table 7</ref>.</p><p>For each resulting 64 ? 64 ? T sequence, where T denotes the number of time steps in the sequence, we extract all individual pedestrian masks based on their object instance identity and create a new sequence for each pedestrian such that each resulting sequence only contains a single pedestrian. We ignore images with masks that have less than 30 pixels as they are too far away or occluded and were not recognizable by the authors. We keep all sequences of two or more frames, as the algorithm only requires pairs of frames for training.</p><p>We leave the maximum distance between time frames within a pair, max(?frames), as a hyperparameter. For a given max(?frames), we report the mean change in physical time in seconds (denoted by mean(?t)). We test adjacent frames (max(?frames) = 1), which corresponds to a mean(?t = 0.05) and max(?frames) = 5, which corresponds to a mean(?t = 0.15). This procedure is motivated by the fact that different sequences were recorded with different frame rates and reporting the mean(?t) in seconds allows for a physical interpretation. The relationship between max(?frames) and mean(?t) is in <ref type="figure" target="#fig_0">Fig. 10</ref>. We show results for testing additional values of mean(?t) in Appendix G.3.</p><p>During training, we augment the data by applying horizontal and vertical translations of ?5 pixels and rotations of ?2 ? degree. We apply the exact same data augmentation to both images within a pair to not change any transition statistics.</p><p>We note that both YouTube-VOS <ref type="bibr" target="#b64">(Xu et al., 2018;</ref><ref type="bibr" target="#b65">Yang et al., 2019)</ref> and KITTI-MOTS <ref type="bibr" target="#b59">(Voigtlaender et al., 2019;</ref><ref type="bibr">Geiger et al., 2012;</ref><ref type="bibr" target="#b31">Milan et al., 2016)</ref> are multi-object datasets, although we consider each unique object (mask) separately. Multi-object representation learning and disentanglement are highly connected, in fact they have recently begun to be used interchangeably <ref type="bibr" target="#b63">(Wulfmeier et al., 2020)</ref>.</p><p>To briefly comment on possible extensions in this direction, we see no reason why our prior would not be beneficial to multi-object methods such as <ref type="bibr" target="#b9">MONet (Burgess et al., 2019)</ref> and <ref type="bibr">IODINE (Greff et al., 2019)</ref>, or video extensions such as ViMON <ref type="bibr" target="#b61">(Weis et al., 2020)</ref> and OP3 <ref type="bibr" target="#b58">(Veerapaneni et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MODEL TRAINING AND SELECTION</head><p>We train all models on all datasets provided in DisLib with the UNI and LAP variants.</p><p>All models are implemented in PyTorch <ref type="bibr" target="#b37">(Paszke et al., 2019)</ref>. To facilitate comparison, the training parameters, e.g. optimizer, batch size, number of training steps, as well as the VAE encoder and decoder architecture are identical to those reported in <ref type="bibr" target="#b22">(Locatello et al., 2018;</ref>. We use this architecture for all datasets, only adjusting the number of input channels (greyscale for dSprites, smallNORB, and KITTI Masks; three color channels for all other datasets).</p><p>The model formulation is agnostic to the direction of time. Therefore, to increase the temporal training signal at a fixed computational cost for each batch of input pairs (x 0 , x 1 ), we optimize the model in both directions i.e. optimizing the model objective for both t 0 = 0, t 1 = 1 as well as t 0 = 1, t 1 = 0. F EXTENDED COMPARISONS AND CONTROLS F.1 COMPARISON TO NONLINEAR ICA F.1.1 THEORETICAL COMPARISON Nonlinear ICA has recently been advanced significantly by several papers from Hyv?rinen and colleagues. Of these studies, the two that are most comparable to our work is Hyv?rinen and <ref type="bibr">Morioka (2017)</ref>, which uses an unsupervised contrastive loss for nonlinear demixing and <ref type="bibr">Khemakhem et al. (2020a)</ref>, which extends the nonlinear ICA framework to include variational autoencoders (VAEs). However, our theory covers an important class of transitions relevant for natural data that is not covered by the identifiability proofs of either of the aforementioned studies.</p><p>As a specific comparison to the first paper, the non-Gaussian autoregressive model that their identifiability proof rests upon (Eq. 8 in Hyv?rinen and <ref type="bibr">Morioka, 2017)</ref> assumes that the second derivative of the innovation probability density function is less than zero to satisfy uniform dependence, which is only met for ? &gt; 1 for generalized Laplace transition distributions. While they denote (footnote 3) that Laplace distributions (? = 1) are not covered by their theory, they offer a suggestion for a smooth approximation. However, they do not demonstrate that this approximation is useful in practice, or offer a solution to a general class of sparse distributions for ? ? 1. We chose a generalized Laplacian to fit our data and for our model assumption as it allows for simple parameterization of fits to data (e.g. ? = 0.5 for natural movie transitions), but is simultaneously quite expressive <ref type="bibr" target="#b46">(Sinz et al., 2009</ref>). Though we use ? = 1 in practice for our estimation method, we prove identifiability up to permutations and sign flips for any ? &lt; 2, covering all sparse distributions under the expressive generalized Laplacian model. In addition, we assume a Gaussian marginal distribution that allows us to derive a fundamentally stronger proof of identifiability -where we identify up to permutation and sign-flips. Hyv?rinen and <ref type="bibr">Morioka (2017)</ref> only identify the sources up to arbitrary non-linear element-wise transformations. Thus they require a subsequent step of ICA (under the typical assumption that at most one marginal source distribution is Gaussian) to recover the signal up to permutations and sign flips for a class of distributions where it is unclear whether they account for temporal sparsity.</p><p>The work of <ref type="bibr">Khemakhem et al. (2020a)</ref> has a couple of differences from our own, most notable of which is the form of the conditional prior, p(z t |z t?1 ). They assume that the conditional posterior is part of the exponential family, which does not include Laplacian conditionals. Though the exponential family contains the Laplace distribution with fixed mean as its member, it does not allow their approach to model sparse transitions. They assume that the natural parameters of the exponential family distribution are conditioned on z t?1 , meaning that only the scale but not the mean of the Laplace prior for z t can be modulated by the previous time step, thus not allowing for sparse transition probabilities. Additionally, their implementation requires the number of classes (i.e. states of the conditioning variable) to equal the number of stationary segments, which is impractical for the datasets we consider.</p><p>Thus, we provide a closer match to natural data transitions, with a stronger identifiability result. We provide validation by performing an extensive evaluation leveraging our contributed datasets as well as the models, metrics, and datasets provided by the Disentanglement Library (DisLib, discussed in section 4). We consider methods from the disentanglement literature  as well as nonlinear ICA (Hyv?rinen and <ref type="bibr">Morioka, 2017)</ref>, that are functionally capable of processing transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.2 EMPIRICAL COMPARISON</head><p>Hyv?rinen and <ref type="bibr">Morioka (2017)</ref> conducted a simulation where the sources in the nonlinear ICA model come from a linear autoregressive (AR) model with non-Gaussian innovations. Specifically, temporally dependent 20-dimensional source signals were randomly generated according to log p(s(t)|s(t ? 1)) = ?|s(t) ? 0.7s(t ? 1)|. Though this generative process was noted to not be covered by the theory presented in (Hyv?rinen and <ref type="bibr">Morioka, 2017)</ref>, the authors demonstrated that PCL could reconstruct the source signals reasonably well even for the nonlinear mixture case. Given our practical use of a Laplacian conditional, we found it a valuable comparison to evaluate our theory in this artificial setting.  <ref type="table">Table 5</ref>: MCC using linear correlation where L denotes the number of mixing layers.</p><p>Given the discussion in Appendix B, we use SlowFlow for these experiments. For computational tractability in demixing highly nonlinear transformations, we consider normalizing flows <ref type="bibr" target="#b20">(Dinh et al., 2017a;</ref><ref type="bibr">Kingma and Dhariwal, 2018)</ref>, namely volume-preserving flows <ref type="bibr" target="#b48">(Sorrenson et al., 2017)</ref>, as we find constraining the Jacobian determinant stabilizes learning. To ensure sufficient expressivity, we consider 6 coupling blocks, each containing a 2-layer MLP with 500 hidden units and ReLU nonlinearities. We compare to the PCL implementation presented in (Hyv?rinen and <ref type="bibr">Morioka, 2017)</ref>, where an MLP with the same number of hidden layers as the mixing MLP was adopted. We use 100 hidden units as we did not find increasing the value improved performance. To account for the architectural difference serving as a possible confounder, we use the same normalizing flow encoder for optimizing the PCL objective, which we term "PCL (NF)".</p><p>While (Hyv?rinen and <ref type="bibr">Morioka, 2017)</ref> used leaky ReLU nonlinearities to make the mixing invertible, said mixing is non-differentiable. This is problematic for SlowFlow, as it involves gradient optimization of the Jacobian term, and more importantly, unlike PCL, aims to explicitly recover the mixing process. We thus use a a smooth version of the leaky-ReLU activation function with a hyperparameter ? (Gresele et al., 2020),</p><formula xml:id="formula_34">s L (x) = ?x + (1 ? ?) log(1 + e x ).<label>(29)</label></formula><p>By ensuring the mixing process is smooth, we find that SlowFlow performs favorably relative to PCL ( <ref type="table">Table 5</ref>) when evaluated in the same setting, converging to a better optimum at higher levels of mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 JOINT FACTOR DEPENDENCE EVALUATION</head><p>In order to consider joint dependencies among natural generative factors, we leverage Natural Sprites to construct modified datasets where time-pairs of factors are shuffled per-factor (e.g. combining the x transition from one clip with the y transition from a different clip). This destroys dependencies between the factors, while maintaining the sparse marginal distributions. In <ref type="figure" target="#fig_0">Fig. 11 (right)</ref>, we show 2D marginals before (blue) and after (orange) this shuffling. The additional density on the diagonals in the unshuffled data reveals dependencies between pairs of factors on both datasets. As mentioned in section 3.4, the observed dependency is mismatched from the theoretical assumptions of our model.</p><p>We test how robust SlowVAE is to such a mismatch by training it on the permuted data and reevaluating disentanglement. In <ref type="table" target="#tab_1">Table 22</ref>, we highlight that the improvement of SlowVAE on the permuted (i.e. independent) continuous Natural Sprites is not significant. In <ref type="table" target="#tab_1">Table 21</ref>, we surprisingly find an overall improved score with non-permuted transitions (i.e. with dependencies), with three out of seven metrics showing a significant improvement. This is in line with <ref type="figure" target="#fig_0">Fig. 1f</ref> in <ref type="bibr">Khemakhem et al. (2020b)</ref>, where, at least for simple mixing, a model (Khemakhem et al., 2020a) that does not account for dependencies performs as well as one that does <ref type="bibr">(Khemakhem et al., 2020b)</ref>. We conclude that these preliminary results do not support the hypothesis that SlowVAE's disentanglement is reliant upon the model assumption that the factors are independent, but do acknowledge that the empirical effect of statistical dependence in natural video warrants further exploration <ref type="bibr" target="#b55">(Tr?uble et al., 2020;</ref><ref type="bibr" target="#b66">Yang et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 TRANSITION PRIOR ABLATION</head><p>We consider an ablated model which minimizes a KL-divergence term between the posteriors at time-step t and time-step t ? 1. This encourages the model to match the posteriors of both time points as closely as possible, and resembles a probabilistic variant of Slow Feature Analysis <ref type="bibr" target="#b57">(Turner and Sahani, 2007)</ref>. Specifically, we set p(z t |z t?1 ) = q(z t?1 |x t?1 ), replacing the Laplace prior with the posterior of the previous time step. This is equivalent to a Gaussian (? = 2) transition prior, where the mean and variance are specified by the previous time step. We ablate over the regularization parameter ? and provide results in <ref type="table" target="#tab_5">Tables 14 and 15</ref>, although we note that we still use the same hyperparameter values for SlowVAE as in all other experiments. As predicted by our theoretical result, ? = 2 leads to entangled representations in aggregate across evaluated datasets and metrics, even when considering a spectrum of ? values, resulting in a drastic reduction in scores, particularly on dSprites and Natural Sprites. YouTube-VOS 234652 0.44 0.52 0.55 <ref type="table">Table 6</ref>: Shape parameters (?) of the fitted generalized Laplace distributions in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p><p>We report the empirical estimates of Kurtosis in <ref type="table" target="#tab_9">Table 7</ref>. We report the log-likelihood scores for the ? area, ? x, ? y statistics in <ref type="table" target="#tab_10">Tables 8, 9</ref>, and 10, respectively for a Normal, a Laplace and a generalized Laplace/Normal distribution. For these distributions, we also report the fit parameters for the ? area, ? x, ? y statistics in <ref type="table" target="#tab_1">Tables 11, 12</ref>, and 13, respectively, where the shape parameter ? of the generalized Laplacian is in bold face. As a higher likelihood indicates a better fit, we can see further evidence that natural transitions are highly leptokurtic; a Laplace distribution (? = 1) is a better fit than a Gaussian (? = 2), while the generalized Laplacian yields the highest likelihood consistently with ? ? 0.5 for all measurements, as indicated in the main paper. For the plots in Figs. 1 and 11, we set the standard deviation of each component to 1 and clipped the minimum (?5) and maximum <ref type="formula" target="#formula_5">(5)</ref> values.</p><p>We note that while the marginal transitions appear sparse in metrics computed from the given object masks, our analysis considers 2D projections of objects instead of the transition statistics in their 3D environment. Understanding the relationship between 3D and 2D transition statistics is a compelling question from a broader perspective of visual processing, but unfortunately, the KITTI-MOTS masks <ref type="bibr" target="#b59">(Voigtlaender et al., 2019;</ref><ref type="bibr">Geiger et al., 2012;</ref><ref type="bibr" target="#b31">Milan et al., 2016)</ref> lack the associated depth data required to answer it. Nonetheless, the natural scene statistics we compute are relevant, given that most computer vision models and vision-based animals see the 3D world as projected onto their 2D receptor arrays.         <ref type="figure" target="#fig_1">Fig. 20</ref>) the known ground truth factors are matched as following: ?-VAE: scale? z 2 , x-position? z 1 and y-position? z 3 ; SlowVAE: scale? z 0 , x-position? z 1 and y-position? z 3 . With these latent visualizations alone, there is no significant difference visible between ?-VAE and SlowVAE. However, we see a quantitative difference with the MCC score (see <ref type="table" target="#tab_1">Table 2</ref>) and a qualitative difference when directly observing latent embeddings (see <ref type="figure" target="#fig_1">Fig. 20</ref>).   , as well as the the more common mean (s.d.) scores for future comparisons and straightforward statistical estimates of significant differences between models. We also consider allowing for static transitions, which we denote with "NC", e.g. "LAP-NC", in the tabular results. As mentioned in Section 5, we use the same parameter settings for SlowVAE in all experiments, while model selection was performed not only per dataset, but per seed, for results from .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 KITTI MASKS ?t ABLATION</head><p>As seen in the main text, considering image pairs separated further apart in time appears beneficial. Here we evaluate a wider range by taking frames which are further apart in a sequence. max(?frames) = N indicates that all pairs differ by at most N frames. We chose an upper bound of N , rather than sampling pairs with a fixed separation, to account for the variable frame rates and sequence lengths in the original dataset <ref type="bibr" target="#b31">(Milan et al., 2016)</ref> without introducing a confounding factor of varying dataset size. We report in <ref type="figure" target="#fig_0">Fig. 10</ref> how the max(?frames) criterion corresponds to the mean time gap between image pairs (mean(?t)) in seconds. For further details, we refer to Appendix D.5.</p><p>In <ref type="figure" target="#fig_0">Fig. 13</ref> we visualize an ablation over mean <ref type="bibr">(?t)</ref>. We find that model performance increased initially with larger temporal separation between data points, then plateaued. We also observe in <ref type="figure" target="#fig_0">Fig. 14</ref>     ). The bottom three rows give mean and standard deviation (s.d.) for the models presented in this paper.</p><p>Increasing mean(?t) leads to increased diversity, and thus more information in the learning signal. However, it is worth noting that since SlowVAE assumes ? = 1 in the transitions, an increase in ? from increasing the temporal gap leads to a reduction in mismatch.</p><p>Our results on increasing the temporal difference within pairs of inputs is in agreement with recent work by Oord et al. <ref type="table" target="#tab_1">(2018, Table 2)</ref>, who show increased performance in representation learning for larger separation between positive samples in a contrastive objective function. Additional related work from <ref type="bibr" target="#b56">Tschannen et al. (2019)</ref> shows that temporal separation between frame embeddings influences the representation that is learned from videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 LATENT SPACE VISUALIZATIONS</head><p>We visualize differences in learned latent representations using image embedding in <ref type="figure" target="#fig_0">Figures 15-28</ref>. We show four different plots for each dataset considered and include all available models. Each figure corresponds to a different dataset.  ). The bottom three rows give mean and standard deviation (s.d.) for the models presented in this paper.</p><p>In <ref type="figure" target="#fig_0">Figures 15-21</ref> we display the mean correlation coefficient matrix and the latent representations for each ground-truth, as described in the main text for <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>The top row is the sorted absolute correlation coefficient matrix between the latents (rows) and the ground truth generating factors (columns). The latent dimensions are permuted such that the sum on the diagonal is maximal. This is achieved by an optimal, non-greedy matching process for each ground truth factor with its corresponding latent, as described in appendix C. As such, a more  20: MPI3D. Median and absolute deviation (a.d.) metric scores across 10 random seeds (first three rows are from ). The bottom three rows give mean and standard deviation (s.d.) for comparison with other tables. prevalent diagonal structure corresponds to a better mapping between the ground-truth factors and latent encoding.</p><p>The middle set of plots are latent embeddings of random training data samples. The x-axis denotes the ground truth generating factor and the y-axis denotes the corresponding latent factor as matched according to the main diagonal of the correlation matrix. For each dataset, we further color-code the latents by a categorical variable as denoted in each figure.</p><p>The bottom set of plots show the ground truth encoding compared to the second best latent as opposed to the diagonally matched latent. This plot can be used to judge how much the correspondence between latents is one-to-one or rather one-to-many.</p><p>To further investigate the latent representations, we show a scatter plot over the best and second best latents in figures 22-28. Here, the color-coding is matched by the ground truth factor denoted in each row.</p><p>When comparing the correlation matrix with the corresponding scatter plots, one can see that embeddings with sinusoidal curves have low correlation, which illustrates a shortcoming of the metric. Another limitation is that categorical variables which have no natural ordering have an order-dependent MCC score, indicating the permutation variance of MCC. With SlowVAE, we can infer three different types of embeddings. First, we have simple ordered ground truth factors with non-  circular boundary conditions. Here, SlowVAE models often show a clear one-to-one correspondence (e.g. <ref type="figure" target="#fig_1">Fig 22 scale,</ref> x-position and y-position; <ref type="figure" target="#fig_1">Fig 25 ?-rotation; Fig 26 ?-rotation)</ref>. Second, we observe circular embeddings due to boundary conditions for certain factors (e.g. <ref type="figure" target="#fig_0">Fig 15, 22 3rd</ref> row; <ref type="figure" target="#fig_0">Fig 16, 23 2nd row)</ref>. Note that not all datasets with orientations exhibit full rotations and thus do not have circular boundary conditions, e.g. smallNORB. Finally, we have categorical variables, where no order exists (e.g. <ref type="figure" target="#fig_0">Fig. 16, 23 top row, Fig 17, 24 top row, Fig 18, 25</ref> top row) resulting in separated but not necessarily ordered clusters.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DIP-VAE-II</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Statistics of Natural Transitions. The histograms show distributions over transitions of segmented object masks from natural videos for horizontal and vertical position as well as object size. The red lines indicate fits of generalized Laplace distributions (Eq. 2) with shape value ?. Data shown is for object masks extracted from YouTube videos. See Appendix G for 2D marginals and corresponding analysis from the KITTI self-driving car dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Proof Intuition. Latent representation and example generated image pairs for ground-truth (blue) and entangled (red) model. See text below for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>KITTI Masks (mean(?t) = 0.15s). (Left) MCC correlation matrix of the top 3 latents corresponding to y-position, x-position and scale. (Right) Images produced by varying the SlowVAE latent unit that corresponds to the corresponding row in the MCC matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Kitti (mean(?t) = 0.15s) 67.6 (6.7) SlowVAE Kitti (mean(?t) = 0.15s) 79.6 (5.8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>DSprites Latent Representations: (Top) shows absolute MCC between generative and model factors (rows are rearranged for maximal correlation on the main diagonal). The columns correspond to generative factors (shape, scale, rotation, x/y-position) and the values correspond to percent correlation. A more diagonal structure in the upper half corresponds to a better one-to-one mapping between generative and latent factors. (Bottom) shows individual latent dimensions (y-axis) over the matched generative factors (x-axis). Colors encode shapes: heart/yellow, ellipse/turquoise, and square/purple.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>VAE failure modes. Rows respectively indicate ? = 0.2, 0.4, 0.6, 0.8, 1.0 from Eq. (24). The left five columns show values for 100 randomly chosen examples, while the ? and ? columns show values for the full training set. Columns in the sets (z * , z), (?z * , ?z), (x * , x) all have the same (arbitrary) scale factors the axes. Lines indicate trajectories from time-point t to t + 1, and color indicates the angle of the trajectory vector with respect to the canonical variable axes. The ? axes is scaled from ?4 to 4, and ? axes are scaled from 0 to 1, where individual dots represent latent encoding values from test images. The rightmost plots show a shift in the relationship between the mean correlation coefficient (MCC) (black, higher is better) and training loss (red, lower is better) as one increases ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>VAEs perform better when data dimensionality exceeds the latent dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Number of changing factors in LAP dataset. For each dataset we sample 10,000 transitions and record the number of changing factors. These are indicated in the histograms. ? = 1, see Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>KITTI Masks ?t. Boxes indicate correspondence to physical time for different max(?frames) in the KITTI Masks datasets. The orange line denotes the median and the green line the mean. The whiskers cover the 5th and 95th percentile of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Statistics of Natural Transitions. Left) Distribution over transitions for horizontal (?x) and vertical (?y) position as well as mask/object size (?area) for both datasets. Orange lines indicate fits of generalized Laplace distributions (Eq. 2). Right) 2D marginal distribution over pairs of factor transitions (blue) and permuted pairs (orange) that indicate the marginal distributions when made</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Ablation over mean(?t) for SlowVAE. Mean and standard deviation (s.d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>KITTI Masks Sparseness. We show the sparseness over time of the transitions for horizontal (?x), vertical (?y) as well as mask/object size (?area) in KITTI Masks by plotting the ? of a generalized Laplace fit for different mean(?t) (top). To display the quality of the fits, we show two exemplary fits at mean(?t) = 0.63 (bottom-left) and mean(?t) = 1.02 (bottom-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :Figure 17 :Figure 18 :Figure 20 :</head><label>15171820</label><figDesc>DSprites Latent Representations. Top, MCC correlation matrices. Middle five rows, model latent over highest correlating ground truth factor. Bottom five rows, model latent over second highest correlating ground truth factor. The color-coding corresponds to the shapes: heart/yellow, ellipse/turquoise and square/purple. SmallNorb Latent Representations. Top, MCC correlation matrices. Middle four rows, model latent over highest correlating ground truth factor. Bottom four rows, model latent over second highest correlating ground truth factor. The color-coding corresponds to the five different GT categories in the dataset. Shapes3D Latent Representations. Top, MCC correlation matrices. Left two columns, model latent over highest correlating ground truth factor. Right two columns, model latent over second highest correlating ground truth factor. The color-coding corresponds to the four different object types (GT-Type) in the dataset. KITTI Masks Latent Representations. Top, MCC correlation matrices. Middle three rows, model latent over highest correlating ground truth factor. Bottom three rows, model latent over second highest correlating ground truth factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 22 :</head><label>22</label><figDesc>DSprites Latent Representations. Best two latents selected from Fig 15. Color-coded by the corresponding ground truth factor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 23 :</head><label>23</label><figDesc>Cars3D Latent Representations. Best two latents selected from Fig 16. Color-coded by ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 24 :</head><label>24</label><figDesc>SmallNorb Latent Representations. Best two latents selected from Fig 17. Color-coded by ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 25 :</head><label>25</label><figDesc>Shapes3D Latent Representations. Best two latents selected from Fig 18. Color-coded by ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 26 :</head><label>26</label><figDesc>MPI3DReal Latent Representations. Best two latents selected from Fig 19. Color-coded by ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 27 :</head><label>27</label><figDesc>KITTI Masks Latent Representations. Best two latents selected from Fig 20. Colorcoded by ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 28 :</head><label>28</label><figDesc>Natural Sprites Latent Representations. Best two latents selected from Fig 21. The left four columns denote the continuous (C) version of Natural Sprites, whereas the right four columns correspond to the discretized (D) version. Color-coded by ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Continuous ground-truth variable datasets. See Table 1 for details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Sunny Duan, Loic Matthey, Andre Saraiva, Nick Watters, Christopher Burgess, Alexander Lerchner, and Irina Higgins. Unsupervised model selection for variational disentangled representation learning. In International Conference on Learning Representations (ICLR), 2020. Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disentangled representations. In In International Conference on Learning Representations, 2018. Peter F?ldi?k. Learning invariance from transformation sequences. Neural Computation, 3(2):194-200, 1991. Lijian Gao, Qirong Mao, Ming Dong, Yu Jing, and Ratna Chinnam. On learning disentangled representation for acoustic event detection. In Proceedings of the 27th ACM International Conference on Multimedia, pages 2006-2014, 2019. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Sch?lkopf, and Stefan Bauer. On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. In Advances in Neural Information Processing Systems, pages 15714-15725, 2019. Will Grathwohl and Aaron Wilson. Disentangling space and time in video with hierarchical variational autoencoders. arXiv preprint arXiv:1612.04440, 2016. Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. arXiv preprint arXiv:1903.00450, 2019. Luigi Gresele, Giancarlo Fissore, Adrian Javaloy, Bernhard Scholkopf, and Aapo Hyvarinen. Relative gradient optimization of the jacobian term in unsupervised deep learning. ArXiv, abs/2006.15090, 2020. Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534, 2019. Olivier J H?naff, Robbe LT Goris, and Eero P Simoncelli. Perceptual straightening of natural videos. Nature neuroscience, 22(6):984-991, 2019. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018. Geoffrey E Hinton. Connectionist learning procedures. In Machine learning, page 208. Elsevier, 1990. Aapo Hyv?rinen and Patrik Hoyer. Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces. Neural computation, 12(7):1705-1720, 2000. Aapo Hyv?rinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ica. In Advances in Neural Information Processing Systems, pages 3765-3773, 2016. Aapo Hyv?rinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources. In Proceedings of Machine Learning Research, 2017. Aapo Hyv?rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks, 12(3):429-439, 1999. Aapo Hyv?rinen, Jarmo Hurri, and Jaakko V?yrynen. Bubbles: a unifying framework for low-level statistical properties of natural image sequences. JOSA A, 20(7):1237-1252, 2003. Khemakhem, Diederik P Kingma, and Aapo Hyv?rinen. Variational autoencoders and nonlinear ica: A unifying framework. International Conference on Artificial Intelligence and Statistics (AISTATS), 2020a. Ilyes Khemakhem, Ricardo Monti, Diederik Kingma, and Aapo Hyvarinen. Ice-beem: Identifiable conditional energy-based deep models based on nonlinear ica. Advances in Neural Information Processing Systems, 33, 2020b. Hyunjik Kim and Andriy Mnih. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in neural information processing systems, pages 10215-10224, 2018. Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In Advances in neural information processing systems, pages 4743-4751, 2016. Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional inverse graphics network. In Advances in neural information processing systems, pages 2539-2547, 2015. Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. In International Conference on Learning Representations, 2018. Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., volume 2, pages II-104. IEEE, 2004. APPENDIX Proof of Identifiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 A.2 Kullback Leibler Divergence of Slow Variational Autoencoder . . . . . . . . . . . 20 Mean Correlation Coefficient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.2 DisLib Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Uniform Transitions (UNI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.2 Laplace Transitions (LAP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.3 YouTube-VOS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.4 Natural Sprites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.5 KITTI MOTS Pedestrian Masks (KITTI Masks) . . . . . . . . . . . . . . . . . . . 29 Comparison to Nonlinear ICA . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 F.2 Joint Factor Dependence Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 33 F.3 Transition Prior Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Extended Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 G.2 All DisLib Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 G.3 KITTI Masks ?t Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 G.4 Latent Space Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38</figDesc><table><row><cell>765-788, 2003. Junxian He, International Conference on Learning Representations (ICLR), 2(5):6, 2017. Irina Higgins, Ilyes A Formal Methods A.1 B Choosing a Latent Variable Model C Disentanglement Metrics C.1 D Natural Datasets D.1 E Model Training and Selection F Extended Comparisons and Controls F.1 G Additional Results Function / variable Description g Generator ? Prior shape ? Prior rate G.1 A FORMAL METHODS p</cell><cell>(4): 18 23 24 27 31 32 34</cell></row></table><note>Wakako Hashimoto. Quadratic forms in natural images. Network: Computation in Neural Systems, 14Aapo Hyv?rinen, Hiroaki Sasaki, and Richard E Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. arXiv preprint arXiv:1805.08651, 2018. Christian Jutten and Jeanny Herault. Blind separation of sources, part i: An adaptive algorithm based on neuromimetic architecture. Signal Processing, 24(1):1-10, 1991.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Natural Sprite Configs. Values in brackets refer to the number of unique values. Shapes presented are predefined in Spriteworld.</figDesc><table><row><cell>? Shape: square, ellipse, heart</cell></row><row><cell>? Scale: 6 values linearly spaced in [0.5, 1]</cell></row><row><cell>? Orientation: 40 values in [0, 2?]</cell></row><row><cell>? Position X: 32 values in [0, 1]</cell></row><row><cell>? Position Y : 32 values in [0, 1]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Empirical estimates of Kurtosis for mask transitions per metric for each dataset.</figDesc><table><row><cell>dataset</cell><cell>N</cell><cell>genlaplace</cell><cell>normal</cell><cell>laplace</cell></row><row><cell>KITTI</cell><cell>82506</cell><cell cols="3">-3.21e+05 -3.79e+05 -3.35e+05</cell></row><row><cell cols="5">YouTube 234652 -1.29e+06 -1.45e+06 -1.33e+06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Maximum likelihood scores for the considered distributions on ? area for each dataset.</figDesc><table><row><cell>dataset</cell><cell>N</cell><cell>genlaplace</cell><cell>normal</cell><cell>laplace</cell></row><row><cell>KITTI</cell><cell>82506</cell><cell cols="3">-8.72e+04 -1.20e+05 -9.25e+04</cell></row><row><cell cols="5">YouTube 234652 -4.50e+05 -5.64e+05 -4.74e+05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Maximum likelihood scores for the considered distributions on ?x for each dataset.</figDesc><table><row><cell>dataset</cell><cell>N</cell><cell>genlaplace</cell><cell>normal</cell><cell>laplace</cell></row><row><cell>KITTI</cell><cell>82506</cell><cell cols="3">-7.59e+04 -1.07e+05 -7.86e+04</cell></row><row><cell cols="5">YouTube 234652 -4.40e+05 -5.45e+05 -4.60e+05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Maximum likelihood scores for the considered distributions on ?y for each dataset.</figDesc><table><row><cell>dataset</cell><cell>N</cell><cell>genlaplace</cell><cell>normal</cell><cell>laplace</cell></row><row><cell>KITTI</cell><cell>82506</cell><cell cols="3">[4.55e-01, 1.00e+00, 1.01e+00] [4.53e-01, 2.39e+01] [1.00e+00, 1.07e+01]</cell></row><row><cell cols="5">YouTube 234652 [4.44e-01, 1.47e-16, 5.04e+00] [2.25e-01, 1.16e+02] [7.73e-09, 5.28e+01]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Parameter fits for the considered distributions on ? area for each dataset. The parameters are (alpha, location, scale) for generalized Laplace/Normal, (location, scale) for the other two distributions.</figDesc><table><row><cell>dataset</cell><cell>N</cell><cell>genlaplace</cell><cell>normal</cell><cell>laplace</cell></row><row><cell>KITTI</cell><cell>82506</cell><cell cols="3">[5.87e-01, 4.76e-02, 1.69e-01] [5.34e-02, 1.04e+00] [5.49e-02, 5.64e-01]</cell></row><row><cell cols="5">YouTube 234652 [5.15e-01, 1.15e-14, 2.57e-01] [2.32e-03, 2.68e+00] [7.54e-09, 1.38e+00]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Parameter fits for the considered distributions on ? x for each dataset. The parameters are (alpha, location, scale) for generalized Laplace/Normal, (location, scale) for the other two distributions.</figDesc><table><row><cell>dataset</cell><cell>N</cell><cell>genlaplace</cell><cell>normal</cell><cell>laplace</cell></row><row><cell>KITTI</cell><cell>82506</cell><cell cols="3">[6.94e-01, 1.02e-02, 2.32e-01] [3.84e-02, 8.86e-01] [1.71e-02, 4.77e-01]</cell></row><row><cell cols="5">YouTube 234652 [5.48e-01, 2.93e-13, 3.08e-01] [8.81e-03, 2.47e+00] [9.15e-04, 1.30e+00]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Parameter fits for the considered distributions on ? y for each dataset. The parameters are (alpha, location, scale) for generalized Laplace/Normal, (location, scale) for the other two distributions.Figure 12: KITTI Masks Latent Representations. We show axis latent traversals along each dimension for the ?-VAE (top) and SlowVAE (bottom). Here, the latents z i are sorted from top to bottom in ascending order according to the mean variance output of the encoder. With MCC correlation (see e.g.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Mean and standard deviation (s.d.) metric scores across 10 random seeds. PM-VAE (?) refers to replacing the Laplace prior with a KL-divergence term between the (Gaussian) posteriors at time-step t and time-step t ? 1, with conditional prior regularization, ?.</figDesc><table><row><cell>G.2 ALL DISLIB RESULTS</cell></row><row><cell>We include results on all DisLib datasets, dSprites (Matthey et al., 2017), Cars3D (Reed et al., 2015),</cell></row><row><cell>SmallNORB (LeCun et al., 2004), Shapes3D (Kim and Mnih, 2018), MPI3D (Gondal et al., 2019), in</cell></row><row><cell>Tables 16, 17, 18, 19, and 20, respectively. We report both median (a.d.) to compare to the previous</cell></row><row><cell>median scores reported in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>that the measured factor marginals remain sparse, with ? &lt; 1, for all tested settings of mean(?t).</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell>MCC</cell></row><row><cell>SlowVAE</cell><cell>Natural (Continuous)</cell><cell>49.1 (4.0)</cell></row><row><cell cols="2">PM-VAE (16) Natural (Continuous)</cell><cell>35.2 (3.7)</cell></row><row><cell cols="2">PM-VAE (10) Natural (Continuous)</cell><cell>33.2 (2.1)</cell></row><row><cell>PM-VAE (8)</cell><cell>Natural (Continuous)</cell><cell>32.7 (3.1)</cell></row><row><cell>PM-VAE (4)</cell><cell>Natural (Continuous)</cell><cell>33.7 (2.3)</cell></row><row><cell>PM-VAE (2)</cell><cell>Natural (Continuous)</cell><cell>32.4 (3.2)</cell></row><row><cell>PM-VAE (1)</cell><cell>Natural (Continuous)</cell><cell>34.2 (3.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 :</head><label>15</label><figDesc>Continuous ground-truth variable datasets. See Table 14 for details.</figDesc><table><row><cell>Model (Data)</cell><cell>BetaVAE</cell><cell cols="2">FactorVAE MIG</cell><cell>DCI</cell><cell cols="2">Modularity SAP</cell></row><row><cell>?-VAE (i.i.d.)</cell><cell>82.3</cell><cell>66.0</cell><cell>10.2</cell><cell>18.6</cell><cell>82.2</cell><cell>4.9</cell></row><row><cell cols="2">Ada-ML-VAE (LOC) 89.6</cell><cell>70.1</cell><cell>11.5</cell><cell>29.4</cell><cell>89.7</cell><cell>3.6</cell></row><row><cell>Ada-GVAE (LOC)</cell><cell>92.3</cell><cell>84.7</cell><cell>26.6</cell><cell>47.9</cell><cell>91.3</cell><cell>7.4</cell></row><row><cell>SlowVAE (UNI)</cell><cell>89.7 (3.8)</cell><cell>81.4 (8.4)</cell><cell>34.5 (9.6)</cell><cell cols="2">50.0 (6.9) 87.1 (2.0)</cell><cell>5.1 (1.5)</cell></row><row><cell>SlowVAE (LAP)</cell><cell cols="2">100.0 (0.0) 99.2 (2.3)</cell><cell>28.2 (8.2)</cell><cell cols="2">65.5 (3.1) 96.8 (1.4)</cell><cell>6.0 (2.4)</cell></row><row><cell cols="3">SlowVAE (LAP-NC) 100.0 (0.2) 97.4 (4.4)</cell><cell>29.1 (7.1)</cell><cell cols="2">62.0 (4.2) 97.4 (1.6)</cell><cell>8.2 (2.9)</cell></row><row><cell>SlowVAE (UNI)</cell><cell>87.0 (5.1)</cell><cell cols="4">75.2 (11.1) 28.3 (11.5) 47.7 (8.5) 86.9 (2.8)</cell><cell>4.4 (2.0)</cell></row><row><cell>SlowVAE (LAP)</cell><cell cols="2">100.0 (0.0) 97.5 (3.0)</cell><cell>29.5 (9.3)</cell><cell cols="2">65.4 (3.6) 96.5 (1.6)</cell><cell>8.1 (3.0)</cell></row><row><cell cols="2">SlowVAE (LAP-NC) 99.8 (0.6)</cell><cell>95.2 (6.0)</cell><cell>27.6 (8.6)</cell><cell cols="2">61.5 (5.3) 96.8 (1.8)</cell><cell>8.4 (3.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 16 :</head><label>16</label><figDesc>dSprites. Median and absolute deviation (a.d.) metric scores across 10 random seeds (first three rows are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18 :</head><label>18</label><figDesc>SmallNORB. Median and absolute deviation (a.d.) metric scores across 10 random seeds (first three rows are from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 22 :</head><label>22</label><figDesc>Impact of removing natural dependence on Continuous Natural Sprites.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">As in slow feature analysis, we consider learning from videos without labels as unsupervised.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For a stationary stochastic process, p(zt?1) represents the instantaneous marginal distribution and p(zt|zt?1) the transition distribution. In case of an autoregressive process with non-Gaussian innovations with finite variance, it follows from the central limit theorem that the marginal distribution converges to a Gaussian in the limit of large ?.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/google-research/disentanglement_lib/blob/master/ disentanglement_lib/methods/weak/train_weak_lib.py#L48 4 https://competitions.codalab.org/competitions/20127</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.vision.rwth-aachen.de/page/mots</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Francesco Locatello for valuable discussions and providing numerical results to facilitate our experimental comparisons. Additionally, we thank Luigi Gresele, Matthias Tangemann, Roland Zimmermann, Robert Geirhos, Matthias K?mmerer, Cornelius Schr?der, Charles Frye, and Sarah Master for helpful feedback in preparing the manuscript. Finally, the authors would</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Factors</head> <ref type="figure">0 1 4 94 16 4 3  1 1 4 2 94 0 3  9 0 8 1 1 30 0  4 1 6 1 0 25 82  15 1 17 17 0 16 72  0 0 1 2 0 26 12  11 4 10 11 10 15 2</ref> <p>SlowVAE (LAP) <ref type="figure">Figure 19</ref>: MPI3DReal Latent Representations. Top, MCC correlation matrices. Left two columns, model latent over highest correlating ground truth factor. Right two columns, model latent over second highest correlating ground truth factor. The color-coding corresponds to the six different object shapes (GT Shape) in the dataset. Middle five rows, model latent over highest correlating ground truth factor (colored by category). Bottom five rows, model latent over second highest correlating ground truth factor. The left two columns denote the continuous (C) version of Natural Sprites, whereas the right two columns correspond to the discretized (D) version.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discovering interpretable representations for both deep generative and discriminative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tameem</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape, albedo, and illumination from a single image of an unknown object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="334" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An information-maximization approach to blind separation and blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1129" to="1159" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2017.1285773</idno>
		<idno>1537-274X. doi: 10.1080/ 01621459.2017.1285773</idno>
		<ptr target="http://dx.doi.org/10.1080/01621459.2017.1285773" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Addressing the topological defects of disentanglement via distributed operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<editor>CoNLL</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding disentangling in beta-vae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christpher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11390</idno>
		<title level="m">Unsupervised scene decomposition and representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihuang</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciyou</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning intermediate-level representations of form and motion from natural movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="827" to="866" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Source separation using higher order moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-F</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="2109" to="2112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discovering hidden factors of variation in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">A</forename><surname>Livezey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6583</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Independent component analysis, a new concept? Signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Comon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flexibly fair representation learning by disentanglement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marissa</forename><forename type="middle">A</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised learning of disentangled representations from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vighnesh</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4414" to="4423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Avoiding latent variable collapse with generative skip models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<idno>abs/1807.04863</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation. ArXiv, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8516</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1605.08803</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Challenging common assumptions in the unsupervised learning of disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>R?tsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12359</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the fairness of disentangled representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Abbati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14611" to="14624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>R?tsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02886</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Weakly-supervised disentanglement without compromises. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Don&apos;t blame the elbo! a linear vae perspective on posterior collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9408" to="9418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disentangled person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Biva: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6551" to="6562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Disentangling disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4402" to="4412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">dsprites: Disentanglement testing sprites dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sur les transformations isom?triques d&apos;espaces vectoriels norm?s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Mazur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Ulam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CR Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="1932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MOT16: A benchmark for multi-object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00831</idno>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Removing time variation with the anti-hebbian differential synapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Mitchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="320" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Time-contrastive learning (tcl)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Morioka</surname></persName>
		</author>
		<ptr target="https://github.com/hirosm/TCL" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning sparse, overcomplete representations of time-varying natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2003 International Conference on Image Processing</title>
		<meeting>2003 International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Time series source separation with slow flows. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Razakarivony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep visual analogy-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1252" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey of inductive biases for factorial representation-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ridgeway</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05299</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning deep disentangled embeddings with the f-statistic loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variational autoencoders pursue pca directions (by accident)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rolinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12406" to="12415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Weakly supervised disentanglement with guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09772</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno A Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Characterization of the p-generalized normal distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gerwinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="817" to="820" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An unsupervised algorithm for learning lie group transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><forename type="middle">Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1001.1027</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disentanglement by nonlinear ica with general incompressible-flow networks (gin)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sorrenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Kothe</surname></persName>
		</author>
		<idno>abs/2001.04872</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the law of frequency of error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikhail Fedorovich Subbotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mat. Sb</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="296" to="301" />
			<date type="published" when="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attila</forename><surname>Szab?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02245</idno>
		<title level="m">Challenges in disentangling independent factors of variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A family of nonparametric density estimation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">V</forename><surname>Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="164" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Density estimation by dual ascent of the log-likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tabak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Quantifying and learning disentangled representations with limited supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loek</forename><surname>Tonnaer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">A P?rez</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlado</forename><surname>Menkovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Holenderski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacobus</forename><forename type="middle">W</forename><surname>Portegies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Tr?uble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07886</idno>
		<title level="m">Is independence all you need? on the generalization of representations learned from correlated data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Self-supervised learning of video-induced visual invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02783</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A maximum-likelihood interpretation for slow feature analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1022" to="1038" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12827</idno>
		<title level="m">Entity abstraction in visual model-based reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mots: Multi-object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljosa</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berin</forename><surname>Balachandar Gnana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Spriteworld: A flexible, configurable reinforcement learning environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/spriteworld/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Unmasking the inductive biases of unsupervised object representations for video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marissa</forename><forename type="middle">A</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07034</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunkumar</forename><surname>Byravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Hertweck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lampe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01758</idno>
		<title level="m">Representation matters: Improving perception and exploration for robotics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04804</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Causalvae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08697</idno>
		<title level="m">Structured causal disentanglement in variational autoencoder</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Efficient inverse graphics in biological face processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Belledonne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winrich</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">5979</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3203" to="3211" />
		</imprint>
	</monogr>
	<note>SlowVAE dSprites (Laplace) 100.0 (0.0) 97.5 (3.0) 29.5 (9.3) 69.8 (2</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada-Ml-Vae</forename></persName>
		</author>
		<imprint>
			<publisher>LOC</publisher>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Table 17: Cars3D. Median and absolute deviation (a.d.) metric scores across 10 random seeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Locatello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>first three rows are from</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada-Ml-Vae</forename></persName>
		</author>
		<imprint>
			<publisher>LOC</publisher>
			<biblScope unit="page">91</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slowvae</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>UNI) 78.8 (2.1) 46.2 (1.9</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada-Ml-Vae</forename></persName>
		</author>
		<imprint>
			<publisher>LOC</publisher>
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Table 19: Shapes3D. Median and absolute deviation (a.d.) metric scores across 10 random seeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Locatello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>first three rows are from</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada-Ml-Vae</forename></persName>
		</author>
		<imprint>
			<publisher>LOC</publisher>
			<biblScope unit="page">72</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slowvae</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>UNI) 58.5 (0.9) 38.6 (2.3) 32.2 (1.0) 29.9 (1.3) 89.2 (2.0</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
