<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Modal Attention-based Fusion Model for Semantic Segmentation of RGB-Depth Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahimeh</forename><surname>Fooladgar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Shohreh</forename><forename type="middle">Kasaei</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Modal Attention-based Fusion Model for Semantic Segmentation of RGB-Depth Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic segmentation</term>
					<term>attention-based fusion</term>
					<term>multi-modal fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 3D scene understanding is mainly considered as a crucial requirement in computer vision and robotics applications. One of the high-level tasks in 3D scene understanding is semantic segmentation of RGB-Depth images. With the availability of RGB-D cameras, it is desired to improve the accuracy of the scene understanding process by exploiting the depth features along with the appearance features. As depth images are independent of illumination, they can improve the quality of semantic labeling alongside RGB images. Consideration of both common and specific features of these two modalities improves the performance of semantic segmentation. One of the main problems in RGB-Depth semantic segmentation is how to fuse or combine these two modalities to achieve more advantages of each modality while being computationally efficient. Recently, the methods that encounter deep convolutional neural networks have reached the state-of-the-art results by early, late, and middle fusion strategies. In this paper, an efficient encoder-decoder model with the attention-based fusion block is proposed to integrate mutual influences between feature maps of these two modalities. This block explicitly extracts the interdependences among concatenated feature maps of these modalities to exploit more powerful feature maps from RGB-Depth images. The extensive experimental results on three main challenging datasets of NYU-V2, SUN RGB-D, and Stanford 2D-3D-Semantic show that the proposed network outperforms the state-of-the-art models with respect to computational cost as well as model size. Experimental results also illustrate the effectiveness of the proposed lightweight attention-based fusion model in terms of accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S EMANTIC segmentation of RGB-Depth images has been considered as one of the main tasks for 3D scene understanding. The popularity of its applications such as autonomous driving, augmented virtual reality, and the inference of support relations among objects in robotics emphasize the importance of scene understanding. Most of the researches in this field have been done on outdoor scenes which are less challenging compared to indoor scenes. The existence of small objects, light-tailed distribution of objects, occlusions, and poor illumination cause major challenges in indoor scenes, to name a few.</p><p>By introducing the Microsoft Kinect camera <ref type="bibr" target="#b0">[1]</ref> which captures both RGB and depth images, some indoor semantic segmentation approaches have been concentrated on the RGB-D dataset which alleviates the challenges of the indoor scene. For instance, when RGB images have a poor illumination in some regions, depth images can improve the labeling accuracy. <ref type="figure" target="#fig_0">Figure 1</ref> shows some examples in which RGB images have poor lightning in some regions while depth images hold discriminative features.</p><p>Utilizing the 3D geometric information in semantic segmentation methods has been provided by the availability of Microsoft Kinect camera <ref type="bibr" target="#b1">[2]</ref>. Extracting this 3D geometric information that might be missed in RGB images aids to diminish some uncertainties in dense prediction and object detection processes <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Early RGB-D semantic segmentation proposed novel engineering features extracted from RGB and depth images by using intrinsic and extrinsic camera parameters <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Then, all of these appearances and 3D features were incorporated into feature vectors fed to common classifiers.</p><p>Recently, Deep Convolutional Neural Networks (DCNNs) <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> have improved the accuary of almost all categories of computer vision methods; such as image classification <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b7">[8]</ref>, object detection <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, action recognition <ref type="bibr" target="#b12">[13]</ref>, depth estimation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, pose estimation <ref type="bibr" target="#b15">[16]</ref> , image segmentation <ref type="bibr" target="#b16">[17]</ref> and semantic segmentation <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>Pooling operations and stride convolutions which are applied in CNNs (to become invariant to most local changes) produce the low spatial resolution outputs for dense prediction applications (such as semantic segmentation, depth estimation, and surface normal estimation). Hence, the early deep learning methods <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> for semantic segmentation utilized the deep networks as feature extractors. They then applied a classifier to categorize each pixel, superpixel, or region. Long et al. <ref type="bibr" target="#b23">[24]</ref> changed CNNs to Fully Convolutional Neural Networks (FCN) which are more appropriate for dense prediction applications. DeconvNet <ref type="bibr" target="#b26">[25]</ref>, dilated convolution <ref type="bibr" target="#b27">[26]</ref>, and unpooling <ref type="bibr" target="#b28">[27]</ref> methods have been proposed to recover this spatial information lost. Among these methods, some nonparametric approaches <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref> have been proposed where they utilize similarity measurements to label each part of images.</p><p>As one of the goals of this paper is the semantic segmentation of RGB-Depth images, the focus is on the main challenges and approaches of RGB-D datasets. The main challenge in RGB-Depth semantic segmentation is how to represent and fuse the RGB and depth channels so that the strong correlations between the depth and photometric channels are considered. Simple methods for fusion of RGB and depth channels are based on the early fusion <ref type="bibr" target="#b21">[22]</ref> and late fusion <ref type="bibr" target="#b17">[18]</ref> polices.</p><p>In this paper, the encoder-decoder architecture with the novel multi-modal Attention-based Fusion Block (AFB) is proposed to fuse these two modalities in order to obtain more powerful and meaningful RGB-Depth fused feature maps. The attention-based fusion block has been inspired by the attention modules in the squeeze and excitation network <ref type="bibr" target="#b31">[30]</ref> which is focused on the channel-wise recalibration of feature maps to model the dependency of channels. The intermediate feature maps extracted from RGB and depth channels of two encoders are considered as the input to the attention-based fusion block. This block computes attention maps which are multiplied by input feature maps for adaptive feature fusion. The attentionbased fusion block consists of two sequentially channel-and spatial-wise attention mechanisms to construct the attention maps. Consequently, feature maps of two modalities are fused based on their interdependencies among different channels. <ref type="figure" target="#fig_1">Fig 2 illustrates</ref> the proposed architecture of attentionbased fusion block. Moreover, each AFB is followed by the lightweight chained residual pooling layers to consider the global contextual information in the proposed decoder side. Consequently, the proposed encoder-decoder architecture is an efficient model in terms of the computational cost and the number of parameters. Main contributions of this work are listed as:</p><p>? Proposing an efficient encoder-decoder architecture for semantic segmentation of RGB-Depth images. ? Proposing an attention mechanism of CNNs for modality fusion. ? Incorporating a channel-wise alongside spatial-wise interdependencies for fusion. ? Proposing a novel representation of evaluation metric for semantic segmentation methods. The remainder of this paper is organized as follows. In Section II, the related work of RGB and RGB-Depth semantic segmentation are categorized. The overall architecture of proposed encoder-decoder model with the fusion block is presented in Section III. The experimental results evaluated on the existing RGB-D dataset by the proposed semantic segmentation criterion are investigated in Section IV. Finally, conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Traditional approaches of semantic segmentation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b33">[32]</ref>, <ref type="bibr" target="#b34">[33]</ref> have two main phases of feature extraction and classification. Engineering or handcrafted features (such as SIFT, HOG, and SURF) are extracted from pixels, super-pixels, or segmented regions. Then, these features are fed to common classifiers; such as Support Vector Machine (SVM) and Random Forest (RF).</p><p>By emerging convolutional neural networks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b7">[8]</ref>, the most successful methods in the field of semantic segmentation have been proposed based on CNNs. Early CNNs methods proposed in semantic segmentation field <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b22">[23]</ref> utilized them as a deep feature extractor. Couprie et al. <ref type="bibr" target="#b21">[22]</ref> extracted deep and dense hierarchical features for each region of the segmentation tree <ref type="bibr" target="#b36">[35]</ref> via the multi-scale CNN model. These deep features are then fed to the SVM classifier to predict the label of each region.</p><p>Well-known CNN models (such as GoogLeNet <ref type="bibr" target="#b37">[36]</ref>, ResNet <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b38">[37]</ref>, and DenseNet <ref type="bibr" target="#b8">[9]</ref>) were originally proposed for the image classification task, where the input was an image and the network output was its predicted label. But, these models need some changes to be appropriate for a dense prediction task. The cascaded down-sampling is performed by max or average pooling and then the stride convolution decrease the spatial resolution of feature maps, hence the outputs of these models for the semantic segmentation are very coarse. In other words, the localization information is lost at the end of the networks. The Fully Convolutional Network <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref> converted the fully connected layers to the convolutional layers and make them suitable for a dense prediction task; such as semantic segmentation, depth estimation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b13">[14]</ref>, surface normal prediction <ref type="bibr" target="#b14">[15]</ref> and video semantic segmentation <ref type="bibr" target="#b39">[38]</ref>. They take an image as input and produce corresponding per pixel labeled image in an end-to-end training procedure. Fu et al. <ref type="bibr" target="#b40">[39]</ref> proposed Refinet model to improve the FCN method via a segmentation-based pooling idea. The goal of their pooling idea is to maintain the fine-grain details and boundary maps of salient objects. To recover the information loss, different approaches have been proposed. Long et al. <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref> up-sampled the feature maps of the last layer and concatenated them with the previous intermediate feature maps in a stage-wise training procedure. The encoder-decoder type models <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b43">[42]</ref> have been proposed to handle the dense per-pixel prediction problem. Commonly, the popular CNN models (such as VGG net, GoogleNet, ResNet, and DenseNet) have been utilized as the encoder to produce intermediate deep feature maps. Then, the goal of decoder branch is to restore the information lost causes in the encoder side.</p><p>Different types of approaches have been presented for the decoder side. DeconvNet <ref type="bibr" target="#b26">[25]</ref> applied the convolution transposed in the decoder side instead of convolution layers of encoder branch. DeepLab <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b44">[43]</ref> eliminated all maxpooling layers of VGG and applied the dilated convolution to enlarge the receptive field of filters to compensate the effect of max pooling operations. Preserving the index of maxpooling and applying un-pooling operations in the decoder was presented by SegNet model <ref type="bibr" target="#b43">[42]</ref>. They proposed an encoderdecoder model in which they utilized the max-pooling indices in the decoder to recover the location of information loss. The fusion of the long-range residual connections of ResNet model was propounded by Lin et al <ref type="bibr" target="#b41">[40]</ref> to refine the resolution loss of the CNN architecture.</p><p>The main goal of this paper is to address the semantic segmentation challenges of RGB-Depth images. The major issue is how to extract the strong feature representations of both photometric and depth channels. In the case of RGB-Depth semantic segmentation, almost all methods exploit the depth images as another channel of the image. As such, the fusion strategy plays an important role. These strategies can be classified into early, middle, and late fusion. Long et al. <ref type="bibr" target="#b23">[24]</ref> proposed the late fusion of FCN while <ref type="bibr" target="#b21">[22]</ref> utilized early fusion, and <ref type="bibr" target="#b45">[44]</ref> applied middle fusion of RGB and depth channels. The FuseNet <ref type="bibr" target="#b45">[44]</ref> considered two encoder branches, one encoder branch for the depth channel and another encoder branch for fusion of RGB and depth channels. Wang et al. <ref type="bibr" target="#b46">[45]</ref> designed a transformation block to fuse the common and specific features of the RGB and depth channels of convolution network to bridge the convolutional and deconvolutional models. Li et al. <ref type="bibr" target="#b47">[46]</ref> incorporated the vertical and horizontal Long Short-Term Memorized (LSTM) method to exploit the interior 2D global contextual relations of RGB and depth channels, separately. Then, the horizontal LSTM has been applied to their concatenated feature maps. Liu et al. in <ref type="bibr" target="#b48">[47]</ref> improved the HHA coding of <ref type="bibr" target="#b6">[7]</ref> via integrating 2D and 3D information. They then extended the VGG <ref type="bibr" target="#b9">[10]</ref> architecture proposed by <ref type="bibr" target="#b18">[19]</ref> for RGB-D semantic segmentation. They proposed the weighted summation of RGB and depth streams of a CNN model followed by a fully connected CRF to enhance the prediction.</p><p>The RDFNet proposed by Park et al. <ref type="bibr" target="#b49">[48]</ref> extended the RefineNet <ref type="bibr" target="#b41">[40]</ref> for RGB-Depth images. They considered two encoder streams (RGB encoder and depth encoder), one fusion stream and one decoder stream. They utilized the cascaded refinement blocks of the RefineNet as their decoder stream. The refinement process was applied to the fusion of the RGB and depth feature maps to emend the resolution loss. The Multi-Modal Multi-Resolution RefineNet (3M2RNet) <ref type="bibr" target="#b50">[49]</ref> proposed the fusion of long-range residual connections of two ResNet encoder branches with focus on the identity mapping idea of RefineNet <ref type="bibr" target="#b41">[40]</ref>. Lin et al. <ref type="bibr" target="#b51">[50]</ref> proposed a context-aware receptive field based on the scene resolution to incorporate the relevant contextual information. Consequently, for each scene resolution, deep features were learned specifically in a cascaded manner to exploit the relevant information of the neighborhood. Depth-aware convolution and pooling operations were presented by <ref type="bibr" target="#b52">[51]</ref> to investigate the 3D geometry of the depth channel. Kang et al. <ref type="bibr" target="#b53">[52]</ref> have proposed the depth-adaptive receptive field in the fully convolutional neural network where the size of each receptive filed has been selected based on the distance of each point from the camera. Hence, they utilize the depth images to determine the size of each filter for each neuron adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The proposed Multi-Modal Attention Fusion Network (MMAF-Net) model is an encoder-decoder CNN architecture with two simultaneously encoder branches of RGB and depth modalities as inputs while including one decoder branch. Both of the encoder branches follow the structure of residual block proposed in the ResNet model <ref type="bibr" target="#b7">[8]</ref>. In the decoder branch, the feature maps of both encoder branches from the same level of resolution have been fused based on the novel proposed attention fusion modules to combine both appearance and 3D feature maps. These fused feature maps have been utilized to recover the information loss of encoders and produce a high resolution prediction output. The overall view of proposed encoder-decoder architecture is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. In the following subsections, the proposed encoder-decoder architecture with a more focus on the multi-modal multi-resolution fusion block of the proposed decoder stream is explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder Stream</head><p>All of the well-known CNN models were primarily proposed for image classification. In those networks, at the end of networks the high semantic but low spatial resolution feature maps produce rough segmentation results for semantic segmentation purposes. To overcome this limitation, an encoderdecoder model is proposed. In the encoder part of the proposed model, the residual blocks of the ResNet model are utilized to benefit from the short and long range skip connections properties. The short-range skip connections immune the networks from the vanishing gradients problem while the longrange skip connections help to refine the information loss caused by the cascaded down-sampling operations and stride convolutions.</p><p>As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the proposed model utilizes the residual blocks of ResNet model (Convi-x) as two separate encoder branches. He et al. in <ref type="bibr" target="#b38">[37]</ref>, analyzed and compared the rule of skip connections of the scaling, gating, 1 ? 1 convolution, and the identity mapping. They showed that using the identity mapping function (as the skip connection) in a deep residual network is more helpful for the generalization of the network as well as the convergence of the optimization algorithm. The building block of one residual unit is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. This short-range skip connection can be formulated as y l = F (x l , W l ) + H(x l ), where x l is the original input to the residual unit l , and x l+1 = G(y l ) is the output of unit l, which is fed to the residual unit of l + 1 as its input. Also, F (x l , W l ), H(x l ), and G(y l ) are the series of the operations applied on the input x l or y l (such as convolution, batch normalization, and nonlinearity). In the first version of the ResNet model <ref type="bibr" target="#b7">[8]</ref>, H is set as an identity function and the Relu function is used for G. Therefore, the information flow of x l is not changed and added by F (x l , W l ). Hence, this residual unit with the identity and Relu functions enhances the performance of very deep networks as the vanishing gradient problem was solved. Consequently, if in the encoder-decoder model the weights of network are very small, the gradient of layers does not completely vanish. Thereupon, the vanishing gradients problem does not occur in such a deep network.</p><p>Between each residual block of the ResNet, the sequential down-sampling operations (applied by the pooling layers) increase the receptive field of the filters to include more context and also prevent the growth in the number of training weights through the encoder stream. Therefore, they preserve efficient and tractable training. But, the network loses some valuable information. This information loss produces the lowresolution prediction in the dense per-pixel classification in which the localization of the semantic labels is more essential than the image classification applications. This means that the higher-level feature maps of deeper layers in the multiencoders which encode the high-level semantic information and carry more object-level information suffer from the lack of localization information. Here, it is proposed to recover this information loss in the up-sampling process of the decoder branch by the attention-based fusion of the long-range residual connections of multi-encoder streams with the preceding decoder output. Therefore, the decoder part is responsible to recover this resolution loss in cascaded multi-modal multiresolution fusion blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoder Stream</head><p>The proposed model applies efficient multi-modal attentionbased fusion modules in the decoder branch of the network to recover the information loss caused by the down-sampling processes in multi encoder streams. The goal of the decoder is to employ the multi-level feature maps coming from the longrange skip connections of two encoder branches to enhance the resolution which is lost by the down-sampling operation performed by the pooling or convolution layers (with stride &gt; 1).</p><p>The output of residual blocks of encoder branches are employed as the long-range skip connections and are fed to the 4-cascaded sub-modules of the decoder, called Multi-Modal Multi-Resolution Fusion (MRF) module. As such, it actually utilizes long and short residual connections. These skip connections, along with the attention-based fusion modules, enable efficient end-to-end training of RGB-Depth encoderdecoder model as well as the efficient high-resolution prediction.</p><p>The overall structure of the MRF module with three modalities as its inputs is illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>. The proposed decoder has 4 cascaded MRF modules. It consists of two main sub-blocks of Attention Fusion Module (AFM), and a Chained Residual Pooling (CRP). It has three input modalities including: i) feature maps extracted from the RGB encoder branch, ii) feature maps extracted from the depth encoder branch at the same resolution level, and iii) the feature maps of the preceding MRF at the lower resolution. In the AFM, two fusion policies have been performed. The first one is the AFB, where the attention-based fusion strategy is applied to two first inputs of this module. The second one is a simple summation strategy to fuse the output of the previous MRF with the output of the current AFB to perform the refinement and produce the high resolution feature maps. The idea of CRP sub-block is to capture the context in multiple region sizes with the chained residual pooling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Attention-based Fusion Block</head><p>The attention mechanism in deep convolutional neural networks is based on visual attention which is consistent with the human visual perception. To perceive the scene and object structure, human visual system focuses on the salient parts. In fact, it concentrates on the most noticeable or important parts of the scene in different sequential glances. We propose to investigate this attention for modality fusion to focus on the salient parts of feature maps in each modality. Recently, different attention-based and salient object detection <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b55">[54]</ref>, <ref type="bibr" target="#b56">[55]</ref> have been proposed where their goal is to model the human attention mechanism. To the best of our knowledge, no previous work has investigated the attention strategy for modality fusion.</p><p>The proposed AFB block sequentially considers channelwise and spatial-wise attention. The goal of channel-wise attention is to determine salient channels of concatenated feature maps, while spatial-wise attention denotes "where" salient feature maps are located.</p><p>The proposed encoder architecture consists of four subblocks with the convolution, non-linearity function, and downsampling operation. Hence, each encoder branch produces four intermediate feature maps. Intermediate feature maps of the same level in two encoder branches are concatenated and fed to the corresponding AFB in their level.</p><p>The structure of the proposed AFB is inspired by the Convolutional Block Attention Module (CBAM) of <ref type="bibr" target="#b57">[56]</ref>. They proposed two sequential channel and spatial attention modules to refine the intermediate extracted feature maps. They illustrated that this module integrates the focus of the network to the target object in an image.</p><p>The  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Chained Residual Pooling</head><p>To efficiently recover the information loss of encoder feature maps, the contextual information is exploited from large regions of an image by utilizing the cascaded pooling operations in a chain residual connections (see <ref type="figure" target="#fig_5">Figure 5</ref>). The 4-cascaded 5 ? 5P ooling ? 1 ? 1Conv layer can capture long-range contextual information with a fixed pooling window size. These pooling feature maps combine with each other via the learnable 1 ? 1 filters of convolutional layers. Consequently, all of these pooling feature maps are located in residual connections; hence each output fuses with input feature maps by a simple summation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Procedures of MMAF</head><p>To learn the proposed network, the training set is defined as</p><formula xml:id="formula_0">{(X RGB i , X D i , Y i )|X RGB i ? R (H?W ?3) , X D i ? R (H?W ) , Y i ? L (H?W ) , i = 1, , N }</formula><p>where L denotes the labeling set defined as L = 1, , C in which C and N determine the number of class labels and training data, respectively. The output of the networks is considered as the function f (x RGB , x D ; W) that is the composition of functions corresponding to each network layer and W denotes all of the network weights. The probability of a pixel x for a given class c with the soft-max function is computed as</p><formula xml:id="formula_1">p(? = c|x RGB , x D , W) = e (fc(x RGB ,x D ;W)) C l=1 e (f l (x RGB ,x D ;W))</formula><p>.</p><p>(3)</p><p>The simple categorical cross-entropy for the loss function is defined as</p><formula xml:id="formula_2">J = ?1 M M i=1 (y i ,? i ) (4) where (y,?) = ? C c=1 y c log p(? = c|x RGB i , x D i , W)</formula><p>. M is the total number of pixels in the training data and y is the one-hot encoding vector of length C that determines the ground-truth label of pixel i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>This section contains two main subsections. In the first one, the proposed attention-based fusion method is evaluated on the challenging SUN-RGBD <ref type="bibr" target="#b58">[57]</ref>, NYU-V2 <ref type="bibr" target="#b3">[4]</ref>, and Stanford-2D-3D-Semantic <ref type="bibr" target="#b59">[58]</ref> datasets. These three datasets contain RGB and depth images with the corresponding dense per pixel ground-truth (GT) images. Then, in the second subsection, the evaluation metrics of semantic segmentation are perused to find a more proper approach to analyze prediction outputs of each model on each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attention-based Fusion Results</head><p>To compare the efficiency of the proposed method, it has been compared with the state-of-the-art CNN models. It is implemented using the PyTorch library. The random cropping, scaling, and flipping operations have been utilized for data augmentation. The weights of ResNet model are employed as the pre-trained weights of two encoder branches. The categorical cross-entropy has been considered as the loss function which is optimized by the Stochastic Gradient Descent (SGD) algorithm. The global accuracy (G), the mean accuracy (M), and the Intersection-over-Union (IoU) score have been computed for evaluation purposes.</p><p>1) Evaluation Results on SUN-RGBD Dataset SUNRGB-D dataset contains 10335 RGB-Depth images as well as dense per-pixel labeling with specific train and test set splits. The dataset has assigned each pixel into one of 37 valid classes, where one class label has been assigned to void. It is worth mentioning that the distribution of labels in this dataset is considerably unbalanced and it is remarkable that approximately 25% of pixels in the training data have not been assigned to any of 37 valid classes and are set as the void class.</p><p>The experiments were performed on three different ResNet models as two encoder streams. To investigate the performance of the proposed multi-modal multi-resolution fusion modules, the performance of the method is compared with the simple middle fusion strategy (SMF-Net) as well as the single modality of RGB and Depth channel. The SMF-Net is the proposed model without attention-based fusion block. In fact, its attention-based fusion block has been replaced with a simple summation operation. As reported in <ref type="table" target="#tab_0">Table I</ref>, the attention fusion policy has attained higher accuracies by adding less than 0.5M parameters. For comparison purposes, the results of the model with the single encoder branch of RGB or depth modality are also reported in <ref type="table" target="#tab_0">Table I</ref> .</p><p>The performance of the proposed method is compared with some previous approaches that utilize RGB and depth channels. As summarized in <ref type="table" target="#tab_0">Table II</ref>, the proposed method has achieved a higher mean and global accuracy as well as IoU score than other well-known methods that utilize just RGB images (such as SegNet, DeepLab, RefineNet, and some others). In comparison with the state-of-the-art methods applied on RGB-D images, the proposed fusion method attains better results than the FuseNet, LSTM-CF, and D-CNN while it achieves comparable accuracy with the RDFNet and 3M2RNet methods.</p><p>The proposed multi-modal multi-resolution fusion method is more computationally efficient than these two methods.</p><p>In <ref type="table" target="#tab_0">Table III</ref>, the proposed method has been compared with them based on computational complexity and model size. The proposed method is more advisable for applications running on embedded devices or even those that require real-time performance. The RDFNet model did not report its computational cost and model size. But, it is an extension of the RefineNet for RGB-Depth images. Hence, it has inevitably more parameters and computational cost than the RefineNet, because it has one extra encoder stream for depth channel and one additional fusion stream to fuse RGB and depth feature maps. It is also notable that the accuracy of RefineNet, RDFNet, and 3M2RNet model have been reported based on a multi-scale evaluation where all of the other accuracies are reported on a single-scale evaluation.</p><p>In <ref type="figure">Figure 8</ref>, some test samples of the SUN-RGBD dataset that are predicted by the proposed method are depicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Evaluation Results on NYU-V2 Dataset</head><p>The NYU-V2 dataset is known as the most popular dataset among indoor RGB-D datasets. It contains images from 646 different scenes with 26 variants of scene types. It includes 1449 RGB and depth images with per-pixel annotation which are splitted to 795 training images and 654 test images. Their class labels are mapped to 40 class labels by Gupta et al. in <ref type="bibr" target="#b5">[6]</ref>. The dataset is unbalanced with respect to the ratio of the number of pixels per class objects and contains the label void showing the pixels which cannot be annotated.</p><p>The proposed method has been compared with the most important CNN models. As the results listed in <ref type="table" target="#tab_0">Table IV</ref> show, it has surpassed all of CNN models using a single RGB modality. For instance, it obtained approximately %6 higher mean IoU than the Context model <ref type="bibr" target="#b60">[59]</ref>. It has also achieved better results than methods that have utilized both RGB and depth channels, while the MMAF-Net did not outperform the RDFNet and 3M2RNet model in terms of accuracy. But, note that it has a lower model size as well as computational complexity than these two models (see <ref type="table" target="#tab_0">Table III</ref>). These two models, as well as the RefineNet, have evaluated their results based on the multi-scale method (determines with ' * ' in <ref type="table" target="#tab_0">Table  IV)</ref>.</p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) Evaluation Results on Stanford-2D-3D-Semantic Dataset</head><p>It contains 70496 RGB and depth images as well as 2D annotation with 13 object categories. It includes 1413 RGB and depth panoramic images as well as their surface normal and semantic annotations of six large-scale indoor areas. It also provides 3D point clouds of these areas. Areas 1, 2, 3, 4, and 6 are utilized as the training and Area 5 is used as the testing set. The attention-based fusion model was applied on RGB-D images (not panoramic ones). Hence, <ref type="table" target="#tab_4">Table V</ref> shows the performance comparison with those approaches that have been evaluated on RGB-D images. The authors of the D-CNN model <ref type="bibr" target="#b52">[51]</ref>, evaluated their model as well as the DeepLab <ref type="bibr" target="#b18">[19]</ref> model on this dataset. They trained these two models from scratch. The proposed MMAF-Net has obtained comparable performance with the 3M2RNet model in terms of accuracy while it enjoys a lower model size and computational complexity (see <ref type="table" target="#tab_0">Table III</ref>). Tateno et al. <ref type="bibr" target="#b63">[62]</ref> and Kong et al. <ref type="bibr" target="#b64">[63]</ref> reported their accuracy on the panoramic images of this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proposed Evaluation Metrics for Semantic Segmentation</head><p>The semantic segmentation problem is actually known as a dense labeling problem. Hence, evaluation metrics that had been utilized for labeling in the machine learning field have also been applied to semantic segmentation methods. Accordingly, the confusion matrix has been computed and then the global, mean, and IoU criteria have been figured out from it. There are two main issues related to these criteria used for semantic segmentation methods which are explained in the following with more details.</p><p>First issue: Almost all of the semantic segmentation approaches calculate these metrics per pixel for whole images of each dataset (per dataset). For example, the global accuracy = 81 means %81 of all pixels of all test images have been classified correctly. It does not carry out any additional information about each image's accuracy. Hence, all pixels of one test image may be classified correctly but the other ones may have a large misclassification error. As a result, it is ambiguous whether the method has approximately the same performance for all images or it has a rich performance for some of them and a poor performance for others. Csurka et al. <ref type="bibr" target="#b65">[64]</ref> proposed to measure the per image accuracy instead of per dataset. They computed the confusion matrix for each image based on the union of classes presented in the ground-truth as well as in the prediction. Therefore, the number of images that have attained an accuracy more than a specific threshold can be reported. But, almost all of the existing methods followed the former accuracy metrics which are computed per dataset. We propose to compute the global, mean, and IoU metrics for each test image, separately and depict their Cumulative Distribution Function (CDF) to illustrate the least number of images with a specified accuracy      level. <ref type="figure" target="#fig_6">Figure 6</ref> shows this CDF for these three famous criteria. The horizontal axis shows the accuracy. Hence, for instance, approximately 70 percent of test images have more than %80 global accuracy. For each CDF, minimum, maximum, median, mean, and standard deviation have been reported. Second issue: Region boundaries are one of the main criteria to determine the quality of image segmentation. The accuracy of these boundaries has not been considered by the global, mean, and IoU metrics, separately. Here, the Boundary Displacement Error (BDE) <ref type="bibr" target="#b66">[65]</ref> was utilized to measure the average displacement error between two segmented boundaries of two images. For each boundary pixel, this error is defined as the distance between the closest pixel in the other boundary image. This metric has been presented for image segmentation where here we propose to utilize it for each segmented region that belongs to the same class label in the ground-truth and the prediction image. Suppose B P is the boundary points of a region with class label l in a prediction image and B G is its corresponding boundary points in the ground-truth image. The two distance distributions are computed from B G to B P and from B P to B G . Then, the minimum distance of each point of B P from B G is considered as d(x, B G ) = min{d E (x, y)} ? y in B P , where d E is an Euclidean distance.</p><p>To apply this metric in semantic segmentation, the BDE is computed for each class label, separately. <ref type="figure">Figure 7</ref> illustrates the CDF of BDE for each class label. For instance, %60 of images have less than 10 pixels discrepancy for Floor and Chair classes (see <ref type="figure">Figure 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>An efficient attention-based fusion method for RGB and depth fusion was proposed. The proposed method focused on salient feature maps generated from RGB and depth encoder branches and suppressed unnecessary ones to efficiently fuse these two modalities. The network model was a type of encoder-decoder CNN architectures with two encoder branches and one decoder. The decoder goal was to refine the resolution loss caused by the down sampling procedures in encoder branches via fusion of long-range residual connections coming from both encoder branches. The proposed architecture achieved approximately comparable accuracy in terms of the IoU score, mean accuracy, and global accuracy, with RGB-D state-of-the-art methods. This same level of accuracy attained remarkably with %50 better performance in terms of model size alongside less computational cost (approximately 250G less floating points operations).    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Pairs of RGB and depth images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Attention-based fusion block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Proposed network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Building block of a residual unit<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>goal of our attention-based fusion is to enhance the representation power of concatenated RGB-Depth feature maps and capture their salient feature maps while suppressing the unnecessary ones. The intermediate feature maps of each encoder stream demonstrate a set of local descriptors where their statistics can be considered as a good representative for each image. These statistics include the average and maximum of each feature map. In the proposed fusion method, the nonlinear and non-mutually exclusive relationships between RGB and Depth intermediate feature maps are exploited via the pooling, non-linearity, convolution, and fully connected layers of the deep neural networks operations. Suppose F RGB ? R n?m?c and F D ? R n?m?c are intermediate feature maps of RGB and Depth modalitis in the same level, respectively, and F = [F RGB ; F D ] ? R n?m?2c shows their concatenation. The channel-wise attention map, M c , is computed as M c (F ) = ?(M LP (AvgP ool(F )) + M LP (M axP ool(F )))= ?(W 1 (W 0 (AvgP ool(F )) + W 1 (W 0 (M axP ool(F )))(1) where ? denotes the sigmoid function. Then, F ? R n?m?2c is determined as an output of the channel-wise attention module (F = M c (F ) ? F ). The spatial-wise attention map, M s , is applied on F and is computed asM s (F ) = ?(Conv([AvgP ool(F ); M axP ool(F )])). (2)Then, F ? R n?m?2c is determined as an output of the spatial-wise attention module (F = M s (F ) ? F ). Consequently, F f used ? R n?m?c is determined by an output of the spatial-wise attention module (F f used = M axP ool3D(F )). This fused feature map focuses on the important features of the channels and concentrates on "where" salient features are located.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Proposed multi-modal multi-resolution fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Semantic segmentation criteria computed per image presented via CDF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Comparison of boundary displacement error for each class label. Qualitative assessments of proposed method on SUN RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative assessments of proposed method on NYU-V2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Performance evaluation of proposed MMAF module on SUN-RGBD dataset.</figDesc><table><row><cell>Methods</cell><cell>Modality</cell><cell>G</cell><cell>M</cell><cell>IoU</cell><cell>W-IoU</cell><cell>No. of Parameters</cell><cell>GFLOPs</cell></row><row><cell>MMAF-Net-50 (ours)</cell><cell>RGB</cell><cell>78.4</cell><cell>53.1</cell><cell>42.3</cell><cell>65.9</cell><cell>27.4M</cell><cell>32.7G</cell></row><row><cell>MMAF-Net-50 (ours)</cell><cell>D</cell><cell>74.8</cell><cell>44.7</cell><cell>35.4</cell><cell>61.3</cell><cell>27.4M</cell><cell>32.7G</cell></row><row><cell>SMF-Net-50 (ours)</cell><cell>RGB-D</cell><cell>79.0</cell><cell>55.2</cell><cell>43.7</cell><cell>67.0</cell><cell>52.6M</cell><cell>56.7G -464.5K</cell></row><row><cell>MMAF-Net-50 (ours)</cell><cell>RGB-D</cell><cell>80.0</cell><cell>57.6</cell><cell>45.5</cell><cell>68.0</cell><cell>53.0M</cell><cell>56.7G</cell></row><row><cell>MMAF-Net-101 (ours)</cell><cell>RGB-D</cell><cell>80.2</cell><cell>58.0</cell><cell>46.0</cell><cell>69.0</cell><cell>91.0M</cell><cell>95.6G</cell></row><row><cell>MMAF-Net-152 (ours)</cell><cell>RGB-D</cell><cell>81.0</cell><cell>58.2</cell><cell>47.0</cell><cell>69.6</cell><cell>122.3M</cell><cell>134.4G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Semantic segmentation results on SUN RGB-D dataset (' * ' denotes multi-scale evalution).</figDesc><table><row><cell>Methods</cell><cell>Modality</cell><cell>G</cell><cell>M</cell><cell>IoU</cell></row><row><cell>Ren et al. [3]</cell><cell>RGB-D</cell><cell>-</cell><cell>36.3</cell><cell>-</cell></row><row><cell>DeconvNet [25]</cell><cell>RGB</cell><cell>66.1</cell><cell>32.3</cell><cell>22.6</cell></row><row><cell>FCN [18]</cell><cell>RGB</cell><cell>68.2</cell><cell>38.4</cell><cell>27.4</cell></row><row><cell>SegNet [27]</cell><cell>RGB</cell><cell>72.6</cell><cell>44.8</cell><cell>31.8</cell></row><row><cell>B-SegNet [41]</cell><cell>RGB</cell><cell>71.2</cell><cell>45.9</cell><cell>30.7</cell></row><row><cell>DeepLab [26]</cell><cell>RGB</cell><cell>71.9</cell><cell>42.2</cell><cell>32.1</cell></row><row><cell>LSTM-CF [46]</cell><cell>RGB-D</cell><cell>-</cell><cell>48.1</cell><cell>-</cell></row><row><cell>FuseNet [44]</cell><cell>RGB-D</cell><cell>76.3</cell><cell>48.3</cell><cell>37.3</cell></row><row><cell>Context [59]</cell><cell>RGB</cell><cell>78.4</cell><cell>53.4</cell><cell>42.3</cell></row><row><cell>D-CNN [51]</cell><cell>RGB-D</cell><cell>-</cell><cell>53.5</cell><cell>42.0</cell></row><row><cell>3D Graph [60]</cell><cell>RGB-D</cell><cell>-</cell><cell>57.0</cell><cell>45.9</cell></row><row><cell>Cheng et al. [61]</cell><cell>RGB-D</cell><cell>-</cell><cell>58.0</cell><cell>-</cell></row><row><cell>RefineNet [40]</cell><cell>RGB</cell><cell>80.6  *</cell><cell>58.5  *</cell><cell>45.9  *</cell></row><row><cell>CFN (VGG-16)[50]</cell><cell>RGB-D</cell><cell>-</cell><cell>-</cell><cell>42.5  *</cell></row><row><cell>CFN (RefineNet)[50]</cell><cell>RGB-D</cell><cell>-</cell><cell>-</cell><cell>48.1  *</cell></row><row><cell>RDFNet [48]</cell><cell>RGB-D</cell><cell>81.5  *</cell><cell>60.1  *</cell><cell>47.7  *</cell></row><row><cell>3M2RNet [49]</cell><cell>RGB-D</cell><cell>83.1  *</cell><cell>63.5  *</cell><cell>49.8  *</cell></row><row><cell>MMAF-Net-152 (ours)</cell><cell>RGB-D</cell><cell>81.0</cell><cell>58.2</cell><cell>47.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Computational complexity and model size comparison on SUN RGB-D dataset (' * ' denotes multi-scale evalution).</figDesc><table><row><cell>Methods</cell><cell>G</cell><cell>M</cell><cell>IoU</cell><cell>Parameters</cell><cell>GFLOPs</cell></row><row><cell>RefineNet [40]</cell><cell>80.6  *</cell><cell>58.5  *</cell><cell>45.9  *</cell><cell>119.0M</cell><cell>234.9G</cell></row><row><cell>3M2RNET [49]</cell><cell>83.1  *</cell><cell>63.5  *</cell><cell>49.8  *</cell><cell>225.4M</cell><cell>384.5G</cell></row><row><cell>RDFNET [48]</cell><cell>81.5  *</cell><cell>60.1  *</cell><cell>47.7  *</cell><cell>-</cell><cell>-</cell></row><row><cell>MMAF-Net (ours)</cell><cell>81.0</cell><cell>58.2</cell><cell>47.0</cell><cell>122.3M</cell><cell>134.4G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Semantic segmentation results on NYU-V2 dataset (' * ' denotes multi-scale evalution).</figDesc><table><row><cell>Methods</cell><cell>Modality</cell><cell>G.</cell><cell>M</cell><cell>IoU</cell></row><row><cell>Silberman et al. [4]</cell><cell>RGB-D</cell><cell>54.6</cell><cell>19.0</cell><cell>-</cell></row><row><cell>Ren et al. [3]</cell><cell>RGB-D</cell><cell>49.3</cell><cell>21.1</cell><cell>21.4</cell></row><row><cell>Gupta et al. [6]</cell><cell>RGB-D</cell><cell>59.1</cell><cell>28.4</cell><cell>29.1</cell></row><row><cell>Gupta et al. [7]</cell><cell>RGB-D</cell><cell>60.3</cell><cell>35.1</cell><cell>31.3</cell></row><row><cell>Eigen et al. [15]</cell><cell>RGB</cell><cell>65.6</cell><cell>45.1</cell><cell>34.1</cell></row><row><cell>FCN [18]</cell><cell>RGB-D</cell><cell>65.4</cell><cell>46.1</cell><cell>34.0</cell></row><row><cell>Wang et al. [45]</cell><cell>RGB-D</cell><cell>-</cell><cell>47.3</cell><cell>-</cell></row><row><cell>Liu et al. [47]</cell><cell>RGB-D</cell><cell>70.3</cell><cell>51.7</cell><cell>41.2</cell></row><row><cell>Context [59]</cell><cell>RGB</cell><cell>70.0</cell><cell>53.6</cell><cell>40.6</cell></row><row><cell>Kang et al. [52]</cell><cell>RGB-D</cell><cell>68.4</cell><cell>49.0</cell><cell>37.6</cell></row><row><cell>LSTM-CF [46]</cell><cell>RGB-D</cell><cell>-</cell><cell>49.4</cell><cell>-</cell></row><row><cell>3D Graph [60]</cell><cell>RGB-D</cell><cell>-</cell><cell>55.7</cell><cell>43.1</cell></row><row><cell>D-CNN [51]</cell><cell>RGB-D</cell><cell>-</cell><cell>56.3</cell><cell>43.9</cell></row><row><cell>Cheng et al. [61]</cell><cell>RGB-D</cell><cell>71.9</cell><cell>60.0</cell><cell>45.9</cell></row><row><cell>RefineNet [40]</cell><cell>RGB</cell><cell>73.6  *</cell><cell>58.9  *</cell><cell>46.5  *</cell></row><row><cell>CFN (VGG-16) [50]</cell><cell>RGB-D</cell><cell>-</cell><cell>-</cell><cell>41.7  *</cell></row><row><cell>CFN (RefineNet)[50]</cell><cell>RGB-D</cell><cell>-</cell><cell>-</cell><cell>47.7  *</cell></row><row><cell>RDFNet [48]</cell><cell>RGB-D</cell><cell>76.0  *</cell><cell>62.8  *</cell><cell>50.1  *</cell></row><row><cell>3M2RNet [49]</cell><cell>RGB-D</cell><cell>76.0  *</cell><cell>63.0  *</cell><cell>48.0  *</cell></row><row><cell>MMAF-Net-152 (ours)</cell><cell>RGB-D</cell><cell>72.2</cell><cell>59.2</cell><cell>44.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Semantic segmentation results on Stanford-2D-3D-Semantic dataset (' * ' denotes multi-scale evalution).</figDesc><table><row><cell cols="3">Methods</cell><cell></cell><cell cols="2">Modality</cell><cell>G.</cell><cell>M</cell><cell>IoU</cell></row><row><cell cols="3">DeepLab [19]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>64.3</cell><cell>46.7</cell><cell>35.5</cell></row><row><cell cols="3">D-CNN [51]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>65.4</cell><cell>55.5</cell><cell>39.5</cell></row><row><cell cols="3">3M2RNet [49]</cell><cell></cell><cell cols="2">RGB-D</cell><cell>79.8  *</cell><cell>75.2  *</cell><cell>63.0  *</cell></row><row><cell cols="4">MMAF-Net-152 (ours)</cell><cell cols="2">RGB-D</cell><cell>76.5</cell><cell>62.3</cell><cell>52.9</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Global Accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell cols="3">Mean Accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Mean IoU</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">min:</cell><cell>0.004</cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell cols="2">max:</cell><cell>1</cell><cell></cell></row><row><cell>No. of Images (%)</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell cols="4">min: max: mean: 0.70 0.012 1 median: 0.70 mean: 0.43 median: 0.41 std: 0.19</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>std:</cell><cell>0.16</cell><cell>min:</cell><cell>0.018</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>max:</cell><cell>1</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mean: 0.81</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>median: 0.85</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>std:</cell><cell>0.16</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhanced computer vision with microsoft kinect sensor: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1334" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="601" to="608" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rgb-(d) scene labeling: Features and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2759" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing for priming object detection in rgb-d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?ecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on Semantic Perception, Mapping and Exploration</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action recognition with trajectorypooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5162" to="5170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep conditional neural network for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical parsing net: Semantic scene parsing from global scene to objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2670" to="2682" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint crf and locality-consistent dictionary learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="875" to="886" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Indoor semantic segmentation using depth information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3572</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mc-ssm: Nonparametric semantic image segmentation with the icm algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Khelifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mignotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TRANSACTIONS ON MULTIMEDIA</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1946" to="1959" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene parsing with integration of parametric and non-parametric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2379" to="2391" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning depth-sensitive conditional random fields for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6232" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic segmentation of rgb-d images using 3d and local neighbouring features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications (DICTA), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep spatio-temporal dependence for semantic video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="939" to="949" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Refinet: A deep segmentation assisted refinement network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="457" to="469" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cvpr</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning common and specific features for rgb-d semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="541" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rgb-d joint modelling with scene geometric information for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3m2rnet: Multi-modal multi-resolution refinement network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fooladgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Science and Information Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="544" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cascaded feature network for semantic segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1320" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Depth-aware cnn for rgb-d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Depth-adaptive deep neural network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2478" to="2490" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Gla: Global-local attention for image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="726" to="737" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised salient object detection via inferring from imperfect saliency models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1101" to="1112" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Saliency detection by fully learning a continuous conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1531" to="1544" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploring context with deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1352" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Localitysensitive deconvolution networks with gated fusion for rgb-d indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="707" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Pixel-wise attentional gating for parsimonious pixel labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.01556</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Yet another survey on image segmentation: Region and boundary information integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mart?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cuf?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="408" to="422" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
