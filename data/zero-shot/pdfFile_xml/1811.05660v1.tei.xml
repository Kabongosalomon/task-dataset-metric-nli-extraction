<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MT-CGCNN: Integrating Crystal Graph Convolutional Neural Network with Multitask Learning for Material Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-14">14 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
							<email>soumyasanyal@iisc.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janakiraman</forename><surname>Balachandran</surname></persName>
							<email>j.balachandran@shell.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naganand</forename><surname>Yadati</surname></persName>
							<email>y.naganand@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
							<email>abhishekkumar12@iisc.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmini</forename><surname>Rajagopalan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchismita</forename><surname>Sanyal</surname></persName>
							<email>suchismita.sanyal@shell.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Shell Technology Centre Bangalore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Shell Technology Centre Bangalore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Shell Technology Centre Bangalore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Indian Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MT-CGCNN: Integrating Crystal Graph Convolutional Neural Network with Multitask Learning for Material Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-14">14 Nov 2018</date>
						</imprint>
					</monogr>
					<note>* contributed equally to this paper. Preprint. Work in progress.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Developing accurate, transferable and computationally inexpensive machine learning models can rapidly accelerate the discovery and development of new materials. Some of the major challenges involved in developing such models are, (i) limited availability of materials data as compared to other fields, (ii) lack of universal descriptor of materials to predict its various properties. The limited availability of materials data can be addressed through transfer learning, while the generic representation was recently addressed by Xie and Grossman [1], where they developed a crystal graph convolutional neural network (CGCNN) that provides a unified representation of crystals. In this work, we develop a new model (MT-CGCNN) by integrating CGCNN with transfer learning based on multi-task (MT) learning. We demonstrate the effectiveness of MT-CGCNN by simultaneous prediction of various material properties such as Formation Energy (?E f ), Band Gap (E g ) and Fermi Energy (E F ) for a wide range of inorganic crystals (46774 materials). MT-CGCNN is able to reduce the test error when employed on correlated properties by upto 8%. The model prediction has lower test error compared to CGCNN, even when the training data is reduced by 10%. We also demonstrate our model's better performance through prediction of end user scenario related to metal/non-metal classification. These results encourage further development of machine learning approaches which leverage multi-task learning to address the aforementioned challenges in the discovery of new materials. We make MT-CGCNN's source code available to encourage reproducible research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The discovery, design and development of new materials with required properties underpin the development of various next generation energy, medical and electronic technologies. Discovery of new materials has historically been made through trial and error process leading to slow development cycles <ref type="bibr" target="#b1">[2]</ref>. The advent of data driven modeling techniques has provided a new approach to develop computationally inexpensive and accurate models, that enables us to rapidly screen large material search spaces to select potential material candidates with desired properties. These approaches have recently been employed to predict new materials for various functionalities such as thermoelectrics <ref type="bibr" target="#b3">[3]</ref>, photovoltaics <ref type="bibr" target="#b4">[4]</ref>, molecular light emitting diodes <ref type="bibr" target="#b5">[5]</ref> and shape memory alloys <ref type="bibr" target="#b6">[6]</ref> among others.</p><p>One of the major challenges in developing data driven models for material discovery is the limited availability of the material datasets compared to other fields. This creates challenges in applying conventional machine learning tools for materials data. Recent works have proposed transfer learning <ref type="bibr" target="#b7">[7]</ref> and augmenting the model with pre-existing physical knowledge <ref type="bibr" target="#b8">[8]</ref> to overcome this data constraint. Multi-task learning (MTL) is an important class of transfer learning algorithms that enables us to overcome such data scarcity challenges. MTL is the procedure of learning several tasks at the same time with the objective of mutually benefitting the performance of individual tasks. In this way, MTL is able to learn generalized representations (embeddings) that can explain multiple aspects of the data. Also, it is able to overcome data limitations by co-learning multiple tasks simultaneously. Using multi-task learning has shown improvements in various fields of machine learning, from natural language processing <ref type="bibr" target="#b15">[9]</ref>, computer vision <ref type="bibr" target="#b16">[10]</ref> to drug discovery <ref type="bibr" target="#b17">[11]</ref> and pharmaceuticals <ref type="bibr" target="#b18">[12]</ref> among others.</p><p>The other major challenge in material science is to be able to come up with a universal material descriptor that can be used to predict various material properties. Until recently most of the work in literature has focused on developing hand crafted descriptors based on domain expertise <ref type="bibr" target="#b19">[13,</ref><ref type="bibr" target="#b20">14]</ref>. However, these approaches typically are difficult to be generalized outside the tasks (properties) for which they were trained. Molecules and crystals can be defined by their chemical composition (atoms) and structure (bonding). Hence, they are naturally amenable to a generalized graph representation. Recent progress in Geometric deep learning <ref type="bibr" target="#b21">[15]</ref> has lead to formulation of graph based deep neural networks for graphical structures <ref type="bibr" target="#b22">[16]</ref><ref type="bibr" target="#b23">[17]</ref><ref type="bibr" target="#b24">[18]</ref><ref type="bibr" target="#b25">[19]</ref>. These deep learning based approaches can automatically learn the best representation (embedding) from raw data of atoms/bonds features for different property predictions. These approaches have been successfully applied to molecules for performing various tasks such as molecular feature extraction <ref type="bibr" target="#b26">[20]</ref><ref type="bibr" target="#b27">[21]</ref><ref type="bibr" target="#b28">[22]</ref> and drug discovery <ref type="bibr" target="#b29">[23]</ref>. Recently, Xie and Grossman <ref type="bibr" target="#b0">[1]</ref> have developed a GCN based approach for inorganic crystals called crystal graph convolutional neural network (CGCNN), to predict various properties of inorganic crystals.</p><p>In this work, we bridge the two approaches by augmenting CGCNN model with multitask learning (MTL) to jointly predict multiple material properties. This approach of simultaneous prediction of different properties ensures that the generic model can automatically transfer the learning of one property to another that results in better performance. We demonstrate this approach through simultaneous prediction of various material properties such as Formation Energy (?E f ), Band Gap (E g ) and Fermi Energy (E F ) for a wide range of inorganic crystals (46774 materials). We also systematically explore the impact of our approach on test errors for different MTL experiments with varying amounts of training data. Finally, we also understand the impact of our method on end user scenario related to metal/non-metal classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Crystal Graph Convolution Neural Network (CGCNN)</head><p>The work by Xie and Grossman <ref type="bibr" target="#b0">[1]</ref> focuses on building a generalized crystal graph convolutional network to represent the crystals and to predict their properties with accuracy of ab initio physics models. A crystal graph G is an undirected multigraph defined by nodes representing atoms and edges representing bonds in a crystal. It allows multiple edges between the same pair of end nodes which represent the different bonds between the atoms. Thus, the graph is defined as G=(A, E, V, U), where A is the set of atoms in the crystal structure, E={(i, j) k : k th bond between atoms i and j where i, j ? A}, is the set of undirected edges and |A|=N is the number of atoms in the crystal graph. v i ? V contains the features of the i th atom encoding properties of the atom. u (i,j) k ? U is the feature vector for the k th bond between atoms i and j. The authors propose a simple convolution function as,</p><formula xml:id="formula_0">v (t+1) i = g ? ? ? ? j,k v (t) j ? u (i,j) k ? ? W (t) c + v (t) i W (t) s + b (t) c + b (t) s ? ? (1)</formula><p>where ? denotes the concatenation of atom and bond feature vectors of the neighbors of i th atom,</p><formula xml:id="formula_1">W (t) c , W (t) s , b (t) c and b (t)</formula><p>s are the convolution weight matrix, self weight matrix, convolution bias and self bias of the t-th layer of GCN respectively, and g(?) is some non-linear activation function between layers.</p><p>As noted by the authors, this formulation has a shortcoming. Since the weight matrix is shared across all neighbors, equal importance is given to all the neighbors. This inherently neglects the differences of interaction strength between neighbors. To overcome this, the authors use the standard edge-gating technique <ref type="bibr" target="#b30">[24]</ref>, where the new convolution function first concatenates neighbor feature vectors z</p><formula xml:id="formula_2">(t) (i,j) k = v (t) i ? v (t) j ? u (i,j) k , and then performs convolution by, v (t+1) i = v (t) i + j,k ?(z (t) (i,j) k W (t) c + b (t) c ) g(z (t) (i,j) k W (t) s + b (t) s )<label>(2)</label></formula><p>where denotes element-wise multiplication and ? denotes a sigmoid function. The ?(?) acts as a learned weight matrix to incorporate different interaction strengths between neighbors.</p><p>The atom features are then pooled (using average pooling <ref type="bibr" target="#b26">[20]</ref>) to get a vector representation of the crystal (v G ). This is then used as an input to a network of fully-connected layers with non-linearities which learn to predict a property value for the crystal. More concretely,</p><formula xml:id="formula_3">v G = 1 N i v i (3) y = f (v G W g + b g )<label>(4)</label></formula><p>where v i is the learned feature representation of i th atom using Eq. 2, v G is the crystal representation learned from pooling and? is the predicted value of the crystal property. W g , b g and f (?) are the weight matrix, bias and non-linearities of the fully-connected network respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task learning</head><p>The fundamental motivation for doing multi-task learning is to achieve better generalization performance. As summarized by <ref type="bibr" target="#b31">[25]</ref>, "MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks". The two main architectures for MTL in the deep learning context <ref type="bibr" target="#b32">[26]</ref> are:</p><p>? Hard parameter sharing: This is the simplest approach to MTL. The architecture shares a common set of layers across all tasks and then some task-specific output layers are present for each individual task. The key motivation is to force the model to learn better representations that can be used to learn multiple related tasks at the same time. ? Soft parameter sharing: Here, there are independent models with own set of parameters for each of the tasks being learned. But then, the distance between the parameters (l 2 distance) are regularized to encourage learning of similar parameters for the different models. This indirectly leads to a generalized representation with the flexibility of unique parameters for each task.</p><p>A more detailed discussion on various aspects of multi-task learning could be found in <ref type="bibr" target="#b31">[25,</ref><ref type="bibr" target="#b32">26]</ref> 3 Proposed method (MT-CGCNN) <ref type="figure">Fig. 1</ref> shows the schematics of the MT-CGCNN model setup. Every atom and bond between atoms in a crystal has some initial vector representation <ref type="bibr" target="#b0">[1]</ref>. The feature embedding for atoms (v i ) and bonds (u (i,j) k ) are the input to the GCN layers. Stacked GCN layers are used to encode these atomic representations using Eq. 2. This is then followed by a pooling layer (Eq. 3) which gives a vector representation for the crystal structure v G . We then use hard parameter sharing MTL, where for each crystal property (p) being learned, there is an independent fully-connected network which takes v G and predicts the property value as,</p><formula xml:id="formula_4">y p = f p (v G W p + b p )<label>(5)</label></formula><p>where y p is the crystal property value for the p th property. W p , b p and f p (?) are the weight matrix, bias and non-linear mapping of the p th fully-connected network respectively. So, each task essentially shares the crystal representation v G and tries to learn functions that can predict a set of crystal properties. In this work, we employ mean squared loss function for each property. The total loss function for the network is the weighted linear sum of individual losses from parts of the network. This formulation of the total loss function is a common setup for the multi-tasking problem <ref type="bibr" target="#b33">[27,</ref><ref type="bibr" target="#b34">28]</ref>. Mathematically,</p><formula xml:id="formula_5">L = 1 |P| p?P w p L p<label>(6)</label></formula><p>where L is the total loss of the network, L p are individual losses from each of the task-specific layers and w p are the weights for the individual losses. A trivial setup is where w p =1 which gives an average loss across tasks. For our experiments, each of L p is mean squared error defined by</p><formula xml:id="formula_6">L p = 1 batchsize p?P ( y p ? y p ) 2 (7)</formula><p>where batchsize is the mini-batch size during an iteration. y p is the model predicted property value and y p is the target property value for the p th property. Finally, back-propagation using gradient descent <ref type="bibr" target="#b35">[29]</ref> is done to train the model. The source code for MT-CGCNN is available at https://github.com/soumyasanyal/mt-cgcnn. <ref type="figure">Figure 1</ref>: (best viewed in color) Overview of MT-CGCNN: Given a crystal structure, a crystal graph is created from it. Note that the graph created can have multiple edges between the atoms representing different atomic bonds. Next, CGCNN is used to extract the crystal representation using Graph Convolutional Networks. The crystal representation is then used as input for different task-specific fully connected layers (F C n ) which predict some property of the crystal. Refer to section 3 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>MT-CGCNN is trained and validated on inorganic crystal data comprising of 46774 materials used by Xie and Grossman <ref type="bibr" target="#b0">[1]</ref> which is obtained from the Materials Project (MP) <ref type="bibr" target="#b37">[30]</ref>. In our experiments, we focus on three correlated properties namely, Formation Energy (?E f ), Band Gap (E g ) and Fermi Energy (E F ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Correlation between properties</head><p>One of the crucial problems in multitasking is to understand which tasks could probably help in an MTL setup <ref type="bibr" target="#b31">[25,</ref><ref type="bibr" target="#b32">26]</ref>. While there have been advancements towards understanding that problem <ref type="bibr" target="#b38">[31,</ref><ref type="bibr" target="#b39">32]</ref>, in our setup we select tasks which have significant correlation. The Pearson correlation coefficients <ref type="bibr" target="#b40">[33]</ref> for the three properties -?E f , E g and E F are shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weighted loss</head><p>Weighted loss as defined in Eq. 6 is useful for cases when we want to give more importance to one task over another. This may be needed in cases when a specific task is harder to learn than the rest and hence would not get equally trained as others <ref type="bibr" target="#b33">[27]</ref>. In our current setup, we consider these weights as hyperparameters for the model and search for the best weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model evaluation</head><p>To evaluate MT-CGCNN, we run a set of experiments with setup as detailed in <ref type="table" target="#tab_0">Table 1</ref>. The results from our experiments are summarized in <ref type="table" target="#tab_1">Table 2 and Table 3</ref>. We report mean absolute error (MAE) over 5 runs with random splits of 60/20/20 ratio of train, validation and test sets, unless specified otherwise. To get the numbers for the CGCNN model, we used the code provided by the authors 2 with the hyperparameters reported in their work. Formation Energy (?E f ) and Band Gap (E g ) E2</p><p>Formation Energy (?E f ) and Fermi Energy (E F ) E3</p><p>Band Gap (E g ) and Fermi Energy (E F ) E4</p><p>Formation Energy (?E f ), Band Gap (E g ) and Fermi Energy (E F )</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, the average MAE (the average of MAEs for individual properties) is tabulated with the relative increase in performance over the baseline due to multi-tasking. Here, we can see that multi-task learning clearly outperforms the single-task CGCNN model across all the experiments. In <ref type="table" target="#tab_2">Table 3</ref> we show how our model performs on individual properties compared to single task setup (CGCNN). For example, we observe a strong reduction in the MAE scores of E g when we do multi-tasking using E g and ?E f . A similar trend is observed for E F when we do multi-tasking using ?E f and E F . These observations indicate that multi-tasking is more helpful when done with a specific combination of tasks. We observe from <ref type="table" target="#tab_2">Table 3</ref> that ?E f prediction shows degradation during multi-task learning, likely due to the strong constraints of hard parameter sharing.</p><p>Further, we do another set of experiment where we systematically reduce the training data available to the different models and check the model performance for the reduced training dataset. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. We observe that MT-CGCNN outperforms CGCNN for the same amount of input data. Specifically, we note that the MAE values of MT-CGCNN using 50% training data is better than CGCNN using 60% training data. This is a reduction of approximately 4.5k training samples for the current setup. This result verifies that multi-tasking leads to comparable performance even with lesser training data. Also, it indirectly shows that multi-tasking leads to a faster learning of the crystal embedding space.  Method  </p><formula xml:id="formula_7">Experiment ?E f (eV/atom) E g (eV) E F (eV) CGCNN ?E f 0.039 ?0.0003 - - E g - 0.323 ?0.006 - E F - - 0.380</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">End user scenarios (chemical insights)</head><p>Beyond test error evaluation, we also evaluate our model on scenarios that are useful for the end users. In the case of material scientists and chemists, this translates into obtaining chemical insights from the predicted data. This, in turn, provides another framework to compare the two approaches.</p><p>Here, we analyze two scenarios that can provide some chemical insights.</p><p>For the first scenario, we compare the ordering of different materials based on Formation energy. The difference between Formation energy helps to understand the relative stability of different materials. Hence, from the end user standpoint, it is more important to rank the crystals correctly using the ?E f rather than the accuracy of prediction. To quantify this ordering (ranking) of materials, we calculate the Spearman's rank correlation coefficient (r s ) <ref type="bibr" target="#b42">[34]</ref> for the predicted ?E f and true ?E f using MT-CGCNN and CGCNN for different amounts of training data as shown in <ref type="figure" target="#fig_1">Fig. 3(c)</ref>. The r s values of both the approaches are very high and comparable. This suggests that the ordering between the crystals based on their ?E f is mostly preserved.</p><p>In case of second scenario, based on E g we classify the materials into two classes namely (i) metals -that can easily conduct electrons and (ii) non-metals such as semiconductors and insulators where electron conduction is constrained. The energy equivalent of a physical system maintained at temperature T is calculated as k B T , where k B is Boltzmann constant. In case of room temperature (T = 300K), this value is 0.025eV. Hence, crystals with E g less than 0.025 eV are considered metals, while the rest of them are considered non-metals comprising of semiconductors and insulators. <ref type="figure" target="#fig_1">Fig. 3(d)</ref> shows the area under the curve (AUC) for crystal classification into metal/non-metal using MT-CGCNN and CGCNN for different amounts of training data. It can be observed that MT-CGCNN has a much higher accuracy in classification compared to CGCNN as measured by the AUC metric.</p><p>In fact, as a function of training data, the lowest AUC of MT-CGCNN is still higher than the highest AUC of CGCNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Hyperparameters</head><p>We divide the dataset into train, validation and test splits. To tune the hyperparameters, we train the model using the training set and then check the test error on the validation set. We perform grid search with early stopping over the hyperparameter space mentioned in <ref type="table" target="#tab_5">Table 5</ref>. For training, we use Adam optimizer <ref type="bibr" target="#b43">[35]</ref> with a learning rate of 0.01. Step size of the Adam optimizer 10 ?4 , 10 ?3 , 10 ?2 , 10 ?1 Weights in the weighted loss (Eq. 6) 1, 2, 3, 4, 5, 6, 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In summary, we propose MT-CGCNN, an effective multi-tasking framework that uses crystal graph convolutions to predict different material properties (?E f , E g , E F ) by exploiting the correlation between them. We also show that MT-CGCNN can achieve comparable accuracy as CGCNN with fewer training samples. Additionally, we demonstrate the effectiveness of MT-CGCNN by testing some end user scenarios relating to the ordering of crystal based on ?E f and classification of materials based on E g . The ability to predict multiple properties shows that the material representation learned is well generalized. This work opens up new research directions for machine learning with material science, where we can continue to build upon the framework of MT-CGCNN (eg. including soft-parameter sharing) to predict other functional properties of materials with limited input data. Also, exploring dynamic weighted loss has the advantage of not requiring extensive hyperparameter tuning. Integrating this with MT-CGCNN is left for future works <ref type="bibr" target="#b33">[27,</ref><ref type="bibr" target="#b34">28]</ref>. We make MT-CGCNN's source code available to encourage reproducible research 3 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Correlation plots between different properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(best viewed in color) (a) Predicted ?E f (vs) true ?E f for 60% training data. (b) Predicted E g (vs) true E g for 60% training data. (c) Spearman's rank correlation coefficient (r s ) of predicted ?E f and true ?E f for MT-CGCNN and CGCNN as a function of training data. Our model is comparable with the baseline. (d) Area under the curve (AUC) of metal/non-metal classification for MT-CGCNN and CGCNN as a function of training data. The lowest AUC of our model is higher than the highest AUC of the baseline. Refer section 4.5 for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental Setup for evaluation</figDesc><table><row><cell>Experiment Setup</cell></row><row><cell>E1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average MAE values with percentage of improvement for different experiments on ?E f , E g and E F . Our model performs consistently better than baseline (CGCNN). Refer section 4.4 for more details.</figDesc><table><row><cell cols="4">Experiment CGCNN MT-CGCNN Improvement(%)</cell></row><row><cell>E1</cell><cell>0.181</cell><cell>0.166</cell><cell>8.3%</cell></row><row><cell>E2</cell><cell>0.210</cell><cell>0.202</cell><cell>3.8%</cell></row><row><cell>E3</cell><cell>0.352</cell><cell>0.346</cell><cell>1.7%</cell></row><row><cell>E4</cell><cell>0.247</cell><cell>0.236</cell><cell>4.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Individual MAE of three properties -?E f , E g and E F using CGCNN and MT-CGCNN models. Our model performs better for E g and E F prediction. Refer section 4.4 for more details.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>MAE values of ?E f and E g with increasing training data split from 20% to 60%. Our model performs better with 50% training data compared to baseline with 60% training data (highlighted in bold). Refer section 4.4 for more details.</figDesc><table><row><cell>Property</cell><cell></cell><cell></cell><cell>CGCNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MT-CGCNN</cell></row><row><cell></cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell></row><row><cell>?E f</cell><cell cols="10">0.062 0.052 0.046 0.043 0.039 0.062 0.053 0.049 0.046 0.043</cell></row><row><cell>E g</cell><cell cols="10">0.424 0.385 0.356 0.332 0.323 0.388 0.346 0.326 0.301 0.290</cell></row><row><cell cols="11">Avg MAE 0.243 0.218 0.201 0.188 0.181 0.225 0.200 0.188 0.174 0.166</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>A list of hyperparameters with values on which grid search is performed Hyperparameter Values Number of convolutional layers 1, 2, 3, 4, 5 Length of learned atom feature vector v i 16, 32, 64, 128 Length of graph hidden representation 16, 32, 64, 128 Number of hidden fully-connected layers per task 1, 2, 3, 4 L 2 Regularization term 0, 10 ?6 , 10 ?4</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/txie-93/cgcnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/soumyasanyal/mt-cgcnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by Shell. We would like to thank Professor Umesh Waghmare from Jawaharlal Nehru Centre for Advanced Scientific Research and Professor Arnab Bhattacharyya from Indian Institute of Science for their insightful discussions. We would also like to thank Tian Xie for providing clarifications on various aspects of the CGCNN code.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kalil</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Materials Genome Initiative for Global Competitiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Wadia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perspective: Web-Based Machine Learning Models for Real-Time Screening of Thermoelectric Materials Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Gaultois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mulholland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APL Materials</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">53213</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerated Discovery of Stable Lead-Free Hybrid Organic-Inorganic Perovskites via Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Design of Efficient Molecular Organic Light-Emitting Diodes by a High-Throughput Virtual Screening and Experimental Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Materials</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1120" to="1127" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accelerated Search for Materials with Targeted Properties by Adaptive Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Balachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hogden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lookman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11241</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Overcoming data scarcity with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paradiso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meredig</surname></persName>
		</author>
		<idno>abs/1711.05099</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kumar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padmini</forename><surname>Rajagopalan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Praveen Pankajakshan</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Arnab Bhattacharyya</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Suchismita Sanyal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janakiraman</forename><surname>Balachandran</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Umesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waghmare</surname></persName>
		</author>
		<title level="m">Machine Learning Constrained with Dimensional Analysis and Scaling Laws: Simple, Transferable and Interpretable Models of Materials from Small Datasets</title>
		<imprint/>
	</monogr>
	<note>in review</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Massively Multitask Networks for Drug Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Konerding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Is Multitask Deep Learning Practical for Pharma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tudor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<idno type="PMID">28692267</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2068" to="2076" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Communication: Understanding Molecular Representations in Machine Learning: The Role of Uniqueness and Target Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page">161102</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gaussian Approximation Potentials: A Brief Tutorial Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bart?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cs?nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Quantum Chemistry</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="1051" to="1057" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning: Going beyond Euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neur. Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semi</surname></persName>
		</author>
		<title level="m">Supervised Classification with Graph Convolutional Networks. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral Networks and Locally Connected Networks on Graphs. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design (CAMD)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML</title>
		<meeting>the 34th International Conference on Machine Learning (ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Low Data Drug Discovery with One-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Multitask</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">An Overview of Multi-Task Learning in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>abs/1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">, C.-Y</forename><forename type="middle">L</forename><surname>Zhao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan; Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gradnorm</surname></persName>
		</author>
		<title level="m">Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks. ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Neurocomputing: Foundations of Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<title level="m">Chapter Learning Representations by Back-propagating Errors</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Persson, K. a. The Materials Project: A materials genome approach to accelerating materials innovation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cholia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ceder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APL Materials</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11002</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Demystifying Multitask Deep Neural Networks for Quantitative Structure-Activity Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Svetnik</surname></persName>
		</author>
		<idno type="PMID">28872869</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2490" to="2504" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Identifying beneficial task relations for multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cohen, I. Noise reduction in speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Research Design and Statistical Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Well</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Research Design and Statistical Analysis v. 1;</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<title level="m">A Method for Stochastic Optimization. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
