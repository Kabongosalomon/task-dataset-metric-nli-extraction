<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate perview 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most recent works on human 3D pose capture has focused on monocular reconstruction, even though multi-view reconstruction is much easier, since multi-camera setups are perceived as being too cumbersome. The appearance of Virtual/Augmented Reality headsets with multiple integrated cameras challenges this perception and has the potential to bring back multi-camera techniques to the fore, but only if multi-view approaches can be made sufficiently lightweight to fit within the limits of low-compute headsets.</p><p>Unfortunately, the state-of-the-art multi-camera 3D pose estimation algorithms tend to be computationally expensive because they rely on deep networks that operate on volumetric grids <ref type="bibr" target="#b16">[17]</ref>, or volumetric Pictorial Structures <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, to combine features coming from different views in ac-  <ref type="figure" target="#fig_3">Figure 1</ref>. Overview of 3D pose estimation from multi-view images. The state-of-the-art approaches project 2D detections to 3D grids and reason jointly across views through computationally intensive volumetric convolutional neural networks <ref type="bibr" target="#b16">[17]</ref> or Pictorial Structures (PSM) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>. This yields accurate predictions but is computationally expensive. We design a lightweight architecture that predicts 2D joint locations from a learned camera-independent representation of 3D pose and then lifts them to 3D via an efficient formulation of differentiable triangulation (DLT). Our method achieves performance comparable to volumetric methods, while, unlike them, working in real-time.</p><p>cordance with epipolar geometry. <ref type="figure" target="#fig_3">Fig. 1(a)</ref> illustrates these approaches.</p><p>In this paper, we demonstrate that the expense of using a 3D grid is not required. <ref type="figure" target="#fig_3">Fig. 1(b)</ref> depicts our approach. We encode each input image into latent representations, which are then efficiently transformed from image coordinates into world coordinates by conditioning on the appropriate camera transformation using feature transform layers <ref type="bibr" target="#b35">[36]</ref>. This yields feature maps that live in a canonical frame of reference and are disentangled from the camera poses. The feature maps are fused using 1D convolutions into a unified latent representation, denoted as p 3D in <ref type="figure" target="#fig_3">Fig. 1(b)</ref>, which makes it possible to reason jointly about the extracted 2D poses across camera views. We then condition this latent code on the known camera transformation to decode it back to 2D image locations using a shallow 2D CNN. The proposed fusion technique, to which we will refer to as Canonical Fusion, enables us to drastically improve the accuracy of the 2D detection compared to the results obtained from each image independently, so much so, that we can lift these 2D detections to 3D reliably using the simple Direct Linear Transform (DLT) method <ref type="bibr" target="#b13">[14]</ref>. Because standard DLT implementations that rely on Singular Value Decomposition (SVD) are rarely efficient on GPUs, we designed a faster alternative implementation based on the Shifted Iterations method <ref type="bibr" target="#b25">[26]</ref>.</p><p>In short, our contributions are: (1) a novel multi-camera fusion technique that exploits 3D geometry in latent space to efficiently and jointly reason about different views and drastically improve the accuracy of 2D detectors, (2) a new GPU-friendly implementation of the DLT method, which is hundreds of times faster than standard implementations.</p><p>We evaluate our approach on two large-scale multi-view datasets, Human3.6M <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref> and TotalCapture <ref type="bibr" target="#b32">[33]</ref>: we outperform the state-of-the-art methods when additional training data is not available, both in terms of speed and accuracy. When additional 2D annotations can be used <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, our accuracy remains comparable to that of the state-of-the-art methods, while being faster. Finally, we demonstrate that our approach can handle viewpoints that were never seen during training. In short, we can achieve real-time performance without sacrificing prediction accuracy nor viewpoint flexibility, while other approaches cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pose estimation is a long-standing problem in the computer vision community. In this section, we review in detail related multi-view pose estimation literature. We then focus on approaches lifting 2D detections to 3D via triangulation.</p><p>Pose estimation from multi-view input images. Early attempts <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref> tackled pose-estimation from multiview inputs by optimizing simple parametric models of the human body to match hand-crafted image features in each view, achieving limited success outside of the controlled settings. With the advent of deep learning, the dominant paradigm has shifted towards estimating 2D poses from each view separately, through exploiting efficient monocular pose estimation architectures <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b29">30]</ref>, and then recovering the 3D pose from single view detections.</p><p>Most approaches use 3D volumes to aggregate 2D predictions. Pavlakos et al. <ref type="bibr" target="#b23">[24]</ref> project 2D keypoint heatmaps to 3D grids and use Pictorial Structures aggregation to estimate 3D poses. Similarly, <ref type="bibr" target="#b24">[25]</ref> proposes to use Recurrent Pictorial Structures to efficiently refine 3D pose esti-mations step by step. Improving upon these approaches, <ref type="bibr" target="#b16">[17]</ref> projects 2D heatmaps to a 3D volume using a differentiable model and regresses the estimated root-centered 3D pose through a learnable 3D convolutional neural network. This allows them to train their system end-to-end by optimizing directly the 3D metric of interest through the predictions of the 2D pose estimator network. Despite recovering 3D poses reliably, volumetric approaches are computationally demanding, and simple triangulation of 2D detections is still the de-facto standard when seeking real-time performance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Few models have focused on developing lightweight solutions to reason about multi-view inputs. In particular, <ref type="bibr" target="#b17">[18]</ref> proposes to concatenate together pre-computed 2D detections and pass them as input to a fully connected network to predict global 3D joint coordinates. Similarly, <ref type="bibr" target="#b24">[25]</ref> refines 2D heatmap detections jointly by using a fully connected layer before aggregating them on 3D volumes. Although, similar to our proposed approach, these methods fuse information from different views without using volumetric grids, they do not leverage camera information and thus overfit to a specific camera setting. We will show that our approach can handle different cameras flexibly and even generalize to unseen ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triangulating 2D detections.</head><p>Computing the position of a point in 3D-space given its images in n views and the camera matrices of those views is one of the most studied computer vision problems. We refer the reader to <ref type="bibr" target="#b13">[14]</ref> for an overview of existing methods. In our work, we use the Direct Linear Triangulation (DLT) method because it is simple and differentiable. We propose a novel GPU-friendly implementation of this method, which is up to two orders of magnitude faster than existing ones that are based on SVD factorization. We provide a more detailed overview about this algorithm in Section 7.2.</p><p>Several methods lift 2D detections efficiently to 3D by means of triangulation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5]</ref>. More closely related to our work, <ref type="bibr" target="#b16">[17]</ref> proposes to back-propagate through an SVDbased differentiable triangulation layer by lifting 2D detections to 3D keypoints. Unlike our approach, these methods do not perform any explicit reasoning about multi-view inputs and therefore struggle with large self-occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We consider a setting in which n spatially calibrated and temporally synchronized cameras capture the performance of a single individual in the scene. We denote with {I i } n i=1 the set of multi-view input images, each captured from a camera with known projection matrix P i . Our goal is to estimate its 3D pose in the absolute world coordinates; we parameterize it as a fixed-size set of 3D point locations {x j } J j=1 , which correspond to the joints. Consider as an example the input images on the left of <ref type="figure" target="#fig_1">Figure 2</ref>. Although exhibiting different appearances, the frames share the same 3D pose information up to a perspective projection and view-dependent occlusions. Building on this observation, we design our architecture (depicted in <ref type="figure" target="#fig_1">Figure 2</ref>), which learns a unified view-independent representation of 3D pose from multi-view input images. This allows us to reason efficiently about occlusions to produce accurate 2D detections, that can be then simply lifted to 3D absolute coordinates by means of triangulation. Below, we first introduce baseline methods for pose estimation from multi-view inputs. We then describe our approach in detail and explain how we train our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lightweight pose estimation from multi-view inputs</head><p>Given input images {I i } n i=1 , we use a convolutional neural network backbone to extract features {z i } n i=1 from each input image separately. Denoting our encoder network as e, z i is computed as</p><formula xml:id="formula_0">z i = e(I i ).<label>(1)</label></formula><p>Note that, at this stage, feature map z i contains a representation of the 3D pose of the performer that is fully entangled with camera view-point, expressed by the camera projection operator P i . We first propose a baseline approach, similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref>, to estimate the 3D pose from multi-view inputs. Here, we simply decode latent codes z i to 2D detections, and lift 2D detections to 3D by means of triangulation. We refer to this approach as Baseline. Although efficient, we argue that this approach is limited because it processes each view independently and therefore cannot handle self-occlusions.</p><p>An intuitive way to jointly reason across different views is to use a learnable neural network to share information across embeddings {z i } n i=1 , by concatenating features from different views and processing them through convolutional layers into view-dependent features, similar in spirit to the recent models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. In Section 4 we refer to this general approach as Fusion. Although computationally lightweight and effective, we argue that this approach is limited for two reasons: (1) it does not make use of known camera information, relying on the network to learn the spatial configuration of the multi-view setting from the data itself, and (2) it cannot generalize to different camera settings by design. We will provide evidence for this in Section 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning a view-independent representation</head><p>To alleviate the aforementioned limitations, we propose a method to jointly reason across views, leveraging the observation that the 3D pose information contained in feature maps {z i } n i=1 is the same across all n views up to camera projective transforms and occlusions, as discussed above. We will refer to this approach as Canonical Fusion.</p><p>To achieve this goal, we leverage feature transform layers (FTL) <ref type="bibr" target="#b35">[36]</ref>, which was originally proposed as a technique to condition latent embeddings on a target transformation so that to learn interpretable representations. Internally, a FTL has no learnable parameter and is computationally efficient. It simply reshapes the input feature map to a point-set, applies the target transformation, and then reshapes the point-set back to its original dimension. This technique forces the learned latent feature space to preserve the structure of the transformation, resulting in practice in a disentanglement between the learned representation and the transformation. In order to make this paper more selfcontained, we review FTL in detail in the Supplementary Section.</p><p>Several approaches have used FTL for novel view synthesis to map the latent representation of images or poses from one view to another <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>. In this work, we leverage FTL to map images from multiple views to a unified latent representation of 3D pose. In particular, we use FTL to project feature maps z i to a common canonical representation by explicitly conditioning them on the camera projection matrix P ?1 i that maps image coordinates to the</p><formula xml:id="formula_1">Algorithm 1: DLT-SII({u i , P i } N i=1 , T = 2) A ? A({u i , P i } N i=1 ); B ? (A T A + ?I) ?1 ; ? ? 0.001 (see Theorem 2); x ? rand(4, 1); for i = 1 : T do x ? Bx; x ? x/ x ; end return y ? x(0 : 3)/x(4); world coordinates z w i = FTL(z i |P ?1 i ).<label>(2)</label></formula><p>Now that feature maps have been mapped to the same canonical representation, they can simply be concatenated and fused into a unified representation of 3D pose via a shallow 1D convolutional neural network f , i.e.</p><formula xml:id="formula_2">p 3D = f (concatenate({z w i } n i=1 )).<label>(3)</label></formula><p>We now force the learned representation to be disentangled from camera view-point by transforming the shared p 3D features to view-specific representations f i by</p><formula xml:id="formula_3">f i = FTL(p 3D |P i ).<label>(4)</label></formula><p>In Section 4 we show both qualitatively and quantitatively that the representation of 3D pose we learn is effectively disentangled from the camera-view point. Unlike the Fusion baseline, Canonical Fusion makes explicit use of camera projection operators to simplify the task of jointly reasoning about views. The convolutional block, in fact, now does not have to figure out the geometrical disposition of the multi-camera setting and can solely focus on reasoning about occlusion. Moreover, as we will show, Canonical Fusion can handle different cameras flexibly, and even generalize to unseen ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoding latent codes to 2D detections</head><p>This component of our architecture proceeds as a monocular pose estimation model that maps view-specific representations f i to 2D Heatmaps H i via a shallow convolutional decoder d, i.e.</p><formula xml:id="formula_4">H j i = d(f i ),<label>(5)</label></formula><p>where H j i is the heatmap prediction for joint j in Image i. Finally, we compute the 2D location u j i of each joint j by simply integrating heatmaps across spatial axes  <ref type="figure">Figure 3</ref>. Evaluation of DLT. We validate the findings of Theorem 2 in (a). We then compare our proposed DLT implementation to the SVD of <ref type="bibr" target="#b16">[17]</ref>, both in terms of accuracy (b) and performance (c),(d). Exploiting Theorem 1, we can choose a suitable approximation for ?min(A * ), and make DLT-SII converge to the desired solution in only two iterations.</p><formula xml:id="formula_5">u j i = x,y xH j i , x,y yH j i / x,y H j i .<label>(6)</label></formula><p>Note that this operation is differentiable with respect to heatmap H j i , allowing us to back-propagate through it. In the next section, we explain in detail how we proceed to lift multi-view 2D detections to 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Efficient Direct Linear Transformation</head><p>In this section we focus on finding the position x j = [x j , y j , z j ] T of a 3D point in space given a set of n 2d detections {u j i } n i=1 . To ease the notation, we will drop apex j as the derivations that follow are carried independently for each landmark.</p><p>Assuming a pinhole camera model, we can write d i u i = P i x, where d i is an unknown scale factor. Note that here, with a slight abuse of notation, we express both 2d detections u i and 3d landmarks x in homogeneous coordinates. Expanding on the components we get</p><formula xml:id="formula_6">d i u i = p 1T i x , d i v i = p 2T i x , d i = p 3T i x,<label>(7)</label></formula><p>where p kT i denotes the k-th row of i-th camera projection matrix. Eliminating d i using the third relation in <ref type="formula" target="#formula_6">(7)</ref>, we obtain</p><formula xml:id="formula_7">(u i p 3T i ? p 1T i )x = 0 (8) (v i p 3T i ? p 2T i )x = 0.<label>(9)</label></formula><p>Finally, accumulating over all available n views yields a total of 2n linear equations in the unknown 3D position x, which we write compactly as</p><formula xml:id="formula_8">Ax = 0, where A = A({u i , v i , P i } N i=1 ).<label>(10)</label></formula><formula xml:id="formula_9">Note that A ? R 2n?4 is a function of {u i , v i , P i } N i=1</formula><p>, as specified in Equations <ref type="formula">(8)</ref> and <ref type="bibr" target="#b8">(9)</ref>. We refer to A as the DLT matrix. These equations define x up to a scale factor, and we seek a non-zero solution. In the absence of noise, Equation (10) admits a unique non-trivial solution, corresponding to the 3D intersection of the camera rays passing by each 2D observation u i (i.e. matrix A does not have full rank). However, considering noisy 2D point observations such as the ones predicted by a neural network, Equation (10) does not admit solutions, thus we have to seek for an approximate one. A common choice, known as the Direct Linear Transform (DLT) method <ref type="bibr" target="#b13">[14]</ref>, proposes the following relaxed version of Equation <ref type="formula" target="#formula_0">(10)</ref>:</p><formula xml:id="formula_10">min x Ax , subject to x = 1.<label>(11)</label></formula><p>Clearly, the solution to the above optimization problem is the eigenvector of A T A associated to its smallest eigenvalue ? min (A T A). In practice, the eigenvector is computed by means of Singular Value Decomposition (SVD) <ref type="bibr" target="#b13">[14]</ref>. We argue that this approach is suboptimal, as we in fact only care about one of the eigenvectors of A T A. Inspired by the observation above that the smallest eigenvalue of A T A is zero for non-noisy observations, we derive a bound for the smallest eigenvalue of matrix A T A in the presence of Gaussian noise. We prove this estimate in the Supplementary Section. , and let us denote as A * the DLT matrix associated to the perturbed system. Then, it follows that:</p><formula xml:id="formula_11">0 ? E[? min (A * )] ? Cs, where C = C({u i , P i } N i=1 )<label>(12)</label></formula><p>In <ref type="figure">Figure 3</ref>(a) we reproduce these setting by considering Gaussian perturbations of 2D observations, and find an experimental confirmation that by having a greater 2D joint measurement error, specified by 2D-MPJPE (see <ref type="bibr">Equation 13</ref> for its formal definition), the expected smallest singular value ? min (A * ) increases linearly.</p><p>The bound above, in practice, allows us to compute the smallest singular vector of A * reliably by means of Shifted Inverse Iterations (SII) <ref type="bibr" target="#b25">[26]</ref>: we can estimate ? min (A * ) with a small constant and know that the iterations will converge to the correct eigenvector. For more insight on why this is the case, we refer the reader to the Supplementary Section. SII can be implemented extremely efficiently on GPUs. As outlined in Algorithm 1, it consists of one inversion of a 4 ? 4 matrix and several matrix multiplication and vector normalizations, operations that can be trivially parallelized. In <ref type="figure">Figure 3</ref>(b) we compare our SII based implementation of DLT (estimating the smallest singular value of A with ? = 0.001) to an SVD based one, such as the one proposed in <ref type="bibr" target="#b16">[17]</ref>. For 2D observation errors up to 70 pixels (which is a reasonable range in 256 pixel images), our formulation requires as little as two iterations to achieve the same accuracy as a full SVD factorization, while being respectively 10/100 times faster on CPU/GPU than its counterpart, as evidenced by our profiling in <ref type="figure">Figures 3(c,d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss function</head><p>In this section, we explain how to train our model. Since our DLT implementation is differentiable with respect to 2D joint locations u i , we can let gradients with respect to 3D landmarks x flow all the way back to the input images {I i } n i=1 , making our approach trainable end-to-end. However, in practice, to make training more stable in its early stages, we found it helpful to first train our model by minimizing a 2D Mean Per Joint Position Error (MPJPE) of the form</p><formula xml:id="formula_12">L 2D-MPJPE = n i=1 1 J J j=1 u j i ?? j i 2 ,<label>(13)</label></formula><p>where? i j denotes the ground truth 2D position of j-th joint in the i-th image. In our experiments, we pre-train our models by minimizing L 2D-MPJPE for 20 epochs. Then, we finetune our model by minimizing 3D MPJPE, which is also our test metric, by</p><formula xml:id="formula_13">L 3D-MPJPE = 1 J J j=1 x j ?x j 2 ,<label>(14)</label></formula><p>wherex j denotes the ground truth 3D position of j-th joint in the world coordinate. We evaluate the benefits of finetuning using L 3D-MPJPE in the Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct our evaluation on two available large-scale multi-view datasets, TotalCapture <ref type="bibr" target="#b32">[33]</ref> and Human3.6M <ref type="bibr" target="#b15">[16]</ref>. We crop each input image around the performer, using ground truth bounding boxes provided by each dataset. Input crops are undistorted, re-sampled so that virtual cameras are pointing at the center of the crop and normalized to 256 ? 256. We augment our train set by performing random rotation(?30 degrees, note that image rotations correspond to camera rotations along the z-axis) and standard color augmentation. In our experiments, we use a ResNet152 <ref type="bibr" target="#b14">[15]</ref> pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref> as the backbone architecture for our encoder. Our fusion block consists of two 1 ? 1 convolutional layers. Our decoder consists of 4 transposed convolutional layers, followed by a 1?1 convolution to produce heatmaps. More details on our architecture are provided in the Supplementary section. The networks are trained for 50 epochs, using a Stochastic Gradient Descent optimizer where we set learning rate to 2.5 ? 10 ?2 .  <ref type="figure">Figure 4</ref>. We visualize randomly picked samples from the test set of TotalCapture and Human3.6M. To stress that the pose representation learned by our network is effectively disentangled from the camera view-point, we intentionally show predictions before triangulating them, rather than re-projecting triangulated keypoints to the image space. Predictions are best seen in supplementary videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets specifications</head><p>TotalCapture: The TotalCapture dataset <ref type="bibr" target="#b32">[33]</ref> has been recently introduced to the community. It consists of 1.9 million frames, captured from 8 calibrated full HD video cameras recording at 60Hz. It features 4 male and 1 female subjects, each performing five diverse performances repeated 3 times: ROM, Walking, Acting, Running, and Freestyle. Accurate 3D human joint locations are obtained from a marker-based motion capture system. Following previous work <ref type="bibr" target="#b32">[33]</ref>, the training set consists of ROM1,2,3, Walking1,3, Freestyle1,2, Acting1,2, Running1 on subjects 1,2 and 3. The testing set consists of Walking2 (W2), Freestyle3 (FS3), and Acting3 (A3) on subjects 1, 2, 3, 4, and 5. The number following each action indicates the video of that action being used, for example Freestyle has three videos of the same action of which 1 and 2 are used for training and 3 for testing. This setup allows for testing on unseen and seen subjects but always unseen performances. Following <ref type="bibr" target="#b24">[25]</ref>, we use the data of four cameras <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref> to train and test our models. However, to illustrate the generalization ability of our approach to new camera settings, we propose an experiment were we train on cameras <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7)</ref> and test on unseen cameras <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8)</ref>.</p><p>Human 3.6M: The Human3.6M dataset <ref type="bibr" target="#b15">[16]</ref> is the largest publicly available 3D human pose estimation benchmark. It consists of 3.6 million frames, captured from 4 synchronized 50Hz digital cameras. Accurate 3D human joint locations are obtained from a marker-based motion capture system utilizing 10 additional IR sensors. It contains a total of 11 subjects (5 females and 6 males) a) In-plane rotations (seen views) <ref type="figure">Figure 5</ref>. In the top row, we synthesize 2D poses after rotating cameras with respect to z-axis. In the bottom row, we rotate camera around the plane going through two consecutive camera views by angle ?, presenting the network with unseen camera projection matrices. Note that after decoding p3D to a novel view, it no longer corresponds to the encoded view. 2D Skeletons are overlaid on one of the original view in order to provide a reference. These images show that the 3D pose embedding p3D is disentangled from the camera view-point. Best seen in supplementary videos.</p><formula xml:id="formula_14">Rz = 0 ? Rz = 10 ? Rz = 20 ? Rz = 30 ? b) Out-of-plane rotations (unseen views) ? = 0 ? ? = 30 ? ? = 150 ? ? = 180 ?</formula><p>performing 15 different activities. For evaluation, we follow the most popular protocol, by training on subjects 1, 5, 6, 7, 8 and using unseen subjects 9, 11 for testing. Similar to other methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>, we use all available views during training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative evaluation of disentanglement</head><p>We evaluate the quality of our latent representation by showing that 3D pose information is effectively disentangled from the camera view-point. Recall from Section 3 that our encoder e encodes input images to latent codes z i , which are transformed from camera coordinates to the world coordinates and latter fused into a unified representation p 3D which is meant to be disentangled from the camera view-point. To verify this is indeed the case, we propose to decode our representation to different 2D poses by using different camera transformations P , in order to produce views of the same pose from novel camera view-points. We refer the reader to <ref type="figure">Figure 5</ref> for a visualization of the synthesized poses. In the top row, we rotate one of the cameras with respect to the z-axis, presenting the network with projection operators that have been seen at train time. In the bottom row we consider a more challenging scenario, where we synthesize novel views by rotating the camera around the plane going through two consecutive camera views. Despite presenting the network with unseen projection operators, our decoder is still able to synthesize correct 2D poses. This experiment shows our approach has effectively learned a representation of the 3D pose that is disentangled from camera view-point. We evaluate it quantitatively in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Seen Subjects (S1,S2,S3) Unseen Subjects (S4,S5) Mean Walking Freestyle Acting Walking Freestyle Acting Qui et al. <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative evaluation on TotalCapture</head><p>We begin by evaluating the different components of our approach and comparing to the state-of-the-art volumetric method of <ref type="bibr" target="#b24">[25]</ref> on the TotalCapture dataset. We report our results in <ref type="table">Table 1</ref>. We observe that by using the feature fusion technique (Fusion) we get a significant 19% improvement over our Baseline, showing that, although simple, this fusion technique is effective. Our more sophisticated Canonical Fusion (no DLT) achieves further 10% improvement, showcasing that our method can effectively use camera projection operators to better reason about views. Finally, training our architecture by back-propagating through the triangulation layer (Canonical Fusion) allows to further improve our accuracy by 3%. This is not surprising as we optimize directly for the target metric when training our network. Our best performing model outperforms the state-ofthe-art volumetric model of <ref type="bibr" target="#b24">[25]</ref> by ? 5%. Note that their method lifts 2D detections to 3D using Recurrent Pictorial Structures (RPSM), which uses a pre-defined skeleton, as a strong prior, to lift 2D heatmaps to 3D detections. Our method doesn't use any priors, and still outperform theirs. Moreover, our approach is orders of magnitude faster than theirs, as we will show in Section 4.6. We show some uncurated test samples from our model in <ref type="figure">Figure 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generalization to unseen cameras</head><p>To assess the flexibility of our approach, we evaluate its performance on images captured from unseen views. To do so, we take the trained network of Section 4.3 and test it on cameras <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8)</ref>. Note that this setting is particularly challenging not only because of the novel camera views, but also because the performer is often out of field of view in camera 2. For this reason, we discard frames where the performer is out of field of view when evaluating our Baseline. We report the results in <ref type="table">Table 2</ref>. We observe that Fusion fails at generalizing to novel views (accuracy drops by 47.1mm when the network is presented with new views). This is not surprising as this fusion technique over-fits by design to the camera setting. On the other hand the accuracy drop of Canonical Fusion is similar to the one of Baseline (? 10mm). Note that our comparison favors Baseline by discarding frames when object is occluded. This experiments validates that our model is able to cope effectively with challenging unseen views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quantitative evaluation on Human 3.6M</head><p>We now turn to the Human36M dataset, where we first evaluate the different components of our approach, and then compare to the state-of-the-art multi-view methods. Note that here we consider a setting where no additional data is used to train our models. We report the results in <ref type="table">Table  3</ref>. Considering the ablation study, we obtain results that are consistent with what we observed on the TotalCapture dataset: performing simple feature fusion (Fusion) yields a 18% improvement over the monocular baseline. A further ? 10% improvement can be reached by using Canonical Fusion (no DLT). Finally, training our architecture by back-propagating through the triangulation layer (Canonical Fusion) allows to further improve our accuracy by 7%. We show some uncurated test samples from our model in <ref type="figure">Figure 4(b)</ref>.</p><p>We then compare our model to the state-of-the-art methods. Here we can compare our method to the one of <ref type="bibr" target="#b24">[25]</ref> just by comparing fusion techniques (see Canonical Fusion  <ref type="table">Table 3</ref>. No additional training data setup. We compare the 3D pose estimation error (reported in MPJPE (mm)) of our method to the stateof-the-art approaches on the Human3.6M dataset. The reported results for our methods are obtained without rigid alignment or further offline post-processing steps.</p><p>(no DLT) vs Qui et al. <ref type="bibr" target="#b24">[25]</ref> (no RPSM) in <ref type="table">Table 3</ref>). We see that our methods outperform theirs by ? 15%, which is significant and indicates the superiority of our fusion technique. Similar to what observed in Section 4.3, our best performing method is even superior to the off-line volumetric of <ref type="bibr" target="#b24">[25]</ref>, which uses a strong bone-length prior (Qui et al. <ref type="bibr" target="#b24">[25]</ref> Fusion + RPSM). Our method outperforms all other multi-view approaches by a large margin. Note that in this setting we cannot compare to <ref type="bibr" target="#b16">[17]</ref>, as they do not report results without using additional data.  <ref type="table">Table 4</ref>. Additional training data setup. We compare our method to the state-of-the-art approaches in terms of performance, inference time, and model size on the Human3.6M dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Exploiting additional data</head><p>To compare to the concurrent model in <ref type="bibr" target="#b16">[17]</ref>, we consider a setting in which we exploit additional training data. We adopt the same pre-training strategy as <ref type="bibr" target="#b16">[17]</ref>, that is we pretrain a monocular pose estimation network on the COCO dataset <ref type="bibr" target="#b19">[20]</ref>, and fine-tune jointly on Human3.6M and MPII <ref type="bibr" target="#b1">[2]</ref> datasets. We then simply use these pre-trained weights to initialize our network. We also report results for <ref type="bibr" target="#b24">[25]</ref>, which trains its detector jointly on MPII and Human3.6M. The results are reported in <ref type="table">Table 4</ref>.</p><p>First of all, we observe that Canonical Fusion outperforms our monocular baseline by a large margin (? 39%). Similar to what was remarked in the previous section, our method also outperforms <ref type="bibr" target="#b24">[25]</ref>. The gap, however, is somewhat larger in this case (? 20%). Our approach also outperforms the triangulation baseline of (Iskakov et al. <ref type="bibr" target="#b16">[17]</ref> Algebraic), indicating that our fusion technique if effective in reasoning about multi-view input images. Finally, we observe that our method reaches accuracy comparable to the volumetric approach of (Iskakov et al. <ref type="bibr" target="#b16">[17]</ref> Volumetric).</p><p>To give insight on the computational efficiency of our method, in <ref type="table">Table 4</ref> we report the size of the trained models in memory, and also measure their inference time (we consider a set of 4 images and measure the time of a forward pass on a Pascal TITAN X GPU and report the average over 100 forward passes). Comparing model size, Canonical Fusion is much smaller than other models and introduces only a negligible computational overhead compared to our monocular Baseline. Comparing the inference time, both our models yield a real-time performance (? 25f ps) in their un-optimized version, which is much faster than other methods. In particular, it is about 50 times faster than (Iskakov et al. <ref type="bibr" target="#b16">[17]</ref> Algebraic) due to our efficient implementation of DLT and about 57 times faster than (Iskakov et al. <ref type="bibr" target="#b16">[17]</ref> Volumetric) due to using DLT plus 2D CNNs instead of a 3D volumetric approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a new multi-view fusion technique for 3D pose estimation that is capable of reasoning across multiview geometry effectively, while introducing negligible computational overhead with respect to monocular methods. Combined with our novel formulation of DLT transformation, this results in a real-time approach to 3D pose estimation from multiple cameras. We report the state-ofthe-art performance on standard benchmarks when using no additional data, flexibility to unseen camera settings, and accuracy comparable to far-more computationally intensive volumetric methods when allowing for additional 2D annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Architectures</head><p>In <ref type="figure" target="#fig_7">Figure 6</ref>, we depict the different architectures (baseline, fusion, canonical fusion) compared in the main article. Recall that our encoder consists of a ResNet152 <ref type="bibr" target="#b14">[15]</ref> backbone pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref> for all three architectures, taking in 256 ? 256 image crops as input and producing 2048?18?18 features maps. Similarly, all methods share the same convolutional decoder, consisting of This produces K ? 64 ? 64 output heatmaps, where K is the number of joints. The only difference between the networks is in the feature fusion module, respectively defined as follows:</p><p>? baseline: no feature fusion.</p><p>? fusion: a 1 ? 1 convolution is first applied to map features from 2048 channels to 300. Then, the feature maps from different views are concatenated to make a feature map of size n ? 300, where n indicates the number of views. This feature map is then processed jointly by two 1 ? 1 convolutional layers, finally producing a feature map with n ? 300 channels, which is later split into view-specific feature maps with 300 channels in each view. Each view-specific feature map is then lifted back to 2048 channels.</p><p>? canonical fusion: a 1 ? 1 convolution is first applied to map features from 2048 channels to 300. The feature maps from different views are then transformed to a shared canonical representation (world coordinate system) by feature transform layers. Once they live in the same coordinate system, they are concatenated into a n?300 feature map and processed jointly by two 1?1 convolutional layers, producing a unified feature map with 300 channels that is disentangled from the camera view-point. This feature map, denoted as p 3D in the main article, is then projected back to each viewpoint by using feature transform layers and the corresponding camera transform matrix. Finally each viewspecific feature map is mapped back to 2048 channels. Note that in contrast to fusion that learns separate latent representations for different views, in canonical fusion all views are reconstructed from the same latent representation, effectively forcing the model to learn a unified representation across all views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Efficient Direct Linear Transformation</head><p>In this section we prove Theorem 2 from the main article, and then illustrate how in practice we use it to design an efficient algorithm for Direct Linear Transformation by using Shifted Inverse Iterations method <ref type="bibr" target="#b25">[26]</ref>. Finally, we provide some insight on why SVD is not efficient on GPUs (see <ref type="figure">Figure 3d</ref> in the main article).</p><p>Theorem 2. Let A be the DLT matrix associated with the non-perturbed case, i.e. ? min (A) = 0. Let us assume i.i.d Gaussian noise ? = (? u , ? v ) ? N (0, s 2 I) in our 2d observations, i.e. (u * , v * ) = (u + ? u , v + ? v ), and let us denote A * the DLT matrix associated with the perturbed system. Then, it follows that:</p><formula xml:id="formula_15">0 ? E[? min (A * )] ? Cs, where C = C({u i , P i } N i=1 )<label>(15)</label></formula><p>Proof. Let us recall the structure of matrix A ? R 2n?4 , which is the DLT matrix for non-noisy 2D observations:</p><formula xml:id="formula_16">A = ? ? ? ? ? ? . . . u i p 3T i ? p 1T i v i p 3T i ? p 2T i . . . ? ? ? ? ? ? .<label>(16)</label></formula><p>Now considering noisy observations</p><formula xml:id="formula_17">(u * i , v * i ) = (u i + ? 2i , v i + ? 2i+1 ),</formula><p>where we drop the subscripts u, v from ? (as noise is i.i.d.), the DLT matrix can be written as</p><formula xml:id="formula_18">A * = ? ? ? ? ? ? . . . (u i + ? 2i ) p 3T i ? p 1T i (v i + ? 2i+1 ) p 3T i ? p 2T i . . . ? ? ? ? ? ? ,<label>(17)</label></formula><p>which is equivalent to</p><formula xml:id="formula_19">A * = A + ? ? ? ? ? ? . . . ? 2i p 3T i ? 2i+1 p 3T i . . . ? ? ? ? ? ? (18) = A + ? ? ? ? ? ? . . . ? 2i ? 2i+1 . . . ? ? ? ? ? ? ? ? ? ? ? ? . . . p 3T i p 3T i . . . ? ? ? ? ? ? (19) = A + ?P,<label>(20)</label></formula><p>where ? ? R 2n?2n and P ? R 2n?4 . Using the classical perturbation theory (see Stewart et al. <ref type="bibr" target="#b28">[29]</ref> for an overview), we can write</p><formula xml:id="formula_20">|? min (A * ) ? ? min (A)| ? A * ? A 2 .<label>(21)</label></formula><p>By exploiting ? min (A) = 0, Equation 20, and the fact that singular values are always positive we can infer</p><formula xml:id="formula_21">? min (A * ) ? ?P 2 .<label>(22)</label></formula><p>Then by leveraging Cauchy-Schwartz inequality <ref type="bibr" target="#b6">[7]</ref> and recalling that the norm 2 of a diagonal matrix is bounded by the absolute value of the biggest element in the diagonal we get</p><formula xml:id="formula_22">? min (A * ) ? ? 2 P 2 ? P 2 max i |? i |.<label>(23)</label></formula><p>Recall that that the max of 2n i.i.d. variables is smaller than their sum, so we can write</p><formula xml:id="formula_23">? min (A * ) ? P 2 2n?1 i=0 |? i |.<label>(24)</label></formula><p>We can then simply take the expected value on both sides of Equation <ref type="bibr" target="#b23">(24)</ref> and obtain</p><formula xml:id="formula_24">E ? min (A * ) ? E P 2 2n?1 i=0 |? i | (25) ? P 2 2n?1 i=0 E[|? i |]<label>(26)</label></formula><formula xml:id="formula_25">? P 2 2n E[|? 0 |].<label>(27)</label></formula><p>Knowing that the expected value of the half-normal distribution is E[|? i |] = s 2/? we finally obtain</p><formula xml:id="formula_26">E[? min (A * )] ? 2n 2/? P 2 s = Cs.<label>(28)</label></formula><p>The other side of inequality <ref type="bibr" target="#b14">(15)</ref> trivially follows from the fact that singular values are always positive.</p><p>In the main article, we proposed (in Algorithm 1) to find the singular vector of A * associated with ? min (A * ) by means of Shifted Inverse Iterations (SII) <ref type="bibr" target="#b25">[26]</ref> applied to matrix A * T A * . This iterative algorithm (which takes as input a singular value estimate ?) has the following properties:</p><p>1. The iterations will converge to the eigenvector that is closest to the provided estimate.</p><p>2. The rate of convergence of the algorithm is geometric,</p><formula xml:id="formula_27">with ratio ? 4 (A * ) + ? ? 3 (A * ) + ? , where ? 3 ? ? 4 = ? min .</formula><p>Combining property 1 with the result of Theorem 2 ascertains that Algorithm 1 will converge to the desired singular vector if we provide it with a small value for ?. Although in theory we could set ? = 0, in practice we choose ? = 0.001 to avoid numerical instabilities when matrix A * T A * is close to being singular.</p><p>Note also that property 2 is confirmed by what we see in <ref type="figure">Figure 3b</ref> in the main article, where the number of iterations needed by the algorithm to reach convergence increases with more Gaussian noise in the 2D observation. In practice, we have found two iterations to be sufficient in our experiments.</p><p>SVD parallelization on GPU. In our experiments, carried in PyTorch v1.3 on a Pascal TITAN X GPU, we found DLT implementations based on Singular Value Decomposition (SVD) to be inefficient on GPU (see <ref type="figure">Figure  3d</ref> in the main paper). Below we provide an insight on why this is the case. SVD numerical implementations <ref type="bibr" target="#b10">[11]</ref> involve two steps:</p><p>1. Two orthogonal matrices Q and P are applied to the left and right of matrix A, respectively, to reduce it to a bidiagonal form, B = Q T AP .</p><p>2. Divide and conquer or QR iteration is then used to find both singular values and left-right singular vectors of B yielding B =? T ?V . Then, singular vectors of B are back-transformed to singular vectors of A by U = Q? and V =V P .</p><p>There are many ways to formulate these problems mathematically and solve them numerically, but in all cases, designing an efficient computation is challenging because of the nature of the reduction algorithm. In particular, the orthogonal transformations applied to the matrix are twosided, i.e., transformations are applied on both the left and the right side of the matrix. This creates data dependencies and prevents the use of standard techniques to increase the computational efficiency of the operation, for example blocking and look-ahead, which are used extensively in the one-sided algorithms (such as in LU, QR, and Cholesky factorizations <ref type="bibr" target="#b10">[11]</ref>). A recent work <ref type="bibr" target="#b33">[34]</ref> has looked into ways to increase stability of SVD while reducing its computational time. Similarly, we also found SVD factorization to be slow, which motivated us to design a more efficient solution involving only GPU-friendly operations (see Algorithm 1 in the main article).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Feature Transform Layer</head><p>Below we first review feature transform layers (FTLs), introduced in <ref type="bibr" target="#b35">[36]</ref> as an effective way to learn interpretable embeddings. Then we explain how FTLs are used in our approach.</p><p>Let us consider a representation learning task, where images X and Y are related by a known transform T and the latent vector x is obtained from X via an encoder network. The feature transform layer performs a linear transformation on x via transformation matrix F T such that the output of the layer is defined as</p><formula xml:id="formula_28">y = F T [x] = F T x,<label>(29)</label></formula><p>where y is the transformed representation. Finally y is decoded to reconstruct the target sample Y. This operation forces the neural network to learn a mapping from imagespace to feature-space while preserving the intrinsic structure of the transformation. In practice, the transforming matrix F T should be chosen such that it is invertible and norm preserving. To this end <ref type="bibr" target="#b35">[36]</ref> proposes to use rotations since they are simple and respect these properties. Periodical transformations can trivially be converted to rotations. Although less intuitive, arbitrary transformation defined on an interval can also be thought of as rotations by mapping them onto circles in feature space. <ref type="figure">Figure 7</ref> illustrates in detail how to compute this mapping.</p><p>Note that if X and Y differ by more than one factor of variation, disentanglement can be achieved by transforming features as follows:</p><formula xml:id="formula_29">y = F T1,...,Tn [x] = ? ? ? F T1 . . . F Tn ? ? ? x.<label>(30)</label></formula><p>In <ref type="bibr" target="#b35">[36]</ref> FTLs are presented as a way to learn representations from data that are 1) interpretable, 2) disentangled, and 3) better suited for down-stream tasks, such as classification.</p><p>In our work, we use FTLs to feed camera transformations explicitly into the network in order to design an architecture that can reason both efficiently and effectively about epipolar geometry in the latent space. As a consequence, the model learns a camera-disentangled representation of 3D pose, that recovers 2D joint locations from multi-view input imagery. This shows that FTLs can be used to learn disentangled latent representations also in supervised learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Additional results</head><p>In <ref type="figure">Figures 8 and 9</ref> we provide additional visualizations, respectively for TotalCapture (using both seen and unseen cameras) and Human3.6M datasets. These uncurated figures illustrate the quality of our predictions. We encourage the reader to look at our supplementary videos for further qualitative results.  . Overview of different multi-view architectures: a) baseline, which detects 2D locations of joints for each view separately and then lifts detections to 3D via DLT triangulation. b) the multi-view feature fusion technique (fusion) that performs joint reasoning in the latent space, similar in spirit to the methods of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. This approach does not exploit epipolar geometry and hence overfits to the camera setting. c) our novel fusion method (canonical fusion), exploiting camera transform layers to fuse views flexibly into a unified pose representation that is disentangled from camera view-points and thus can generalize to novel views.  . Randomly picked samples from the test set of TotalCapture. Numbers denote cameras. In the two left columns we test our model on unseen images captured from seen camera view-points. In the right column, instead, we use images captured from unseen camera view-points. To stress that the pose representation learned by our network is effectively disentangled from the camera view-point, we intentionally show predictions before triangulating them, rather than re-projecting triangulated keypoints to the image space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Canonical Fusion. The proposed architecture learns a unified view-independent representation of the 3D pose from multi-view inputs, allowing it to reason efficiently across multiple views. Feature Transform Layers (FTL) use camera projection matrices (Pi) to map features between this canonical representation, while Direct Linear Transform (DLT) efficiently lifts 2D keypoints into 3D. Blocks marked in gray are differentiable (supporting backpropagation) but not trainable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 1 .</head><label>1</label><figDesc>Let A be the DLT matrix associated to the nonperturbed case, i.e. ? min (A) = 0. Let us assume i.i.d Gaussian noise ? = (? u , ? v ) ? N (0, s 2 I) in our 2d observations, i.e. (u * , v * ) = (u + ? u , v + ? v )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>ConvTranspose2D(2048, 256) + BatchNorm + ReLU ? ConvTranspose2D(256, 256) + BatchNorm + ReLU ? ConvTranspose2D(256, 256) + BatchNorm + ReLU ? Conv2D(256, K).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6</head><label>6</label><figDesc>Figure 6. Overview of different multi-view architectures: a) baseline, which detects 2D locations of joints for each view separately and then lifts detections to 3D via DLT triangulation. b) the multi-view feature fusion technique (fusion) that performs joint reasoning in the latent space, similar in spirit to the methods of [18, 25]. This approach does not exploit epipolar geometry and hence overfits to the camera setting. c) our novel fusion method (canonical fusion), exploiting camera transform layers to fuse views flexibly into a unified pose representation that is disentangled from camera view-points and thus can generalize to novel views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 ?</head><label>1</label><figDesc>cos ? = ? ? 0.5 * (? max + ? min ) 0.5 * (? max ? ? min ) sin ? = ? cos 2 ? R ? = cos ? sin ? ? sin ? cos ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .Figure 8 .Figure 9</head><label>789</label><figDesc>FTL encodes transformations by mapping them onto circles in the feature space. Consider the setting in which a factor of variation ? (e.g. x-component of camera position in world coordinates), defined in the interval ? ? [?min, ?max], changes from ? = ?1 to ? = ?2. Exploiting trigonometry, we can map this transformation onto a circle, as depicted on the right-hand side of the figure, where the transformation is defined as a rotation. Randomly picked samples from the test set of Human3.6M. Numbers denote cameras. To stress that the pose representation learned by our network is effectively disentangled from the camera view-point, we intentionally show predictions before triangulating them, rather than re-projecting triangulated keypoints to the image space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1. 3D pose estimation error MPJPE (mm) on the TotalCapture dataset. The results reported for our methods are obtained without rigid alignment or further offline post-processing.</figDesc><table><row><cell cols="2">25] Baseline + RPSM</cell><cell>28</cell><cell>42</cell><cell>30</cell><cell>45</cell><cell>74</cell><cell>46</cell><cell>41</cell></row><row><cell cols="2">Qui et al. [25] Fusion + RPSM</cell><cell>19</cell><cell>28</cell><cell>21</cell><cell>32</cell><cell>54</cell><cell>33</cell><cell>29</cell></row><row><cell>Ours, Baseline</cell><cell></cell><cell>31.8</cell><cell>36.4</cell><cell>24.0</cell><cell>43.0</cell><cell>75.7</cell><cell>43.0</cell><cell>39.3</cell></row><row><cell>Ours, Fusion</cell><cell></cell><cell>14.6</cell><cell>35.3</cell><cell>20.7</cell><cell>28.8</cell><cell>71.8</cell><cell>37.3</cell><cell>31.8</cell></row><row><cell cols="2">Ours, Canonical Fusion(no DLT)</cell><cell>10.9</cell><cell>32.2</cell><cell>16.7</cell><cell>27.6</cell><cell>67.9</cell><cell>35.1</cell><cell>28.6</cell></row><row><cell>Ours, Canonical Fusion</cell><cell></cell><cell>10.6</cell><cell>30.4</cell><cell>16.3</cell><cell>27.0</cell><cell>65.0</cell><cell>34.2</cell><cell>27.5</cell></row><row><cell>Methods</cell><cell cols="3">Seen Subjects (S1,S2,S3)</cell><cell></cell><cell cols="3">Unseen Subjects (S4,S5)</cell><cell>Mean</cell></row><row><cell></cell><cell cols="7">Walking Freestyle Acting Walking Freestyle Acting</cell></row><row><cell>Ours, Baseline</cell><cell>28.9</cell><cell>53.7</cell><cell cols="2">42.4</cell><cell>46.7</cell><cell>75.9</cell><cell>51.3</cell><cell>48.2</cell></row><row><cell>Ours, Fusion</cell><cell>73.9</cell><cell>71.5</cell><cell cols="2">71.5</cell><cell>72.0</cell><cell>108.4</cell><cell>58.4</cell><cell>78.9</cell></row><row><cell>Ours, Canonical Fusion</cell><cell>22.4</cell><cell>47.1</cell><cell cols="2">27.8</cell><cell>39.1</cell><cell>75.7</cell><cell>43.1</cell><cell>38.2</cell></row></table><note>Table 2. Testing the generalization capabilities of our approach on unseen views. We take the networks of Section 4.3, trained on cameras (1,3,5,7) of the TotalCapture training set, and test on the unseen views captured with cameras (2,4,6,8). We report 3D pose estimation error MPJPE (mm).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Methods Dir. Disc. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Mean Martinez et al.</figDesc><table><row><cell>[22]</cell><cell>46.5 48.6 54.0 51.5</cell><cell>67.5</cell><cell>70.7</cell><cell>48.5</cell><cell>49.1</cell><cell>69.8 79.4</cell><cell>57.8</cell><cell>53.1</cell><cell>56.7</cell><cell>42.2</cell><cell>45.4</cell><cell>57.0</cell></row><row><cell>Pavlakos et al. [24]</cell><cell>41.2 49.2 42.8 43.4</cell><cell>55.6</cell><cell>46.9</cell><cell>40.3</cell><cell>63.7</cell><cell>97.6 119.0</cell><cell>52.1</cell><cell>42.7</cell><cell>51.9</cell><cell>41.8</cell><cell>39.4</cell><cell>56.9</cell></row><row><cell>Tome et al. [31]</cell><cell>43.3 49.6 42.0 48.8</cell><cell>51.1</cell><cell>64.3</cell><cell>40.3</cell><cell>43.3</cell><cell>66.0 95.2</cell><cell>50.2</cell><cell>52.2</cell><cell>51.1</cell><cell>43.9</cell><cell>45.3</cell><cell>52.8</cell></row><row><cell cols="2">Kadkhodamohammadi et al. [18] 39.4 46.9 41.0 42.7</cell><cell>53.6</cell><cell>54.8</cell><cell>41.4</cell><cell>50.0</cell><cell>59.9 78.8</cell><cell>49.8</cell><cell>46.2</cell><cell>51.1</cell><cell>40.5</cell><cell>41.0</cell><cell>49.1</cell></row><row><cell>Qiu et al. [25]</cell><cell>34.8 35.8 32.7 33.5</cell><cell>34.5</cell><cell>38.2</cell><cell>29.7</cell><cell>60.7</cell><cell>53.1 35.2</cell><cell>41.0</cell><cell>41.6</cell><cell>31.9</cell><cell>31.4</cell><cell>34.6</cell><cell>38.3</cell></row><row><cell>Qui et al. [25] + RPSM</cell><cell>28.9 32.5 26.6 28.1</cell><cell>28.3</cell><cell>29.3</cell><cell>28.0</cell><cell>36.8</cell><cell>41.0 30.5</cell><cell>35.6</cell><cell>30.0</cell><cell>28.3</cell><cell>30.0</cell><cell>30.5</cell><cell>31.2</cell></row><row><cell>Ours, Baseline</cell><cell>39.1 46.5 31.6 40.9</cell><cell>39.3</cell><cell>45.5</cell><cell>47.3</cell><cell>44.6</cell><cell>45.6 37.1</cell><cell>42.4</cell><cell>46.7</cell><cell>34.5</cell><cell>45.2</cell><cell>64.8</cell><cell>43.2</cell></row><row><cell>Ours, Fusion</cell><cell>31.3 37.3 29.4 29.5</cell><cell>34.6</cell><cell>46.5</cell><cell>30.2</cell><cell>43.5</cell><cell>44.2 32.4</cell><cell>35.7</cell><cell>33.4</cell><cell>31.0</cell><cell>38.3</cell><cell>32.4</cell><cell>35.4</cell></row><row><cell cols="2">Ours, Canonical Fusion (no DLT) 31.0 35.1 28.6 29.2</cell><cell>32.2</cell><cell>34.8</cell><cell>33.4</cell><cell>32.1</cell><cell>35.8 34.8</cell><cell>33.3</cell><cell>32.2</cell><cell>29.9</cell><cell>35.1</cell><cell>34.8</cell><cell>32.5</cell></row><row><cell>Ours, Canonical Fusion</cell><cell>27.3 32.1 25.0 26.5</cell><cell>29.3</cell><cell>35.4</cell><cell>28.8</cell><cell>31.6</cell><cell>36.4 31.7</cell><cell>31.2</cell><cell>29.9</cell><cell>26.9</cell><cell>33.7</cell><cell>30.4</cell><cell>30.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>We would like to thank Giacomo Garegnani for the numerous and insightful discussions on singular value decomposition. This work was completed during an internship at Facebook Reality Labs, and supported in part by the Swiss National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu Catalin Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustin-Louis</forename><surname>Cauchy</surname></persName>
		</author>
		<idno>1821. 11</idno>
		<title level="m">Sur les formules qui resultent de lemploie du signe et sur? ou?, et sur les moyennes entre plusieurs quantites. Cours dAnalyse, 1er Partie: Analyse algebrique</title>
		<imprint>
			<biblScope unit="page" from="373" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10895" to="10904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular neural image based rendering with continuous view control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4090" to="4100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating numerical dense linear algebra calculations with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azzam</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Kurzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanimire</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichitaro</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Numerical computations with GPUs</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deepfly3d: A deep learning-based approach for 3d limb and appendage tracking in tethered, adult drosophila. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>G?nel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Campagnolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavan</forename><surname>Ramdya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05754</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A generalizable approach for multi-view 3d human pose regression. ArXiv, abs/1804.10462</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdolrahim</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d pose detection of closely interactive humans using multiview cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markerless motion capture of interacting characters using multi-view image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Numerical mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Quarteroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Sacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Saleri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural scene decomposition for multi-person motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7703" to="7713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Perturbation theory for the singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stewart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09212</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3d: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09023</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Backpropagation-friendly eigendecomposition. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
