<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>117417</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Factorization Machines for Sparse Predictive Analytics *</title>
					</analytic>
					<monogr>
						<title level="m">SIGIR&apos;17</title>
						<meeting> <address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 7-11, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3077136.3080777</idno>
					<note>978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ?Information systems ? Information retrieval</term>
					<term>Recommender systems</term>
					<term>?Computing methodologies ? Neural networks</term>
					<term>Fac- torization methods</term>
					<term>KEYWORDS Factorization Machines, Neural Networks, Deep Learning, Sparse Data, Regression, Recommendation *</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data e ectively, it is crucial to account for the interactions between features.</p><p>Factorization Machines (FMs) are a popular solution for e ciently using the second-order feature interactions. However, FM models feature interactions in a linear way, which can be insu cient for capturing the non-linear and complex inherent structure of real-world data. While deep neural networks have recently been applied to learn non-linear feature interactions in industry, such as the Wide&amp;Deep by Google and DeepCross by Microso , the deep structure meanwhile makes them di cult to train.</p><p>In this paper, we propose a novel model Neural Factorization Machine (NFM) for prediction under sparse se ings. NFM seamlessly combines the linearity of FM in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM without hidden layers. Empirical results on two regression tasks show that with one hidden layer only, NFM signi cantly outperforms FM with a 7.3% relative improvement. Compared to the recent deep learning methods Wide&amp;Deep and DeepCross, our NFM uses a shallower structure but o ers be er performance, being much easier to train and tune in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Predictive analytics is one of the most important techniques for many information retrieval (IR) and data mining (DM) tasks, ranging from recommendation systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">16]</ref>, targeted advertising <ref type="bibr" target="#b21">[21]</ref>, to search ranking <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b39">39]</ref>, visual analysis <ref type="bibr" target="#b35">[35]</ref>, and event detection <ref type="bibr" target="#b40">[40]</ref>. Typically, a predictive task is formulated as estimating a function that maps predictor variables to some target, for example, real valued target for regression and categorical target for classi cation. Distinct from continuous predictor variables that are naturally found in images and audios, such as raw features, the predictor variables for web applications are mostly discrete and categorical. For example, in online advertising, we need to predict how likely (target) a user ( rst predictor variable) of a particular occupation (second predictor variable) will click on an ad (third predictor variable). To build predictive models with these categorical predictor variables, a common solution is to convert them to a set of binary features (a.k.a. feature vector) via one-hot encoding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31]</ref>. erea er, standard machine learning (ML) techniques such as logistic regression and support vector machines can be applied.</p><p>Depending on the possible values of categorical predictor variables, the generated feature vector can be of very high dimension but sparse. To build e ective ML models with such sparse data, it is crucial to account for the interactions between features <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b31">31]</ref>. Many successful solutions in both industry and academia largely rely on manually cra ing combinatorial features <ref type="bibr" target="#b8">[9]</ref>, i.e., constructing new features by combining multiple predictor variables, also known as cross features. For example, we can cross variable occupation = {banker, doctor } with ender = {M, F } and get a new occupation ender = {banker M, banker F , doctor M, doctor F }. It is well known that top data scientists are usually masters of cra ing combinatorial features, which play a key role in their winning formulas <ref type="bibr" target="#b31">[31]</ref>. However, the power of such features comes at a high cost, since it requires heavy engineering e orts and useful domain knowledge to design e ective features. us these solutions can be di cult to generalize to new problems or domains.</p><p>Instead of augmenting feature vectors manually, another solution is to design a ML model to learn feature interactions from raw data automatically. A popular approach is factorization machines (FMs) <ref type="bibr" target="#b27">[27]</ref>, which embeds features into a latent space and models the interactions between features via inner product of their embedding vectors. While FM has yielded great promise 1 in many prediction tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24]</ref>, we argue that its performance can be limited by its linearity, as well as the modelling of pairwise (i.e., secondorder) feature interactions only. Speci cally, for real-world data <ref type="bibr" target="#b0">1</ref> h ps://securityintelligence.com/factorization-machines-a-new-way-of-looking-atmachine-learning arXiv:1708.05027v1 <ref type="bibr">[cs.</ref>LG] <ref type="bibr" target="#b16">16</ref> Aug 2017 that have complex and non-linear underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed <ref type="bibr" target="#b27">[27]</ref>, they still belong to the family of linear models and are claimed to be di cult to estimate <ref type="bibr" target="#b28">[28]</ref>. Moreover, they are known to have only marginal improvements over FM, which we suspect the reason might be due to the modelling of higher-order interactions in a linear way.</p><p>In this work, we propose a novel model for sparse data prediction named Neural Factorization Machines (NFMs), which enhances FMs by modelling higher-order and non-linear feature interactions. By devising a new operation in neural network modelling -Bilinear Interaction (Bi-Interaction) pooling -we subsume FM under the neural network framework for the rst time.</p><p>rough stacking non-linear layers above the Bi-Interaction layer, we are able to deepen the shallow linear FM, modelling higher-order and nonlinear feature interactions e ectively to improve FM's expressiveness. In contrast to traditional deep learning methods that simply concatenate <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b44">44]</ref> or average <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b36">36]</ref> embedding vectors in the low level, our use of Bi-Interaction pooling encodes more informative feature interactions, greatly facilitating the following "deep" layers to learn meaningful information. We conduct extensive experiments on two public benchmarks for context-aware prediction and personalized tag recommendation. With one hidden layer only, our NFM signi cantly outperforms FM (the ocial LibFM <ref type="bibr" target="#b28">[28]</ref> implementation) with a 7.3% improvement. Compared to the state-of-the-art deep learning methods -the 3-layer Wide&amp;Deep <ref type="bibr" target="#b8">[9]</ref> and 10-layer DeepCross [31] -our 1-layer NFM shows consistent improvements with a much simpler structure and fewer model parameters. Our implementation is available at: h ps://github.com/hexiangnan/neural factorization machine. e main contributions of this work are summarized as follows.</p><p>(1) To the best of our knowledge, we are the rst to introduce the Bi-Interaction pooling operation in neural network modelling, and present a new neural network view for FM. (2) Based on this new view, we develop a novel NFM model to deepen FM under the neural network framework for learning higher-order and non-linear feature interactions. (3) We conduct extensive experiments on two real-world tasks to study the Bi-Interaction pooling and NFM model, demonstrating the e ectiveness of NFM and great promise in using neural networks for prediction under sparse se ings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODELLING FEATURE INTERACTIONS</head><p>Due to the large space of combinatorial features, traditional solutions typically rely on manual feature engineering e orts or feature selection techniques like boosted decision trees to select important feature interactions. One limitation of such solutions is that they cannot generalize to combinatorial features that have not appeared in the training data. In recent years, embedding-based methods become increasingly popular, which try to learn feature interactions from raw data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b44">44]</ref>. By embedding high-dimensional sparse features into a low-dimensional latent space, the model can generalize to unseen feature combinations. Regardless of domain, we can categorize the approaches into two types: 1) factorization machine-based linear models, and 2) neural network-based non-linear models. In what follows, we shortly recapitulate the two representative techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factorization Machines</head><p>Factorization machines are originally proposed for collaborative recommendation <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b30">30]</ref>. Given a real valued feature vector x ? R n , FM estimates the target by modelling all interactions between each pair of features via factorized interaction parameters:</p><formula xml:id="formula_0">F M (x) = w 0 + n i=1 w i x i + n i=1 n j=i+1 v T i v j ? x i x j ,<label>(1)</label></formula><p>where w 0 is the global bias, w i models the interaction of the i-th feature to the target. e v T i v j term denotes the factorized interaction, where v i ? R k denotes the embedding vector for feature i, and k is the size of embedding vector, also termed as number of latent factors in literature. Note that due to the coe cient x i x j , only interactions of non-zero features are considered.</p><p>One main power of FM stems from its generality -in contrast to matrix factorization (MF) that models the relation of two entities only <ref type="bibr" target="#b17">[17]</ref>, FM is a general predictor working with any real valued feature vector for supervised learning. Clearly, it enhances linear/logistic regression (LR) using the second-order factorized interactions between features. By specifying input features, Rendle <ref type="bibr" target="#b27">[27]</ref> showed that FM can mimic many speci c factorization models such as the standard MF, parallel factor analysis, and SVD++ <ref type="bibr" target="#b22">[22]</ref>, Owing to such genericity, FM has been recognized as one of the most e ective embedding methods for sparse data prediction. It has been successfully applied to many predictive tasks, ranging from online advertising <ref type="bibr" target="#b21">[21]</ref>, microblog retrieval <ref type="bibr" target="#b26">[26]</ref>, to open relation extraction <ref type="bibr" target="#b25">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Expressiveness Limitation of FM.</head><p>Despite e ectiveness, we point out that FM still belongs to the family of (multivariate) linear models. In other words, the predicted target? (x) is linear w.r.t. each model parameter <ref type="bibr" target="#b28">[28]</ref>. Formally, for each model parameter ? ? {w 0 , {w i }, { if }}, we can have? (x) = + h? , where and h are expressions independent of ? . Unfortunately, the underlying structure of real-world data is o en highly non-linear and cannot be accurately approximated by linear models <ref type="bibr" target="#b11">[12]</ref>. As such, FM may su er from insu cient representation ability for modelling real data with complex inherent structure and regularities.</p><p>In terms of methodology, many variants <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">38]</ref> of FM have been developed. For example, Hong et al. <ref type="bibr" target="#b18">[18]</ref> proposed Co-FMs to learn from multi-view data; Oentaryo et al. <ref type="bibr" target="#b24">[24]</ref> encoded prior knowledge of features to FM by designing a hierarchical regularizer; and Lin et al. <ref type="bibr" target="#b21">[21]</ref> proposed eld-aware FM, which learned multiple embedding vectors for a feature to di erentiate its interaction with features of di erent elds. More recently, Xiao et al. <ref type="bibr" target="#b38">[38]</ref> proposed a entional FM, using an a ention network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">39]</ref> to learn the importance of each feature interaction. However, these variants are all linear extensions of FM and model the second-order feature interactions only. As such, they can su er from the same expressiveness issue for modelling real-world data.</p><p>In this work, we contribute improvements on the expressiveness of FM by endowing it the ability of non-linearity modelling. e idea is to perform non-linear transformation on the latent space s-no-pretrain of the second-order feature interactions; meanwhile, higher-order feature interactions can be captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Neural Networks</head><p>In recent ve years, deep neural networks (DNNs) have achieved immense success and have been widely used on speech recognition, computer vision and natural language processing. However, the use of DNNs is not as widespread among the IR and DM community. In our view, we think one reason might be that most data of IR and DM tasks are naturally sparse, such as user behaviors, documents/queries and various features converted from categorical variables. Although DNNs have exhibited strong ability to learn pa erns from dense data <ref type="bibr" target="#b14">[14]</ref>, the use of DNNs on sparse data has received less scrutiny, and it is unclear how to employ DNNs for e ectively learning feature interactions under sparse se ings. Until very recently, some work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b44">44]</ref> started to explore DNNs for some scenarios of sparse predictive analytics. In <ref type="bibr" target="#b16">[16]</ref>, He et al. presented a neural collaborative ltering (NCF) framework to learn interactions between users and items. Later, the NCF framework was extended to model a ribute interactions for a ribute-aware CF <ref type="bibr" target="#b37">[37]</ref>. However, their methods are only applicable to learn interactions between two entities and do not directly support the general se ing of supervised learning. Zhang et al. <ref type="bibr" target="#b44">[44]</ref> developed a FM-supported Neural Network (FNN), which uses the feature embeddings learned by FM to initialize DNNs. Cheng et al. <ref type="bibr" target="#b8">[9]</ref> proposed Wide&amp;Deep for App recommendation, where the deep part is a multi-layer perceptron (MLP) on the concatenation of feature embedding vectors to learn feature interactions. Shan et al. <ref type="bibr" target="#b31">[31]</ref> proposed DeepCross for ads prediction, which shares a similar framework with Wide&amp;Deep by replacing the MLP with the state-of-the-art residual network <ref type="bibr" target="#b14">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Optimization Di iculties of DNN.</head><p>It is worthwhile to mention the common structure of these neural network-based approaches, i.e., stacking multiple layers above the concatenation of embedding vectors to learn feature interactions. e expectation is that the multiple layers can learn combinatorial features of arbitrary orders in an implicit way <ref type="bibr" target="#b31">[31]</ref>. However, we nd a key weakness of such an architecture is that simply concatenating feature embedding vectors carries too li le information about feature interactions in the low level. An empirical evidence is from He et al.'s recent work <ref type="bibr" target="#b16">[16]</ref>, which shows that simply concatenating user and item embedding vectors leads to very poor results for collaborative ltering. To remedy this, one has to rely on the following deep layers to learn meaningful interaction function. While it is claimed that multiple non-linear layers are able to learn feature interactions well <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">31]</ref>, such a deep architecture can be di cult to optimize in practice due to the notorious problems of vanishing/exploding gradients, over ing, degradation, among others <ref type="bibr" target="#b14">[14]</ref>.</p><p>To demonstrate optimization di culties of DNNs empirically, we plot the training and test error of each epoch of Wide&amp;Deep and DeepCross on the Frappe data in <ref type="figure" target="#fig_0">Figure 1</ref>. We use the same architecture and parameters as reported in their papers, where Wide&amp;Deep applies a 3-layer tower structure and DeepCross uses a 10-layer residual network with successively decreasing hidden units. From <ref type="figure" target="#fig_0">Figure 1a</ref>, we can see that training the two models from scratch leads to a performance much worse than the shallow FM model. For Wide&amp;Deep, the training error is relatively high, which is likely because of the degradation problem <ref type="bibr" target="#b14">[14]</ref>. For DeepCross, the very low training error but high test error implies that the model is overing. Inspired by FNN <ref type="bibr" target="#b44">[44]</ref>, we further explore the use of feature embeddings learned by FM to initialize DNNs, which can be seen as a pre-training step. As can be seen from <ref type="figure" target="#fig_0">Figure 1b</ref>, both models achieve much be er performance (over 11% improvements). For Wide&amp;Deep, the degradation problem is well addressed, evidenced by the much lower training error, and the test performance is be er than LibFM. However for the 10-layer DeepCross, it still su ers from severe over ing and underperforms LibFM. ese relatively negative results reveal optimization di culties for training DNNs.</p><p>In this work, we present a new paradigm of neural networks for sparse data prediction. Instead of concatenating feature embedding vectors, we propose a new Bi-Interaction operation that models the second-order feature interactions. is results in a much more informative representation in the low level, greatly helping the subsequent non-linear layers to learn higher-order interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEURAL FACTORIZATION MACHINES</head><p>We rst present the NFM model that uni es the strengths of FMs and neural networks for sparse data modelling. We then discuss the learning procedure and how to employ some useful techniques in neural networks -dropout and batch normalization -for NFM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">e NFM Model</head><p>Similar to factorization machine, NFM is a general machine learner working with any real valued feature vector. Given a sparse vector x ? R n as input, where a feature value x i = 0 means the i-th feature does not exist in the instance, NFM estimates the target as:</p><formula xml:id="formula_1">N F M (x) = w 0 + n i=1 w i x i + f (x),<label>(2)</label></formula><p>where the rst and second terms are the linear regression part similar to that for FM, which models global bias of data and weight of features. e third term f (x) is the core component of NFM for modelling feature interactions, which is a multi-layered feedforward neural network as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. In what follows, we elaborate the design of f (x) layer by layer.</p><p>Embedding Layer. e embedding layer is a fully connected layer that projects each feature to a dense vector representation. Formally, let v i ? R k be the embedding vector for the i-th feature. A er embedding, we obtain a set of embedding vectors V x = {x 1 v 1 , ..., x n v n } to represent the input feature vector x. Owing to sparse representation of x, we only need to include the embedding vectors for non-zero features, i.e., V x = {x i v i } where x i 0. Note that we have rescaled an embedding vector by its input feature value, rather than simply an embedding table lookup, so as to account for the real valued features <ref type="bibr" target="#b27">[27]</ref>.</p><p>Bi-Interaction Layer. We then feed the embedding set V x into the Bi-Interaction layer, which is a pooling operation that converts a set of embedding vectors to one vector:</p><formula xml:id="formula_2">f BI (V x ) = n i=1 n j=i+1 x i v i x j v j ,<label>(3)</label></formula><p>where denotes the element-wise product of two vectors, that is,</p><formula xml:id="formula_3">(v i v j ) k = ik jk .</formula><p>Clearly, the output of Bi-Interaction pooling is a k-dimension vector that encodes the second-order interactions between features in the embedding space.</p><p>It is worth pointing out that our proposal of Bi-Interaction pooling does not introduce extra model parameter, and more importantly, it can be e ciently computed in linear time.</p><p>is property is the same with average/max pooling and concatenation that are rather simple but commonly used in neural network approaches <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36]</ref>. To show the linear time complexity of evaluating Bi-Interaction pooling, we reformulate Equation (3) as:</p><formula xml:id="formula_4">f BI (V x ) = 1 2 ( n i=1 x i v i ) 2 ? n i=1 (x i v i ) 2 ,<label>(4)</label></formula><p>where we use the symbol v 2 to denote v v. By considering the sparsity of x, we can actually perform Bi-Interaction pooling in O(kN x ) time, where N x denotes the number of non-zero entries in x. is is a very appealing property, meaning that the bene t of Bi-Interaction pooling in modelling pairwise feature interactions does not involve any additional cost. Hidden Layers. Above the Bi-Interaction pooling layer is a stack of fully connected layers, which are capable of learning higherorder interactions between features <ref type="bibr" target="#b31">[31]</ref>. Formally, the de nition of fully connected layers is as follows:</p><formula xml:id="formula_5">z 1 = ? 1 (W 1 f BI (V x ) + b 1 ), z 2 = ? 2 (W 2 z 1 + b 2 ), ...... z L = ? L (W L z L?1 + b L ),<label>(5)</label></formula><p>where L denotes the number of hidden layers, W l , b l and ? l denote the weight matrix, bias vector and activation function for the l-th layer, respectively. By specifying non-linear activation functions, such as sigmoid, hyperbolic tangent (tanh), and Recti er (ReLU), we allow the model to learn higher-order feature interactions in a non-linear way. is is advantageous to existing methods for higher-order interaction learning, such as higher-Order FM <ref type="bibr" target="#b2">[3]</ref> and Exponential Machines <ref type="bibr" target="#b23">[23]</ref>, which only support the learning of higher-order interactions in a linear way. As for the structure of fully connected layers (i.e., size of each layer), one can freely choose tower <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">16]</ref>, constant <ref type="bibr" target="#b31">[31]</ref>, and diamond <ref type="bibr" target="#b44">[44]</ref>, among others.</p><p>Prediction Layer. At last, the output vector of the last hidden layer z L is transformed to the nal prediction score:</p><formula xml:id="formula_6">f (x) = h T z L ,<label>(6)</label></formula><p>where vector h denotes the neuron weights of the prediction layer.</p><p>To summarize, we give the formulation NFM's predictive model as:</p><formula xml:id="formula_7">N F M (x) = w 0 + n i=1 w i x i + h T ? L (W L (...? 1 (W 1 f BI (V x ) + b 1 )...) + b L ),<label>(7)</label></formula><p>with all model parameters</p><formula xml:id="formula_8">? = {w 0 , {w i , v i }, h, {W l , b l }}.</formula><p>Compared to FM, the additional model parameters of NFM are mainly {W l , b l }, which are used for learning higher-order interactions between features. In remainder of this subsection, we rst show how NFM generalizes FM and discuss the connection of NFM between existing deep learning methods; we then analyze the time complexity of evaluating NFM model.</p><formula xml:id="formula_9">3.1.1 NFM Generalizes FM.</formula><p>FM is a shallow and linear model, which can be seen as a special case of NFM with no hidden layer. To show this, we set L to zero and directly project the output of Bi-Interaction pooling to prediction score. We term this simpli ed model as NFM-0, which is formulated as:</p><formula xml:id="formula_10">N F M ?0 = w 0 + n i=1 w i x i + h T n i=1 n j=i+1 x i v i x j v j = w 0 + n i=1 w i x i + n i=1 n j=i+1 k f =1 h f if jf ? x i x j .<label>(8)</label></formula><p>As can be seen, by xing h to a constant vector of (1, ..., 1), we can exactly recover the FM model 2 .</p><p>It is worth pointing out that, to our knowledge, this is the rst time FM has been expressed under the neural network framework. While a recent work by Blondel et al. <ref type="bibr" target="#b3">[4]</ref> has uni ed FM and Polynomial network via kernelization, their kernel view of FM only provides a new training algorithm and does not provide insight for improving the FM model. Our new view of FM is highly instructive and provides more insight for improving FM. Particularly, we allow the use of various neural network techniques on FM to improve its learning and generalization ability. For example, we can use dropout [33] -a well-known technique in deep learning community to prevent over ing -on the Bi-Interaction layer as a way to regularize FM, which we nd can be even more e ective than the conventional L 2 regularization (see <ref type="figure" target="#fig_3">Figure 3</ref> of Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.2</head><p>Relation to Wide&amp;Deep and DeepCross. NFM has a similar multi-layered neural architecture with several existing deep learning solutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>e key di erence is in the Bi-Interaction pooling component, which is uniquely used in NFM. Speci cally, if we replace the Bi-Interaction pooling with concatenation and apply a tower-structure MLP (or residual units <ref type="bibr" target="#b14">[14]</ref>) to hidden layers, we can recover the Wide&amp;Deep (or DeepCross) model. An obvious limitation of the concatenation operation is that it does not account for any interaction between features. As such, these deep learning approaches have to rely entirely on the following deep layers to learn meaningful feature interactions, which unfortunately can be di cult to train in practice (cf. Section 2.2.1). Our use of Bi-Interaction pooling captures second-order feature interactions in the low level, which is more informative than the concatenation operation. is greatly facilitates the subsequent hidden layers of NFM to learn useful higher-order feature interactions in a much easier way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Time Complexity Analysis.</head><p>We have shown in Equation (4) that Bi-Interaction pooling can be e ciently computed in O(kN x ) time, which is the same as FM. en the additional costs are caused by the hidden layers. For hidden layer l, the matrixvector multiplication is the main operation which can be done in O(d l ?1 d l ), where d l denotes the dimension of the l-th hidden layer and d 0 = k. e prediction layer only involves inner product of two vectors, for which the complexity is O(d L ). As such, the overall time complexity for evaluating a NFM model is</p><formula xml:id="formula_11">O(kN x + L l =1 d l ?1 d l )</formula><p>, which is the same as that of Wide&amp;Deep and DeepCross.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>NFM can be applied to a variety of prediction tasks, including regression, classi cation and ranking. To estimate model parameters of NFM, we need to specify an objective function to optimize. For regression, a commonly used objective function is the squared loss:</p><formula xml:id="formula_12">L r e = x?X (? (x) ? (x)) 2 ,<label>(9)</label></formula><p>where X denotes the set of instances for training, and (x) denotes the target of instance x. e regularization terms are optional and omi ed here, since we found that some techniques in neural network modelling such as dropout can well prevent NFM from over ing. For classi cation task, we can optimize the hinge loss or log loss <ref type="bibr" target="#b16">[16]</ref>. For ranking task, we can optimize pairwise personalized ranking loss <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b37">37]</ref> or contrastive max-margin loss <ref type="bibr" target="#b43">[43]</ref>. In this work, we focus on the regression task and optimize the squared loss of Equation <ref type="bibr" target="#b8">(9)</ref>. e optimization for ranking/classi cation tasks can be done in the same way.</p><p>Stochastic gradient descent (SGD) is a universal solver for optimizing neural network models. It iteratively updates the parameters until convergence. In each time, it randomly selects a training instance x, updating each model parameter towards the direction of its negative gradient:</p><formula xml:id="formula_13">? = ? ? ? ? 2(? (x) ? (x)) d? (x) d? ,<label>(10)</label></formula><p>where ? ? ? is a trainable model parameter, and ? &gt; 0 is the learning rate that controls the step size of gradient descent. As NFM is a multi-layered neural network model, the gradient of? (x) w.r.t. to each model parameter can be derived using the chain rule. Here we give only the di erentiation of the Bi-Interaction pooling layer, since other layers are just standard operations in neural network modelling and have been widely implemented in ML toolkits like TensorFlow and Keras.</p><formula xml:id="formula_14">d f BI (V x ) dv i = ( n j=1 x j v j )x i ? x 2 i v i = n j=1, j i x i x j v j .<label>(11)</label></formula><p>As such, for end-to-end neural methods, upon plugging in the Bi-Interaction pooling layer, they can still be learned end-to-end. To leverage the vectorization and parallelism speedup of modern computing platforms, mini-batch SGD is more widely used in practice, which samples a batch of training instances and updates model parameters based on the batch. In our implementation, we use mini-batch Adagrad <ref type="bibr" target="#b9">[10]</ref> as the optimizer, rather than the vanilla SGD. Its main advantage is that the learning rate can be self adapted during the training phase, which eases the pain of choosing a proper learning rate and leads to faster convergence than the vanilla SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Dropout. While neural network models have strong representation ability, they are also easy to over t the training data. Dropout <ref type="bibr" target="#b33">[33]</ref> is a regularization technique for neural networks to prevent over ing. e idea is to randomly drop neurons (along with their connections) of the neural network during training. at is, in each parameter update, only part of the model parameters that contributes to the prediction of? (x) will be updated. rough this process, it can prevent complex co-adaptations of neurons on training data. It is important to note that in the testing phase, dropout must be disabled and the whole network is used for estimating? (x). As such, dropout can also be seen as performing model averaging with smaller neural networks <ref type="bibr" target="#b33">[33]</ref>.</p><p>In NFM, to avoid feature embeddings co-adapt to each other and over t the data, we propose to adopt dropout on the Bi-Interaction layer. Speci cally, a er obtaining f BI (V x ) which is a k-dimensional vector of latent factors, we randomly drop ? percent of latent factors, where ? is termed as the dropout ratio. Since NFM with no hidden layer degrades to the FM model, it can be seen as a new way to regularize FM. Moreover, we also apply dropout on each hidden layer of NFM to prevent the learning of higher-order feature interactions from co-adaptations and over ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.2</head><p>Batch Normalization. One di culty of training multilayered neural networks is caused by the fact of covariance shi <ref type="bibr" target="#b20">[20]</ref>. It means that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. As a result, the later layer needs to adapt to these changes (which are o en noisy) when updating its parameters, which adversely slows down the training. To address the problem, Io e and Szegedy <ref type="bibr" target="#b20">[20]</ref> proposed batch normalization (BN), which normalizes layer inputs to a zero-mean unit-variance Gaussian distribution for each training mini-batch. It has been shown that BN leads to faster convergence and be er performance in several computer vision tasks <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b42">42]</ref>.</p><p>Formally, let the input vector to a layer be x i ? R d and all input vectors to the layer of the mini-batch be B = {x i }, then BN normalizes x i as:</p><formula xml:id="formula_15">BN (x i ) = ? ( x i ? ? B ? B ) + ?,<label>(12)</label></formula><p>where <ref type="bibr" target="#b1">2</ref> denotes the mini-batch variance, and ? and ? are trainable parameters (vectors) to scale and shi the normalized value to restore the representation power of the network. Note that in testing, BN also needs to be applied, where ? B and ? B are estimated from the whole training data.</p><formula xml:id="formula_16">? B = 1 | B | i ?B x i denotes the mini-batch mean, ? 2 B = 1 | B | i ?B (x i ? ? B )</formula><p>In NFM, to avoid the update of feature embeddings changing the input distribution to hidden layers or prediction layer, we perform BN on the output of the Bi-Interaction pooling. For each successive hidden layer, the BN is also applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>As the key contributions of this work are on Bi-Interaction pooling in neural network modelling and the design of NFM for sparse data prediction, we conduct experiments to answer the following research questions: In what follows, we rst present the experimental se ings, followed by answering the above research questions one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets. We experimented with two publicly accessible datasets: Frappe 3 and MovieLens 4 :</head><p>1. Frappe. Frapp? is a context-aware app discovery tool. is dataset is constructed by Baltrunas et al. <ref type="bibr" target="#b0">[1]</ref>. It contains 96, 203 app usage logs of users under di erent contexts. Besides user ID and app ID, each log contains 8 context variables, including weather, city and daytime (e.g., morning or a ernoon). We converted each log (i.e., user ID, app ID and all context variables) to a feature vector using one-hot encoding, resulting in 5, 382 features in total. A target value of 1 means the user has used the app under the context. is is the Full version of the latest Movie-Lens data published by GroupLens <ref type="bibr" target="#b12">[13]</ref>. As this work concerns higher-order interactions between features, we study the task of personalized tag recommendation rather than collaborative ltering <ref type="bibr" target="#b16">[16]</ref> that considers the second-order interactions only. e tagging part of the data includes 668, 953 tag applications of 17, 045 users on 23, 743 items with 49, 657 distinct tags. We converted each tag application (i.e., user ID, movie ID and tag) to a feature vector, resulting in 90, 445 features in total. A target value of 1 means the user has assigned the tag to the movie.</p><p>As both original datasets contain positive instances only (i.e., all instances have target value 1), we sampled two negative instances to pair with one positive instance to ensure the generalization of the predictive model. For each log of Frappe, we randomly sampled two apps that the user has not used in the context; for each tag application of MovieLens, we randomly sampled two tags that the user has not assigned to the movie. Each negative instance is assigned to a target value of ?1. <ref type="table" target="#tab_0">Table 1</ref> summarizes the statistics of the nal evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Protocols.</head><p>We randomly split the dataset into training (70%), validation (20%), and test (10%) sets. e validation set was used for tuning hyper-parameters and the nal performance comparison was conducted on the test set. e study of NFM properties (i.e., the answering of RQ1 and RQ2) was performed on the validation set, which can also re ect how we choose the optimal hyper-parameters. To evaluate the performance, we adopted root mean square error (RMSE), where a lower RMSE score indicates a be er performance. Note that RMSE has been widely used for evaluating regression tasks such as recommendation with explicit ratings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">30]</ref> and click-through rate prediction <ref type="bibr" target="#b24">[24]</ref>. We rounded up the prediction of each model to 1 or ?1 if it was out of the range. e one-sample paired t-test was performed to judge the statistical signi cance where necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.3</head><p>Baselines. We implemented NFM using TensorFlow 5 . We compared with the following competitive embedding-based models that are speci cally designed for sparse data prediction:</p><p>-LibFM <ref type="bibr" target="#b28">[28]</ref>. is is the o cial implementation 6 of FM released by Rendle. It has shown strong performance for personalized tag recommendation and context-aware prediction <ref type="bibr" target="#b30">[30]</ref>. We used the SGD learner for a fair comparison with other methods which were all optimized with SGD (or its variants).</p><p>-HOFM. is is the TensorFlow implementation 7 of higherorder FM, as described in <ref type="bibr" target="#b27">[27]</ref>. We experimented with order size 3, since the MovieLens data concerns the ternary relationship between users, movies and tags.</p><p>-Wide&amp;Deep <ref type="bibr" target="#b8">[9]</ref>. As introduced in Section 2.2, the deep part rst concatenates feature embeddings, followed by a MLP to model ut vs. L2_reg feature interactions. As the structure of a DNN is di cult to be fully tuned, we used the same structure as reported in their paper, which has three layers with size 1024, 512 and 256, respectively. While the wide part (which is a linear regression model) is subjected to design to incorporate cross features, we used the raw features only for a fair comparison with FM and NFM.</p><p>-DeepCross <ref type="bibr" target="#b31">[31]</ref>. It applies a multi-layered residual network on the concatenation of feature embeddings for learning feature interactions. We used the same structure as reported in their paper, which stacks 5 residual units (each unit has two layers) with the hidden dimension 512, 512, 256, 128 and 64, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Parameter Se ings.</head><p>To fairly compare models' capability, we learned all models by optimizing the square loss (Equation <ref type="formula" target="#formula_12">(9)</ref>). e learning rate was searched in [0.005, 0.01, 0.02, 0.05] for all methods. To prevent over ing, we tuned the L 2 regularization for linear models LibFM and HOFM in [1e ?6 , 5e ?6 , 1e ?5 , ..., 1e ?1 ], and the dropout ratio for neural network models Wide&amp;Deep, Deep-Cross and NFM in [0, 0.1, 0.2, ..., 0.9]. Note that we found that dropout can well regularize the hidden layers of Wide&amp;Deep and NFM; however it did not work well for the residual units of Deep-Cross. Besides LibFM that optimized FM with the vanilla SGD, all other methods were optimized with mini-batch Adagrad <ref type="bibr" target="#b9">[10]</ref>, where the batch size was set to 128 for Frappe and 4096 for Movie-Lens. Note that the batch size was selected by considering both training time and convergence rate, as a larger batch size usually led to faster training per epoch but slower convergence. For all methods, the early stopping strategy was performed, where we stopped training if the RMSE on validation set increased for 4 successive epochs. Without special mention, we show the results of embedding size 64, and more results of larger embedding sizes are shown in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Study of Bi-Interaction Pooling (RQ1)</head><p>We empirically study the Bi-Interaction pooling operation. To avoid other components (e.g., hidden layers) a ecting the analysis, we study the NFM-0 model that directly projects the output of Bi-Interaction pooling to prediction score with no hidden layer. As discussed in Section 3.1.1, NFM-0 is identical to FM as the trainable h does not impact model's expressiveness. We rst compare dropout with traditional L 2 regularization for preventing model over ing, and then explore the impact of batch normalization. layer and L 2 regularization on feature embeddings. e performance of linear regression (LR) is also shown for benchmarking the performance of prediction that does not consider feature interactions. First, LR leads to very poor performance, highlighting the importance of modelling interactions between sparse features for prediction. Second, we see that both L 2 regularization and dropout can well prevent over ing and improve NFM-0's generalization to unseen data. Between the two strategies, dropout o ers be er performance. Speci cally, on Frappe, using a dropout ratio of 0.3 leads to a lowest validation error of 0.3562, which is signi cantly be er than that of L 2 regularization 0.3799. One reason might be that enforcing L 2 regularization only suppresses the values of parameters in each update numerically, while using dropout can be seen as ensembling multiple sub-models <ref type="bibr" target="#b33">[33]</ref>, which can be more e ective. Considering the genericity of FM that subsumes many factorization models, we believe this is a new interesting nding, meaning that dropout can also be an e ective strategy to address over ing of linear latent-factor models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dropout Improves Generalization.</head><p>To be more clear about the e ect of dropout, we show the training and validation error of each epoch of NFM-0 with and without dropout in <ref type="figure">Figure 4</ref>. Both datasets show that with a dropout ratio of 0.3, although the training error is higher, the validation error becomes lower. is demonstrates the ability of dropout in preventing over ing and as such, be er generalization can be achieved.  enabled with a ratio of 0.3, and the learning rate is set to 0.02. Focusing on the training error, we can see that BN leads to a faster convergence; on Frappe, when BN is applied, the training error of epoch 20 is even lower than that of epoch 60 without BN; and the validation error indicates that the lower training error is not over ing -in fact, <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b20">20]</ref> showed that by addressing the internal covariate shi with BN, the model's generalization ability can be improved. Our result also veri es this point, where using BN leads to slight improvement (although the improvement is not statistically signi cant). Furthermore, we notice that BN makes the learning less stable, as evidenced by the larger performance uctuation of blue lines. is is caused by our use of dropout and BN together, as randomly dropping neurons can change the input distribution normalized by BN. It is an interesting direction to e ectively combine BN and dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Hidden Layers (RQ2)</head><p>e hidden layers of NFM play a pivotal role in capturing higherorder interactions between features. To explore the impact, we rst add one hidden layer above the Bi-Interaction layer and slightly overuse the name NFM to indicate this speci c model. To ensure the same model capability with NFM-0, we set the size of hidden layer the same as the embedding size. <ref type="figure" target="#fig_6">Figure 6</ref> shows the validation error of NFM w.r.t. di erent activation functions and dropout ratios for the hidden layer. e performance of LibFM and NFM-0 are also shown for benchmarking purposes. First and foremost, we observe that by using nonlinear activations, NFM's performance is improved with a large margin -compared to NFM-0 which has a similar performance with LibFM, the relative improvement is 11.3% and 5.2% for Frappe and MovieLens, respectively.</p><p>is highlights the importance of modelling higher-order feature interactions for quality prediction. Among the di erent non-linear activation functions, there is no obvious winner. Second, when we use the identity function as the activation function, i.e., the hidden layer performs a linear transformation, NFM does not perform that well. is provides evidence to the necessity of learning higher-order feature interactions with non-linear functions.</p><p>To see whether a deeper NFM can further improve the performance, we stack more ReLU layers above the Bi-Interaction layer. As it is computationally expensive to tune the size and dropout ratio for each hidden layer separately, we use the same se ing for all layers and tune them the same way as NFM-1. As can be seen from <ref type="table" target="#tab_1">Table 2</ref>, when we stack more layers, the performance is not further improved, and best performance is when we use one hidden layer only. We have also explored other designs for hidden layers, such as the tower structure and residual units, however, the performance is still not improved. We think the reason is because the Bi-Interaction layer has encoded informative second-order feature interactions, and based on which, a simple non-linear function is su cient to capture higher-order interactions. To verify this, we replaced the Bi-Interaction layer with concatenation (which leads to the same architecture as Wide&amp;Deep), and found that the performance can be gradually improved with more hidden layers (up to three); however, the best performance achievable is still inferior to that of NFM-1. is demonstrates the value of using a more informative operation for low-level layers, which can ease the burden of higher-level layers for learning meaningful information. As a result, a deep structure becomes not necessarily required.  <ref type="figure" target="#fig_7">Figure 7</ref> shows the state of each epoch of NFM-1 with and without pre-training. First, we can see that by using FM embeddings as pre-training, NFM exhibits extremely fast convergence -on both datasets, with 5 epochs only, the performance is on par with 40 epochs of NFM that is trained from scratch (with BN enabled). Second, we nd that pre-training does not improve NFM's nal performance, and a random initialization can achieve a result that is slightly be er than that with pre-training.</p><p>is demonstrates the robustness of NFM, which is relatively insensitive to parameter initialization. In contrast to the huge impact of pre-training on Wide&amp;Deep and DeepCross (cf. <ref type="figure" target="#fig_0">Figure 1</ref>) that improves both their convergence and nal performance, we draw the conclusion that  NFM is much easier to train and optimize, which is due largely to the informative and e ective Bi-Interaction pooling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison (RQ3)</head><p>We now compare with state-of-the-art methods. For NFM, we use one hidden layer with ReLU as the activation function, since the baselines DeepCross and Wide&amp;Deep also choose ReLU in their original papers. Note that the most important hyper-parameter for NFM is the dropout ratio, which we use 0.5 for the Bi-Interaction layer and tune the value for the hidden layer. <ref type="figure" target="#fig_8">Figure 8</ref> plots the test RMSE w.r.t. di erent number of latent factors (i.e., embedding sizes), where Wide&amp;Deep and DeepCross are pre-trained with FM to be er explore the two methods. <ref type="table" target="#tab_2">Table 3</ref> shows the concrete scores obtained on factors 128 and 256, and the number of model parameters of each method. e scores of Wide&amp;Deep and DeepCross without pre-training are also shown. We have the following three key observations. First and foremost, NFM consistently achieves the best performance on both datasets with the fewest model parameters besides FM. is demonstrates the e ectiveness and rationality of NFM in modelling higher-order and non-linear feature interactions for prediction with sparse data. e performance is followed by Wide&amp;Deep, which uses a 3-layer MLP to learn feature interactions. We have also tried deeper layers for Wide&amp;Deep, however the performance has not been improved. is further veri es the utility of using the informative Bi-Interaction pooling in the low level.</p><p>Second, we observe that HOFM shows slight improvement over FM with 1.45% and 1.04% average improvement on Frappe and MovieLens, respectively. is sheds light on the limitation of FM that models only the second-order feature interactions, and thus the usefulness of modelling higher-order interactions. Meanwhile, the large performance gap between HOFM and NFM re ects the value of modelling higher-order interactions in a non-linear way, since HOFM models higher-order interactions linearly and uses much more parameters than NFM.</p><p>Lastly, the relatively weak performance of DeepCross reveals that deeper learnings are not always be er, as DeepCross is the deepest method among all baselines that utilizes a 10-layer network. On Frappe, DeepCross only achieves a comparable performance with the shallow FM model, while it underperforms FM signi cantly on MovieLens. We believe that the reasons are due to optimization di culties and over ing (as evidenced by the worse performance on factors 128 and 256).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this work, we proposed a novel neural network model NFM, which brings together the e ectiveness of linear factorization machines with the strong representation ability of non-linear neural networks for sparse predictive analytics. e key of NFM's architecture is the newly proposed Bi-Interaction operation, based on which we allow a neural network model to learn more informative feature interactions at the lower level. Extensive experiments on two real-world datasets show that with one hidden layer only, NFM signi cantly outperforms FM, higher-order FM, and state-of-the-art deep learning approaches Wide&amp;Deep and DeepCross. e work represents the rst step towards bridging the gap between linear models and deep learning. Linear models, such as various factorization methods, have shown to be e ective for many IR and DM tasks and are easy to interpret. However, their limited expressiveness may hinder the performance when modelling real-world data with complex inherent pa erns. While deep learning models have exhibited great expressive power and yielded immense success on speech processing and computer vision, their performance is still unsatisfactory for IR tasks, such as collaborative ltering <ref type="bibr" target="#b16">[16]</ref>. In our view, one reason is that most data of IR and DM tasks are naturally sparse; and to date, there still lacks e ective deep learning solutions for prediction with sparse data. By connecting neural networks with FM -one of the most powerful linear models for supervised learning -we are able to design a simple yet e ective deep learning solution for sparse data prediction. With recent developments on GPU platforms, it is not technically di cult to build very deep models with hundreds or even thousands of layers <ref type="bibr" target="#b14">[14]</ref>. However, deeper models do not necessarily lead to be er results, since deeper models are less transparent and more di cult to optimize and tune. As such, we expect future research on deep learning for IR should focus more on designing be er neural components or architectures for speci c tasks, rather than relying on deeper models for minor improvements. Our proposed Bi-Interaction pooling is an e ective neural component for sparse feature modelling, reducing the demand for deeper structure for quality prediction. In future, we will improve the e ciency of NFM by resorting to hashing techniques <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b41">41]</ref> to make it more suitable for large-scale applications and study its performance for other IR tasks, such as search ranking and targeted advertising. While this work endows FM with non-linearities from predictive model perspective, another viable solution for incorporating nonlinearities is to extend the objective function with regularizers like the graph Laplacian <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b34">34]</ref>. Lastly, we are interested in exploring the Bi-Interaction pooling for recurrent neural networks (RNNs) for sequential data modelling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Training and test error of each epoch of Wide&amp;Deep and DeepCross on Frappe. Random initialization of embeddings leads to poor performance, worse than LibFM (a). Initializing using the embeddings learned by FM improves the performance signi cantly (b). is reveals optimization di culties for training deep neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Neural Factorization Machines model (the rstorder linear regression part is not shown for clarity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>RQ1</head><label></label><figDesc>Can Bi-Interaction pooling e ectively capture the secondorder feature interactions? How does dropout and batch normalization work for Bi-Interaction pooling? RQ2 Are hidden layers of NFM useful for capturing higherorder interactions between features and improving the expressiveness of FM? RQ3 How does NFM perform as compared to higher-order FM and the state-of-the-art deep learning methods Wide&amp;Deep and DeepCross?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Validation error of NFM-0 w.r.t. dropout on the Bi-Interaction layer and L 2 regularization on embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 3shows the validation error of NFM-0 w.r.t. dropout ratio on the Bi-InteractionDropout vs. No Dropout Training and validation error of each epoch of NFM-0 with and without dropout on Bi-Interaction layer. Training and validation error of each epoch of NFM-0 with and without BN on the Bi-Interaction layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 2 . 2</head><label>22</label><figDesc>Batch Normalization Speeds up Training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5shows the training and validation error of each epoch of NFM-0 with and without BN on the Bi-Interaction layer. e dropout is Validation error of LibFM, NFM-0 and NFM with di erent activation functions on the rst hidden layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Training and validation error of each epoch of NFM-1 with and without pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Performance comparison on the test set w.r.t. di erent embedding sizes. LibFM, HOFM and HOFM are trained from random initialization; Wide&amp;Deep and DeepCross are pre-trained with FM feature embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the evaluation datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Instance# Feature# User# Item#</cell></row><row><cell>Frappe</cell><cell>288,609</cell><cell>5, 382</cell><cell>957</cell><cell>4,082</cell></row><row><cell cols="2">MovieLens 2,006,859</cell><cell>90, 445</cell><cell cols="2">17,045 23,743</cell></row><row><cell cols="2">2. MovieLens.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>NFM w.r.t. di erent number of hidden layers. Pre-training Speeds up Training. It is known that parameter initialization can greatly a ect the convergence and performance of DNNs [11, 16], since gradient-based methods can only nd local optima for DNNs. As have been shown in Section 2.2.1, initializing with feature embeddings learned by FM can signi cantly enhance Wide&amp;Deep and DeepCross. Now the question arises, how does pre-training impact NFM?</figDesc><table><row><cell cols="3">Methods Frappe MovieLens</cell></row><row><cell>NFM-0</cell><cell>0.3562</cell><cell>0.4901</cell></row><row><cell>NFM-1</cell><cell>0.3133</cell><cell>0.4646</cell></row><row><cell>NFM-2</cell><cell>0.3193</cell><cell>0.4681</cell></row><row><cell>NFM-3</cell><cell>0.3219</cell><cell>0.4752</cell></row><row><cell>NFM-4</cell><cell>0.3202</cell><cell>0.4703</cell></row><row><cell>4.3.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test error and number of trainable parameters for di erent methods on latent factors 128 and 256. M denotes "million"; * and * * denote the statistical signi cance for p &lt; 0.05 and p &lt; 0.01, respectively, compared to the best baseline.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Frappe</cell><cell></cell><cell></cell><cell></cell><cell>MovieLens</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Factors=128</cell><cell cols="2">Factors=256</cell><cell></cell><cell cols="2">Factors=128</cell><cell>Factors=256</cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>Param#</cell><cell>RMSE</cell><cell>Param#</cell><cell cols="2">RMSE</cell><cell>Param#</cell><cell>RMSE</cell><cell>Param#</cell><cell>RMSE</cell></row><row><cell></cell><cell cols="2">LibFM [28]</cell><cell></cell><cell>0.69M</cell><cell>0.3437</cell><cell>1.38M</cell><cell cols="2">0.3385</cell><cell>11.67M</cell><cell>0.4793</cell><cell>23.24M</cell><cell>0.4735</cell></row><row><cell></cell><cell>HOFM</cell><cell></cell><cell></cell><cell>1.38M</cell><cell>0.3405</cell><cell>2.76M</cell><cell cols="2">0.3331</cell><cell>23.24M</cell><cell>0.4752</cell><cell>46.40M</cell><cell>0.4636</cell></row><row><cell></cell><cell cols="2">Wide&amp;Deep [9]</cell><cell></cell><cell>2.66M</cell><cell>0.3621</cell><cell>4.66M</cell><cell cols="2">0.3661</cell><cell>12.72M</cell><cell>0.5323</cell><cell>24.69M</cell><cell>0.5313</cell></row><row><cell cols="4">Wide&amp;Deep (pre-train) DeepCross [31] DeepCross (pre-train) rmance Compare</cell><cell>2.66M 4.47M 4.47M</cell><cell>0.3311 0.4025 0.3388</cell><cell>4.66M 8.93M 8.93M</cell><cell cols="2">0.3246 0.4071 0.3548</cell><cell>12.72M 12.71M 12.71M</cell><cell>0.4595 0.5885 0.5084</cell><cell>24.69M 25.42M 25.42M</cell><cell>0.4512 0.5907 0.5130</cell></row><row><cell></cell><cell>NFM</cell><cell></cell><cell></cell><cell>0.71M</cell><cell>0.3127  *  *</cell><cell>1.45M</cell><cell cols="4">0.3095  *  *  11.68M 0.4557  *  23.31M 0.4443  *</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell>Frappe</cell><cell>LibFM</cell><cell></cell><cell></cell><cell></cell><cell>0.54</cell><cell></cell><cell>MovieLens</cell></row><row><cell></cell><cell>0.38</cell><cell></cell><cell></cell><cell cols="2">HOFM Wide&amp;Deep</cell><cell></cell><cell></cell><cell>0.52</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DeepCross</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RMSE</cell><cell>0.34 0.36</cell><cell></cell><cell></cell><cell>NFM</cell><cell></cell><cell></cell><cell>RMSE</cell><cell>0.5 0.48</cell><cell cols="2">LibFM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HOFM</cell></row><row><cell></cell><cell>0.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.46</cell><cell cols="2">Wide&amp;Deep DeepCross</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.44</cell><cell>NFM</cell></row><row><cell></cell><cell>16</cell><cell>32</cell><cell>64 Factors</cell><cell>128</cell><cell>256</cell><cell></cell><cell></cell><cell></cell><cell>16</cell><cell>32</cell><cell>64 Factors</cell><cell>128</cell><cell>256</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that for NFM-0, a trainable h can not improve the expressiveness of FM, since its impact on prediction can be absorbed into feature embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">h p://baltrunas.info/research-menu/frappe 4 h p://grouplens.org/datasets/movielens/latest</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Codes are available at h ps://github.com/hexiangnan/neural factorization machine 6 h p://www.libfm.org/ 7 h ps://github.com/ge y/t m</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Frappe: Understanding the usage and perception of mobile app recommendations in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<idno>abs/1505.03014</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A generic coordinate descent framework for learning from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Higher-order factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishihata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<title level="m">Polynomial networks and factorization machines: New insights and e cient training algorithms. In ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cross-platform app recommendation by jointly modeling ratings and texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM TOIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep ctr prediction in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A entive collaborative ltering: Multimedia recommendation with feature-and item-level a ention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context-aware image tweets modelling and recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DLRS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A mathematical framework for feature selection from real-world data with non-linear observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Genzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08852</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<title level="m">e movielens datasets: History and context</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Interactive Intelligent Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predicting the popularity of web 2.0 items based on user comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural collaborative ltering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-factorization machines: Modeling user interests and predicting individual decisions in twi er</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Doumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning visual semantic relationships for e cient visual retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Io E</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Field-aware factorization machines for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<editor>RecSys</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: A multifaceted collaborative ltering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exponential machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting response in mobile advertising with hierarchical importance-aware factorization machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finegold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Core: Context-aware open relation extraction with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Del</forename><surname>Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploiting ranking factorization machines for microblog retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Ieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast contextaware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Ieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep crossing: Web-scale modeling without manually cra ed combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classi cation by retrieval: Binarizing data and classi er</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from over ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable semi-supervised learning by e cient anchor graph regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visual classi cation by l1-hypergraph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning hierarchical representation model for nextbasket recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Item silk road: Recommending items from information domains to social users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A entional factorization machines: Learning the weight of feature interactions via a ention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to a end and to rank with word-entity duets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Geoburst: Real-time local event detection in geo-tagged tweet streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discrete collaborative ltering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Play and rewind: Optimizing binary representations of videos by self-supervised temporal hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A ribute-augmented semantic hierarchy: Towards bridging semantic gap and intention gap in image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep learning over multi-eld categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
