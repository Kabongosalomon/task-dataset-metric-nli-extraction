<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Searching the Deployable Convolution Neural Networks for GPUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhan</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Salian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Kierat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Migacz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Fit</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florea</forename><surname>Nvidia</surname></persName>
						</author>
						<title level="a" type="main">Searching the Deployable Convolution Neural Networks for GPUs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Customizing Convolution Neural Networks (CNN) for production use has been a challenging task for DL practitioners. This paper intends to expedite the model customization with a model hub that contains the optimized models tiered by their inference latency using Neural Architecture Search (NAS). To achieve this goal, we build a distributed NAS system to search on a novel search space that consists of prominent factors to impact latency and accuracy. Since we target GPU, we name the NAS optimized models as GPUNet, which establishes a new SOTA Pareto frontier in inference latency and accuracy. Within 1ms, GPUNet is 2x faster than EfficientNet-X and FBNetV3 with even better accuracy. We also validate GPUNet on detection tasks, and GPUNet consistently outperforms EfficientNet-X and FB-NetV3 on COCO detection tasks in both latency and accuracy. All of these data validate that our NAS system is effective and generic to handle different design tasks. With this NAS system, we expand GPUNet to cover a wide range of latency targets such that DL practitioners can deploy our models directly in different scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The progress of neural networks has decoupled from the actual deployment for a long time. Deep Learning (DL) researchers have been dedicated to inventing new building blocks, while DL engineers deploy these building blocks in real-world tasks, painstakingly recombine them to find architectures that meet the design requirements. Most of the time, we can simplify these requirements to find the best-performing architecture on the target device (e.g., GPUs) within a specific latency budget. Though there are many exciting advancements in the neural network designs, e.g., the residual connection <ref type="bibr" target="#b11">[13]</ref>, Inverted Residual Block (IRB) <ref type="bibr" target="#b26">[28]</ref> and the attention <ref type="bibr" target="#b30">[32]</ref>, deploying these network designs remains challenging and laborious; and this is the problem to be addressed in this paper.</p><p>Our solution to alleviate the gap between the DL research and the actual deployment is to propose a set of optimized Expert (A) Accelerate the model customization using GPUNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPU TensorRT Latency</head><p>Top-1 GPUNet  Convolution Neural Networks for each type of GPUs tiered by their optimized inference latency (e.g., post-processed by TensorRT <ref type="bibr" target="#b1">[3]</ref> or OpenVINO <ref type="bibr" target="#b0">[2]</ref>). Specifically, we deliver a table of models, an entry of which is the result of model optimization from maximizing the accuracy subject to the limit of inference latency on a GPU. This table enables DL engineers to directly query the optimized neural architecture w.r.t the design requirements to expedite the customization process on expensive models. We resort to Neural Architecture Search (NAS) to design models in this table. Recently NAS has shown promising results to automate the design of network architectures in many tasks <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37]</ref>. Therefore, NAS can be a handy tool since we need to design many models for many latency limits for different GPUs. When models are ready to deploy, we measure post-processed TensorRT engine latency, i.e., including quantization, layer/tensor fusion, kernel tun-ing, and other system side model optimizations. Finally, we design our model toward NVIDIA enterprise GPU products for their broad adoption by the community today.</p><p>We built a novel distributed NAS system to achieve our goal. Following the prior works, our NAS consists of 3 modules, a search space, an evaluation module, and a search method. The search space provisions networks following the predefined patterns; the search method proposes the most promising network based on the priors. The evaluation module returns the performance of the proposed network either by training or estimation from a supernet <ref type="bibr" target="#b43">[45]</ref>. Our search space constructs a network by stacking convolution layers, IRBs, and Fused-IRBs used in EfficientNet <ref type="bibr" target="#b29">[31]</ref>. However, our search space is the most comprehensive by far that includes filter numbers (#filters), kernel sizes, the number of layers (#layers) or IRBs (#IRBs) in a stage, and the input resolution. Within an IRB or Fused-IRB, we also search for the expansion ratio, the activation type, with or without the Squeeze-Excitation (SE) layer. All of these factors are identified as prominent factors to affect latency and accuracy. Therefore, this search space enables us to better leverage the accuracy and latency than prior works, e.g., the fixed filter pattern in NASNet <ref type="bibr" target="#b44">[46]</ref> and the fixed activation and SE pattern in FBNetV3 <ref type="bibr" target="#b8">[10]</ref>; and the search also enables us to find a better policy than the fixed scaling strategy in EfficientNet <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">30]</ref>. To support such a complex search space, we choose to evaluate a network candidate by training. Although this approach is far more expensive than the supernet approach, the evaluation is more accurate in ranking the architectures <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b43">45]</ref>. And we can avoid many unresolved problems in building a supernet for our search space, e.g., supporting multiple types of activation, activating/deactivating SE, and variable filter sizes. We built a client-server-style distributed system to tackle the computation challenges, and it has robustly scaled to 300 A100 GPUs (40 DGX-A100 nodes) in our experiments. Finally, we adopt the LA-MCTS guided Bayesian Optimization (BO) <ref type="bibr" target="#b33">[35]</ref> as the search method for its superior sample efficiency demonstrated in the recent black-box optimization challenges [1].</p><p>We name the NAS optimized CNNs as GPUNet, and GPUNet has established a new SOTA Pareto front in the latency and accuracy in <ref type="figure" target="#fig_1">Fig. 1</ref>. We measure the latency of GPUNet using TensorRT, so GPUNet is directly reusable to DL practitioners. Particularly, GPUNet-1 is nearly 2x faster and 0.5% better in accuracy than FBNetV3-B and EfficientNet-X-B2-GPU, respectively. We also validate GPUNet on COCO detection tasks, and GPUNet still consistently outperforms EfficientNet-X and FBNetV3. All of these data validate that our NAS system is effective and generic in designing various tasks. Although this paper only shows a few GPUNet for comparisons, the complete model hub tiered by the inference latency is still ongoing, and we will release them with the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Today running DL models locally on an edge device or hosting the model as a service on the enterprise-level GPUs in data centers are two major ways for the model deployment. Edge devices, such as your smartphones or the embedded system for the self-driving, are often equipped with a small CPU, limited RAM, and slow inter-connect. Given the constrained resource, a model's #FLOPS or MACs can correlate well with the latency on such devices, driving many works to propose low FLOPS operators for the faster deployment on the edge devices <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b26">28]</ref>, e.g., depthwise separable convolutions. Accelerating the inference on mobile devices has been a hot research topic in recent years <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b36">38]</ref>.</p><p>Deploying models on the GPU requires different optimizations from the edge devices. Since GPU has a far more powerful architecture than edge devices, we need to consider the device saturation, parallelism and compute/memory-bound operators, etc., for the deployment <ref type="bibr" target="#b16">[18]</ref>. Although GPU has dominated the MLPerf inference benchmark for years, only a few works optimize the CNN deployment on GPUs. One line of these works is to propose fast operators. RegNet <ref type="bibr" target="#b23">[25]</ref> accelerates the inference by optimizing the search space to select simple, GPUfriendly operators. ResNetXt proposes the split attention operator to improve the accuracy and inference latency <ref type="bibr" target="#b42">[44]</ref>. Another line of work is to optimize the structure. TRes-Net <ref type="bibr" target="#b25">[27]</ref> optimizes operators in ResNet-50, including SE layers and BatchNorm, to improve the inference on GPU. At the same time, EfficientNet-X <ref type="bibr" target="#b16">[18]</ref> proposes a latencyaware scaling method for designing the fast EfficientNet to the GPU/TPU and uses a roofline model to explain the gap between the FLOPS and latency on GPUs. Rather than using a fixed scaling policy, our work treats the model optimization as a black box, searching for the fast architectures for GPUs using the TensorRT optimized inference latency. Therefore, we can better trade-off the latency and accuracy than EfficientNet-X, and our final networks are directly deployable on GPUs.</p><p>We use NAS to build proposed networks, and here we review the recent advances in NAS. Early works in NAS, e.g., NASNet <ref type="bibr" target="#b44">[46]</ref>, models CNN as a Direct Acycle Graph (DAG). While EfficientNet quickly gains popularity for its good performance on ImageNet <ref type="bibr" target="#b37">[39]</ref>. This paper reuses the building blocks from EfficientNetV2 to find fast architectures. Recently transformer <ref type="bibr" target="#b19">[21]</ref> and Multi-Layer Perception (MLP) <ref type="bibr" target="#b18">[20]</ref> starts to emerge as promising alternatives to ConvNet; we leave the NAS on these search spaces as future work. Our paper evaluates each network independently by training end-to-end despite the popularity of the supernet approach. ENAS <ref type="bibr" target="#b22">[24]</ref> proposed the supernet, which is an over-parameterized network to approximate the performance of sub-networks. Although the supernet significantly reduces the computation requirement for NAS, the rank predicted by supernet can be inaccurate <ref type="bibr" target="#b43">[45]</ref>. Besides, training the supernet is non-trivial <ref type="bibr" target="#b39">[41]</ref> and the construction of supernet to support variable activation, expansion ratio, and filter sizes, etc., still remains an open problem. Therefore, we perform NAS using a distributed system to avoid unresolved issues of supernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We built a novel distributed NAS framework to automate the model design for GPUs. Our NAS system consists of a search space, a search algorithm, and an evaluation method following the existing NAS framework. First, the search algorithm selects networks from the search space, querying its performance using the evaluation method. Then the search algorithm refines its decision in the next iteration by leveraging all the evaluated network accuracy pairs.</p><p>Our NAS consists of two stages, 1) categorizing networks by the inference latency and 2) performing NAS on networks within a latency group to optimize the accuracy. In the first stage ( <ref type="figure">Fig. 2</ref>.A), we use Sobol sampling <ref type="bibr" target="#b27">[29]</ref> to draw network candidates from the high-dimensional search space evenly, approximate the network latency by using the latency look-up table, then categorize the network into a sub-search space, e.g., networks &lt; 0.5ms. We approximate the inference latency by summing up the latency of each layer from a latency lookup table. The latency table uses the input data shape and layer configurations as the key to the latency of a layer. In the second stage ( <ref type="figure">Fig. 2</ref>.B), Bayesian optimization consumes a sub-space to find the best performing network within the latency range of the sub-space. We built a client-server distributed framework to perform NAS. The search algorithm runs on the server, proposing network for a client. The client will return the accuracy and network after training. The following elaborates each components and its design justifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space</head><p>The search space prescribes the general structure of network candidates, and our search space is inspired by Effi-cientNet <ref type="bibr" target="#b29">[31]</ref>. Please note our search framework is generic to support various search spaces, e.g., designing visual transformer or MLP for visual tasks. Although transformer has shown excellent performance recently <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b19">21]</ref>, here we focus on ConvNet due to the better support from current TensorRT that performs critical performance optimizations for the fast inference on GPUs. We leave the NAS on the transformer-or MLP-based vision models as future work. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the details of our search space used in this paper. Our search space consists of 8 stages. Here we search for the configurations of each stage, and the lay- ers within a stage share the same configurations. The first two stages are to search for the head configurations using convolutions. Inspired by EfficientNet-V2 <ref type="bibr" target="#b29">[31]</ref>, the 2 and 3 stages uses Fused-IRB <ref type="bibr" target="#b29">[31]</ref>. But we observed the increasing latency after replacing the rest IRB with Fused-IRB. From the stage 4 to 7, we use IRB as the basic layers. The column #Layers shows the range of #layers in the stage, e.g., <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b8">10]</ref> at stage 4 means that the stage can have 3 to 10 IRBs. And the column Filters shows the range of filters for the layers in the stage (see table note for details). Our search space also tunes the expansion ratio, activation types, kernel sizes, and the Squeeze Excitation(SE) <ref type="bibr" target="#b14">[16]</ref> layer inside the IRB/Fused-IRB. Finally, the dimensions of the input image increase from 224 to 512 at the step of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Justifications of the Search Space</head><p>Unlike prior works, our search is guided by the accuracy and the TensorRT optimized inference latency. In a good experiment design, we should identify the most relevant factors <ref type="bibr" target="#b15">[17]</ref> to the design targets, i.e., fast and accurate networks. <ref type="table">Table.</ref> 1 demonstrates the several prominent factors found by us to impact the latency and accuracy. Here we provide the empirical data to support our decisions.</p><p>? #Layers and Filters: extensive evidence from published results demonstrates that a deep or wide network can perform better than the shallow or narrow variants <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref> while adding layers or increasing filters slows down the inference. #Layers and Filters are essential design choices to the accuracy and latency.</p><p>? Activation: many past works have demonstrated that a good activation design can notably improve the final accuracy on ImageNet <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b24">26]</ref>, whereas <ref type="figure" target="#fig_3">Fig. 3</ref>.A shows a network using ReLU can be 4x faster than using PReLU. In general, the activation is the memory-  <ref type="figure">Figure 2</ref>. The work flow of proposed NAS framework. A proposed search space is first pruned by TensorRT inference latency in (B). Then we use a black box optimizer to iteratively explore the search space in (A). We implement a distributed search framework to exploit the parallelism in (C).  bound operation. And TensorRT supports ReLU, Sigmoid, and Tanh in the layer fusion, which explains the speed gap. Therefore, the choice of activation is an important factor in the latency and accuracy trade-off.</p><p>? Expansion: IRB or Fused-IRB internally expands the channel size using a 1x1 convolution, and the expansion ratio controls the size of the internal channel, i.e., expansion ratio x the input channel. The MobileNet <ref type="bibr" target="#b26">[28]</ref> paper claims that the larger channel expansion will help improve the capacity of the network and expressiveness. Our empirical results are also consistent with the claim. For example, the accuracy of a network drops 4 points on ImageNet top-1 after reducing the expansion from 6 to 2, whereas increasing the expansion incurs non-negligible costs <ref type="figure" target="#fig_3">(Fig 3.C)</ref>. These data suggest the expansion ratio is an important factor to search.</p><p>? Kernel: a large convolution kernel can increase the receipt field to improve the accuracy (more details  in <ref type="bibr" target="#b4">[6]</ref>). Still, it also increases the latency <ref type="figure" target="#fig_3">(Fig. 3.D)</ref>, which validates the choice of kernel size into the search space.</p><p>? SE: Squeeze-Excitation <ref type="bibr" target="#b14">[16]</ref> was introduced by the winning entry to ILSVRC 2017 that improved 25% over the previous year. After adding SE, <ref type="figure" target="#fig_3">Fig. 3</ref> shows the latency significantly increases. This justifies SE to be a factor in the search space.</p><p>? Image Resolution: EfficientNet <ref type="bibr" target="#b28">[30]</ref> clearly demonstrates the accuracy improvement by increasing the resolution, and <ref type="figure" target="#fig_3">Fig. 3</ref>.B shows the latency also significantly increases. So we search for the input image resolution for better accuracy and latency trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Network and Search Space Representations</head><p>Now we have the general picture of search space; the next is to find the proper representation that embodies the design. We use a vector of integers to encode a network sampled from the search space described in <ref type="table">Table.</ref> 1. The length of the vector is 41, and  for the stages from 2 to 7. So a network is an instance of the vector described in <ref type="table">Table.</ref> 2, and the range of every digit collectively define the search space in <ref type="table">Table.</ref> 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stratify Networks by Inference Latency</head><p>To design networks tiered by the inference latency, we choose to directly measure the latency of networks in the search space. Because the size of search space is exponentially large, we approximate the search space by sampling millions of networks from it. The sampling techniques is critical to capture the true distribution of search space, and here we use the Sobol sequence <ref type="bibr" target="#b3">[5]</ref>, the advantages of which is straightforward in <ref type="figure" target="#fig_6">Fig. 5</ref>. The sampling is a low cost operations that we can get millions of samples within a minute. The challenge is to measure the latency of sampled networks. Since TensorRT has dominated the MLPerf inference benchmark, we want to measure the inference latency optimized by TensorRT. Whereas, TensorRT takes minutes to build the inference engine for the measurement, which makes it infeasible to measure all the sampled networks.</p><p>We approximate a network's latency by adding up the latency of each layer. Although the search space renders 10 30 networks, the layers have limited configurations, e.g., 10 4 in our case. Therefore we can significantly speed up the latency measurement by building a latency table with the input data shape and the layer configurations as the key. Given a network, we iterate over layers to look up the latency. If a layer does not exist in the table, we only benchmark it and record its latency in the table. Finally, the network latency is the sum of the latency of all the layers. <ref type="figure" target="#fig_4">Fig. 4</ref> demonstrates that the table estimated latency is close to the network's actual latency, and the table estimation is on average 75?s higher than the actual end-to-end measurement. Because a whole network subjects more opportunities for the layer fusion to TensorRT than the single layer. Benchmarking ? 10 4 layers is still an expensive task, and we parallelize the curation of the latency table over multi-GPUs to speed up the process from weeks to days.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Distributed Neural Architecture Search</head><p>We treat the network design as a black box. Guided by the reward, e.g. the validation accuracy, the search tunes the hyper-parameters prescribed in the search space to optimize the model performance ( <ref type="figure">Fig.2.A)</ref>. Here we elaborate the details of search algorithms and the evaluation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Search Algorithms</head><p>We choose LA-MCTS boosted Bayesian optimization (BO) <ref type="bibr" target="#b33">[35]</ref> as the search algorithm, which is one of the top entries to the 2020 NeurIPS black-box optimization competition [1]. Since we evaluate a network by training, the sample efficiency is critical to the overall cost. The competition results show that LA-MCTS boosted BO has demonstrated the leading sample efficiency among other BO variants and evolutionary algorithms; therefore, we adopt it in our experiments, and <ref type="figure">Fig. 2</ref> depicts the workflow. Some prior works <ref type="bibr" target="#b21">[23]</ref> define the problem as a Multi-Objective Optimization (MOO) to find the optimal Pareto frontier to the latency and accuracy. However, finding the Pareto Optimality is too fine-grained to the practice. For example, two solutions located on the Pareto frontier may have trivial differences in accuracy and latency, but finding these Pareto solutions is very expensive. Therefore, we organize the search space by latency before maximizing the accuracy. This also allows us to build a table of networks tiered by their inference latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evaluation</head><p>We choose to evaluate each proposed network by training and apply the early-stopping if the training curve is not promising. For networks from the same search space with similar latency, we use the same training receipt, as our practical experience suggests that tuning the training receipt brings up to 1% accuracy improvement at a tremendous cost. The details of our training receipts can be found at sec 6.1 in the supplemental material. The training will return the best validation accuracy to the search algorithm after 450 epochs.</p><p>Rather than democratizing NAS, this paper intends to maintain a set of NAS-optimized models using hundreds of GPUs for the community. We believe the training approach is necessary, although it is far more expensive than recent supernet approaches <ref type="bibr" target="#b22">[24]</ref>. First, extensive evidence in <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b43">45]</ref> demonstrates that supernet can be inaccurate in ranking the network candidates, and training a good supernet is non-trivial <ref type="bibr" target="#b39">[41]</ref>. Second, no supernet properly supports variable expansion ratios, image resolutions, and with/without SE. The training approach can circumvent all these problems at additional costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Distributed NAS</head><p>Now we're ready to put everything together. <ref type="figure">Fig. 2</ref> demonstrates that we implement a client and server distributed system to run NAS. Following sec 3.2, we start with generating networks in a latency range as the search space by sampling. Then we integrate the pruned search space into the search algorithm to run on the server. The server and clients exchange data via sockets. The clients will request a network from the server to evaluate if they are free and return the network and the best validation accuracy to the server. The search algorithm can leverage this information to propose the next network candidate. To validate the framework, we test the system with a few synthetic functions to ensure the performance metric increases along with the #samples. This framework is also generic to different search problems, and we can also use the same framework to search the architecture for Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section demonstrates the details of using the proposed search space and NAS system in designing GPUNet. Compared to existing works, GPUNet significantly improves the SOTA Pareto frontier in both the accuracy and inference latency <ref type="figure" target="#fig_1">(Fig. 1)</ref>. With a similar latency of 1.8ms, the accuracy of GPUNet is 1% better than the corresponding FBNet-V3 on ImageNet. With a similar 80.5 accuracy, GPUNet is 1.6x faster than FBNet. We start with describing the experiment setup, then discuss the main results. Finally, we show that GPUNet also effectively improves the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setup</head><p>Software Setup: we perform NAS directly on Ima-geNet <ref type="bibr" target="#b9">[11]</ref> that contains 1.28 million training and 50000 validation images in 1000 classes. Each network candidate is pre-trained with 300 epochs for the performance ranking with automatic mixed precision (AMP), then we finetune the top network for another 150 epochs. We use a modified training script from Pytorch Image Models <ref type="bibr" target="#b2">[4]</ref>   <ref type="figure">Figure 6</ref>. The architecture of searched GPUNet in <ref type="table">Table.</ref> 3. For stages 2, 3, 4 and 6, only the stride of first layer is 2 and the stride of rest layers is 1.</p><p>to train models. The training only uses random augmentation at the magnitude of 9 and a standard deviation of 0.5. The learning rate decays by .97 for every 2.4 epochs. Exponential Moving Average (EMA) is also in use with a decay rate of 0.9999. The crop percentage is set to 1, and the optimizer is RMSprop. We set NAS to focus on models with FP32+FP16 TensorRT GPU compute time 1 &lt; 2ms, which is more relevant in practice. For latency measurement, we use TensorRT-8.0.1. We export the onnx model and measure FP16 GPU compute latency using the trtexec --fp16 command-line on a standalone PCI-E NVIDIA GV100 GPU.</p><p>Machine Setup: we perform the training on DGX A100 with 8x A100 80 GB. Our system is flexible to allow training on preemptible (spot) instances. We launch the server at a dedicated node to propose networks and launch clients at preemptible instances. Each client checkpoints per epoch during the training in case of preemption. The server also consistently checkpoints its state for fault tolerance. <ref type="table">Table.</ref> 3 lists the recent SOTA baselines used in comparisons; There are two set of baselines that include and exclude distillation, respectively. The accuracy and the inference latency are two key metrics in our evaluations. Comparing the accuracy is easy to be fair but not on the inference latency, since the latency can be impacted by the software stack (e.g. runtime efficiency and system optimizations), GPUs, batch size and etc.. To ensure fairness, we transform (Only random augmentation) ? : We measure the latency (FP16 GPU compute time) using an explicit-shape at batch size 1. <ref type="table">Table 3</ref>. Comparisons of GPUNet to SOTA results. <ref type="figure" target="#fig_1">Fig. 1</ref> visualizes the table and shows that GPUNet-D dominates the baseline models in both the accuracy and inference latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Preparation of Baselines</head><p>all the baseline models into ONNX models for benchmarking with TensorRT. The inference benchmark exclusively runs on an NVIDIA GV100; and the workspace of Ten-sorRT is fixed at 10G across all runs. We benchmark the latency at the batch size = 1 with a explicit-shape, and report the average latency from 1000 runs. We take baseline models from either their original implementations or from PyTorch Image Models <ref type="bibr" target="#b2">[4]</ref>. Appendix Sec. 6.1.1 provides the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results on ImageNet1K</head><p>Table. 3 compares the performance of searched GPUNet to baselines, and <ref type="figure" target="#fig_1">Fig. 1</ref> visualizes the Pareto frontier of different models in the inference latency and accuracy. Please note we prepare two sets of different networks GPUNet and GPUNet-D, for the cases of with/without distillation. <ref type="figure" target="#fig_1">Fig. 1</ref>.B clearly shows GPUNet-D dominates other models in both target objectives, and <ref type="table">Table.</ref> 3 suggests both GPUNet and GPUNet-D are significantly faster than SOTA networks while maintaining similar accuracy. At the similar 80.5 top-1 accuracy, GPUNet-1 is nearly 2 times faster than EfficientNet-B2, EfficientNetX-B2-GPU, and FBNetV3-B. For other accuracy groups, GPUNet consistently demonstrates the speedup from 1.27? to 3.24? than baselines. While EfficientNet-V2 <ref type="bibr" target="#b29">[31]</ref> shows slightly better results than GPUNet when latency &gt; 3ms in <ref type="figure" target="#fig_1">Fig. 1.B</ref>, EfficientNet-V2 utilizes a far more sophisticated training scheme that includes Mixup <ref type="bibr" target="#b41">[43]</ref> and progressive training in regularizing the network for better accuracy. These regularizations are orthogonal to NAS, and they can be an excellent future work to improve the accuracy of GPUNet further.</p><p>Interestingly, we also note that the #FLOPS and #Parameters of GPUNet are larger than baselines, though GPUNet is significantly faster. These results indicate that low FLOPS models are not necessarily fast on GPUs. EfficientNet-X explains this with the roofline model <ref type="bibr" target="#b16">[18]</ref>, and we will provide more results in Sec. <ref type="bibr">4.2.4.</ref> Please note that our ultimate goal is to provide a table of models tiered by their inference latency to expedite the customization. <ref type="table">Table.</ref> 3 and <ref type="figure" target="#fig_1">Fig. 1</ref> only show a few models to demonstrate that our NAS system can effectively design fast and accurate networks on the proposed search space in <ref type="table">Table.</ref> 1. We will release this table of models after the paper. <ref type="figure">Fig. 6</ref> shows that the architectures of NAS optimized GPUNet are too irregular to be human design. For example, the two adjacent stride=2 ER (Fused-IRB) blocks in GPUNet-2 consecutively halve H and W twice, while the human-designed networks usually have multiple stride = 1 layer between two stride = 2 layers. There is no obvious pattern for the activation functions and expansions in IRB as well. However, these NAS optimized networks show one common characteristic in the filter distribution, which are skinny in the beginning/middle stages and very wide in the last few stages though the search space in <ref type="table">Table.</ref> 1 permits large filters at the beginning and small filters in the end. For example, the filters of GPUNet-2 follow the pattern of 32 ? 32 ? 116 ? 144 ? 160 ? 224 ? 832; GPUNet-0 and GPUNet-1 also follow a similar filter pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">GPUNet Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Why GPUNet Are Faster and Better?</head><p>We also compare the architecture of FBNet and EfficientNet to GPUNet. Here are a few key differences found by us that explain the GPUNet performance. Let's use GPUNet-1 as an example.</p><p>? Mixed types of activation: <ref type="figure">Fig.6</ref> suggests that GPUNet switches between RELU and Swish, but EfficientNet and FBNet use Swish across all the layers. <ref type="figure" target="#fig_3">Fig.3</ref>.A suggests Swish greatly increases the latency. Some layers of GPUNet uses RELU to reduce the latency for other opportunities to improve the accuracy, e.g., larger filters.</p><p>? Fewer expansions in IRB: <ref type="figure" target="#fig_3">Fig.3</ref>.C shows the network latency almost doubles by increasing the expansions in all IRB from 1 to 6. The expansion is part of our search space, so some GPUNet layers tend to have small expansions to save the latency. Besides, GPUNet-2 has 33 layers, 2 more than FBNetV3-F and 5 more than EfficientNet-B3. It is known that deep and wide networks have better accuracy; therefore, the accuracy of GPUNet is better than baselines within each group.</p><p>? Larger Resolution: GPUNet-(1 and 2) are 32 and 64 larger than EfficientNet-B2 and B3 in resolutions, 72 and 120 larger than FBNetV3-B and FBNetV3-F, respectively. Using large resolution generally improves the accuracy; therefore, GPUNet shows better accuracy and higher FLOPS than baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluating on Detection tasks</head><p>We test GPUNet on COCO detection tasks. We evaluate GPUNet, FBNetV3-F, and EfficientNet-B3 on COCO detection tasks by replacing the backbone in the cascade RCNN <ref type="bibr" target="#b7">[9]</ref>. <ref type="table" target="#tab_7">Table. 4</ref> shows GPUNet-2 is not only faster, but also delivers higher mAP than baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Model customization is challenging for DL engineering. In this work, we proposes to build a model hub tiered by their inference latency to expedite model customization, which facilitates practitioners to reuse the our pretrained models with known TensorRT latency and compatibility in mind. With a novel distributed NAS system and an enhanced search space to design a set of fast and accurate GPUNet, we establish a new SOTA Pareto frontier in latency and accuracy, validating the effectiveness of our NAS system. Although this paper primarily focus on EfficientNet search space, our NAS system is generic to support various tasks and search spaces. Ultimately we intend to maintain a hub of NAS optimized models that track the latest technology so that ML practitioners can directly reuse them.   <ref type="table">Table.</ref> 5 shows the full details of training hyperparameters. We used Pytorch Image Models in training, and we applied the same configurations to all GPUNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Sources of Baseline</head><p>The baseline models are from their original public release to ensure fair evaluations. We only convert their models to ONNX so that we can benchmark them in TensorRT. The conversion is invasive to the model latency and structure, and we use the Pytorch and Tensorflow native support for ONNX conversions. Here is the list that shows the source of the original implementation.  In <ref type="figure" target="#fig_7">Fig. 7</ref>, we have tested GPUNet optimized for GV100 on NVIDIA AGX Orin, and GPUNet consistently dominates other networks in the accuracy and latency Pareto frontier. GPUNet maintains the advantages because it replaces some memory-bound operators (e.g., high expansion ratio in SE layers) to compute bound operators, such as larger filters or deeper networks. An interesting observation is that the advantages of GPUNet-2 and GPUNet-3 (latency &gt; 3000 on Orin) decrease on Orin w.r.t on GV100. Because GV100 has more execution units than Orin, increasing filters or layers can better saturate the device. Therefore, the optimization strategies generalized from GPUNet in sec.4.2.4 are still applicable to other devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The performance of GPUNet w.r.t SOTA networks in the inference latency and the top 1 accuracy on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>GPUNet establishes the new SOTA Pareto frontier in the accuracy and inference latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The latency impact of different hyper-parameters and configurations on EfficentNet-B0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The error between the actual measured TensorRT latency and the table predicted latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The distribution sampled by (A) the uniform random on each dimensions and (B) sobol sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>GPUNet performance on AGX Orin and GV100. ? ResNeSt:https : / / github . com / zhanghang1989/ResNeSt ? LaNet:https : / / github . com / facebookresearch/LaMCTS ? OFA:https://github.com/mithan-lab/ once-for-all 6.1.2 Verify the models on more devices</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>?: R is ReLU and S is Swish. ?: F-IRB indicates Fused-Inverse-Residual-Block (Fused-IRB). ?: E indicates the range of IRB expansion rate. * : number of filters increase from 24 to 32 at the step of 8. The proposed convnet search space.</figDesc><table><row><cell cols="2">Stage Type</cell><cell cols="3">Stride Kernel #Layers Act</cell><cell>E  ?</cell><cell>Filters</cell><cell>SE</cell></row><row><cell>0</cell><cell cols="2">Conv 2</cell><cell>[3, 5] 1</cell><cell>[R,S]  ?</cell><cell>[24, 32, 8]  *</cell></row><row><cell>1</cell><cell cols="2">Conv 1</cell><cell cols="2">[3, 5] [1, 4] [R,S]</cell><cell>[24, 32, 8]</cell></row><row><cell>2</cell><cell cols="2">F-IRB ? 2</cell><cell cols="3">[3, 5] [1, 8] [R,S] [2, 6] [32, 80, 16] [0, 1]</cell></row><row><cell>3</cell><cell cols="2">F-IRB 2</cell><cell cols="3">[3, 5] [1, 8] [R,S] [2, 6] [48, 112, 16] [0, 1]</cell></row><row><cell>4</cell><cell>IRB</cell><cell>2</cell><cell cols="3">[3, 5] [1, 10] [R,S] [2, 6] [96, 192, 16] [0, 1]</cell></row><row><cell>5</cell><cell>IRB</cell><cell>1</cell><cell cols="3">[3, 5] [0, 15] [R,S] [2, 6] [112, 224, 16] [0, 1]</cell></row><row><cell>6</cell><cell>IRB</cell><cell>2</cell><cell cols="3">[3, 5] [1, 15] [R,S] [2, 6] [128, 416, 32] [0, 1]</cell></row><row><cell>7</cell><cell>IRB</cell><cell>1</cell><cell cols="3">[3, 5] [0, 15] [R,S] [2, 6] [256, 832, 64] [0, 1]</cell></row><row><cell>8</cell><cell></cell><cell cols="3">Conv1x1 &amp; Pooling &amp; FC</cell><cell>1792</cell></row><row><cell></cell><cell>Res</cell><cell cols="4">[224, 256, 288, 320, 352, 384, 416, 448, 480, 512]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Client Exchanging networks or accuracy Client Client Training Receipts Evaluation</head><label></label><figDesc></figDesc><table><row><cell>Network</cell><cell></cell><cell></cell><cell>Search</cell><cell></cell><cell></cell></row><row><cell>Search Search Space</cell><cell>Layer Type Conv~~0.18ms Input Shape Config TRT Latency IRB~~0.31ms</cell><cell>Sobol sampling</cell><cell>Space Subspace &gt; 0.5ms &lt; 1.0ms &gt; 1.0ms Subspace</cell><cell>Server Search</cell><cell>Networks &lt; 1.5ms &gt; 1.0ms</cell></row><row><cell>Network performance</cell><cell></cell><cell></cell><cell>&lt; 1.5ms</cell><cell></cell><cell></cell></row><row><cell>(A) NAS workflow</cell><cell cols="3">(B) Curating the search space by the latency</cell><cell></cell><cell>(C) Distributed NAS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>The encoding scheme of networks in the search space.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>?Table 4 .</head><label>4</label><figDesc>Wider and Deeper: the filters (wide) and the number of layers (deep) in a stage are part of our search space. Because of the latency saving from mixed activation Applying GPUNet to COCO object detection tasks. The latency was measured using the resolution of 1333x800. and fewer expansions, GPUNet tends to be wider and deeper than FBNet and EfficientNet. In the same accuracy group, the filters of FBNetV3-B follow the pattern of 16 ? 24 ? 40 ? 72 ? 120 ? 183 ? 224, and the filter pattern of EfficientNet-B2 is 32 ? 16 ? 24 ? 48 ? 88 ? 120 ? 208 ? 352, but GPUNet-1 is a lot wider than FBNetV3-B and EfficientNet-B2 that has a pattern of 24 ? 64 ? 96 ? 160 ? 288 ? 448.</figDesc><table><row><cell>Backbone</cell><cell cols="2">ImageNet top1 Method</cell><cell cols="2">TRT Latency(ms) mAP</cell></row><row><cell>GPUNet-2</cell><cell>82.2</cell><cell cols="2">Cascade RCNN 5.2</cell><cell>40.0</cell></row><row><cell>ResNet-50</cell><cell>80.3</cell><cell cols="2">Cascade RCNN 5.8</cell><cell>40.4</cell></row><row><cell>FBNetV3-F</cell><cell>82.1</cell><cell cols="2">Cascade RCNN 7.90</cell><cell>26.5</cell></row><row><cell cols="2">EfficientNet-B3 81.6</cell><cell cols="2">Cascade RCNN 10.65</cell><cell>28.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>The training hyper-parameters: we use Pytorch Image Models to train GPUNet, and here<ref type="bibr" target="#b2">[4]</ref> further explains the usage of these hyper-parameters.</figDesc><table><row><cell>6. Supplemental Material</cell></row><row><cell>6.1. Training Receipts</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">TensorRT also reports throughput, end-to-end, device-to-host, hostto-device time. We use GPU compute time to better capture the latency impact on architecture difference.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel&amp;apos;s Openvino</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorrt</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/tensorrt.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="https://github.com/rwightman/pytorch-image-models.6" />
		<title level="m">Pytorch image models</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The sobol sequence implemented in the pytorch</title>
		<ptr target="https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Computing receptive fields of convolutional neural networks. Distill</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<ptr target="https://distill.pub/2019/computing-receptive-fields.4" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint architecture-recipe search using predictor pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The design and analysis of experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Kempthorne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Searching for fast model families on datacenter accelerators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman P Jouppi</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8085" to="8095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6232" to="6240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nsga-net: neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="419" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Searching for activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tresnet: High performance gpu-dedicated architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><forename type="middle">Ben</forename><surname>Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the distribution of points in a cube and the approximate evaluation of integrals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Il&amp;apos;ya Meerovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sobol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zhurnal Vychislitel&apos;noi Matematiki i Matematicheskoi Fiziki</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="784" to="802" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnetv2</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00298</idno>
		<title level="m">Smaller models and faster training</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12965" to="12974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Alphanet: Improved training of supernet with alpha-divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07954</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning search space partition for black-box optimization using monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00708</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sample-efficient neural architecture search by learning actions for monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuu</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bignas: Scaling up neural architecture search with big single-stage models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">How to train your super-net: An analysis of training heuristics in weight-sharing nas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04276</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Guo</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
