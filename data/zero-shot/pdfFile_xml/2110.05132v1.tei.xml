<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Bras?</surname></persName>
							<email>guillem.braso@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kister</surname></persName>
							<email>n.kister@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
							<email>lealtaixe@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce CenterGroup, an attention-based framework to estimate human poses from a set of identity-agnostic keypoints and person center predictions in an image. Our approach uses a transformer to obtain context-aware embeddings for all detected keypoints and centers and then applies multi-head attention to directly group joints into their corresponding person centers. While most bottom-up methods rely on non-learnable clustering at inference, CenterGroup uses a fully differentiable attention mechanism that we train end-to-end together with our keypoint detector. As a result, our method obtains state-of-the-art performance with up to 2.5x faster inference time than competing bottom-up approaches. Our code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Localizing the anatomical 2D keypoints of all humans in an image is a fundamental task in computer vision, with the ability to enable progress in applications such as virtual reality, human computer interaction, and human behavior analysis. It is also a common key component of algorithms for tasks such as action recognition <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b12">13]</ref>, multi-object tracking <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref>, and generative models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b52">52]</ref>.</p><p>Current methods typically follow one of two paradigms: bottom-up and top-down. Top-down approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56]</ref> divide the problem into two subtasks: (i) bounding box detection for all persons in the image, and (ii) joint localization for each person individually. Despite their success in some benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35]</ref>, these two-step approaches lack efficiency due to their need to use a separate object detector, and their performance tends to degrade severely under heavy occlusions <ref type="bibr" target="#b33">[34]</ref>. Bottom-up methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref> follow a different approach, as they first detect identity-agnostic keypoints, and then group them into separate poses. Their lack of reliance on external object detectors and their ability to operate jointly over the entire set of keypoints in the image has allowed them to outperform top-down approaches in benchmarks where occlusions are common <ref type="bibr" target="#b33">[34]</ref>. While recent work has significantly advanced the ability of bottom-up methods to accurately predict identity-free keypoints <ref type="bibr" target="#b11">[12]</ref>, current grouping algorithms still face significant drawbacks: since they generally rely on optimization algorithms, they cannot be trained end-to-end, and are often slow.</p><p>The keypoint grouping task can be formulated as a graph optimization problem in which nodes represent keypoints, and edge weights, which can be learned, represent their likelihood of belonging to the same human pose. Approaches ranging from integer linear programming <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49]</ref>, heuristics <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b5">6]</ref> or graph clustering <ref type="bibr" target="#b28">[29]</ref> are then used to find the correct assignment. A common problem of bottom-up methods is that their learning objectives are poorly aligned with the real inference procedure: they learn affinities between keypoints but, at test time, grouping is performed by a separate algorithm which is not differentiable per se.</p><p>One-shot methods are an efficient alternative <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b62">62</ref>] to optimization-based bottom-up methods. Their general formulation consists in regressing a root node location per person, and then predicting offsets to keypoint locations. Since they are able to avoid the optimization-based group-ing stage, they are significantly faster than their counterparts. However, given the inherent difficulty of predicting offsets under occlusions and scale variation, they are also significantly less accurate, and therefore have to rely on additional postprocessing techniques to obtain competitive performance <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b62">62]</ref>.</p><p>We propose to tackle the limitations of current bottomup grouping and one-shot algorithms with a novel framework based on attention. Instead of regressing offsets from a set of center nodes, our proposed CenterGroup uses attention to search for the best match between person centers and keypoints over the entire image. Our method retains the ability of bottom-up approaches to precisely predict keypoints from heatmaps, while maintaining the efficiency of one-shot methods. Furthermore, unlike standard bottom-up methods, CenterGroup does not require any test-time optimization and is end-to-end trainable.</p><p>More specifically, we first obtain proposals for person centers and identity-agnostic keypoints via heatmap regression. We then feed centers and keypoints to a transformer <ref type="bibr" target="#b58">[58]</ref> to encode contextual information into their updated embeddings. Finally, the embeddings are used in a simple keypoint grouping scheme which maximizes the attention scores between person centers and keypoints belonging to the same pose. At test time, we extract poses by assigning to centers those keypoints with the corresponding highest attention score. Due to the simplicity of our grouping algorithm and the parallel nature of attention computation, CenterGroup is 2.5x faster than the current state-of-the-art bottom-up method <ref type="bibr" target="#b11">[12]</ref>, while having better performance.</p><p>Overall, we make the following contributions:</p><p>? We propose to tackle the pose estimation problem by grouping keypoints and person center predictions with a multi-head attention formulation that allows to train the model in an end-to-end fashion.</p><p>? We use a transformer to encode dependencies between bottom-up detected keypoints and centers to obtain context-enhanced embeddings, efficiently boosting the performance of our proposed grouping scheme.</p><p>? We achieve state-of-the-art results within an end-toend framework that yields a speedup increase of up to 2.5x with respect to state-of-the-art <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Top-down methods. Top-down methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b13">14]</ref> split the task into two steps. They first apply a person detector on the image and then perform single person pose estimator for each detected image region which is given by the bounding boxes. While being particularly strong at handling scale variaion, these methods struggle in cases of occlusion. To address these limitations, previous work has explored refining poses by exploiting the graph structure of the human skeletons with additional modules such as graph networks <ref type="bibr" target="#b60">[60,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b49">50]</ref> or probabilistic graphical models <ref type="bibr" target="#b57">[57]</ref>. While being more robust, these methods still rely on external detectors, and therefore cannot recover from missing boxes.</p><p>Bottom-up methods. Bottom-up methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> start by detecting identity-free keypoints over the entire image. In a second step, a grouping algorithm assembles poses using pairwise similarity scores between keypoints. To predict these similarity scores, Deep-Cut and Person-Lab <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref> predict offset fields that link joints belonging to the same person. Openpose and Pif-Paf <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33]</ref> predict part affinity fields which resemble human limbs and encode the position and orientation between pairs of keypoints. Associative embeddings <ref type="bibr" target="#b40">[41]</ref> are a popular approach currently used by state-of-the-art <ref type="bibr" target="#b11">[12]</ref>. They predict an embedding for every detected keypoint from convolutional features, and then use their pairwise euclidean distances as similarity scores. For all these methods, grouping is done by either graph partitioning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b27">28]</ref> or heuristic greedy parsing <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b43">44]</ref>. HGG <ref type="bibr" target="#b28">[29]</ref> makes progress towards learning to group keypoints by using a graph neural network <ref type="bibr" target="#b51">[51]</ref> on top of associative embeddings and training an edge and node classifier to hierarchically predict which keypoints belong together. While its graph network is trainable, it still relies on an external nondifferentiable clustering algorithm <ref type="bibr" target="#b14">[15]</ref> for grouping. Cen-terGroup does not need this clustering step and, instead, it uses attention as a form of learnable keypoint grouping.</p><p>One-shot methods. One-shot methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b62">62]</ref> avoid the grouping task by directly regressing keypoint locations from a set of predicted centers <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b69">69]</ref> or anchors <ref type="bibr" target="#b62">[62]</ref>. Both SPM <ref type="bibr" target="#b42">[43]</ref> and CenterNet <ref type="bibr" target="#b69">[69]</ref> regress offsets at center locations to regress each person's joints. In addition, <ref type="bibr" target="#b69">[69]</ref> predicts keypoint heatmaps as standard bottom-up methods. It then combines both predictions by heuristically matching offsets to their closest predicted joints. While being more efficient than grouping-based approaches, this heuristic still does not perform on par with them, and suffers from the same problem of not being end-to-end learnable.</p><p>Transformers and attention. Transformers were initially introduced for machine translation, and became recently popular for computer vision tasks ranging from image classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref>, object detection <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b70">70]</ref>, semantic segmentation <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b61">61]</ref>, video processing <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b68">68]</ref>, image generation <ref type="bibr" target="#b46">[47]</ref>, and hand pose estimation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. They employ self-attention layers to model relations between entities in a global context. Their use for human pose estimation is still relatively unexplored: <ref type="bibr" target="#b53">[53]</ref> employs transformer for human pose tracking, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b20">21]</ref> apply them to estimate 3D human poses and <ref type="bibr" target="#b65">[65]</ref> use a transformer based architecture for explainable single person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background: Multi-Head Attention</head><p>Our model uses multi-head attention and a transformer as main tools to perform grouping. Therefore, we start by providing a brief review of these techniques.</p><p>Multi-Head Attention (MHA) <ref type="bibr" target="#b58">[58]</ref>, the core component of the transformer model, aims at obtaining contextual representations from an unordered set of vectors by letting each vector attend over multiple representation subspaces of a (possibly different) set of vectors. More precisely, given a set of n d?dimensional query feature vectors, Q :</p><formula xml:id="formula_0">= {q i ? R d } n i=1</formula><p>, and a set of m pairs of key and value vectors,</p><formula xml:id="formula_1">K := {k j ? R d } m j=1 and V := {v j ? R d } m j=1</formula><p>1 , MHA updates the query embeddings by linearly projecting the concatenation of h attention heads:</p><formula xml:id="formula_2">MHA(Q, K, V ) = concat(head 1 , . . . , head h )W O , (1) where W O ? R (d H * h)</formula><p>?d is a learnable matrix, and d H is the dimensionality of each attention head. Each attention head computes, at every index l ? {1, . . . n}:</p><formula xml:id="formula_3">(head i ) l = m j=1 attn i (q l , k j )W V i v j ,<label>(2)</label></formula><p>where the attention scores are computed as softmaxnormalized 2 dot-products between keys and queries:</p><formula xml:id="formula_4">attn i (q l , k j ) := exp((W Q i q l ) T (W K i k j )) m t=1 exp((W Q i q l ) T (W K i k t ))<label>(3)</label></formula><p>where,</p><formula xml:id="formula_5">W Q i , W K i , W V i ? R d?d H are learnable projec- tion matrices.</formula><p>Whenever these sets of key, query and values are the same, i.e., Q = K = V , one refers to self-attention, which is the core component of the transformer encoder architecture. Overall, transformer encoders are formed by stacking blocks of an initial layer of self-attention with skipconnection and layer normalization <ref type="bibr" target="#b2">[3]</ref>, followed by a feedforward network and a second instance of layer normalization. For completeness, we provide a more detailed explanation of their architecture in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Problem Formulation</head><p>We first provide an overview of the general formulation of our method and introduce notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Statement</head><p>Given an input image, we aim to obtain the set of poses P corresponding to all persons in the image. Let J be the number of joints being considered. Each pose can be uniquely determined by the 2D location and visibility of its J joints. Formally, for every pose p ? P, we refer to its joint locations as loc i p ? R 2 for every i ? {1, . . . , J}. We denote the visibility of each joint of pose p as vis i p ? {0, 1}, and assign it 1 whenever the joint is visible, and 0 otherwise.</p><p>Our approach operates over a set of predicted identityagnostic joint keypoints in the image, which we refer to as K. Each keypoint k ? K can be identified by its 2D location loc k ? R and its predicted type type k ? {1, . . . , J}. Our method also predicts an additional set of targets corresponding to the center locations of persons in the image. We denote as C the set of detected person centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Grouping Keypoints and Centers</head><p>Standard bottom-up methods learn a similarity score sim(k 1 , k 2 ) for every pair of detected keypoints k 1 , k 2 ? K, and use them to form poses by clustering keypoints that are most similar. One-shot methods, instead, directly use predicted person centers and regress displacement offsets from centers to joint locations to avoid expensive grouping.</p><p>Inspired by these approaches, we propose to perform human pose estimation by learning similarity score sim i (c, k) ? R + between every pair of detected person center c ? C and keypoints k ? K and type i ? {1, . . . , J}. By being able to estimate the similarity between center nodes and keypoints, as opposed to the similarity between pairs of keypoints, we are able to reduce the complexity of the grouping task significantly. Instead of requiring a graph clustering algorithm, we formulate the grouping task as a simple nearest-neighbor search problem. Namely, for every predicted center c ? C, we obtain its corresponding pose by retrieving the locations of its most similar detected keypoint k * ? K of a target type i ? {1, . . . , J} according to sim i . Formally, the predicted location loc i c of joint type i for center c can be obtained as loc</p><formula xml:id="formula_6">i c = loc k * , where k * = arg max k?K sim i (c, k)<label>(4)</label></formula><p>Since our approach operates directly over the set of detected keypoint locations, there is no need for additional postprocessing to obtain precise joint locations, unlike offset-based methods <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b42">43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attention as Differentiable Keypoint Selection</head><p>The main drawback from the aforementioned procedure is that it is not end-to-end trainable, as it involves an arg max operation over the detected keypoints. We circumvent this issue by formulating the nearest neighbor search <ref type="figure">Figure 2</ref>: Our method receives a single RGB image as input and predicts a set of identity-agnostic keypoints and person centers with a HigherHRNet <ref type="bibr" target="#b11">[12]</ref> by heatmap regression. It then extracts features from our CNN's last layer, augments them with positional encodings and feeds them to a transformer encoder that returns context-aware embeddings for every keypoint and center. These embeddings are then fed to an attention module that predicts which joints correspond to each center. task as a differentiable attention mechanism. To do so, we treat person centers as our set of queries, and keypoints as our set of keys, and obtain their similarity scores by computing their dot-product in a learned embedding space for every joint type i ? {1, . . . , J}. We then normalize the scores with a softmax operator to replace the non-differentiable arg max. The resulting coefficients are used during training to directly predict keypoint locations for every joint type i ? {1, . . . , J} and every person center c as:</p><formula xml:id="formula_7">loc i c := k?K attn i (c, k)loc k<label>(5)</label></formula><p>where loc k are the coordinates of the detected keypoint k.</p><p>Predictions resulting from equation 5 can then be minimized by directly computing their L1 loss with respect to the ground truth location. Note that since the detected keypoint coordinates are fixed in <ref type="bibr">Equation 5</ref>, in order to minimize the loss our network needs to assign the highest attention score to the keypoints which location is closest to the ground truth coordinates loc i c . Moreover, in the limit, when max k?K attn i (c, k) = 1, Equation 5 becomes equivalent to just computing a standard arg max. Therefore, the attention coefficients act as a differentiable mechanism for selecting keypoints from person centers based on their dotproduct similarity. This procedure still allows us to use the simple arg max operator at test-time to efficiently retrieve keypoints from centers as in Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Method</head><p>We exploit the formulation described in the previous section within an end-to-end bottom-up pipeline for pose estimation. In this section, we first provide a general overview of it, and then explain each of its components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Overview</head><p>Our method, CenterGroup, consists of three main stages, which are summarized in <ref type="figure">Figure 2:</ref> 1. Keypoint and center detection. The location of identity-agnostic keypoints and person centers is obtained by heatmap regression following HigherHRNet <ref type="bibr" target="#b11">[12]</ref>. The output is a variable number of high-scoring joint and person center detections.</p><p>2. Encoding keypoints and centers. For every detected keypoint and center, we extract features from a CNN backbone, and augment them with additional embeddings encoding their spatial position. These embeddings are fed to a transformer <ref type="bibr" target="#b58">[58]</ref>, yielding updated embeddings with enhanced contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Keypoint and Center Detection</head><p>In the first stage of our pipeline, we start by detecting identity agnostic-keypoints and person centers. Heatmap regression. We follow HigherHRNet <ref type="bibr" target="#b11">[12]</ref> to obtain identity-agnostic keypoint proposals for each of J joint types being considered. HigherHRNet uses an HRNet <ref type="bibr" target="#b55">[55]</ref> backbone, followed by two keypoint prediction heads that regress heatmaps at 1/4 and 1/2 of the original image scale for every joint type. Heatmaps are trained to follow a gaussian distribution centered at ground truth keypoint locations. During training, both heatmaps are supervised independently with a minimum-squared error loss. At inference, heatmaps are upsampled and aggregated to obtain a single heatmap at full image resolution. Person centers. In addition to joints, we regress a new heatmap corresponding to person centers, also at resolutions 1/4 and 1/2. Following <ref type="bibr" target="#b42">[43]</ref>, given a ground truth pose p ? P with joint locations {loc i p ? R 2 } J i=1 , the location of its center is computed as the center of mass of the visible joints, i.e.,</p><formula xml:id="formula_8">loc p := 1 N p vis i p =1 loc i p .<label>(6)</label></formula><p>where N p := J i=1 vis i p is the number of visible joints in pose p. Note that we identify the pose location as that of its center, and hence write loc p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Encoding Keypoints and Centers</head><p>Given the set of predicted keypoints and centers from the first stage, our goal is to obtain discriminative embeddings encoding contextual information. These embeddings will then be used in our grouping module in order to predict associations among keypoints and centers. Hence, it is desirable for them to encode global context.Towards this end, we use a transformer encoder that yields updated embeddings for every keypoint and center. Initial features. We add one additional residual block <ref type="bibr" target="#b19">[20]</ref> to our backbone's last feature map at 1/4 of the original resolution. For every detected keypoint and center, we obtain an initial embedding vector by extracting the vector at its corresponding location from the resulting feature map, and feed it to a two-layer Multi-Layer Perceptron (MLP) to project it onto a higher dimensionality. Positional encodings. CNN features struggle to encode the position of different keypoints <ref type="bibr" target="#b36">[37]</ref>. However, spatial information offers an important cue for keypoint grouping. Hence, similarly to previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61]</ref>, we use fixed sinusoidal features encoding the absolute x and y axis locations at different frequencies. As a result, we obtain a new vector of dimensionality d and sum it element-wise to the initial features of every detected keypoint and center. Transformer encoder. In order to encode global context among every detected person and keypoint, we take their initial features, augmented with positional encodings, and feed them to a transformer encoder. As a result, we obtain updated embeddings h k and h c for every detected keypoint k ? K and center c ? C. Our transformer architecture follows the one described in Section 3, and is described in detail in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Keypoint Grouping</head><p>In the last stage of our pipeline, we construct poses by using the embeddings produced by our transformer to determine which keypoints belong to which person centers via pairwise attention scores. As explained in Section 4.3, we use attention as a differentiable approximation of keypoint selection from centers. In addition, we predict two additional targets for every center: the visibility of each of their keypoints, and the probability that they represent a true pose. This module is summarized in <ref type="figure" target="#fig_1">Figure 3</ref>. Classifying centers. We start by identifying which predicted centers locations correspond to the ground truth poses by matching them based on their locations 3 . As a result, each predicted center is labelled with a binary target y c center , set to 1 if the center is matched and 0 otherwise. We then use a small multi-layer perceptron to classify the center embeddings produced by our transformer, h c , and supervise the resulting prediction MLP center (h c ) with a focal loss <ref type="bibr" target="#b35">[36]</ref>. Predicting joint locations. For every predicted center c ? C such that y c center = 1, we aim to predict the 2D coordi-nates of each of its joints loc i c of every type i ? {1, . . . , J}. For each joint type i ? {1, . . . , J}, we define a pair of learnable projection matrices W K i and W Q i , and a learned type encoding vector ? i ? R d . The goal of the projection matrices is to map the center embeddings h c and keypoint embeddings h k , into a discriminative representation in which their dot-product will encode their likelihood being a good match for type i. We compute their similarity as:</p><formula xml:id="formula_9">sim i (c, k) = (W Q i h c ) T W K i (h k + ? type k )<label>(7)</label></formula><p>Note that the learned embedding ? type k is added to the keypoint embedding h k before the multiplication. Its goal is to encode the initial type predicted by our keypoint detector for keypoint k. Intuitively, when searching for joints of a target type i, it is desirable for our network to still be able to consider joints of all predicted types in order to recover from type errors made by the keypoint detector. For instance, for a target type i, such as left ankle, some predicted types by the detector (e.g., right ankle) are more likely to be better matching candidates than others (e.g. nose). By using the learnable encoding ? type k for each keypoint before computing the projected embedding with W K i , we allow our network to explicitely account for the relationships between the target type i, and the predicted type of k, type k , in a learnable manner.</p><p>With the similarity scores from <ref type="bibr">Equation 7</ref>, the final attention scores are computed by normalizing them with a softmax operation over the entire set of keypoints:</p><formula xml:id="formula_10">attn i (c, k) = exp (sim i (c, k)) k ?K exp(sim i (c,k))<label>(8)</label></formula><p>Finally, we obtain the predicted locations as in Eq. 5:</p><formula xml:id="formula_11">loc i c = k?K attn i (c, k)loc k ,<label>(9)</label></formula><p>and supervise them with an L1 loss as:</p><formula xml:id="formula_12">L loc = c?C|y c center =1 vis i c =1 | loc i c ? loc i c |,<label>(10)</label></formula><p>where the learnable ground locations of every joint for center loc i c are those of its matched ground truth pose. Overall, this procedure can be interpreted as an instance of an attention head in which center and keypoint embeddings are queries and keys, respectively, and keypoint locations act as values. Note that we use different matrices W K i and W Q i for every target type i ? {1, . . . , J}, which is equivalent to having J different heads. Predicting keypoint visibility. One drawback of the attention mechanism we have described is that, due to the softmax normalization, it may still predict high attention scores between centers and keypoints for a target type i even if a given center has no corresponding visible keypoint of that type in the image. We address this problem by exploiting the attention mechanism to explicitly classify whether the predicted keypoints are visible. To do so, we introduce an additional projection matrix for every head W V i , and reuse the type encodings and attention scores already computed to predict locations to compute a weighted aggregation analogous to that in Equation <ref type="bibr" target="#b8">9</ref>:</p><formula xml:id="formula_13">h i c = k?K attn(c, k)W V i (h k + ? type k )<label>(11)</label></formula><p>we then concatenateh i c and h c , and classify the resulting vector with an additional multilayer perceptron, MLP vis as either visible or not visible. We supervise the result with a focal loss 4 . Intuitively, whenever keypoints are not visible, the original embeddings h k and h c will not be aligned, and therefore, neither will beh c and h c . Hence, their concatenation can be discriminatively used to identify when a joint has no good keypoint candidate for the target joint type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we detail the experimental evaluation of our method. We divide it into ablation studies and comparison to state-of-the-art on two large-scale public datasets. For implementation details, we refer the reader to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Datasets and Evaluation Metrics</head><p>COCO Keypoint Detection. The COCO dataset <ref type="bibr" target="#b34">[35]</ref> is a large-scale benchmark containing large variety of everyday life situations. It contains over 200,000 images and 17 keypoints annotations for more than 250,000 human instances, which are split in approximately 150,000, 80,000 and 20,000 instances are for training, testing and, validation, respectively. We train our models on the train2017 split only, perform our ablation studies on val2017, and report our final results on the test-dev2017 split. CrowdPose. The CrowdPose dataset <ref type="bibr" target="#b33">[34]</ref> is a challenging benchmark with the goal to evaluate the robustness of methods in crowded scenes. Unlike COCO, in which the majority of images contain few instances, the crowd index in CrowdPose follows a uniform distribution <ref type="bibr" target="#b33">[34]</ref>. The dataset contains a total of 20,000 images and a total of 80,000 instances annotated with 14 keypoints. Images are split in a ratio 5:4:1 for training, validation, and testing. Following <ref type="bibr" target="#b11">[12]</ref>, we train our model on the train and validation splits combined, and report the final performance on the test set.  <ref type="table">Table 1</ref>: Ablation study on the COCO2017 val split.</p><p>Evaluation metrics. The aforementioned datasets use average precision (AP) as their main metric. AP computation is based on the Object Keypoint Similarity (OKS) <ref type="bibr" target="#b34">[35]</ref> score among detected and ground truth poses. AP is the result of average precision scores for OKS thresholds 0.50, 0.55..., 0.90, 0.95. We also report AP for thresholds 0.5 and 0.75, namely, AP 50 and AP 75 . In addition, for COCO we report AP L and AP M , which corresponds to AP over medium and large-sized instances respectively. For CrowdPose, we also report AP E , AP M , AP H , which stands for AP scores over easy, medium and hard instances, according to dataset annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Study</head><p>To determine the individual contribution of each of our model's main components, we perform an ablation study on the COCO val2017 split, with HRNet32 backbone and input size 512x512. All results are reported with flip-testing, following <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b28">29]</ref>, and without top-down refinement. Baselines. CenterGroup can be naturally compared to two alternative frameworks. First, associative embeddings <ref type="bibr" target="#b40">[41]</ref> (Tab. 1, row #1), since they are the method used originally by our keypoint detection network <ref type="bibr" target="#b11">[12]</ref>. Second, one-shot or offset-based methods <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b42">43]</ref>, which also use person center predictions, but use offset regression to obtain the final results. For a fair comparison, we reimplement <ref type="bibr" target="#b69">[69]</ref> with our HigherHRNet backbone and report its performance for its strongest variant, which predicts keypoint heatmaps, center heatmaps and center offsets, and matches centers to their closest predicted keypoint (Tab. 1, row #2). Grouping Module. We consider our model without the transformer encoder to isolate the effect of our Grouping Module. We compare three versions of it. In the first one, the attention head corresponding to the prediction of each keypoint type is only allowed to attend over keypoints of the same type that our keypoint detection network detects, and therefore is not able to overcome joint type mistakes made by the detector. This setting (Tab. 1, row #3) already outperforms our baselines, which confirms the superiority of CenterGroup over AE-based grouping and offset-based methods. In rows #4 and #5, we allow each head to attend over keypoints from the entire set of predicted heatmaps, and refer to them as type-agnostic. In row #5, we further use type encoding in the attention computation, as explained in Section 5.4, and observe that they significantly improve upon type-agnostic grouping. Feature encoding. In rows #6 and #7 of <ref type="table">Table 1</ref>, we further analyze the effect of using the keypoint and center transformer encoding before the routing module. This yields a significant performance boost, which confirms the importance of encoding long-range interactions between keypoints. Further enhancing the initial embeddings with positional encodings allows the transformer to explicitly use spatial information and gives up to 0.4 AP points of improvement for large persons. Loss terms. We also assess the importance of our additional center and visibility classification losses, the results can be found in <ref type="table" target="#tab_2">Table 2</ref>. Without them, we score our predicted poses by directly assigning them the confidence of its predicted center from heatmaps. We observe that replacing the heatmap score with the classification score obtained from our transformer's embeddings (row #2) already provides a significant boost. We then experiment with either using a simple MLP over those features to predict the visibility of every keypoint (row #3), compared to using our attention-based model shown in <ref type="figure" target="#fig_1">Figure 3</ref>, results in row #4. We observe that both yield a significant improvement, but our attention-based model performs best. Runtime analysis. In <ref type="table" target="#tab_3">Table 3</ref>, we report the overall speed of our method when compared to our baselines. All models are run on the same machine with a single NVIDIA RTX5000 GPU, with batch size 1 and flip testing. We report: grouping runtime, i.e., all computations after keypoint detection, which in our case includes the transformer and grouping attention forward pass, and the overall runtime, which always adds 126ms corresponding to HigherHRNet. The overall runtime of CenterGroup is similar to <ref type="bibr" target="#b69">[69]</ref>, while we get significantly better results. Compared to AE-based grouping, our keypoint attention grouping is over 6x faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Benchmark Evaluation</head><p>COCO Keypoint Detection. In <ref type="table" target="#tab_4">Table 4</ref>, we compare Cen-terGroup against state-of-the-art methods on the COCO dataset. Our method achieves the best performance among    all bottom-up methods, for both single and multi-scale testing, and outperforms HigherHRNet, which uses AE-based Grouping <ref type="bibr" target="#b40">[41]</ref>, by approximately 1 AP. We observe that our achievements are most significant in AP L . This can be explained by the ability of our attention module to capture long-range interactions between joints that are far apart.</p><p>Overall, strong results in COCO, combined with our faster inference speed, show that CenterGroup is a more efficient alternative to current bottom-up methods <ref type="bibr" target="#b69">[69]</ref>. We provide additional analysis in the supplementary material. CrowdPose. In  methods outperform their top-down counterparts in Crowd-Pose, since this dataset is focused on much more challenging images with severe occlusions. In this setting, Center-Group shows its full potential and obtains state-of-the-art performance among all methods by 1.8 AP points. Most importantly, our improvement is most significant in the hard regime (AP H ), where we improve upon state-of-the-art by 2.4 and 2.6 AP points for single and multi-scale testing, respectively.</p><p>This proves that our end-to-end learnable formulation does benefit from being trained on a dataset in which occlusions are common, and results in better generalization to new, challenging images. Overall, we show that our end-to-end trainable method can outperform top-down and bottom-up approaches on difficult scenarios with severe occlusions, where reasoning about keypoint detection and grouping jointly has a clear benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have proposed an end-to-end attention-based framework for bottom-up human pose estimation. We have demonstrated that CenterGroup has better performance than existing state-of-the-art methods, particularly in crowded images, while being significantly more efficient. We hope that our approach will inspire future work to explore the potential of attention mechanisms, as well as general learning-based alternatives to optimization-based grouping for bottom-up human pose estimation.</p><p>normalize it by the scale ofc, sc:</p><formula xml:id="formula_14">dist(c,c) := exp ? ||loc c ? locc|| 2 2sc * k 2<label>(12)</label></formula><p>where k is a fixed constant set to 0.15 <ref type="bibr" target="#b5">6</ref> , and the scale sc is computed as 0.53 multiplied byc's bounding box height and width, following <ref type="bibr" target="#b33">[34]</ref>. This formula is adapted from the OKS metric, and simply normalizes distances between 0 and 1 by using a pre-defined standard deviation that depends on the object size. With the distances from Equation 12, we define an instance of a bipartite matching problem. For every c ? C and c ? P, their corresponding cost cost(c,c) := 1 ? dist(c,c), whenever dist(c,c) &lt; 0.5 and ? otherwise. We obtain matches between centers and ground truth centers by solving the problem with the hungarian algorithm, similarly to <ref type="bibr" target="#b6">[7]</ref>. Note that running this algorithm takes on average significantly less than 1ms since the cost matrix is, at most, of size 20x30, and therefore it adds no significant computational burden. Additionally, note that this procedure is only necessary at training time in order define ground truth assignments. At test-time, as explained in the main paper, we do not require any form of optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Training</head><p>We pretrain our backbone and keypoint detection module following HigherHRNet <ref type="bibr" target="#b11">[12]</ref>. We then randomly initialize our encoding and grouping modules and train our entire model end-to-end for 27, 000 iterations with batch size 130, which corresponds to approximately 50 epochs on COCO, and 270 epochs on CrowdPose, and use learning rate linear warm-up during the first 1, 000 iterations <ref type="bibr" target="#b17">[18]</ref>. We use an Adam optimizer <ref type="bibr" target="#b30">[31]</ref> with learning rate set to 1e ? 5 for pretrained layers and 3e ? 4 for the remaining parts of the network, which we drop by a factor of 10 at 10,000 and 20,000 iterations. In addition, use use automatic mixed precision for training <ref type="bibr" target="#b38">[39]</ref>, which reduces the memory requirements by approximately half, and allows training on 4 NVIDIA RTX6000 with 24GB of RAM memory in approximately 24 hours. We observe that our training loss shows high stability and allows training with mixed precision without any divergence problems, in contrast to Associative Embeddings <ref type="bibr" target="#b40">[41]</ref>. For data augmentation, we use the same techniques as <ref type="bibr" target="#b11">[12]</ref>, which include random flipping, rotation, scale variation, and generating a random crop of size 512x512, when using an HRNet32 backbone, or 640x640 when using an HRNet48 backbone.</p><p>We add one grouping module at the output of every transformer encoder block and compute the location, visibility and center losses, and then average them over the output of every transformer encoder block. Loss terms are balanced as follows: the heatmap loss, L heatmap is weighted by factor 10, the location loss, L loc is averaged over all visible keypoints in the image and weighted by 0.02, the center and visibility losses, L vis and L center , are both weighted by factor 1. The overall set of weights is determined by ensuring that each loss term has a comparable magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Inference</head><p>At inference, we resize images to preserve their aspect ratio and have their shorter side of size 512 if using a HR-Net32 backbone, or 640 if using HRNet48. Following <ref type="bibr" target="#b11">[12]</ref>, predicted heatmaps are upsampled to full image resolution. We then extract peaks by applying heatmap Non-Maximum Suppression (NMS) with a max-pooling kernel of size 5x5 for keypoints and 17x17 for person centers, and select all peaks that either have score over 0.01 or are within the top-5 scoring peaks in the heatmap.</p><p>For every predicted center c ? C, we build its pose by assigning it the keypoints with highest attention score according to the attention score corresponding to every type, as explained in Section 4.2 in the main paper. Formally, given center c ? C the location of each of its joint types i ? {1, . . . , J} is determined as:</p><formula xml:id="formula_15">loc i c = arg max k?K attn i (c, k)<label>(13)</label></formula><p>In order to score the resulting poses, we use the predicted visibility scores for every keypoint, vis i c , as well as the predicted probability that center c represents a true positive center,? c center , as follows:</p><formula xml:id="formula_16">score c = avg { vis i c | vis i c ? 0.5} J i=1</formula><p>if? c center ? 0.5 y c center otherwise (14) Intuitively, since visibility scores are only computed for those centers such that y c center = 1 during training (i.e. matched centers), we only use them whenever our network predicts centers to represent true pose centers with probability over 0.5. In that case, the overall pose score is the average visibility confidence score of keypoints that are predicted to be visible (i.e., vis i c ? 0.5). Unlike <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b5">6]</ref>, we do not perform top-down refinement, nor ensembling <ref type="bibr" target="#b31">[32]</ref>, and all results are reported with flip-testing as it is common practice <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref>. For postprocessing, following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12]</ref>, keypoint coordinates are shifted by 0.25 towards the contiguous second maximal activation in each heatmap, to account for quantization errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Exact Architecture</head><p>Our keypoint detection network is minimally modified from HigherHRNet, as explained in Section 5.2 in the main paper. Our newly added modules include an additional residual block and a multi-layer perceptron (MLP) to generate initial keypoint and person features, a transformer encoder and the grouping module. Our transformer en-   coder has 3 blocks, each with input dimension 128, 4 selfattention heads and MLP hidden dimension set to 512. We found no significant performance benefits from further increasing the transformer's size. The architecture of each transformer encoder block is not modified from the original one <ref type="bibr" target="#b58">[58]</ref>, and shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>All of the MLPs in the grouping module, as well as the one generating the transformer's input contain two hidden layers. We detail the number of parameters of each component in <ref type="table" target="#tab_9">Table 7</ref>. The overall parameter count of our pro-posed keypoint encoding and grouping module is below 2M, which is relatively small, and only accounts for &lt;6% (resp. &lt;3%) of the overall count when using an HRNet32 (resp. HRNet48) backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Qualitative Examples</head><p>In <ref type="figure">Figure 5</ref>, we visualize results produced by our method in comparison to those from our baselines: HigherHRNet <ref type="bibr" target="#b11">[12]</ref> and CenterNet <ref type="bibr" target="#b69">[69]</ref>. As explained in the main paper, we reimplement CenterNet to use an HRNet <ref type="bibr" target="#b55">[55]</ref> backbone and HigherHRNet's scale-aware heatmaps <ref type="bibr" target="#b11">[12]</ref> for keypoint heatmap regression for a fair comparison.</p><p>We observe that our method's performance is robust under severe occlusion and challenging conditions. In comparison, CenterNet often fails whenever there is significant overlap among different poses, as can be seen in rows 1, 4, 5, 6 and 7. Moreover, since it always predicts joint locations for a given pose regardless of whether they are visible or not, it often hallucinates joints and produces unfeasible pose estimates (all rows).</p><p>HigherHRNet generally does a better job at grouping, as can be seen in rows 1, 4, 5, and 6, but this comes at a significantly increased computational cost of 2.5x inference time. Moreover, we observe that it tends to miss or assign very low confidence to large-sized poses (rows 2, 4, 5, 6).</p><p>Our method, instead, has a runtime inference time comparable to CenterNet's, due to its fast optimization-free testtime procedure, and has increased robustness where our baselines fail. Namely, it performs well in images with heavy occlusion, and, due to its ability to capture longrage connections with our attention mechanism, it does not struggle with large-sized poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Visualizing Attention Activations</head><p>In <ref type="figure">Figures 6 and 7</ref> we visualize the attention output scores with which the results in <ref type="figure">Figure 5</ref> were obtained. We observe that despite the large amount of keypoints over which each center attends, particularly in crowded scenes, attention scores are heavily concentrated over a small subset of keypoints, for each center. Indeed, most attention scores for a given type have magnitude over 0.95%, which can be seen from the dark color of most lines. This can be explained due to our loss formulation: to achieve low training error, our model must concentrate attention weights in the most promising keypoint locations, as otherwise it'd incur in large L1 loss values. Overall, <ref type="figure">Figures 6 and 7</ref> show how our model is able to consider a large number of centerkeypoint association candidates but still focus on those keypoints belonging to each pose, even in highly challenging scenarios.</p><p>(a) Input Image (b) HigherHRNet <ref type="bibr" target="#b11">[12]</ref> (c) CenterNet <ref type="bibr" target="#b69">[69]</ref> (d) Ours <ref type="figure">Figure 5</ref>: Qualitative examples of our method's performance in comparison to HigherHRNet <ref type="bibr" target="#b11">[12]</ref> and CenterNet <ref type="bibr" target="#b69">[69]</ref>. Best viewed in color and in a screen. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Given a set of predicted identity-agnostic keypoints and person centers, CenterGroup learns to assign keypoints into their corresponding centers with attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Overview of our Grouping Module. Attention between keypoint and center embeddings is used in order to predict joint locations and visibility for a given center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Overview of the architecture of a Transformer Encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Visualization of predicted attention scores by our grouping module. In (b) we show all pairwise connections between detected keypoints and centers classified as true positives. In (c) we show all final attention scores predicted with attention weight over 0.5 and as visible. The attention weight is color-coded in the color's intensity. Best viewed in color and in a screen. Visualization of predicted attention scores by our grouping module. In (b) we show all pairwise connections between detected keypoints and centers classified as true positives. In (c) we show all final attention scores predicted with attention weight over 0.5 and as visible. The attention weight is color-coded in the color's intensity. Best viewed in color and in a screen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>#</head><label></label><figDesc>Method Group. Type Agnostic Type Encoding Transformer Pos. Encoding AP AP 50 AP 75 AP M AP L 1 Offsets + keypoint match.[69] 65.3 86.4 71.4 59.1 75.0 2 AE [12, 41] 67.1 86.2 73.0 61.5 76.1 3 Ours w/o K&amp;C transformer enc. ? 67.5 86.7 72.7 62.0 76.6 4 Ours w/o K&amp;C transformer enc.</figDesc><table><row><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>67.5 86.8 72.9 60.8 77.3</cell></row><row><cell cols="2">5 Ours w/o K&amp;C transformer enc. ?</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>67.9 87.4 73.2 61.4 77.4</cell></row><row><cell>6 Ours</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>68.4 87.5 73.9 62.0 77.6</cell></row><row><cell>7 Ours</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>68.6 87.6 74.1 62.0 78.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>#</head><label></label><figDesc>Class. Cent. Vis. w/ MLP Vis. w/ Attn. AP AP M AP L</figDesc><table><row><cell>1</cell><cell></cell><cell></cell><cell>66.5 61.4 75.5</cell></row><row><cell>2</cell><cell>?</cell><cell></cell><cell>67.1 61.0 76.2</cell></row><row><cell>3</cell><cell>?</cell><cell>?</cell><cell>68.2 61.8 77.5</cell></row><row><cell>4</cell><cell>?</cell><cell>?</cell><cell>68.6 62.0 78.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on loss terms.</figDesc><table><row><cell>Method</cell><cell cols="3">Group. Time (ms) Time (ms) AP AP M AP L</cell></row><row><cell>Offsets + match[69]</cell><cell>20</cell><cell>146</cell><cell>65.3 59.1 75.0</cell></row><row><cell>AE[41]</cell><cell>327</cell><cell>453</cell><cell>67.1 60.7 76.0</cell></row><row><cell>Ours</cell><cell>52</cell><cell>178</cell><cell>68.6 62.0 78.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Runtime analysis of different grouping methods.</figDesc><table><row><cell>Method</cell><cell cols="4">AP AP 50 AP 75 AP M AP L</cell></row><row><cell cols="3">Top-down methods</cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN [19]</cell><cell cols="4">63.1 87.3 68.7 57.8 71.4</cell></row><row><cell>G-RMI [46]</cell><cell cols="4">64.0 85.5 71.3 62.3 70.0</cell></row><row><cell cols="5">Integral Pose Regression [56] 67.8 88.2 74.8 63.9 74.0</cell></row><row><cell>CPN [11]</cell><cell cols="4">72.1 91.4 80.0 68.7 77.2</cell></row><row><cell>RMPE [17]</cell><cell cols="4">72.3 86.1 79.1 68.0 78.6</cell></row><row><cell>CFN [26]</cell><cell cols="4">72.6 86.1 69.7 78.3 64.1</cell></row><row><cell>SimpleBaseline [63]</cell><cell cols="4">73.7 91.9 81.1 70.3 80.0</cell></row><row><cell>HRNet-W48 [55]</cell><cell cols="4">75.5 92.5 83.3 71.9 81.5</cell></row><row><cell cols="3">Bottom-up methods</cell><cell></cell><cell></cell></row><row><cell>OpenPose* [6]</cell><cell cols="4">61.8 84.9 67.5 57.1 68.2</cell></row><row><cell>Hourglass* + [41]</cell><cell cols="4">65.5 86.8 72.3 60.6 72.6</cell></row><row><cell>PifPaf[33]</cell><cell>66.7</cell><cell>-</cell><cell>-</cell><cell>62.4 72.9</cell></row><row><cell>SPM* + [43]</cell><cell cols="4">66.9 88.5 72.9 62.6 73.1</cell></row><row><cell>HGG + [29]</cell><cell cols="4">67.6 85.1 73.7 62.7 74.6</cell></row><row><cell>PersonLab + [44]</cell><cell cols="4">68.7 89.0 75.4 66.6 75.8</cell></row><row><cell>HrHRNet-W32[12]</cell><cell cols="4">66.4 87.5 72.8 61.2 74.2</cell></row><row><cell>HrHRNet-W48[12]</cell><cell cols="4">68.4 88.2 75.1 64.4 74.2</cell></row><row><cell>HrHRNet-W48 + [12]</cell><cell cols="4">70.5 89.3 77.2 66.6 75.8</cell></row><row><cell>Ours w/ HrHRNet-W32</cell><cell cols="4">67.6 88.7 73.6 61.9 75.6</cell></row><row><cell>Ours w/ HrHRNet-W48</cell><cell cols="4">69.6 89.7 76.0 64.9 76.3</cell></row><row><cell>Ours w/ HrHRNet-W32 +</cell><cell cols="4">70.3 90.0 76.9 65.4 77.5</cell></row><row><cell>Ours w/ HrHRNet-W48 +</cell><cell cols="4">71.4 90.4 78.1 67.2 77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with state of the art methods on the COCO2017 test-dev split. * means top-down refinement, and + means multi-scale testing.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="4">AP AP 50 AP 75 AP E AP M AP H</cell></row><row><cell></cell><cell cols="2">Top-down methods</cell><cell></cell><cell></cell></row><row><cell>Mask-RCNN [19]</cell><cell cols="4">57.2 83.5 60.3 69.4 57.9 45.8</cell></row><row><cell>SimpleBaseline [63]</cell><cell cols="4">60.8 81.4 65.7 71.4 61.2 51.2</cell></row><row><cell>AlphaPose [17]</cell><cell cols="4">61.0 81.3 66.0 71.2 61.4 51.1</cell></row><row><cell cols="4">Top-down with refinement</cell><cell></cell></row><row><cell>SPPE [34]</cell><cell cols="4">66.0 84.2 71.5 75.5 66.3 57.4</cell></row><row><cell></cell><cell cols="3">Bottom-up methods</cell><cell></cell></row><row><cell>OpenPose*[6]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.7 48.7 32.3</cell></row><row><cell>HrHRNet-W48[12]</cell><cell cols="4">65.9 86.4 70.6 73.3 66.5 57.9</cell></row><row><cell>HrHRNet-W48 + [12]</cell><cell cols="4">67.6 87.4 72.6 75.8 68.1 58.9</cell></row><row><cell>Ours w/ HrHRNet-W48</cell><cell cols="4">67.6 87.7 72.7 73.9 68.2 60.3</cell></row><row><cell cols="5">Ours w/ HrHRNet-W48 + 70.0 88.9 75.1 76.8 70.7 62.2</cell></row></table><note>, we show the test-set results for our model trained on CrowdPose. Unlike COCO, where top-down methods show superior performance, bottom-up</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparisions with state of the art methods on the CrowdPose test set. Superscripts E, M, H mean easy, medium and hard. + means multi-scale test, and * means topdown refinement.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of published bottom-up methods on the COCO2017 test-dev split. * means top-down refinement. w/ optimization refers to the use of bipartite matching solvers during inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Parameter count breakdown among components in each stage of our model's pipeline. For the backbone, 28.5M refers to a HRNet32 backbone, and 63.7M refers to a HRNet48. Note that the overall number of parameters of our proposed encoding and grouping modules combined is relatively small, at 1.7M.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For notational simplicity, we assume queries, keys and values have the same dimensinality.<ref type="bibr" target="#b1">2</ref> Transformers use scaled dot product attention, which means that they normalize softmax outputs with the dimensionality of the projected embeddings, d H . However, we omit this term as we will not use it in the remaining of the paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. Keypoint grouping. We use the embeddings obtained from the previous stage and compute dotproduct attention scores between person centers and keypoints, and normalize them in order to obtain a soft-assignment between persons and keypoints. Additionally, we use the transformer embeddings to classify center nodes into true and false positives, and determine the visibility of each keypoint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">More details are provided in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This loss is only computed whenever the given joint in the predicted center is labelled as not visible in the ground truth, or the predicted keypoint has is has small euclidean distance with respect to the with the ground truth keypoint.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">i.e. it applies a single person pose estimation model over the predicted poses.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">This number is determined by increasing by 50% the constant that the COCO dataset uses for hip joints for OKS computation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended COCO Comparison</head><p>In <ref type="table">Table 6</ref>, we provide a detailed comparison of Cen-terGroup against published bottom-up approaches on the COCO test-dev dataset. For each method, we specify its backbone network, grouping procedure, input size, and parameter count. We observe that most top-performing methods rely on greedy decoding schemes, which often involve optimization in the form of solving a sequence of bipartite matching problems. Alternatively, SPM <ref type="bibr" target="#b42">[43]</ref> uses offsets, but relies on top-down refinement to achieve competitive results 5 , and HGG <ref type="bibr" target="#b28">[29]</ref> uses a hierarchical clustering algorithm that operates on the output of graph network predictions.</p><p>CenterGroup outperforms all previous methods with our proposed attention-based grouping module, which does not rely on optimization and is end-to-end trainable. Note that this module only introduces a slight increase in the number of parameters with respect to HigherHRNet <ref type="bibr" target="#b11">[12]</ref>, and combined with our keypoint detector, yields a model with significantly fewer parameters than other methods.</p><p>Regarding performance, we note that the increase in accuracy is most significant for large persons, where our improvement is of 2.1 AP points for single-scale, and 1.7 for multi-scale, which can be explained by the ability of our transformer to capture relationships among distant joints in the image. Overall, it outperforms the current state-of-theart method, HigherHRNet <ref type="bibr" target="#b11">[12]</ref> by approximately 1.2 AP for single-scale and 0.9 AP for multi-scale, while having the exact same backbone and input size, and being 2.5x faster, which confirms CenterGroup's increased efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Matching Centers</head><p>In order to train our grouping module, we need to determine which detected centers in the image correspond to a ground truth pose. As explained in Section 5.4 in the main paper, this allows us to define a target y c center for every detected center c ? C indicating whether it represents a ground truth pose (i.e., y c center = 1) or not (y c center = 0). These labels are used to train our center classification module. Moreover, for those detected centers that do correspond to a ground truth pose, we obtain the visibility of their corresponding keypoints as well as the locations of those that are visible by simply using the annotations of the ground truth center that the detected center is matched with.</p><p>In order to determine correspondences between detected centers (C) and ground truth centers (P), we compute the euclidean distance between every c ? C andc ? P, and</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PoseTrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ensafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Everybody dance now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>PMLR, 2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06-01" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple people tracking using body and joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5700" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handtransformer: Non-autoregressive structured modeling for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hot-net: Non-autoregressive transformer for 3d handobject pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3028" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Motion segmentation &amp; multiple object tracking by correlation co-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salih</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="417" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. 1, 6</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Liftformer: 3d human pose estimation using attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Llopart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00348</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7773" to="7781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingteng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Peeking into occluded joints: A novel framework for crowd pose estimation</title>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="488" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Appearance and pose-conditioned human image generation using deformable gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">15 keypoints is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Snower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farley</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6738" to="6748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with enhanced channelwise and spatial information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5674" to="5682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00759</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph-pcnn: Two stage human pose estimation with graph pose refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="492" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Point-set anchors for object detection, instance segmentation and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Transpose: Towards explainable human pose estimation by transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14214</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7093" to="7102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Objects as points. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Method Backbone Grouping Input size # Params</title>
		<idno>AP AP 50 AP 75 AP M AP L</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Hourglass Graph Network + Graclus clustering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">512</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
