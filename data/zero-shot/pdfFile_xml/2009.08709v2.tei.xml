<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Progressive Semantic-Aware Style Transformation for Blind Face Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computing</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Digital Media</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhui</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Progressive Semantic-Aware Style Transformation for Blind Face Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LQ PULSE [24] HiFaceGAN [36] DFDNet [20] Ours Figure 1: Result comparison with state-of-the-art methods. (Please zoom in to see the details)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Face restoration is important in face image processing, and has been widely studied in recent years. However, previous works often fail to generate plausible high quality (HQ) results for real-world low quality (LQ) face images. In this paper, we propose a new progressive semantic-aware style transformation framework, named PSFR-GAN, for face restoration. Specifically, instead of using an encoderdecoder framework as previous methods, we formulate the restoration of LQ face images as a multi-scale progressive restoration procedure through semantic-aware style transformation. Given a pair of LQ face image and its corresponding parsing map, we first generate a multi-scale pyramid of the inputs, and then progressively modulate different scale features from coarse-to-fine in a semantic-aware style transfer way. Compared with previous networks, the proposed PSFR-GAN makes full use of the semantic (parsing maps) and pixel (LQ images) space information from different scales of input pairs. In addition, we further introduce a semantic aware style loss which calculates the feature style loss for each semantic region individually to improve the details of face textures. Finally, we pretrain a face parsing network which can generate decent parsing maps from * This work was done when he was an intern at Alibaba. real-world LQ face images. Experiment results show that our model trained with synthetic data can not only produce more realistic high-resolution results for synthetic LQ inputs but also generalize better to natural LQ face images compared with state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Blind face restoration refers to recovering the HQ images from the LQ inputs which suffer from unknown degradation such as low resolution, noise, blur and lossy compression. It has drawn more and more interest due to its wide applications. However, most current restoration methods still focus on a specific type of restoration, especially super resolution, and few of them can generalize well to real LQ images.</p><p>Unlike general image restoration, face restoration can exploit strong prior knowledge of the face to recover details of the face components even when the images are severely degraded. Therefore, many recent works about face super resolution incorporate face prior knowledge to improve the performance, such as parsing maps <ref type="bibr" target="#b3">[4]</ref>, face landmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b36">37]</ref> and identity prior <ref type="bibr" target="#b41">[42]</ref>. Most of these works are based on an encoder-decoder like structure, which follow the practice of general image restoration and aim to learn a direct black-box mapping from LQ to HQ images. Al-though they managed to get better results with extra face prior knowledge as inputs or supervision, few of them reports satisfactory results for real LQ images. Other methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> try to utilize high quality references to facilitate the restoration of LQ images. However, their practical applications are limited when there are no high quality references.</p><p>In this work, we propose a new progressive framework, named PSFR-GAN, which formulates face restoration as multi-scale semantic-aware style transformation procedure. Inspired by recent success of style based GAN, i.e., Style-GAN <ref type="bibr" target="#b14">[15]</ref>, we use a semantic-aware style transfer approach to modulate the features of different scales progressively. To be specific, the proposed PSFR-GAN starts with a learned constant latent code and then generates features of different scales through several upsample layers. We modulate the "styles" of different scale features by generating the corresponding style transformation parameters from different scale inputs. The LQ input provides color information and the parsing map provides shape and semantic information. In this way, more details are added to the final features in a coarse-to-fine manner. In addition, we proposed a semantic aware style loss which calculates the gram matrix loss for each semantic region separately. Gram matrix loss is usually applied in neural style transfer <ref type="bibr" target="#b6">[7]</ref>, recent works <ref type="bibr" target="#b7">[8]</ref> found it also effective in recovering textures. In this work, we show that the semantic aware style loss can help to improve the restoration of textures and alleviate the occurrence of artifacts in different face regions.</p><p>Finally, to make our framework more practical, we pretrain a face parsing network (FPN) for LQ face images. Intuitively, predicting face parsing maps is easier than face restoration because we do not need to care the texture details. Experiments demonstrate that the FPN is pretty robust on parsing real-world LQ face images. During test time, we first generate parsing maps for LQ inputs with FPN, and then produce the HQ outputs with PSFR-GAN.</p><p>Our contributions are summarized as follows:</p><p>1. We propose a novel multi-scale progressive framework for practical blind face restoration, i.e. PSFR-GAN. Our model can recover high quality face details progressively through semantic aware style transformation with multi-scale LQ images and parsing maps as inputs. Compared with previous works, the proposed PSFR-GAN can make better use of multi-scale inputs in both pixel domain and semantic domain.</p><p>2. We introduce the semantic aware style loss which helps to improve the texture restoration of different semantic regions and reduce the occurrence of artifacts.</p><p>3. Extensive experiments demonstrate that our model trained with synthetic dataset generalizes better to natural LQ images than current state-of-the-arts.</p><p>4. By introducing a pretrained LQ face parsing network, our model can generate HQ images given only LQ inputs, making it highly practical and applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we briefly review the existing methods for face super-resolution, blind face restoration and HQ face generation based on generative adversarial networks. Face Super-Resolution. Face super resolution is often studied as a basic task for face restoration. Zhu et al. <ref type="bibr" target="#b44">[45]</ref> proposed a cascaded two-branch network to optimize face hallucination and dense correspondence field estimation in a unified framework. Yu et al. <ref type="bibr" target="#b38">[39]</ref> exploited generative adversarial networks (GAN) to directly super-resolve LR inputs. They further improved their model to handle unaligned faces <ref type="bibr" target="#b39">[40]</ref>, noisy faces <ref type="bibr" target="#b40">[41]</ref> and faces with different attributes <ref type="bibr" target="#b37">[38]</ref>. Instead of directly inferring HR face images, Huang et al. <ref type="bibr" target="#b11">[12]</ref> proposed to predict wavelet coefficients from LR images to reconstruct HR images. More recent works exploited extra face prior information to improve the SR performance. Chen et al. <ref type="bibr" target="#b3">[4]</ref> used face landmark heatmaps and parsing maps to super-resolve unaligned LR faces. They first predicted landmark heatmaps and parsing maps from LR faces, and then concatenated them with feature maps to fine tune the SR results. Adrian et al. <ref type="bibr" target="#b0">[1]</ref> jointly learned face SR and landmark prediction. Yu et al. <ref type="bibr" target="#b36">[37]</ref> employed face component heatmaps to preserve face structure while super-resolving LR faces. They used one CNN branch to predict face component heatmaps and then concatenated the heatmaps to the SR CNN branch. Kim et al. <ref type="bibr" target="#b4">[5]</ref> proposed a multi-scale facial attention loss by multiplying the heatmap values to the pixel differences of different scales to better restore pixels around landmarks. However, most of these works followed a general encoder-decoder framework and did not fully utilize the face prior, making them unsuitable to handle real-world LQ images. Blind Face Restoration To deal with blind face restoration, recent works either tried to improve the framework or adopted reference based approach. Adrian et al. <ref type="bibr" target="#b1">[2]</ref> proposed a two-stage GAN framework to learn real degradation. Yang et al. <ref type="bibr" target="#b35">[36]</ref> introduced the HiFaceGAN which progressively replenish face details. As for reference based methods, Song et al. <ref type="bibr" target="#b30">[31]</ref> constructed a high-resolution dictionary to help enhance face components. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed GRFNet which learns to warp a guidance image for blind face restoration. They further improve their work by replacing single reference with multiple reference images <ref type="bibr" target="#b20">[21]</ref> and feature dictionaries <ref type="bibr" target="#b19">[20]</ref>. The aforementioned model based methods fail to make full use of face prior, and the reference based approaches require HQ references which limits their practical application. High Quality Face Generation with GAN. Among all GAN based image generation methods, the style based GANs (StyleGAN) proposed by Karras et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> exceed other works by a large margin and generate HQ faces which are almost indistinguishable from real photos. Instead of directly generating the faces from random latent vector, they first generate style transform parameters from latent vector, and modulated the features with adaptive instance normalization (AdaIN) <ref type="bibr" target="#b12">[13]</ref>. SEAN <ref type="bibr" target="#b43">[44]</ref> and Gau-GAN <ref type="bibr" target="#b26">[27]</ref> extend StyleGAN to generate realistic faces from parsing maps. PULSE <ref type="bibr" target="#b23">[24]</ref> describes a self-supervised face restoration approach by exploring the latent space of a pretrained StyleGAN. mGANprior <ref type="bibr" target="#b9">[10]</ref> extended PULSE with multiple latent codes. Inspired by the above works, we introduce the style transformation approach to face restoration in this paper. Compared with PULSE, our model does not need a time consuming optimization process and preserves the shape and identity better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we first describe our formulation and framework in detail, then introduce the semantic-aware style loss and the other objectives used to train our networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Progressive Semantic-Aware Style Transformation</head><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the architecture of our PSFR-GAN is inspired by GauGAN <ref type="bibr" target="#b26">[27]</ref> and StyleGANs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. It starts with a learned constant of size C ? 16 ? 16, denoted as F 0 , where C is the channel size. Then, F 0 goes through several upsample residual blocks and generates the final features with the same size as HQ images. Let's define the output features of i-th residual block as F i , then the features are progressively upsampled in the following way</p><formula xml:id="formula_0">F i = ? ? ? ? ST ? RES (F i?1 ) , i = 1 ? ST ? U P (F i?1 ) , 1 &lt; i ? 6<label>(1)</label></formula><p>where ? RES (?) denotes the residual convolution block, ? U P (?) denotes the upsample residual convolution block and ? ST (?) denotes the style transformation block. The last feature F 6 goes through a single ToRGB convolution layer and predicts the final output? H . The ? ST (?) blocks are the key parts of our framework. Each of them learns the style transformation parameters y i = (y s,i , y b,i ) for F i from corresponding scale of input pairs, i.e., LQ images and parsing maps denoted as (I i L , I i P ). (I i L , I i P ) are resized to the same size as F i through bicubic interpolation. Then, ? ST (?) can be formulated as follows</p><formula xml:id="formula_1">y i = ?(I i L , I i P ),<label>(2)</label></formula><formula xml:id="formula_2">F i = y i,s ? U P (F i?1 ) ? ? ? U P (F i?1 ) ? ? U P (F i?1 ) + y i,b , 1 &lt; i ? 6<label>(3)</label></formula><p>where ?(?) is a lightweight network composed of several convolution layers. ?(?) and ?(?) are the mean and standard variation of features. Compared with StyleGAN which adpots spatially invariant styles, we follow <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref> and compute the spatially adaptive style parameters y i with the same size as F i . This helps to make full use of the spatial-wise color and texture information from I L as well as shape and semantic guidance from I P . We use the pretrained face parsing network (FPN) to generate I P from LQ inputs I L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semantic-Aware Style Loss</head><p>Recent super-resolution work <ref type="bibr" target="#b7">[8]</ref> has demonstrated that gram matrix loss which is usually used in style transfer helps a lot in recovering textures. To better synthesize texture details, we introduce the semantic-aware style loss L ss which calculates the gram matrix loss for each semantic region separately. We use VGG19 features of layer relu3 1, relu4 1 and relu5 1 to calculate L ss . Denote ? i as the i-th layer feature in VGG19 and the parsing mask with label j as M j (the background is denoted as M 0 ), the semantic aware style loss is formulated as</p><formula xml:id="formula_3">L ss = 5 i=3 18 j=0 G ? i (? H ), M j ? G ? i (I H ), M j 2 , (4) G(?) computes the gram matrix of feature ? i (?) with seman- tic label mask M j as below G(? i , M j ) = ? i M j ) T ? i M j ) M j + ,<label>(5)</label></formula><p>where is the element-wise product, and = 1e?8 is used to avoid zero division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Objectives</head><p>Following previous image restoration works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20]</ref>, we apply reconstruction loss and adversarial loss apart from L ss . Reconstruction Loss. It is the combination of pixel and feature space mean square error (MSE) which aims to constrain the network output? H close to ground truth I H . We formulate the reconstruction loss as</p><formula xml:id="formula_4">L rec = ? H ? I H 2 + s 4 k=1 D k s (? s H ) ? D k s (I s H ) 2 (6)</formula><p>The second term in L rec is the multi-scale feature matching loss <ref type="bibr" target="#b31">[32]</ref>   tasks. In this work, we use multi-scale discriminators and hinge loss as the objective function, defined as</p><formula xml:id="formula_5">L GAN G = s ?E(? s H ),<label>(7)</label></formula><formula xml:id="formula_6">L GAN D = s E max(0, 1 ? D s (I s H )) + E max(0, 1 + D s (? s H )) ,<label>(8)</label></formula><p>For training stability, we incorporate spectral normalization <ref type="bibr" target="#b25">[26]</ref> in both generator and discriminators. In summary, the final loss function for our generator network is defined as</p><formula xml:id="formula_7">L G = ? ss L ss + ? rec L rec + ? adv L GAN G ,<label>(9)</label></formula><p>where ? are the weights for different terms. PSFR-GAN is trained by minimizing L G and L GAN D alternatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Degradation Model</head><p>According to previous works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> and common practice in SISR framework, we generate the LR image I r L with the following degradation model:</p><formula xml:id="formula_8">I r L = ((I H ? k ) ? r +n ? ) JP EGq ,<label>(10)</label></formula><p>where ? represents the convolution operation between the HQ image I H and a blur kernel k with parameter . ? r is the downsampling operation with a scale factor r. n ? denotes the additive white Gaussian noise (AWGN) with a noise level ?. (?) JP EGq indicates the JPEG compression operation with quality factor q. The hyper parameters , r, ?, q are randomly selected for each HR image I H , and I r L is generated online. More details are described in the appendix. After we get I r L , I L = (I r L ) ? r is used for the parsing map prediction and face restoration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Training Data. We adopt the FFHQ <ref type="bibr" target="#b14">[15]</ref> as the training dataset. This dataset consists of 70, 000 high-quality images at a size of 1024 ? 1024. All images are automatically cropped and aligned. We resize the images to 512 ? 512 with bilinear downsampling as the ground-truth HR images, and synthesize the LQ inputs online with Eq. 12 where the parameters are randomly selected.</p><p>Testing Data. We construct two testing datasets, a synthetic one and a real one. For the synthetic test dataset, we randomly choose 2, 800 HQ images from CelebAHQ <ref type="bibr" target="#b13">[14]</ref> which has no identity intersection with FFHQ, and then generate the corresponding LQ images in the same way as training dataset. We denote this synthetic test dataset as CelebAHQ-Test. For the real LQ test dataset, we collect 1, 020 faces smaller than 48 ? 48 from CelebA <ref type="bibr" target="#b22">[23]</ref> and 106 images provided by GFRNet <ref type="bibr" target="#b21">[22]</ref>. The GFRNet-Test contains LR images from VGGFace2 <ref type="bibr" target="#b2">[3]</ref> and IMDB-WIKI <ref type="bibr" target="#b29">[30]</ref>. We also collect some old photos from the internet for testing. All images are cropped and aligned in the same manner as FFHQ, and then resized to 512 ? 512 using bicubic upsampling. We merge all these images together and create a new dataset containing 1, 157 real LQ faces, denoted as PSFR-RealTest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Details</head><p>We use Adam optimizer <ref type="bibr" target="#b16">[17]</ref> to train our networks. We choose ? 1 = 0.5, ? 2 = 0.999, and set the learning rate of the generator and discriminator to 0.0001 and 0.0004 respectively. The trade-off parameters of different losses are set as ? rec = 10, ? ss = 100 and ? adv = 1. The training batch size is set to 4. All models were implemented by PyTorch <ref type="bibr" target="#b27">[28]</ref> and trained on a Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we conduct experiments to compare our framework with other methods on both synthetic and real LQ test datasets, and carry out extensive ablation studies to evaluate the effectiveness of the multi-scale parsing map guidance and semantic aware style loss.  <ref type="table" target="#tab_1">Table 1</ref>. Please zoom in to see the details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics</head><p>For CelebAHQ-Test with ground truth, we take the widely used PSNR, SSIM and MSSIM metrics. However, these pixel space metrics prefer smooth results and are not consistent with human perception, therefore we also adopt LPIPS score <ref type="bibr" target="#b42">[43]</ref> to evaluate the perceptual realism of generated faces. For natural LQ images without ground truth, we use FID score <ref type="bibr" target="#b10">[11]</ref> to measure the statistic distance between the restoration results and a reference HQ face dataset. Compared with other no reference metrics such as NIQE <ref type="bibr" target="#b24">[25]</ref> and IS (Inception Score) <ref type="bibr" target="#b28">[29]</ref> which focus on natural images, FID utilizes a reference HQ face dataset and gives better measurement. We use the ground truth images from CelebAHQ-Test as the reference dataset to evaluate results of PSFR-RealTest. We also provide FID scores for We show results with top-5 FID scores in <ref type="table" target="#tab_1">Table 1</ref>. More results are provided in appendix.</p><p>results of CelebAHQ-Test for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison on Synthetic Datasets</head><p>We first evaluate the performance of different methods on the synthetic CelebAHQ-Test dataset. Following Hi-FaceGAN <ref type="bibr" target="#b35">[36]</ref>, we compare the proposed PSFR-GAN with architectures designed for different restoration tasks: AR-CNN <ref type="bibr" target="#b5">[6]</ref> for JPEG artifacts removal; DeblurGANv2 <ref type="bibr" target="#b17">[18]</ref> for image deblurring; ESRGAN <ref type="bibr" target="#b33">[34]</ref> for natural image super-resolution; Super-FAN <ref type="bibr" target="#b0">[1]</ref> and WaveletSRNet <ref type="bibr" target="#b11">[12]</ref> for face super-resolution; and recent methods HiFaceGAN <ref type="bibr" target="#b35">[36]</ref> and DFDNet <ref type="bibr" target="#b19">[20]</ref> for blind face restoration. <ref type="table" target="#tab_1">Table 1</ref> shows the performance of both statistical metrics (PSNR, SSIM, MSSIM) and perceptual metrics (LPIPS, FID). We can observe that methods designed for specific tasks generally show low perceptual scores than blind face restoration methods. Both DFDNet and PSFR-GAN utilize extra face prior, and their performance is much better than HiFace-GAN. Compared with DFDNet, the proposed PSFR-GAN outperforms by a large margin in terms of most evaluation metrics, especially in FID score (50% improvement), which indicates the superiority of our proposed PSFR-GAN.</p><p>The visualization examples in <ref type="figure" target="#fig_1">Fig. 3</ref> help us understand the quantitative results better. We show three kinds of LQ inputs in each row of <ref type="figure" target="#fig_1">Fig. 3</ref>: LQ with light degradation, LQ with severe degradation, LQ with large pose and different skin color. It can be observed that results of both ESR-GAN and Super-FAN are over smoothed and fail to recover clear face components and textures compared with blind face restoration methods, see 2-nd and 3-rd rows. This indicates that methods designed for specific task cannot handle LQ inputs with complicated blind degradation. The results of HiFaceGAN are clearer and sharper but contain too much artifacts in the mouth, eyes and backgrounds. This is most likely because of the unstable training of GAN without face prior guidance. Different from PSFR-GAN which utilizes parsing map, DFDNet needs to first detect the LQ face components and then matches them to a HQ dictionary. It may fail to find the correct reference when the LQ are too blurry or with large pose. For example, the results of DFDNet in the 3-rd row seems to have two eyeballs which do not exist in the other methods. Meanwhile, PSFR-GAN incorporates the semantic information through parsing map which is more generic than explicit HQ reference. Therefore, the results of PSFR-GAN are more realistic and robust with all kinds of LQ inputs. Moreover, our results are generated using the predicted parsing maps of the pretrained FPN, see last column in <ref type="figure" target="#fig_1">Fig. 3</ref>. This means PSFR-GAN does not need extra information such as HQ dictionaries during the test time, making PSFR-GAN available in most situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison on Real World LQ Images</head><p>The final target of all methods is to restore real world LQ face images. To evaluate the generalization ability of different methods, we also compare the performance of PSFR-GAN on PSFR-RealTest dataset with methods in Table 1. The same as previous results on CelebAHQ-Test, methods designed for blind face restoration still outperform the others. FID score of PSFR-GAN surpasses other methods by a large margin, and is 20% higher than the second best result of DFDNet. <ref type="figure" target="#fig_2">Fig. 4</ref> gives some examples from PSFR-RealTest dataset. We can observe that there are many artifacts in the results of HiFaceGAN. DFDNet seems to have difficulties finding the correct component references for real LQ inputs, for example, teeth in the first row and right eye in the second row. In contrast, the results of our method is much more natural with few conspicuous artifacts. This is because PSFR-GAN generates the results in a progressive way, and the multi-scale parsing maps provide multi-scale style guidance which makes PSFR-GAN able to synthesis realistic textures for each semantic regions, e.g., teeth textures and eyes. The parsing map results in the last column illustrate that the pretrained FPN works well for real LQ inputs. We show more results of PSFR-GAN on PSFR-RealTest datasets in appendix, which demonstrate that PSFR-GAN is practical for real world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with PULSE</head><p>PULSE <ref type="bibr" target="#b23">[24]</ref> is a recent popular method for face restoration. Different from other methods, PULSE is an optimization based method which needs carefully finetune for each LQ input. Therefore, it is unfair to compare the quantitative result of PULSE with others on PSFR-RealTest. Instead, we follow HiFaceGAN and use the historic group photograph of famous physicists taken at the 5th Solvay Conference 1927 for visual comparison. We carefully finetune PULSE on these photos and get the best results as much as we can. Even so, we still observe several typical fail-  ure cases of PULSE shown in <ref type="figure" target="#fig_3">Fig. 5:</ref> (1) age mismatch and shape deformation in the first column; (2) large pose in the second column; (3) background interference in the third column. The final column shows the best result of the test photos, but there are many slight shape changes in the eyes, nose and mouth which make it look like another person. To conclude, although PULSE can generate better details, for example hair textures, the results of our PSFR-GAN are still much better than PULSE in face restoration. Besides, PSFR-GAN is 40 times faster than PULSE (0.1s vs. 4s) to process the same image on GPU. Complete results of Solvay conference are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions and Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Analysis of PSFR-GAN</head><p>Why PSFR-GAN works ? There are two key designs in PSFR-GAN: (1) the features are modulated progressively from coarse-to-fine; (2) the multi-scale input pairs I L and I P work together to provide sufficient color, shape and semantic information. In <ref type="figure" target="#fig_5">Fig. 6(b)</ref>, we show how details are progressively added to the final outputs by first zeroing all inputs and then adding them back from coarse to fine. We can observe that the restoration process is consistent with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LQ Inputs</head><p>Our Results <ref type="figure">Figure 7</ref>: Results for different upscale factors while fixing other degradation parameters.</p><formula xml:id="formula_9">X 16 X 12 X 16 X 8 X 4 X 2</formula><p>our hypothesis, and happens in the following order: high level semantic information comes first, then the mid-level shape and edges, and finally low-level color and details. Similarly, we analyze the effect of I i P and I i L separately in <ref type="figure" target="#fig_5">Fig. 6</ref>(c) and <ref type="figure" target="#fig_5">Fig. 6(d)</ref>. The first column of <ref type="figure" target="#fig_5">Fig. 6</ref>(c) and <ref type="figure" target="#fig_5">Fig. 6(d)</ref> show the result of using I i L and I i P as inputs separately. We can see that in the first image of <ref type="figure" target="#fig_5">Fig. 6(c)</ref>, the nose and mouth borders are not clear and there are many artifacts in the cheek region. This indicates that network without parsing map as inputs only makes the bicubic results (first row and last column in <ref type="figure" target="#fig_5">Fig. 6(b)</ref>) sharper and has difficulties in understanding the semantic meaning of each region. When we add I i P progressively, the artifacts are gradually removed and the edges are clearer. As for <ref type="figure" target="#fig_5">Fig. 6(d)</ref>, we can observe that the semantic regions are clear in all stages, for example the nose and teeth part. Then, color and texture details are added with the entry of I i L . In summary, PSFR-GAN restores the LQ images in a progressive way by modulating the features with multi-scale inputs, where I i L provides the low level color and texture informaiton and I i P contributes the semantic and shape information. Robustness to different degrees of degradation As an example, we verify the effectiveness of the PSFR-GAN for different upscale factors in Eq. 12 and fix other parameters. We can observe from <ref type="figure">Fig. 7</ref> that our network works well for upscale factors ? 12 and can produce reasonable result for ?16, which demonstrates the robustness of PSFR-GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Study</head><p>To explore the effectiveness of parsing map guidance and semantic-aware style loss L ss , we evaluate four variants of our framework in <ref type="table" target="#tab_2">Table 2</ref>: A, baseline model with only I L as inputs; B, baseline model with (I L , I P ) as inputs but without L ss ; C, baseline model with I L as inputs and L ss ; D, the proposed PSFR-GAN. We can observe that parsing map plays an important role in face image restoration and achieves the most improvements, and the L ss can also benefit the restoration results. With the combination of them, our PSFR-GAN can achieve the best performance. <ref type="figure" target="#fig_7">Figure 8</ref> shows some example results on PSFR-RealTest. We can ob-    <ref type="figure">A and B)</ref> produce apparent artifacts especially in the eyes. In contrast, model D with parsing map and L ss has none of the above flaws. It can be inferred from the above observation that 1) parsing map helps to regularize face structure, and 2) L ss helps to synthesize realistic textures for each semantic region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper proposes a multi-scale progressive face restoration network, named PSFR-GAN, which restores LQ face inputs in a coarse to fine manner through semanticaware style transformation. We also proposed the semanticaware style loss based on original gram matrix loss. Experiments on both synthetic and real LQ test datasets demonstrate the superority and robustness of our PSFR-GAN. By pretraining the face parsing network (FPN) for LQ inputs, our framework can generate high-resolution and realistic HQ outputs without requiring extra inputs. In summary, PSFR-GAN provides a robust and easy-to-use solution for face restoration in real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>Given a LQ face image of any size, we first upsample it to 512 ? 512 and treat it as the input I L . FPN is trained to produce a parsing map? P and a HR face? H that approximate the ground-truth parsing map I P and ground-truth HR face I H respectively, i.e., ? p = arg min ?p L parse (? P , I P ) + L pix (? H , I H ), <ref type="bibr" target="#b10">(11)</ref> where ? p denotes the parameters of FPN, L parse is the parsing loss, and L pix is the pixel space L2 loss. As shown in <ref type="figure">Fig. 9, FPN</ref> adopts an encoder-resnet-decoder architecture. It begins with 4 downsample blocks, followed by 10 resnet blocks and 4 upsample blocks. Finally, two output convolution layers are used to generate? P and? H . We adopt BatchNorm-LeakyRelu after every convolution layer. We use multi-task learning for FPN because we found that L pix is quite helpful for the prediction of? P . Since I L is degraded, both the pixel values and pattern of the face components are not clear and stable. The network is not able to understand the meaning of each label without the extra supervision of I H . <ref type="figure" target="#fig_9">Fig. 10</ref> shows that the parsing results with multi-task learning are much better than that without it, especially in the eyes and eyebrows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Datasets and Implementation</head><p>We use CelebA-Mask-HQ <ref type="bibr" target="#b18">[19]</ref> to train FPN. The CelebA-Mask-HQ contains 30, 000 HR faces with a size of 1024 ? 1024 selected from the CelebA <ref type="bibr" target="#b22">[23]</ref> dataset. Each image has a segmentation mask of facial attributes corresponding to CelebA. The masks of CelebA-Mask-HQ are manually-annotated with a size of 512 ? 512 and 19 classes including background, skin, nose, eyes (left and right), eyebrows (left and right), ears (left and right), mouth, lips (up and bottom), hair, hat, eyeglass, earring, necklace, neck, and cloth. The whole dataset is split into a training set (24, 183 images), a validation set (2, 993 images), and a test set (2, 824 images). We use the training set as ground-truth HQ faces and parsing maps, and the LQ faces are generated online with Eq. 12.</p><p>We use Adam optimizer <ref type="bibr" target="#b16">[17]</ref> to train the FPN. We set ? 1 = 0.9, ? 2 = 0.999 and learning rate to 0.0002. The training batch size is set to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Degradation Model</head><p>As described in the paper, our degradation model used the following equation: ? JP EG q is the JPEG operation. The compression level is randomly chosen from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">65]</ref>, in which higher means stronger compression and lower image quality.</p><formula xml:id="formula_10">I r L = ((I H ? k ) ? r +n ? ) JP EGq ,<label>(12)</label></formula><p>We implement the degradation model using imgaug 1 library with code snippets in <ref type="figure">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Results</head><p>In this section, we show more results on PSFR-RealTest and Solvay conference test. We mainly compare our model with DFDNet because they provide public codes and test models, and their results are current state-of-the-art. We also provide carefully finetuned results of PULSE on Solvay test. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Results of PSFR-RealTest</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Results of Solvay Conference Test</head><p>We give the overall results of the 5-th Solvay conference test images in <ref type="figure" target="#fig_3">Fig. 15</ref>. All faces are cropped out and aligned first, then enhanced by our model and finally paste back to the original photo. Complete results and detailed comparison with other methods are presented in <ref type="figure" target="#fig_5">Fig. 16, Fig. 17</ref>, <ref type="figure" target="#fig_7">Fig. 18, Fig. 19</ref>   <ref type="figure">Figure 9</ref>: Architecture details of face parsing network (FPN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LR Images</head><p>Without</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task</head><p>With Multi-Task  <ref type="figure" target="#fig_1">GaussianBlur((3, 15)</ref>), iaa.AverageBlur <ref type="figure" target="#fig_1">(k=(3, 15)</ref>), iaa.MedianBlur <ref type="figure" target="#fig_1">(k=(3, 15)</ref>), iaa.MotionBlur((5, 25)) ])), iaa.Resize(scale_size, interpolation=ia.ALL), iaa.Sometimes(0.2, iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.1 * 255), per_channel=0.5)) , iaa.Sometimes(0.7, iaa.JpegCompression(compression=(10, 65))), iaa.Resize(org_size), ])  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Visulization of the proposed progressive semantic-aware style transformation network for face restoration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visual comparisons on CelebAHQ-Test dataset. The proposed PSFR-GAN shows the best results for LQ inputs with light degradation, severe degradation, large pose and different skin colors. The predicted parsing maps of LQ inputs through pretrained FPN are overlapped at the corner of GT images. For better visualization experience, we only show results with top-5 FID scores in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparisons on PSFR-RealTest dataset. Results of the proposed PSFR-GAN are clearer and more realistic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visual comparison between PULSE and PSFR-GAN. Complete results are provided in the appendix. Please zoom in to see the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Multi-scale LQ and parsing map input pairs (I i L , I i P ). Results of adding multi-scale inputs (I i L , I i P ) progressively. (c) Keep all scales of LQ inputs I i L , and add different scales of parsing maps I i P progressively. (d) Keep all scales of parsing map inputs I i P , and add different scales of LQ inputs I i L progressively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Analysis of PSFR-GAN. Zoom in to see details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Visual comparison between different variations of our model. Please zoom in to see the details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 ,</head><label>12</label><figDesc>Fig. 13and Fig. 14 show more examples from PSFR-RealTest dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of parsing results of natural LR faces with and without supervision of I H . import imgaug as ia import imgaug.augmenters as iaa scale_size = random(32, 256) org_size = 512 aug_seq = iaa.Sequential([ iaa.Sometimes(0.5, iaa.OneOf([ iaa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :Figure 12 :Figure 13 :Figure 14 :</head><label>11121314</label><figDesc>Code snippets for degradation model. LQ DFDNet PSFR-GAN (ours) Parsing Map (ours) More results from PSFR-RealTest Dataset. LQ DFDNet PSFR-GAN (ours) Parsing Map (ours) More results from PSFR-RealTest Dataset. LQ DFDNet PSFR-GAN (ours) Parsing Map (ours) More results from PSFR-RealTest Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :</head><label>151617181920</label><figDesc>Overall result of the 5-th Solvay conference taken in 1927. Please zoom in to see the details. LQ PULSE DFDNet PSFR-GAN (ours) Parsing Map (ours) Results of 5-th Solvay conference test. LQ PULSE DFDNet PSFR-GAN (ours) Parsing Map (ours) Results of 5-th Solvay conference test. LQ PULSE DFDNet PSFR-GAN (ours) Parsing Map (ours) Results of 5-th Solvay conference test. LQ PULSE DFDNet PSFR-GAN (ours) Parsing Map (ours) Results of 5-th Solvay conference test. Results of 5-th Solvay conference test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>denotes the k-th layer features in D s . Adversarial Loss. It has been proved to be effective and critical in generating realistic textures in image restoration</figDesc><table><row><cell></cell><cell>2</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv.</cell><cell>Instance Norm</cell><cell>LReLU</cell></row><row><cell>32</cell><cell>Style Transformation Block</cell><cell>16</cell><cell>8</cell><cell>4</cell><cell>ST Block</cell><cell>2 2</cell><cell>ST Block</cell><cell>ST Block</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ST Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ST Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>To</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell cols="2">RGB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv.</cell><cell>Instance Norm</cell><cell>LReLU</cell></row></table><note>which matches the discriminator features of? H and I H . s ? {1, 1 2 , 1 4 } is the downscale factors, and D k s (?)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Task</cell><cell>Methods</cell><cell cols="4">CelebAHQ-Test PSNR? SSIM? MSSIM? LPIPS?</cell><cell>FID?</cell><cell>PSFR-RealTest FID?</cell></row><row><cell>JPEG artifacts removal</cell><cell>ARCNN</cell><cell>22.78</cell><cell>0.6538</cell><cell>0.7462</cell><cell cols="2">0.5862 133.38</cell><cell>124.46</cell></row><row><cell>Deblur</cell><cell>DeblurGANv2</cell><cell>22.66</cell><cell>0.6587</cell><cell>0.7493</cell><cell cols="2">0.5546 113.85</cell><cell>97.42</cell></row><row><cell></cell><cell>ESRGAN</cell><cell>21.95</cell><cell>0.6096</cell><cell>0.7293</cell><cell>0.5515</cell><cell>97.02</cell><cell>57.51</cell></row><row><cell>Super-Resolution</cell><cell>Super-FAN</cell><cell>22.71</cell><cell>0.6527</cell><cell>0.7459</cell><cell>0.4908</cell><cell>94.95</cell><cell>65.45</cell></row><row><cell></cell><cell>WaveletSRNet</cell><cell>23.50</cell><cell>0.6595</cell><cell>0.7542</cell><cell cols="2">0.5409 111.60</cell><cell>108.21</cell></row><row><cell></cell><cell>HiFaceGAN</cell><cell>21.50</cell><cell>0.5495</cell><cell>0.6900</cell><cell>0.4569</cell><cell>57.81</cell><cell>56.48</cell></row><row><cell>Blind-Restoration</cell><cell>DFDNet</cell><cell>22.28</cell><cell>0.6589</cell><cell>0.7650</cell><cell>0.3791</cell><cell>37.34</cell><cell>37.63</cell></row><row><cell></cell><cell>PSFR-GAN (ours)</cell><cell>23.64</cell><cell>0.6557</cell><cell>0.7740</cell><cell>0.3042</cell><cell>23.20</cell><cell>30.39</cell></row></table><note>Quantitative comparison on different restoration tasks with state-of-the-art methods. The test datasets are generated with FFHQ-Test using random parameters of each specific degradation type.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the proposed method.</figDesc><table><row><cell>ID Model Variations</cell><cell>FID?</cell></row><row><cell>A Baseline with I L</cell><cell>47.48</cell></row><row><cell>B + I P</cell><cell>32.37</cell></row><row><cell>C + L ss</cell><cell>34.22</cell></row><row><cell>D + I P + L ss</cell><cell>30.39</cell></row><row><cell cols="2">serve from the bottom row that models without parsing map</cell></row><row><cell>(i.</cell><cell></cell></row></table><note>e., model A and C) fail to generate clear shapes when the face border in LQ image is not clear. On the other hand, models without L ss (i.e., model</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>where ? k is the blur kernel. We randomly choose one of the following four kernels: Gaussian Blur (3 &lt;= &lt;= 15), Average Blur (3 &lt;= &lt;= 15), Median Blur (3 &lt;= &lt;= 15), Motion Blur (5 &lt;= &lt;= 25);? ? s is the downsample operation. The scale factor r is randomly selected in [ 32 512 , 256 512 ]; ? n ? is the addictive white gaussian noise (AWGN) with 0 &lt;= ? &lt;= 0.1 ? 255;</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>and Fig. 20. https://github.com/aleju/imgaug</figDesc><table><row><cell>64</cell><cell></cell><cell></cell><cell></cell><cell cols="3">128 256 512 512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>256 128 64 19</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>To RGB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResBlock</cell><cell></cell><cell>Conv.</cell><cell></cell><cell cols="3">Batch Norm</cell><cell cols="2">LReLU</cell><cell></cell><cell></cell></row><row><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>19</cell></row></table><note>1</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by Alibaba DAMO Academy, Hong Kong RGC RIF grant (R5001-18), and Hong Kong RGC GRF grant (project# 17203119).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To learn image super-resolution, use a gan to learn how to do image degradation first</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FSRNet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressive face super-resolution via attention to facial landmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Deokyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Minseon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwon</forename><surname>Gihyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Dae-Shik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of texture transfer for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Muhammad Waleed Gondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="80" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Blind super-resolution with iterative kernel correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1604" to="1613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image processing using multi-code gan prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3012" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1689" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<title level="m">Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8878" to="8887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maskgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11922</idno>
		<title level="m">Towards diverse and interactive facial image manipulation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind face restoration via deep multi-scale component dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning warped guidance for blind face restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pulse: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2437" to="2445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to hallucinate face images via component generation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4537" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshops (ECCVW)</title>
		<meeting>the European Conference on Computer Vision Workshops (ECCVW)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to superresolve blurry face and text images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hifacegan: Face renovation via collaborative suppression and replenishment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face super-resolution guided by facial component heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="217" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face hallucination with tiny unaligned images by transformative discriminative neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3760" to="3768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Superidentity convolutional neural network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="183" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sean: Image synthesis with semantic region-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
