<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-29">29 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hritam</forename><surname>Basak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Kundu</surname></persName>
							<email>rohitkunduju@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Sarkar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">INDIA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-29">29 Mar 2022</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Elsevier March 30, 2022</note>
					<note>* Corresponding author (Rohit Kundu )</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lesion Segmentation</term>
					<term>Deep Learning</term>
					<term>Parallel Partial Decoder</term>
					<term>Attention Modules</term>
					<term>Skin Melanoma</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: P H 2 , ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework. The relevant codes for the proposed approach are accessible at (a) Raw dermoscopy image (b) Ground truth mask Figure 1: Example of a skin lesion image and its ground truth mask. https://github.com/Rohit-Kundu/MFSNet</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Melanoma is the most severe and deadly type of skin cancer, causing more than 13 thousand incidences globally. Though less prevalent than its nonmalignant counterpart, malignant melanoma is increasing at an alarming rate of 4% per year. Research has shown its correlation with genetic and physical variations. The primary cause of melanoma is long-term exposure to ultraviolet (UV) rays. With the increase in greenhouse gases, the protective ozone layer in the stratosphere is depleting rapidly, causing the harmful solar UV rays to reach the earth's surface. This causes the global incidence of melanoma to rise rapidly. Fortunately, studies like Siegel et al. <ref type="bibr" target="#b41">[40]</ref> show that early detection can decrease the chances of fatality by 97%. Surgical treatment of melanoma is often disfiguring and extremely painful, justifying the importance of early detection of the disease. Dermoscopy is a non-invasive test for detecting and diagnosing pigmented skin lesions and malignant melanoma in the early stages. It is often considered the golden standard for melanoma localization. However, manual labeling and reviewing are extremely grueling and cumbersome even for expert clinicians, relying on their perceptions and vision. Therefore, to mitigate the problem, Computer-Aided Diagnosis (CAD) systems have been widely preferred as a support system to aid clinicians in automated segmentation and analysis of malignant melanoma.</p><p>Semantic segmentation refers to the pixel-level classification of the images.</p><p>Each pixel in an image is classified as part of the object class or background class. This is beneficial for localizing the region of interest (ROI) from the raw images for further analysis and thus is a vital preprocessing step in automated disease diagnosis. <ref type="figure" target="#fig_6">Figure 1(a)</ref> shows an example of a raw skin-lesion image.</p><p>Its segmented image, called "ground truth," is shown in <ref type="figure" target="#fig_6">Figure 1(b)</ref>. Here, the image is classified into two classes, namely "lesion" and "background," which led to the generation of a "binary mask" image. The task of semantic segmentation is to generate a segmentation map like <ref type="figure" target="#fig_6">Figure 1</ref> Segmentation of skin melanoma from non-invasive dermoscopy images relies upon several emerging and traditional methods. Among them, segmentation methods based on artificial intelligence have widely been explored and adopted due to their excellent accuracy, robustness, and reliability. Extensive research has been conducted in the last few years, using neural networks <ref type="bibr" target="#b3">[4]</ref>, fuzzy logic <ref type="bibr" target="#b22">[21]</ref>, attention-gated networks <ref type="bibr" target="#b49">[48]</ref>, or their combinations with traditional image processing methods to improve the segmentation performance. The significant variations in texture, size, shape, the position of lesions, and obscure boundaries in dermoscopy images make it extremely challenging to obtain accurate and prominent tissue-level segmentation maps for developing CAD systems.</p><p>Pre and post-processing are the other essential aspects and used in most of the current segmentation methods <ref type="bibr" target="#b14">[14]</ref> for effectively removing artifacts, enhancing image quality, removing unnecessary noises in images for effective and accurate segmentation of pigmented skin lesions from images. Beuren et al. <ref type="bibr" target="#b7">[8]</ref> proposed a series of morphological operations for image enhancement of image resolution and denoising before segmentation. Later Chatterjee et al. <ref type="bibr" target="#b14">[14]</ref> proposed the Fractal Region Texture Analysis (FRTA) method for quantification of texture information integrated with Recursive Feature Elimination (RFE) and several morphological operations as preprocessing before classification of dermoscopic images. Verma et al. <ref type="bibr" target="#b47">[46]</ref> showed that median filters and anisotropic diffusion filters can be helpful in not only smoothing the images but also removal of thick hairlines, preserving sufficient lesion edge information. Recently, morphological operations and image inpainting methods have been modified and used in research for dermoscopy image analysis <ref type="bibr" target="#b39">[38]</ref>. In the preprocessing step, this research has incorporated the image inpainting method for unwanted hair removal from the input images.</p><p>In literature, the skin lesion segmentation methods are broadly classified into the following categories: (a) edge detection and thresholding <ref type="bibr" target="#b32">[31]</ref>, (b) active contour models <ref type="bibr" target="#b46">[45]</ref>, and (c) segmentation based on convolutional neural network (CNN) <ref type="bibr" target="#b53">[50,</ref><ref type="bibr" target="#b6">7]</ref>, etc. Symmetrical encoder-decoder architecture, also known as U-Net, proposed by Ronneberger et al. <ref type="bibr" target="#b37">[36]</ref>, is widely used and considered as the golden standard for several image segmentation tasks. It consists of a downsampling path that captures sufficient semantics and context, connected to an expanding path for accurate localization of the ROI. Later Zhou et al. <ref type="bibr" target="#b60">[57]</ref> proposed a novel architecture UNet++ by redesigning the series of nested dense skip connections to reduce the semantic gap between the feature representations and the encoder-decoder sub-networks. Their proposed model outperformed the previous U-Net architecture in multiple biomedical image segmentation tasks.</p><p>Weng et al. <ref type="bibr" target="#b54">[51]</ref> proposed another modification in the U-Net backbone by incorporating neural architecture search (NAS), thereby improving the segmentation performance significantly. SegNet <ref type="bibr" target="#b4">[5]</ref> is a similar encoder-decoder model, for instance, segmentation, that uses a VGG16 backbone followed by a decoder path integrated with a pixel-wise classification layer. This is a well-known segmentation model for binary or multi-class segmentation problems and has been proven to produce state-of-the-art results in various domains. Yuan et al. <ref type="bibr" target="#b57">[54]</ref> proposed a fully convolution-deconvolution network that was able to produce a dice similarity score of 76.5% on the ISIC 2017 dataset. Later Abraham et al. <ref type="bibr" target="#b0">[1]</ref> proposed a novel Focal Tversky Loss function, then integrated with attention U-Net, produced a state-of-the-art result on BUS 2017B and ISIC2018 datasets with average dice scores of 80.4% and 85.6% respectively. Double U-Net <ref type="bibr" target="#b27">[26]</ref>, another modification of U-Net that used two different upsampling branches instead of one, was used to produce two different segmentation maps, slightly different from one another. The paper reported an average dice score of 89.2% on the ISIC 2017 dataset.</p><p>Recently meta-heuristic-based optimization algorithms have been explored for thresholding-based segmentation and image enhancement operations in different applications. Aljanabi et al. <ref type="bibr" target="#b2">[3]</ref> proposed an image thresholding method by selecting an optimum threshold level using the artificial bee colony (ABC) algorithm. The algorithm was able to produce segmentation maps with high confidence on several widely known skin datasets. Attention mechanisms are also widely known for boosting the performance of CNN-based models in different computer vision applications. Chattopadhyay et al. <ref type="bibr" target="#b15">[15]</ref> proposed a multi-scale attention mechanism which is inspired by the work of <ref type="bibr" target="#b8">[9]</ref> for accurate localization and segmentation of objects. The dual attention mechanism was proposed by <ref type="bibr" target="#b20">[19]</ref> adaptively integrates the local features with their corresponding global dependencies. Though used in scene segmentation application, it inspired several similar works in the biomedical domain <ref type="bibr" target="#b5">[6]</ref>. Generative Adversarial Networks (GAN) have also been instrumental for extensive research for biomedical image segmentation recently <ref type="bibr" target="#b28">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Overview and Contributions</head><p>To address the issues mentioned before, we propose a novel skin lesion segmentation framework, called Multi-Focus Segmentation Network (MFSNet), that produces the final segmentation map by focusing on image information at multiple scales. Taking a clue from the standard clinical practice, we can say that the area and boundary are the two essential aspects to produce the accurate pixel-level segmentation map based on local appearance from a coarse localization of the melanoma region. The proposed model generates a coarse segmen-tation map implicitly by aggregating image features at multiple levels, followed by a series of reverse and boundary attention networks by iteratively learning pixel-level information of area and boundary by explicitly using the coarse map and ground truth the global guidance. We have evaluated the performance of the proposed model on three publicly available skin melanoma datasets: The P H 2 dataset by Mendoncca et al. <ref type="bibr" target="#b34">[33]</ref>, the ISIC 2017 dataset by Codella et al. <ref type="bibr" target="#b17">[17]</ref> and the HAM10000 dataset by Tschandl et al. <ref type="bibr" target="#b43">[42]</ref>. The proposed model outperforms state-of-the-art models on the same datasets justifying the reliability and robustness of the framework.</p><p>The contributions of the present research are as follows:</p><p>1. The use of differently focused segmentation maps in various stages of the proposed MFSNet helps accurately map both the lesion's coarse structure and its fine edges. 3. We evaluate the proposed MFSNet model on three publicly available datasets: P H 2 , ISIC 2017 and HAM10000 datasets, and obtain dice similarity coefficient values of 0.954, 0.987, and 0.906 respectively on the datasets, outperforming state-of-the-art methods on the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Method</head><p>This section describes the architecture of our proposed MFSNet, which combines the high-level semantics and the low-level edge information by using a series of RA modules, BA block, and a PPD module. We propose a hybrid loss function that integrates the widely used Binary Cross-Entropy (BCE) loss with the Weighted IoU loss functions. The whole segmentation process is followed by image inpainting and a preprocessing step for artifact removal, described in Section 2.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Image Preprocessing</head><p>Dermoscopy images vary in terms of size, pixel intensity and may suffer from unwanted artifacts in the form of noises or body hair. These artifacts may lead to abrupt segmentation results in some images and may diminish the overall model performance. Hence, to address these problems, we used standard image preprocessing methods before segmenting the images. All the images have been resized to a shape of 256 ? 256 for faster convolution and resolving excessive memory constraints. Next, we perform image normalization to resolve the uneven image contrast issues. Finally, we introduce the image inpainting method for hair removal.</p><p>Following the work of <ref type="bibr" target="#b42">[41]</ref>, we have used several morphological operations for hair removal from dermoscopy images. First, the input RGB images are converted to grayscale images, followed by blackhat transformation as proposed by <ref type="bibr" target="#b48">[47]</ref>. In this regard, we define a structuring element: a cross-shaped twodimensional array of shape 17 ? 17, i.e., an array whose middle row and column are composed of 1's and all other places contain 0.</p><p>Similar to <ref type="bibr" target="#b48">[47]</ref>, closing is also performed to remove small hollows inside a region while keeping the original region shape and size unaltered. Thus the blackhat transformation results in an output image containing elements darker than the surrounding pixel values, whereas smaller than the structuring element.</p><p>A suitable threshold value is applied to the obtained output from the blackhat transformation to obtain the hair-like artifacts.</p><p>Fast marching method <ref type="bibr" target="#b31">[30]</ref> is widely used for segmentation purposes. In this research, we have used this algorithm for image inpainting. We used the thresholded image output from blackhat transformation and the original input image and replaced the artifacts or hair structures with the neighboring pixels.   supervised with the ground truth G through a loss function (described in Section 2.7). Thus, by using the parallel RA and the residual connections between the segmentation of multiple scales and the ground truth, the errors can be removed by "larger-scale adaptability" <ref type="bibr" target="#b16">[16]</ref>. Finally, the output O 2 is passed through the sigmoid activation function to produce the final segmentation map S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MFSNet Architecture</head><p>In general, the error between the input and output of the RA unit is minor (zero in the extreme case), thus making the learning comparatively easy with very few parameters. Hence, the network can be very effective in region segmentation with fewer parameters. As the learning procedure of the network is focused on generating multiple levels of outputs from multiple branches, the network is named as MFSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Workflow of the MFSNet</head><p>As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, the proposed model consists of a series of convolution operations, RA branches, and BA modules. For the convenience of the readers, we have described below the flow of information from the input image through the layers and branches to produce the segmentation output finally.  Output of the RA block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Parallel Reverse Attention branch</head><p>In medical diagnosis, clinicians go for a rough estimation of skin melanoma before looking into the tissue-level finer details for proper localization and labeling. Though, it is not easy for a network to learn residual refinement for saliency detection without proper supervision, leading to inaccurate segmentation results. As most of the existing methods heavily rely on image classification networks, fine-tuned for responsiveness to very few discriminatory regions in images, it deviates from the requirement of exploration of pixel-wise prediction of dense regions. We propose a two-stage segmentation method using a parallel RA unit to mitigate this problem and replicate the real-world clinical approach.</p><p>The deep layers of the CNN produce coarse-level and a rough estimation of the melanoma region, with small structural details <ref type="bibr" target="#b13">[13]</ref>. Next, followed by the idea of progressive erasing of the foreground region <ref type="bibr" target="#b51">[49]</ref>, we mine discriminative melanoma regions using the RA unit. Instead of aggregating features from all the CNN layers, <ref type="bibr" target="#b16">[16]</ref>, our proposed RA model guides the learning of the whole network, starting from the coarse saliency map produced by the deepest CNN layer, containing the highest semantic confidence, by sequentially discovering new information about complementary melanoma regions from the side-output of the last three layers only. <ref type="figure" target="#fig_7">Figure 4</ref> shows with the i th level feature set F i , Con is the operation of passing the feature through a couple of convolutional layers with filter size set to 64, and is the element-wise multiplication of the concatenated feature with the RA mask</p><formula xml:id="formula_0">M RA . O R (F i ) = M i RA Con[F i ? D(O B )],<label>(1)</label></formula><p>Chen et al. <ref type="bibr" target="#b16">[16]</ref> defined the RA mask as in <ref type="bibr">Equation 2</ref>, where is the operation of forming a 64-channel tensor by repeating the single-channel output, to match the dimension, is the subtraction operation, Sof tmax indicates the sigmoid activation, S i+1 is the segmentation mask obtained from the (i + 1) th layer of the CNN, U is the upsampling operation.</p><formula xml:id="formula_1">M i RA = [1 Sof tmax{U(S i+1 (j)}],<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Boundary Attention</head><p>Edge information can guide the task of feature extraction for segmentation by providing helpful supervision with fine-grained boundary constraints as shown in <ref type="bibr" target="#b59">[56]</ref>. Hence, being inspired by the Edge Guidance Module (EGM), proposed by <ref type="bibr" target="#b58">[55]</ref>, we have used a BA module along with the parallel RA branches for extracting accurate boundary information. Based on the fact that only low-level features contain substantial edge information, we have fed the shallow feature F 2 from the encoder network to the BA module as shown in <ref type="figure" target="#fig_9">Figure 5</ref>.  where j is the pixel position index, U i denotes the i th level upsampled prediction.</p><formula xml:id="formula_2">O B (F i ) = M i B F i ,<label>(3)</label></formula><formula xml:id="formula_3">S i (j) = ? ? ? ? ? 1, if ?[U i (j)] &gt; 0.5 0, otherwise ,<label>(4)</label></formula><p>The value of i is set to 2, 3, i.e., we only consider the second and third level features from the CNN to feed into the BA module. ? is the softmax activation function given by the Equation <ref type="bibr" target="#b4">5</ref>.</p><formula xml:id="formula_4">?(x i ) = exp x m n exp x n<label>(5)</label></formula><p>Next, distance transformation <ref type="bibr" target="#b19">[18]</ref> is applied over S i to fill each pixel position of the melanoma region with the distance to the melanoma boundary. Conversely, the distances of the pixels of non-melanoma regions can be obtained by simply transposing S i followed by distance transformation. The overall distance map is produced by normalizing and summing up these two distance maps as given by <ref type="bibr">Equation 6</ref>, where S i is the transpose of segmentation map S i which can be obtained as S i = 1 ? S i .</p><formula xml:id="formula_5">D i = DT (S i ) max j DT [S i (j)] + DT (S i ) max j DT [S i (j)] ,<label>(6)</label></formula><p>In <ref type="bibr">Equation 6</ref>, D i has values equal to 0 and 1 at the melanoma boundary and the farthest point from the boundary, respectively. Here, we define the i th level boundary mask M B i as</p><formula xml:id="formula_6">M B i = 1 ? D i<label>(7)</label></formula><p>Finally, we calculate the boundary map G B from the ground truth using its gradient, which is constrained by the BCE loss to measure the dissimilarity between the produced boundary map O B with the actual boundary map G B</p><p>given by Equation <ref type="bibr" target="#b7">8</ref>.</p><formula xml:id="formula_7">L B = ? j [G B log(O B ) + (1 ? G B ) log(1 ? O B )]<label>(8)</label></formula><p>The overall architecture of the BA module is shown in <ref type="figure" target="#fig_9">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Partial Parallel Decoder module</head><p>As suggested by <ref type="bibr" target="#b55">[52]</ref>, low-level features contribute very little toward the final prediction map with a massive requirement of computation due to their high spatial resolution. However, in literature, most of the existing models like <ref type="bibr" target="#b60">[57,</ref><ref type="bibr" target="#b24">23]</ref> are designed to aggregate both high and low-level semantics, leading to unnecessary wastage of resources and inefficient segmentation map. To mitigate this problem, we have used a PPD module to capture the global context information, being inspired from the Receptive Field Block (RFB) module by <ref type="bibr" target="#b30">[29]</ref>.</p><p>Specifically, we have used the first five convolution layers of the Res2Net <ref type="bibr" target="#b21">[20]</ref>, among which the first three layers are considered as the low-level features and are discarded for the decoder module. To accelerate the feature propagation, we add a series of convolution and batch normalization operations as shown in <ref type="figure" target="#fig_11">Figure 6</ref>.</p><p>Short connections are added in the PPD module, similar to the original RFB module. After obtaining different discriminating features from different layers, we finally multiply them to reduce the gap between multiple feature levels. Thus, the PPD module produces a global segmentation map O S through a series of element-wise multiplication and concatenation operations, serving as the global guidance of the parallel RA branches. Proper downsampling and upsampling operations are performed throughout, whenever required, to match the feature dimensions before concatenation. Finally, the generated segmentation map is of a similar dimension as that of the input of the MFSNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Deep Supervision</head><p>To supervise the segmentation performance, we have used a hybrid loss func- </p><formula xml:id="formula_8">L S = ?L wBCE + (1 ? ?)L wIoU ,<label>(9)</label></formula><p>The experimental analysis is shown in <ref type="figure" target="#fig_13">Figure 7</ref>. The L wIoU and L wBCE are effective to increase the weights of the hard pixels rather than giving equal weights to each pixel like the standard IoU loss and BCE loss functions.</p><p>The side outputs from the CNN are upsampled to form segmentation map O U P i ; i = 4, 5 of the same size of the ground truth G. Thus, the overall loss function is extended to <ref type="bibr">Equation 10</ref>.</p><formula xml:id="formula_9">L = L S (G, O S ) + i=2,3 L B (G B , O B,i ) + i=4,5 L S (G, O U P i )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results and Discussion</head><p>This section evaluates the proposed framework on three publicly available datasets of skin lesion segmentation, using 5-fold cross-validation. We discuss the significance of the obtained results and compare the model with other stateof-the-art models to justify the superiority of the proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Description</head><p>Three dermatology datasets have been used in the current research to evaluate the performance of MFSNet:</p><p>1. P H 2 dataset by <ref type="bibr" target="#b34">[33]</ref> consisting of 200 images.</p><p>2. ISIC 2017 dataset by <ref type="bibr" target="#b17">[17]</ref> consisting of 2379 images.</p><p>3. HAM10000 dataset by <ref type="bibr" target="#b43">[42]</ref> consisting of 10015 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Metrics</head><p>To evaluate the performance of the proposed model on the supervised skin lesion segmentation problem, we use five popularly used metrics which are described as follows: .</p><formula xml:id="formula_10">Spe(S, G) = |(1 ? S) ? (1 ? G)| |1 ? G|<label>(15)</label></formula><p>For all the mentioned evaluation metrics, the mean value over all the test images has been reported in this study for evaluation denoted by mIoU , mDSC, mF M , mSen and mSpe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>The proposed MFSNet is implemented in PyTorch and is accelerated using an NVIDIA Tesla K80 GPU. <ref type="table" target="#tab_0">Table 1</ref> shows the results obtained by MFSNet on the three publicly available datasets using 5-fold cross-validation. The high values of DSC and IoU suggest that the segmentation is reasonably accurate.</p><p>In contrast, the high Sensitivity and Specificity values suggest the maintenance  of these experiments are presented in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ablation Study</head><p>We have experimented by removing different components from the proposed model to justify their impact on the overall performance. We have performed an ablation study of RA, PPD, BA modules and their different orientations concerning the convolution layers of the backbone Res2Net model to assert the importance of the proposed configuration used in the MFSNet architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Orientation of BA and RA</head><p>We have experimented with different combinations and orientations of BA and RA branches to explore the best possible combinations for boosting performance. We have slightly better performance in instance three than in instance 5, establishing the importance of the RA module at the Conv4 layer. Based on these observations, we have finalized the orientations of different RA and BA blocks to optimize the segmentation performance and add their clinical importance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">Importance of BA</head><p>In this work, we have also performed an ablation study to investigate the importance of the proposed BA module in the overall model. Row 3 in <ref type="table" target="#tab_4">Table 4</ref> shows the performance of the proposed architecture has improved by a considerable margin in terms of significant evaluation metrics by using the BA module along with the Res2N et backbone as compared to the backbone only in row 1.</p><p>Besides, using BA along with RA boosts the model performance as compared to only the RA module, shown in row 4 and row 5 of <ref type="table" target="#tab_4">Table 4</ref>, leading to the conclusion that BA has an essential contribution towards achieving better segmentation outcome. <ref type="bibr" target="#b58">[55]</ref> also exemplified that optimal edge guidance can boost the segmentation performance significantly, justifying the results obtained in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Importance of RA</head><p>Row 4 in <ref type="table" target="#tab_4">Table 4</ref> shows that RA is another essential component of the proposed module as removing it may reduce the DSC, IoU, and other evaluation results significantly. The optimal combination of BA and RA modules (shown in <ref type="table" target="#tab_2">Table 3</ref>) is another essential feature of the proposed MFSNet, where the addition of RA has boosted the model performance as compared to the mere BA module, shown in row 5 of <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4.">Importance of PPD</head><p>PPD is another vital component of our proposed method, as removal of this can affect the model performance as shown in <ref type="table" target="#tab_4">Table 4</ref>. We can observe from row 2 of <ref type="table" target="#tab_4">Table 4</ref> that adding PPD to the baseline model can increase the performance, unparalleled to the contribution of RA and BA. Again, combining it with the RA module, as shown in row 6, can produce an almost similar performance to that of the proposed architecture. The improvements establish that PPD, combined with RA, is the prime component of the proposed MFSNet. <ref type="table" target="#tab_5">Table 5</ref> compares the proposed method to several state-of-the-art methods on the three datasets used. The proposed MFSNet performs significantly better than the said methods and can be justified as a reliable framework for skin lesion segmentation. To further prove the superiority of the MFSNet framework, we use some popular segmentation models prevalent in literature for comparison:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Comparison to State-of-the-art</head><p>U-Net <ref type="bibr" target="#b37">[36]</ref>, SegNet <ref type="bibr" target="#b4">[5]</ref> and Double U-Net <ref type="bibr" target="#b27">[26]</ref>, the results of which are also compared in <ref type="table" target="#tab_5">Table 5</ref>. Some visual results of the predicted segmented masks by these models and the proposed MFSNet are shown in <ref type="figure" target="#fig_15">Figure 8</ref>. From the visual results, it can be seen that SegNet consistently produces unsatisfactory results.</p><p>U-Net can segment most images well, but it fails to perform well for relatively challenging images. Double U-Net performs closest to the MFSNet. However evidently MFSNet outperforms all these models as justified from both <ref type="table" target="#tab_5">Table 5</ref> and <ref type="figure" target="#fig_15">Figure 8</ref>.  We have also compared the computational cost of MFSNet in terms of the total execution (training) time with existing methods, as shown in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>It is clear from the table that our proposed method is computationally effi- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>The emergence of CAD systems has facilitated several seemingly daunting tasks, like the segmentation of skin lesions. Skin cancer affects a large population worldwide, and hence its early detection is essential for eradicating cancer.</p><p>Localization of tumors and lesion segmentation poses a challenge since an esoteric group of clinicians can only perform manual segmentation, and it is also a time-demanding task. To bolster the efforts of the medical practitioners, in this research, we develop a fully automated framework for accurate skin lesion segmentation from raw dermoscopy images. The proposed framework uses multi-scaled maps using a PPD module and two RA and BA modules to produce the final segmentation mask. The use of the multi-focus-based approach helps determine the overall lesion structure from the coarse map. The use of the finer maps helps in determining more refined edges, leading to increased segmentation accuracy. Upon evaluating the proposed MFSNet model on three publicly available datasets of varied sizes, the proposed method displayed robust performance, outperforming the state-of-the-art methods on the respective datasets.</p><p>In the future, we may extend the segmentation model to other domains like brain MRIs, lung CT scans, etc. Also, we might incorporate semi-supervision for the segmentation, similar to <ref type="bibr" target="#b29">[28]</ref> to extend the models to unlabelled datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) from raw input image similar to Figure 1(a). To address this, extensive research attempts have been made since the last decade to automate the segmentation of lesions, monitor their growth, and aid physicians in making surgical decisions, thereby increasing the clinical significance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 .</head><label>2</label><figDesc>Unlike the commonly used segmentation frameworks in literature, the proposed model upsamples the encoded features in subsequent steps of attention modules instead of coarse upsampling applied in U-Net type architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Outputs of the image inpainting method used for artefact removal on the PH2 dataset: (a) &amp; (f)-Original images; (b) &amp; (g)-Corresponding grayscale images; (c) &amp; (h)-Blackhat filtered images; (d) &amp; (i)-Thresholding for the inpainting operation; (e) &amp; (j)-final preprocessed (inpainted) images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>shows the image outputs from different intermediate steps of image artifact removal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3</head><label>3</label><figDesc>shows the architecture of the proposed MFSNet. It consists of the Res2Net as a backbone, which is a recently proposed CNN model<ref type="bibr" target="#b21">[20]</ref>, for feature extraction combined with a series of RA branches, explained in Section 2.4. Only five initial convolution layers of the network are used for this purpose. The first three layers are used to extract low-level features with high resolution but very little spatial information. The second and third level features, F 2 and F 3 , with important edge information, are fed to the BA module to improve melanoma boundary representation. F 2 and F 3 are further used for two different purposes.They are fed to the following two layers of the CNN, whose output is fed to the PPD module to generate the global segmentation map O S , which is used as the global map for coarse localization of melanoma segmentation. Secondly, they are fed to the RA branches, along with O S , to be used as the global guidance for the entire learning process of the network. The subsequent two layers of featuresF i ; i = 4, 5 from the successive two consecutive layers of the CNN are fed to the corresponding RA module to produce output O R (F i ),which is concatenated with the upsampled O R (F i+1 ) from the next branch, thereby ensuring the multi-level feature representation. This results in the output O i from each branch, which is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Overall structure of the proposed MFSNet model for the segmentation of skin lesions. The inputs to the boundary attention and reverse attention blocks have been shown in different colored arrows. The inputs of BA 1 , BA 2 , RA 1 , and RA 2 are marked using green, red, pink and brown arrows respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 .</head><label>1</label><figDesc>The input image is initially passed through a series of convolution layers for feature extraction using the Res2Net backbone, where downsampling is performed. Among those, only the features of the second and third Convolution layers are considered useful for edge guidance of the learning process because the low-level features preserve sufficient boundary information<ref type="bibr" target="#b58">[55]</ref>. Hence they are used for the BA module that explicitly learns the boundary information. Upsampling is performed in the PPD module.2. The BA module simultaneously takes input from the global segmentation map (output from PPD) and the shallow features from the convolution layers. By performing a series of distance transformations and other mathematical operations, an enhanced boundary map is obtained, further used by the RA modules. The detailed algorithm and workflow of BA are described later in Section 2.5.3. The RA module takes the features from the corresponding convolution layer, BA module, and the upsampled segmentation map from the next layer. The RA module uses two separate input branches to learn features to produce segmentation masks associated with two different classes -foreground and background. Thus the RA module generates a per-class mask to amplify the reverse-class response in the regions that contain high-level semantic information shared between two adjacent classes. Finally, the prediction of these two branches is fused to generate the segmentation output from the RA branch. The detailed workflow of the RA module is explained in Section 2.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of the RA module used in the proposed MFSNet model. O B : Output from the BA module; U i+1 : Upsampled output from the next layer; O R (F i ):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>the architecture of the RA module used in the proposed MFSNet. Let us consider the last two layers of the CNN have features F i ; i = {4, 5}, the BA module has output O B , and the RA mask of the i th level is M i RA , then the RA output O R of the i th level is given by Equation 1,where D is the downsampling operation, ? is the concatenation operation of downsampled O B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Architecture of the BA module used in the MFSNet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>U</head><label></label><figDesc>i : Global map output from PPD module; S i : Segmentation Map;S i : Inverted Segmentation Map; DT (x): Distance Transform; M i B : i th level boundary mask; O B : Boundary Attention output The BA module helps the network capture important boundary information, which is complementary to the amplified reverse class response for the regions of shared semantic information extracted by the RA module. This additional edge information acts as a helpful signal to confusing segment regions near the lesion boundaries. The i th level feature F i from the encoder, when fed to the BA module, produces an output O B , given by Equation 3, where is the elementwise multiplication of feature F i and the i th level boundary mask M i B , which is obtained by formulating the binary segmentation map S i given by Equation 4,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Architecture of the Partial Parallel Decoder module used in the proposed framework. The convolution layers 4 and 5 denote the 4th and fifth layer respectively of the Res2Net CNN backbone used in MFSNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>tion in this research. For the BA module, we have used the standard BCE loss function, shown in Equation 8. However, for the supervision of segmentation, we have used a mixing loss function for effective global and local supervision to enhance both image-level and pixel-level segmentation, respectively. The proposed loss function involves the weighted BCE loss function L wBCE and weighted IoU loss function L w IoU , given by Equation 9, where ? is the weight, set to 0.9 in our case experimentally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Experimental analysis of mean DSC and mean IoU on ISIC2017 dataset against different ? values, that defined the weights of different components in the proposed loss function (Equation 9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>1 . 11 ) 2 . 14 ) 5 .</head><label>1112145</label><figDesc>Dice Similarity Coefficient (DSC): It is a spatial overlap metric which is computed as in Equation 11 for predicted image S and ground truth G. DSC(S, G) = 2 ? |S ? G| |S| + |G| (Intersection over Union (IoU): IoU, also known as Jaccard Index (JI), measures segmentation accuracy by computing the ratio of the intersection of objects and their union when projected on the same plane. Mathematically it is expressed as in Equation 12, where S is the predicted segmentation mask, and G is the original ground truth mask of the image. IoU (S, G) = |S ? G| |S ? G| (12) 3. F-Measure (FM): F-Measure is a standard metric that evaluates the harmonic mean of the pixel-wise precision and recall and is mathematically expressed as in Equation 13. F M = 2 ? P recision ? Recall Recall + P recision (13) 4. Sensitivity (Sen): It characterizes the percentage of pixels of the object that are accurately classified as the object class and it is computed by Equation 14. Sen(S, G) = |S ? G| |G| (Specificity (Spe): It characterizes the percentage of pixels of the background class that are accurately classified as the background, and it is computed using Equation 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 :</head><label>8</label><figDesc>A few instances of segmentation masks obtained by some standard models in literature compared to the proposed MFSNet model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>cient compared to several state-of-the-art methods like SegNet, DoubleUNet, etc. However, we could not calculate the execution time for all the methods in the literature compared in this study due to the unavailability of open-source implementations. The improvement in the values of the evaluation metrics by the MFSNet model as compared to state-of-the-art methods in the literature can significantly impact the diagnosis process. Higher values of IoU, DSC, etc., indicate a more accurate skin lesion segmentation while preserving structural similarity. Thus, when the segmented lesions are used for further diagnosis, more robust and informative features can be extracted for the automatic classification of the lesions into benign and malignant classes as stated by Mahbod et al. [32]. This reduces the chances of faulty diagnosis and helps control skin cancer early and more effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results obtained by MFSNet on the three datasets using 5-fold cross-validation.</figDesc><table><row><cell>Dataset</cell><cell>Fold</cell><cell>mDSC</cell><cell>mIoU</cell><cell>mFM</cell><cell>mSen</cell><cell>mSpe</cell></row><row><cell></cell><cell>1</cell><cell>0.955</cell><cell>0.917</cell><cell>0.947</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell></cell><cell>2</cell><cell>0.956</cell><cell>0.918</cell><cell>0.941</cell><cell>0.991</cell><cell>0.986</cell></row><row><cell>P H 2</cell><cell>3 4</cell><cell>0.951 0.949</cell><cell>0.915 0.920</cell><cell>0.945 0.943</cell><cell>0.995 1.000</cell><cell>0.999 1.000</cell></row><row><cell></cell><cell>5</cell><cell>0.958</cell><cell>0.899</cell><cell>0.941</cell><cell>0.989</cell><cell>0.999</cell></row><row><cell></cell><cell cols="6">Average 0.954?0.003 0.914?0.008 0.944?0.002 0.995?0.004 0.997?0.002</cell></row><row><cell></cell><cell>1</cell><cell>0.991</cell><cell>0.976</cell><cell>0.989</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell></cell><cell>2</cell><cell>0.985</cell><cell>0.971</cell><cell>0.980</cell><cell>1.000</cell><cell>1.000</cell></row><row><cell>ISIC 2017</cell><cell>3 4</cell><cell>0.983 0.986</cell><cell>0.967 0.980</cell><cell>0.980 0.991</cell><cell>0.998 0.999</cell><cell>0.999 0.999</cell></row><row><cell></cell><cell>5</cell><cell>0.990</cell><cell>0.975</cell><cell>0.989</cell><cell>0.997</cell><cell>0.998</cell></row><row><cell></cell><cell cols="6">Average 0.987?0.003 0.974?0.004 0.986?0.005 0.999?0.001 0.999?0.001</cell></row><row><cell></cell><cell>1</cell><cell>0.911</cell><cell>0.910</cell><cell>0.905</cell><cell>1.000</cell><cell>0.999</cell></row><row><cell></cell><cell>2</cell><cell>0.900</cell><cell>0.901</cell><cell>0.899</cell><cell>0.997</cell><cell>0.998</cell></row><row><cell>HAM10000</cell><cell>3 4</cell><cell>0.905 0.904</cell><cell>0.903 0.894</cell><cell>0.906 0.892</cell><cell>0.999 1.000</cell><cell>1.000 1.000</cell></row><row><cell></cell><cell>5</cell><cell>0.910</cell><cell>0.900</cell><cell>0.914</cell><cell>0.998</cell><cell>0.998</cell></row><row><cell></cell><cell cols="6">Average 0.906?0.004 0.902?0.005 0.903?0.007 0.999?0.001 0.999?0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the results obtained with the MFSNet model on the three datasets with and without image preprocessing of structural coherence between the segmented mask and the available groundtruth mask. Further, to evaluate the importance of image preprocessing (artifact removal) in this research, we evaluate and compare the performance of the MFSNet model with the raw images and the preprocessed images. The results</figDesc><table><row><cell>Dataset</cell><cell cols="3">Preprocessing mDSC mIoU mSen mSpe</cell></row><row><cell>PH2</cell><cell>NO YES</cell><cell>0.931 0.954</cell><cell>0.895 0.914 0.995 0.997 0.978 0.978</cell></row><row><cell>ISIC2017</cell><cell>NO YES</cell><cell>0.963 0.987</cell><cell>0.942 0.974 0.999 0.999 0.969 0.970</cell></row><row><cell>HAM10000</cell><cell>NO YES</cell><cell>0.872 0.906</cell><cell>0.869 0.902 0.999 0.999 0.954 0.961</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>shows the results on the P H 2 dataset, where we have used RA and BA modules at different levels of feature extraction. Comparing instances 1 and 4 from the table shows that the performance can be boosted if we use BA at the Conv2 layer instead of RA. This behavior can be justified because the shallow layers of the CNN can extract features rich in boundary information.Hence adding BA there will provide additional edge guidance to the model. Conv1 layer does not decrease the segmentation performance significantly but effectively reduces computation of an additional BA module.</figDesc><table><row><cell>Similar conclusions can be drawn by comparing instances 1 and 2. Again, com-</cell></row><row><cell>paring instances 2 and 3, we can observe experimentally that removing the BA</cell></row><row><cell>module from the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of quantitative results obtained from different orientations of RA and BA blocks in the proposed MFSNet model on the P H 2 dataset. The highlighted row indicates the orientations and results of our proposed model.</figDesc><table><row><cell>Instance</cell><cell cols="5">Combinations Conv1 Conv2 Conv3 Conv4 Conv5</cell><cell>mDSC</cell><cell cols="3">Average Result (on 5 fold) mIoU mFM mSen</cell><cell>mSpe</cell></row><row><cell>1</cell><cell>BA</cell><cell>BA</cell><cell>BA</cell><cell>RA</cell><cell>RA</cell><cell>0.944?0.002</cell><cell>0.897?0.003</cell><cell>0.928?0.004</cell><cell>0.984?0.008</cell><cell>0.989?0.004</cell></row><row><cell>2</cell><cell>BA</cell><cell>BA</cell><cell>RA</cell><cell>RA</cell><cell>RA</cell><cell>0.926?.006</cell><cell>0.872?0.002</cell><cell>0.902?0.006</cell><cell>0.971?0.004</cell><cell>0.969?0.006</cell></row><row><cell>3</cell><cell>-</cell><cell>BA</cell><cell>RA</cell><cell>RA</cell><cell>RA</cell><cell>0.930?0.004</cell><cell>0.876?0.003</cell><cell>.911?0.007</cell><cell>0.979?0.005</cell><cell>0.972?0.006</cell></row><row><cell>4</cell><cell>BA</cell><cell>RA</cell><cell>BA</cell><cell>RA</cell><cell>RA</cell><cell>0.926?0.004</cell><cell>0.876?0.002</cell><cell>0.916?0.005</cell><cell>0.966?0.004</cell><cell>0.963?0.002</cell></row><row><cell>5</cell><cell>-</cell><cell>BA</cell><cell>RA</cell><cell>BA</cell><cell>RA</cell><cell>0.926?0.006</cell><cell>0.871?0.005</cell><cell>0.902?0.003</cell><cell>0.978?0.006</cell><cell>0.970?0.004</cell></row><row><cell>Proposed</cell><cell>-</cell><cell>BA</cell><cell>BA</cell><cell>RA</cell><cell>RA</cell><cell cols="5">0.954?0.003 0.914?0.008 0.944?0.002 0.995?0.004 0.997?0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results of the ablation study considering various components of the MFSNet model on the P H 2 dataset. Best results are highlighted.</figDesc><table><row><cell>Architecture</cell><cell>mDSC</cell><cell>mIoU</cell><cell>mFM</cell><cell>mSen</cell><cell>mSpe</cell></row><row><cell>Res2Net</cell><cell>0.794?0.006</cell><cell>0.758?0.008</cell><cell>0.761?0.005</cell><cell>0.816?0.009</cell><cell>0.821?0.008</cell></row><row><cell>Res2Net+PPD</cell><cell>0.877?0.005</cell><cell>0.852?0.004</cell><cell>0.873?0.003</cell><cell>0.915?0.006</cell><cell>0.906?0.004</cell></row><row><cell>Res2Net+BA</cell><cell>0.843?0.003</cell><cell>0.820?0.006</cell><cell>0.871?0.004</cell><cell>0.911?0.007</cell><cell>0.904?0.004</cell></row><row><cell>Res2Net+RA</cell><cell>0.842?0.002</cell><cell>0.834?0.005</cell><cell>0.866?0.006</cell><cell>0.909?0.006</cell><cell>0.929?0.004</cell></row><row><cell>Res2Net+BA+RA</cell><cell>0.906?0.003</cell><cell>0.861?0.004</cell><cell>0.894?0.006</cell><cell>0.947?0.004</cell><cell>0.936?0.007</cell></row><row><cell>Res2Net+RA+PPD</cell><cell>0.927?0.003</cell><cell>0.895?0.007</cell><cell>0.912?0.005</cell><cell>0.963?0.007</cell><cell>0.959?0.006</cell></row><row><cell cols="6">Res2Net+BA+RA+PPD (Proposed) 0.954?0.003 0.914?0.008 0.944?0.002 0.995?0.004 0.997?0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of the proposed MFSNet model to state-of-the-art models on the three publicly available datasets used in this study. (Total training time is calculated on implementation using NVIDIA Tesla K80 GPU)</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="4">mDSC mIoU mSen mSpe</cell><cell>Training time</cell></row><row><cell></cell><cell>Double U-Net [26]</cell><cell>0.907</cell><cell>0.899</cell><cell>0.945</cell><cell>0.966</cell><cell>1hr 2min12sec</cell></row><row><cell></cell><cell>U-Net [36]</cell><cell>0.876</cell><cell>0.780</cell><cell>0.816</cell><cell>0.978</cell><cell>30min 54 sec</cell></row><row><cell></cell><cell>SegNet [5]</cell><cell>0.894</cell><cell>0.808</cell><cell>0.865</cell><cell>0.966</cell><cell>58min 21 sec</cell></row><row><cell></cell><cell>Goyal et al. [22]</cell><cell>0.907</cell><cell>0.839</cell><cell>0.932</cell><cell>0.929</cell><cell>-</cell></row><row><cell></cell><cell>Hasan et al. [24]</cell><cell>-</cell><cell>0.870</cell><cell>0..929</cell><cell>0.969</cell><cell>35min 08sec</cell></row><row><cell></cell><cell>Al et al. [2]</cell><cell>0.918</cell><cell>0.848</cell><cell>0.937</cell><cell>0.957</cell><cell>-</cell></row><row><cell>P H 2</cell><cell>Ozturk et al. [35]</cell><cell>0.930</cell><cell>0.871</cell><cell>0.969</cell><cell>0.953</cell><cell>-</cell></row><row><cell></cell><cell>Xie et al. [53]</cell><cell>0.919</cell><cell>0.857</cell><cell>0.963</cell><cell>0.942</cell><cell>-</cell></row><row><cell></cell><cell>Unver et al. [44]</cell><cell>0.881</cell><cell>0.795</cell><cell>0.836</cell><cell>0.940</cell><cell>-</cell></row><row><cell></cell><cell>Yuan et al. [54]</cell><cell>0.915</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Bi et al. [11]</cell><cell>0.907</cell><cell>0.840</cell><cell>0.949</cell><cell>0.940</cell><cell>-</cell></row><row><cell></cell><cell>Bi et al. [10]</cell><cell>0.921</cell><cell>0.859</cell><cell>0.962</cell><cell>0.945</cell><cell>-</cell></row><row><cell></cell><cell>Proposed MFSNet</cell><cell>0.954</cell><cell cols="3">0.914 0.995 0.997</cell><cell>46min 37sec</cell></row><row><cell></cell><cell>Double U-Net [26]</cell><cell>0.913</cell><cell>0.918</cell><cell>0.963</cell><cell>0.974</cell><cell>4hr 18min 07sec</cell></row><row><cell></cell><cell>U-Net [36]</cell><cell>0.778</cell><cell>0.683</cell><cell>0.812</cell><cell cols="2">0.805 3hr 22min 44sec</cell></row><row><cell></cell><cell>SegNet [5]</cell><cell>0.821</cell><cell>0.696</cell><cell>0.801</cell><cell>0.954</cell><cell>4hr 04min 17sec</cell></row><row><cell></cell><cell>Tschandl et al. [43]</cell><cell>0.853</cell><cell>0.770</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Navarro et al. [34]</cell><cell>0.938</cell><cell>0.846</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Saha et al. [37]</cell><cell>0.855</cell><cell>0.772</cell><cell>0.824</cell><cell>0.981</cell><cell>-</cell></row><row><cell>ISIC2017</cell><cell>Goyel et al. [22]</cell><cell>0.793</cell><cell>0.871</cell><cell>0.899</cell><cell>0.950</cell><cell>-</cell></row><row><cell></cell><cell>Hasan et al. [24]</cell><cell>-</cell><cell>0.775</cell><cell>0.875</cell><cell>0.955</cell><cell>3hr 37min 17sec</cell></row><row><cell></cell><cell>Al et al. [2]</cell><cell>0.871</cell><cell>0.771</cell><cell>0.854</cell><cell>0.967</cell><cell>-</cell></row><row><cell></cell><cell>Ozturk et al. [35]</cell><cell>0.886</cell><cell>0.783</cell><cell>0.854</cell><cell>0.981</cell><cell>-</cell></row><row><cell></cell><cell>Xie et al. [53]</cell><cell>0.862</cell><cell>0.783</cell><cell>0.870</cell><cell>0.964</cell><cell>-</cell></row><row><cell></cell><cell>Unver et al. [44]</cell><cell>0.843</cell><cell>0.748</cell><cell>0.908</cell><cell>0.927</cell><cell>-</cell></row><row><cell></cell><cell>Proposed MFSNet</cell><cell>0.987</cell><cell cols="3">0.974 0.999 0.999</cell><cell>3hr 51min 20sec</cell></row><row><cell></cell><cell>Double U-Net [26]</cell><cell>0.843</cell><cell>0.812</cell><cell>0.861</cell><cell>0.845</cell><cell>11hr 21min 53sec</cell></row><row><cell></cell><cell>U-Net [36]</cell><cell>0.781</cell><cell>0.774</cell><cell>0.799</cell><cell>0.802</cell><cell>9hr 05min 31sec</cell></row><row><cell></cell><cell>SegNet [5]</cell><cell>0.816</cell><cell>0.821</cell><cell>0.867</cell><cell>0.854</cell><cell>11hr 04min 10sec</cell></row><row><cell></cell><cell>Saha et al. [37]</cell><cell>0.891</cell><cell>0.819</cell><cell>0.824</cell><cell>0.981</cell><cell>-</cell></row><row><cell>HAM10000</cell><cell>Abraham et al. [1]</cell><cell>0.856</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Shahin et al. [39]</cell><cell>0.903</cell><cell>0.837</cell><cell>0.902</cell><cell>0.974</cell><cell>-</cell></row><row><cell></cell><cell>Bissoto et al. [12]</cell><cell>0.873</cell><cell>0.792</cell><cell>0.934</cell><cell>0.936</cell><cell>-</cell></row><row><cell></cell><cell>Ibtehaz et al. [25]</cell><cell>-</cell><cell>0.803</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Proposed MFSNet</cell><cell>0.906</cell><cell cols="4">0.902 0.999 0.999 9hr 41min 34sec</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the Centre for Microprocessor Applications for Training, Education and Research (CMATER) laboratory of the Computer Science and Engineering Department, Jadavpur University, Kolkata, India, for providing the infrastructural support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>All the authors declare that there is no conflict of interest.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel focal tversky loss function with improved attention u-net for lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="683" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Skin lesion segmentation in dermoscopy images via deep full resolution convolutional networks. Computer methods and programs in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Masni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Antari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation method for dermoscopy images using artificial bee colony algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aljanabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>?zok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rahebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Abdullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">347</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skin melanoma segmentation using recurrent and convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yazdabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 14th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="292" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explainable skin lesion diagnosis using taxonomies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107413</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparative study of maturation profiles of neural cells in different species with the help of computer vision and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Signal Processing and Intelligent Recognition Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="352" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skin melanoma segmentation by morphological approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Beuren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Janasieivicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Grando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Facon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on advances in computing, communications and informatics</title>
		<meeting>the international conference on advances in computing, communications and informatics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="972" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-label classification of multi-modality skin lesion via hyper-connected convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107502</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Step-wise integration of deep class-specific learning for dermoscopic image segmentation</title>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="78" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dermoscopic image segmentation via multistage fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="2065" to="2074" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep-learning ensembles for skin-lesion segmentation, analysis, classification: Recod titans at isic challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fornaciali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08480</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking skin lesion segmentation in a convolutional classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weinthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="435" to="440" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Integration of morphological preprocessing and fractal based feature extraction with recursive feature elimination for skin lesion types classification. Computer methods and programs in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Munshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="201" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-scale attention u-net (msaunet): A modified u-net architecture for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Basak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06911</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</title>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 15th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Torelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>2d euclidean distance transform algorithms: A comparative survey</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Segmentation of skin lesions in dermoscopy images using fuzzy classification of pixels and histogram thresholding. Computer methods and programs in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Garcia-Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garcia-Zapirain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation in dermoscopic images with ensemble deep learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oakley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dancey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="4171" to="4181" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ce-net: Context encoder network for 2d medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2281" to="2292" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dsnet: Automatic dermoscopic skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dahal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Samarakoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Tushar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">103738</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the unet architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation via generative adversarial networks with dual discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">101716</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single annotated pixel based weakly supervised semantic segmentation under driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page">107979</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A novel approach to segment skin lesions in dermoscopic images based on a deformable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M R</forename><surname>Tavares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="615" to="623" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The effects of skin lesion segmentation on the performance of dermatoscopic image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahbod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ellinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page">105725</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ph 2-a dermoscopic image database for research and benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mendon?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Marcal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rozeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 35th annual international conference of the IEEE engineering in medicine and biology society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5437" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accurate segmentation and registration of skin lesion images to evaluate lesion change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Escudero-Vi?olo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besc?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="501" to="508" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation with improved convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>?zt?rk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>?zkaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="958" to="970" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Leveraging adaptive color augmentation in convolutional neural networks for deep skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2014" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using deep learning to detect melanoma in dermoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A A</forename><surname>Salido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep convolutional encoder-decoders with aggregated multi-resolution skip connections for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 16th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="451" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CA: a cancer journal for clinicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jemal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cancer statistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="7" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An image inpainting technique based on the fast marching method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of graphics tools</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain-specific classificationpretrained fully convolutional network encoders for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="111" to="116" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Skin lesion segmentation in dermoscopic images with combination of yolo and grabcut algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>?nver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic skin lesions segmentation based on a new morphological approach via geodesic active contour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F X</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Medeiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Peixoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Filho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="44" to="59" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An enhancement in adaptive median filter for edge preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thoke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Morphological background detection and illumination normalization of text image with poor lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">110991</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automated segmentation of skin lesion based on pyramid attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning in Medical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="435" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Attention-based denseunet network with adversarial training for skin lesion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="136616" to="136629" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nas-unet: Neural architecture search for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="44247" to="44257" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Skin lesion segmentation using high-resolution convolutional neural network. Computer methods and programs in biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page">105241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automatic skin lesion segmentation using deep fully convolutional networks with jaccard distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1876" to="1886" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Et-net: A generic edge-attention guidance network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8779" to="8788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
