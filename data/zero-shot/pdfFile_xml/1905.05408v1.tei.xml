<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghwan</forename><surname>Son</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daewoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename><forename type="middle">Ju</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hostallero</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung</forename><surname>Yi</surname></persName>
						</author>
						<title level="a" type="main">QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint actionvalue function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN's superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning aims to instill in agents a good policy that maximizes the cumulative reward in a given environment. Recent progress has witnessed success in various tasks, such as Atari games <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>, Go <ref type="bibr" target="#b18">(Silver et al., 2016;</ref>, and robot control <ref type="bibr" target="#b11">(Lillicrap et al., 2015)</ref>, just to name a few, with the development of deep learning techniques. Such advances largely consist of deep neural networks, which can represent action-value functions and policy functions in reinforcement learning problems as a high-capacity function approximator. However, more complex tasks such as robot swarm control and autonomous driving, often modeled as cooperative multi-agent learning problems, still remain unconquered due to their high scales and operational constraints such as distributed execution.</p><p>The use of deep learning techniques carries through to cooperative multi-agent reinforcement learning (MARL). MAD-DPG <ref type="bibr" target="#b12">(Lowe et al., 2017)</ref> learns distributed policy in continuous action spaces, and COMA  utilizes a counterfactual baseline to address the credit assignment problem. Among value-based methods, value function factorization <ref type="bibr" target="#b10">(Koller &amp; Parr, 1999;</ref><ref type="bibr" target="#b3">Guestrin et al., 2002a;</ref><ref type="bibr" target="#b22">Sunehag et al., 2018;</ref><ref type="bibr" target="#b17">Rashid et al., 2018)</ref> methods have been proposed to efficiently handle a joint action-value function whose complexity grows exponentially with the number of agents.</p><p>Two representative examples of value function factorization include VDN <ref type="bibr" target="#b22">(Sunehag et al., 2018)</ref> and QMIX <ref type="bibr" target="#b17">(Rashid et al., 2018)</ref>. VDN factorizes the joint action-value function into a sum of individual action-value functions. QMIX extends this additive value factorization to represent the joint action-value function as a monotonic function -rather than just as a sum -of individual action-value functions, thereby covering a richer class of multi-agent reinforcement learning problems than does VDN. However, these value factorization techniques still suffer structural constraints, namely, additive decomposability in VDN and monotonicity in QMIX, often failing to factorize a factorizable task. A task is factorizable if the optimal actions of the joint action-value function are the same as the optimal ones of the individual action-value functions, where additive decomposability and monotonicity are only sufficient -somewhat excessively restrictive -for factorizability.</p><p>Contribution In this paper, we aim at successfully factorizing any factorizable task, free from additivity/monotonicity concerns. We transform the original joint action-value function into a new, easily factorizable one with the same optimal actions in both functions. This is done by learning a state-value function, which corrects for the severity of the partial observability issue in the agents. arXiv:1905.05408v1 <ref type="bibr">[cs.</ref>LG] 14 May 2019</p><p>We incorporate the said idea in a novel architecture, called QTRAN, consisting of the following inter-connected deep neural networks: (i) joint action-value network, (ii) individual action-value networks, and (iii) state-value network. To train this architecture, we define loss functions appropriate for each neural network. We develop two variants of QTRAN: QTRAN-base and QTRAN-alt, whose distinction is twofold: how to construct the transformed action-value functions for non-optimal actions; and the degree of stability and convergence speed. We assess the performance of QTRAN by comparing it against VDN and QMIX in three environments. First, we consider a simple, singlestate matrix game that does not satisfy additivity or monotonicity, where QTRAN successfully finds the joint optimal action, whereas neither VDN nor QMIX does. We then observe a similarly desirable cooperation-inducing tendency of QTRAN in more complex environments: modified predator-prey games and multi-domain Gaussian squeeze tasks. In particular, we show that the performance gap between QTRAN and VDN/QMIX increases with environments having more pronounced non-monotonic characteristics.</p><p>Related work Extent of centralization varies across the spectrum of cooperative MARL research. While more decentralized methods benefit from scalability, they often suffer non-stationarity problems arising from a trivialized superposition of individually learned behavior. Conversely, more centralized methods alleviate the non-stationarity issue at the cost of complexity that grows exponentially with the number of agents.</p><p>Prior work tending more towards the decentralized end of the spectrum include <ref type="bibr" target="#b24">Tan (1993)</ref>, whose independent Qlearning algorithm exhibits the greatest degree of decentralization. <ref type="bibr" target="#b23">Tampuu et al. (2017)</ref> combines this algorithm with deep learning techniques presented in DQN <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>. These studies, while relatively simpler to implement, are subject to the threats of training instability, as multiple agents attempt to improve their policy in the midst of other agents, whose policies also change over time during training. This simultaneous alteration of policies essentially makes the environment non-stationary.</p><p>The other end of the spectrum involves some centralized entity to resolve the non-stationarity problem. <ref type="bibr" target="#b4">Guestrin et al. (2002b)</ref> and <ref type="bibr" target="#b9">Kok &amp; Vlassis (2006)</ref> are some of the earlier representative works. <ref type="bibr" target="#b4">Guestrin et al. (2002b)</ref> proposes a graphical model approach in presenting an alternative characterization of a global reward function as a sum of conditionally independent agent-local terms. <ref type="bibr" target="#b9">Kok &amp; Vlassis (2006)</ref> exploits the sparsity of the states requiring coordination compared to the whole state space and then tabularize those states to carry out tabular Q-learning methods as in <ref type="bibr" target="#b26">Watkins (1989)</ref>.</p><p>The line of research positioned mid-spectrum aims to put together the best of both worlds. More recent studies, such as COMA , take advantage of CTDE <ref type="bibr" target="#b14">(Oliehoek et al., 2008)</ref>; actors are trained by a joint critic to estimate a counterfactual baseline designed to gauge each agent's contribution to the shared task. <ref type="bibr" target="#b5">Gupta et al. (2017)</ref> implements per-agent critics to opt for better scalability at the cost of diluted benefits of centralization. MADDPG <ref type="bibr" target="#b12">(Lowe et al., 2017)</ref> extends DDPG <ref type="bibr" target="#b11">(Lillicrap et al., 2015)</ref> to the multi-agent setting by similar means of having a joint critic train the actors. <ref type="bibr" target="#b28">Wei et al. (2018)</ref> proposes Multi-Agent Soft Q-learning in continuous action spaces to tackle the relative overgeneralization problem <ref type="bibr" target="#b27">(Wei &amp; Luke, 2016)</ref> and achieves better coordination. Other related work includes CommNet <ref type="bibr" target="#b21">(Sukhbaatar et al., 2016)</ref>, DIAL <ref type="bibr" target="#b1">(Foerster et al., 2016)</ref>, ATOC <ref type="bibr" target="#b7">(Jiang &amp; Lu, 2018)</ref>, and SCHEDNET <ref type="bibr" target="#b8">(Kim et al., 2019)</ref>, which exploit inter-agent communication in execution.</p><p>On a different note, two representative examples of valuebased methods have recently been shown to be somewhat effective in analyzing a class of games. Namely, VDN <ref type="bibr" target="#b22">(Sunehag et al., 2018)</ref> and QMIX <ref type="bibr" target="#b17">(Rashid et al., 2018)</ref> represent the body of literature most closely related to this paper. While both are value-based methods and follow the CTDE approach, the additivity and monotonicity assumptions naturally limit the class of games that VDN or QMIX can solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model and CTDE</head><p>DEC-POMDP We take DEC-POMDP <ref type="bibr" target="#b15">(Oliehoek et al., 2016)</ref> as the de facto standard for modelling cooperative multi-agent tasks, as do many previous works: as a tuple G =&lt; S, U, P, r, Z, O, N, ? &gt;, where s ? S denotes the true state of the environment. Each agent i ? N := {1, ..., N } chooses an action u i ? U at each time step, giving rise to a joint action vector, u :</p><formula xml:id="formula_0">= [u i ] N i=1 ? U N . Function P (s |s, u) : S ? U N ? S ? [0, 1]</formula><p>governs all state transition dynamics. Every agent shares the same joint reward function r(s, u) : S ? U N ? R, and ? ? [0, 1) is the discount factor. Each agent has individual, partial observation z ? Z, according to some observation function O(s, i) : S ? N ? Z. Each agent also has an actionobservation history ? i ? T := (Z ? U) * , on which it conditions its stochastic policy ? i (u i |? i ) : T ? U ? [0, 1].</p><p>Training and execution: CTDE Arguably the most na?ve training method for MARL tasks is to learn the individual agents' action-value functions independently, i.e., independent Q-learning. This method would be simple and scalable, but it cannot guarantee convergence even in the limit of infinite greedy exploration. As an alternative solution, recent works including VDN <ref type="bibr" target="#b22">(Sunehag et al., 2018)</ref> and QMIX <ref type="bibr" target="#b17">(Rashid et al., 2018)</ref> employ centralized training with decentralized execution (CTDE) <ref type="bibr" target="#b14">(Oliehoek et al., 2008)</ref> to train multiple agents. CTDE allows agents to learn and construct individual action-value functions, such that optimization at the individual level leads to optimization of the joint action-value function. This in turn, enables agents at execution time to select an optimal action simply by looking up the individual action-value functions, without having to refer to the joint one. Even with only partial observability and restricted inter-agent communication, information can be made accessible to all agents at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">IGM Condition and Factorizable Task</head><p>Consider a class of sequential decision-making tasks that are amenable to factorization in the centralized training phase. We first define IGM (Individual-Global-Max):</p><p>Definition 1 (IGM). For a joint action-value function</p><formula xml:id="formula_1">Q jt : T N ? U N ? R, where ? ? T N is a joint action- observation histories, if there exist individual action-value functions [Q i : T ? U ? R] N i=1 , such that the following holds arg max u Q jt (? , u) = ? ? ? arg max u1 Q 1 (? 1 , u 1 ) . . . arg max u N Q N (? n , u N ) ? ? ? , (1)</formula><p>then, we say that [Q i ] satisfy IGM for Q jt under ? . In this case, we also say that Q jt (? , u) is factorized by</p><formula xml:id="formula_2">[Q i (? i , u i )], or that [Q i ] are factors of Q jt .</formula><p>Simply put, the optimal joint actions across agents are equivalent to the collection of individual optimal actions of each agent. If Q jt (? , u) in a given task is factorizable under all ? ? T N , we say that the task itself is factorizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">VDN and QMIX</head><p>Given Q jt , one can consider the following two sufficient conditions for IGM:</p><formula xml:id="formula_3">(Additivity) Q jt (? , u) = N i=1 Q i (? i , u i ), (Monotonicity) ?Q jt (? , u) ?Q i (? i , u i ) ? 0, ?i ? N .<label>(2)</label></formula><p>VDN <ref type="bibr" target="#b22">(Sunehag et al., 2018)</ref> and QMIX <ref type="bibr" target="#b17">(Rashid et al., 2018)</ref> are methods that attempt to factorize Q jt assuming additivity and monotonicity, respectively. Thus, joint actionvalue functions satisfying those conditions would be wellfactorized by VDN and QMIX. However, there exist tasks whose joint action-value functions do not meet the said conditions. We illustrate this limitation of VDN and QMIX using a simple matrix game in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">QTRAN: Learning to Factorize with Transformation</head><p>In this section, we propose a new method called QTRAN, aiming at factorizing any factorizable task. The key idea is to transform the original joint action-value function Q jt into a new one Q jt that shares the optimal joint action with Q jt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditions for the Factor Functions [Q i ]</head><p>For a given joint observation ? , consider an arbitrary factorizable Q jt (? , u). Then, by Definition 1 of IGM, we can find individual action-value functions <ref type="bibr">u)</ref>. Theorem 1 states the sufficient condition for [Q i ] that satisfy IGM. Let? i denote the optimal local action arg max</p><formula xml:id="formula_5">[Q i (? i , u i )] that fac- torize Q jt (? ,</formula><formula xml:id="formula_6">ui Q i (? i , u i ) and? = [? i ] N i=1 ,. Also, let Q = [Q i ] ? R N , i.e., a column vector of Q i , i = 1, . . . , N. Theorem 1. A factorizable joint action-value function Q jt (? , u) is factorized by [Q i (? i , u i )], if N i=1 Q i (? i , u i )?Q jt (? , u)+V jt (? ) = 0 u =?, (4a) ? 0 u =?, (4b) where V jt (? ) = max u Q jt (? , u) ? N i=1 Q i (? i ,? i ).</formula><p>The proof is provided in the Supplementary. We note that conditions in (4) are also necessary under an affine transformation. That is, there exists an affine transformation</p><formula xml:id="formula_7">?(Q) = A ? Q + B, where A = [a ii ] ? R N ?N + is a symmet- ric diagonal matrix with a ii &gt; 0, ?i and B = [b i ] ? R N , such that if Q jt is factorized by [Q i ]</formula><p>, then (4) holds by replacing Q i with a ii Q i + b i . This is because for all i, b i cancels out, and a ii just plays the role of re-scaling the value of N i=1 Q i in multiplicative (with a positive scaling constant) and additive manners, since IGM is invariant to ? of [Q i ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Factorization via transformation</head><p>We first define a new function Q jt as the linear sum of individual factor functions [Q i ]:</p><formula xml:id="formula_8">Q jt (? , u) := N i=1 Q i (? i , u i ).<label>(5)</label></formula><p>We call Q jt (? , u) the transformed joint-action value function throughout this paper. Our idea of factorization is as follows: from the additive construction of Q jt based on [Q i ], [Q i ] satisfy IGM for the new joint action-value function Q jt , implying that [Q i ] are also the factorized individual action-value functions of Q jt . From the fact that arg max u Q jt (? , u) = arg max u Q jt (? , u), finding [Q i ] satisfying (4) is precisely the factorization of Q jt (? , u). One interpretation of this process is that rather than directly factorizing Q jt , we consider an alternative joint action-value function (i.e., Q jt ) that is factorized by additive decomposition. The function V jt (? ) corrects for the discrepancy between the centralized joint action-value function Q jt and the sum of individual joint action-value functions [Q i ]. This discrepancy arises from the partial observability of agents. If bestowed with full observability, V jt can be re-defined as zero, and the definitions would still stand. Refer to the Supplementary for more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method</head><p>Overall framework In this section, we propose a new deep RL framework with value function factorization, called QTRAN, whose architectural sketch is given in <ref type="figure" target="#fig_0">Figure 1</ref>. QTRAN consists of three separate estimators: (i) each agent i's individual action-value network for Q i , (ii) a joint actionvalue network for Q jt to be factorized into individual actionvalue functions Q i , and (iii) a state-value network for V jt , i.e.,</p><formula xml:id="formula_9">(Individual action-value network) f q : (? i , u i ) ? Q i , (Joint action-value network) f r : (? , u) ? Q jt , (State-value network) f v : ? ? V jt .</formula><p>Three neural networks are trained in a centralized manner, and each agent uses its own factorized individual actionvalue function Q i to take action during decentralized execution. Each network is elaborated next.</p><p>Individual action-value networks For each agent, an action-value network takes its own action-observation history ? i as input, and produces action-values Q i (? i , ?) as output. This action-value network is used for each agent to determine its own action by calculating the action-value for a given ? i . As defined in <ref type="formula" target="#formula_8">(5)</ref>, Q jt is just the summation of the outputs of all agents.</p><p>Joint action-value network The joint action-value network approximates Q jt . It takes as input the selected action and produces the Q-value of the chosen action as output. For scalability and sample efficiency, we design this network as follows. First, we use the action vector sampled by all individual action-value networks to update the joint actionvalue network. Since the joint action space is U N , finding an optimal action requires high complexity as the number of agents N grows, whereas obtaining an optimal action in each individual network is done by decentralized policies with linear-time individual arg max operations. Second, the joint action-value network shares the parameters at the lower layers of individual networks, where the joint action-value network combines hidden features with summa-</p><formula xml:id="formula_10">tion i h Q,i (? i , u i ) of h i (? i , u i ) = [h Q,i (? i , u i ), h V,i (? i )]</formula><p>from individual networks. This parameter sharing is used to enable scalable training with good sample efficiency at the expense of expressive power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-value network</head><p>The state-value network is responsible for computing a scalar state-value, similar to V (s) in the dueling network <ref type="bibr" target="#b25">(Wang et al., 2016)</ref>. V jt is required to provide the flexibility to match Q jt and Q jt + V jt at arg max. Without state-value network, partial observability would limit the representational complexity of Q jt . The state-value is independent of the selected action for a given ? . Thus, this value network does not contribute to choosing an action, and is instead used to calculate the loss of (4). Like the joint action-value network, we use the combined hidden features i h V,i (? i ) from the individual networks as input to the value network for scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>There are two major goals in centralized training. One is that we should train the joint action-value function Q jt to estimate the true action-value; the other is that the transformed action-value function Q jt should "track" the joint action-value function in the sense that their optimal actions are equivalent. We use the algorithm introduced in DQN <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref> to update networks, where we maintain a target network and a replay buffer. To this end, we devise the following global loss function in QTRAN, combining three loss functions in a weighted manner:</p><formula xml:id="formula_11">L(? , u, r, ? ; ?) = L td + ? opt L opt + ? nopt L nopt ,<label>(6)</label></formula><p>where r is the reward for action u at observation histories ? with transition to ? . L td is the loss function for estimating the true action-value, by minimizing the TD-error as Q jt is learned. L opt and L nopt are losses for factorizing Q jt by [Q i ] satisfying condition (4). The role of L nopt is to check at each step if the action selected in the samples satisfied <ref type="formula">(4b)</ref>, and L opt confirms that the optimal local action obtained satisfies (4a). One could implement (4) by defining a loss depending on how well the networks satisfy (4a) or (4b) with actions taken in the samples. However, in this way, verifying whether (4a) is indeed satisfied would take too many samples since optimal actions are seldom taken at training. Since we aim to learn Q jt and V jt to factorize for a given Q jt , we stabilize the learning by fixing Q jt when learning with L opt and L nopt . We letQ jt denote this fixed Q jt . ? opt and ? nopt are the weight constants for two losses. The detailed forms of L td , L opt , and L nopt are given as follows, where we omit their common function arguments (? , u, r, ? ) in loss functions for presentational simplicity:</p><formula xml:id="formula_12">L td (; ?) = Q jt (? , u) ? y dqn (r, ? ; ? ? ) 2 , L opt (; ?) = Q jt (? ,?) ?Q jt (? ,?) + V jt (? ) 2 , L nopt (; ?) = min Q jt (? , u) ?Q jt (? , u) + V jt (? ), 0 2 , where y dqn (r, ? ; ? ? ) = r + ?Q jt (? ,? ; ? ? ),? = [arg max ui Q i (? i , u i ; ? ? )] N i=1</formula><p>, and ? ? are the periodically copied parameters from ?, as in DQN <ref type="bibr" target="#b13">(Mnih et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Tracking the Joint Action-value Function Differently</head><p>We name the method previously discussed QTRAN-base, to reflect the basic nature of how it keeps track of the joint action-value function. Here on, we consider a variant of QTRAN, which utilizes a counterfactual measure. As mentioned earlier, Theorem 1 is used to enforce IGM by (4a) and determine how the individual action-value functions [Q i ] and the State-value function V jt jointly "track" Q jt by (4b), which governs the stability of constructing the correct factorizing Q i 's. We found that condition (4b) is often too loose, leading the neural networks to fail their mission of constructing the correct factors of Q jt . That is, condition (4b) imposes undesirable influence on the non-optimal actions, which in turn compromises the stability and/or convergence speed of the training process. This motivates us to study conditions stronger than (4b) that would still be sufficient for factorizability, but at the same time would also be necessary under the aforementioned affine transformation ?, as in Theorem 1.</p><p>Theorem 2. The statement presented in Theorem 1 and the necessary condition of Theorem 1 holds by replacing (4b) with the following <ref type="formula" target="#formula_13">(7)</ref>: if u =?,</p><formula xml:id="formula_13">min ui?U Q jt (? , u i , u ?i ) ? Q jt (? , u i , u ?i ) + V jt (? ) = 0, ?i = 1, . . . , N,<label>(7)</label></formula><p>where u ?i = (u 1 , . . . , u i?1 , u i+1 , . . . , u N ), i.e., the action vector except for i's action.</p><p>The proof is presented in the Supplementary. The key idea behind <ref type="formula" target="#formula_13">(7)</ref> lies in what conditions to enforce on non-optimal actions. It stipulates that</p><formula xml:id="formula_14">Q jt (? , u) ? Q jt (? , u) + V jt (? )</formula><p>be set to zero for some actions. Now, it is not possible to zero this value for every action u, but it is available for at least one action whilst still abiding by Theorem 1. It is clear that condition <ref type="formula" target="#formula_13">(7)</ref> is stronger than condition (4b), as desired. For non-optimal actions u =?, the conditions of Theorem 1 are satisfied when</p><formula xml:id="formula_15">Q jt (? , u) ? V jt (? ) ? Q jt (? , u) ? Q jt (? ,?)</formula><p>for any given ? . Under this condition, however, there can exist a non-optimal action u =? whose Q jt (? , u) is comparable to Q jt (? ,?) but Q jt (? , u) is much smaller than Q jt (? ,?). This may cause instability in the practical learning process. However, the newly devised condition <ref type="formula" target="#formula_13">(7)</ref> compels Q jt (? , u) to track Q jt (? , u) even for the problematic non-optimal actions mentioned above. This helps in widening the gap between Q jt (? , u) and Q jt (? ,?), and this gap makes the algorithm more stable. Henceforth, we call the deep MARL method outlined by Theorem 2 QTRAN-alt, to distinguish it from the one due to condition (4b).</p><p>Counterfactual joint networks To reflect our idea of <ref type="formula" target="#formula_13">(7)</ref>, we now propose a counterfactual joint network, which replaces the joint action-value network of QTRAN-base, to efficiently calculate Q jt (? , ?, u ?i ) and Q jt (? , ?, u ?i ) for all i ? N with only one forward pass. To this end, in the QTRAN-alt module, each agent has a counterfactual joint network with the output of Q jt (? , ?, u ?i ) for each possible action, given other agents' actions. As a joint action-value network, we use h V,i (? i ) and the combined hidden features j =i h Q,j (? j , u j ) from other agents. Finally, </p><formula xml:id="formula_16">Q jt (? , ?, u ?i ) is calculated as Q i (? i , ?) + j =i Q j (? j , u j )<label>for</label></formula><formula xml:id="formula_17">L nopt-min (? , u, r, ? ; ?) = 1 N N i=1 ( min ui?U D(? , u i , u ?i )) 2 , where D(? , u i , u ?i ) = Q jt (? , u i , u ?i ) ?Q jt (? , u i , u ?i )+V jt (? ).</formula><p>In QTRAN-alt, L td , L opt are also used, but they are also computed for all agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Example: Single-state Matrix Game</head><p>In this subsection, we present how QTRAN performs compared to existing value factorization ideas such as VDN and QMIX, and how the two variants QTRAN-base and QTRAN-alt behave. The matrix game and learning results are shown in <ref type="table" target="#tab_1">Table 1</ref> 1 . This symmetric matrix game has the optimal joint action (A, A), and captures a very simple cooperative multi-agent task, where we have two users with three actions each. Evaluation with more complicated tasks are provided in the next subsection. We show the results of VDN, QMIX, and QTRAN through a full exploration (i.e., = 1 in -greedy) conducted over 20,000 steps. Full exploration guarantees to explore all available game states. Therefore, we can compare only the expressive power of the methods. Other details are included in the Supplementary.</p><p>Comparison with VDN and QMIX <ref type="table" target="#tab_1">Tables 1b-1f</ref> show the learning results of QTRAN, VDN, and QMIX. <ref type="table" target="#tab_1">Table 1b</ref> shows that QTRAN enables each agent to jointly take the optimal action only by using its own locally optimal action, <ref type="bibr">1</ref> We present only Qjt and Q jt , because in fully observable cases (i.e., observation function is bijective for all i) Theorem 1 holds for Vjt(? ) = 0. We discuss further in Supplementary. meaning successful factorization. Note that <ref type="table" target="#tab_1">Tables 1c and  1d</ref> demonstrate the difference between Q jt and Q jt , stemming from our transformation, where their optimal actions are the nonetheless same. <ref type="table" target="#tab_1">Table 1d</ref> shows that QTRAN also satisfies (4), thereby validating our design principle as described in Theorem 1. However, in VDN, agents 1's and 2's individual optimal actions are C and B, respectively. VDN fails to factorize because the structural constraint of additivity Q jt (u) = i=1,2 Q i (u i ) is enforced, leading to deviations from IGM, whose sufficient condition is additivity, i.e., Q jt (u) = i=1,2 Q i (u i ) for all u = (u 1 , u 2 ). QMIX also fails in factorization in a similar manner due to the structural constraint of monotonicity.</p><p>Impact of QTRAN-alt In order to see the impact of QTRAN-alt, we train the agents in a matrix game where two agents each have 21 actions. <ref type="figure">Figure 2</ref> illustrates the joint action-value function and its transformations of both QTRAN-base and QTRAN-alt. The result shows that both algorithms successfully learn the optimal action by correctly estimating the Q jt for u =? for any given state. Q jt values for non-optimal actions are different from Q jt , but it has a different tendency in each algorithm as follows. As shown in <ref type="figure">Figures 2e-2h</ref>, all Q jt values in QTRAN-base have only a small difference from the maximum value of Q jt , whereas <ref type="figure">Figures 2i-2l</ref> show that QTRAN-alt has the ability to more accurately distinguish optimal actions from non-optimal actions. Thus, in QTRAN-alt, the agent can smartly explore and have better sample efficiency to train the networks. This feature of QTRAN-alt also prevents learning unsatisfactory policies in complex environments. Full details on the experiment are included in the Supplementary.  <ref type="bibr">(2018)</ref>. In GS, multiple homogeneous agents need to work together for efficient resource allocation whilst avoiding congestion. We use GS in conjunction with Multi-domain Gaussian Squeeze (MGS) as follows: we have ten agents; each agent i takes action u i , which controls the resource usage level, ranging over {0, 1, ..., 9}. Each agent has its own amount of unit-level resource s i ? [0, 0.2], given by the environment a priori. Then, a joint action u determines the overall resource usage x(u) = i s i ? u i . We assume that there exist K domains, where the above resource allocation takes place. Then, the goal is to maximize the joint reward defined as</p><formula xml:id="formula_18">G(u) = K k=1 xe ?(x?? k ) 2 /? k 2 ,</formula><p>where ? k and ? k are the parameters of each domain. Depending on the number of domains, GS has only one local maximum, whereas MGS has multiple local maxima. In our MGS setting, compared to GS, the optimal policy is similar to that in GS, and through this policy, the reward similar to that in GS can be obtained. Additionally, in MGS, a new sub-optimal "pitfall" policy that is easier to achieve but is only half as rewarding as the optimal policy. The case when K &gt; 1 is usefully utilized to test the algorithms that are required to avoid sub-optimal points in the joint space of actions. The full details on the environment setup and hyperparameters are described in the Supplementary.</p><p>Modified predator-prey (MPP) We adopt a more complicated environment by modifying the well-known predator-prey <ref type="bibr" target="#b20">(Stone &amp; Veloso, 2000)</ref> in the grid world, used in many other MARL research. State and action spaces are constructed similarly to those of the classic predator-prey game. "Catching" a prey is equivalent to having the prey within an agent's observation horizon. We extend it to the scenario that positive reward is given only if multiple predators catch a prey simultaneously, requiring a higher degree of cooperation. The predators get a team reward of 1, if two or more catch a prey at the same time, but they are given negative reward ?P , when only one predator catches the prey.</p><p>Note that the value of penalty P also determines the degree of monotonicity, i.e., the higher P is, the less monotonic the task is. The prey that has been caught regenerated at random positions whenever caught by more than one predator. In our evaluation, we tested up to N = 4 predators and up to two prey, and the game proceeds over fixed 100 steps. We experimented with six different settings with varying P values and numbers of agents, where N = 2, 4 and P = 0.5, 1.0, 1.5. For the N = 4 case, we placed two prey; otherwise, just one. The detailed settings are available in the Supplementary. <ref type="figure" target="#fig_2">Figure 3a</ref> shows the result of GS, where it is not surprising to observe that all algorithms converge to the optimal point. However, QTRAN noticeably differs in its convergence speed; it is capable of handling the non-monotonic nature of the environment more accurately and of finding an optimal policy from wellexpressed action-value functions. VDN and QMIX have some structural constraints, which hinder the accurate learning of the action-value functions for non-monotonic structures. These algorithms converge to the locally optimal point -which is the globally optimal point in GS, where K = 1 -near the biased samples by a wrong policy with epsilon-decay exploration. To support our claim, we experiment with the full exploration without epsilon-decay for the same environment, as shown in <ref type="figure" target="#fig_2">Figure 3b</ref>. We observe that QTRAN learns more or less the same policy as in <ref type="figure" target="#fig_2">Figure 3a</ref> ure 3c shows the result for a more challenging scenario of MGS, with each of [s i ] being the same, where agents can always achieve higher performance than GS. VDN and QMIX are shown to learn only a sub-optimal policy in MGS, whose rewards are even smaller than those in GS. QTRAN-base and QTRAN-alt achieve significantly higher rewards, where QTRAN-alt is more stable, as expected. This is because QTRAN-alt's alternative loss increases the gap between Q jt for non-optimal and optimal actions, and prevents it from being updated to an undesirable policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-domain Gaussian Squeeze</head><p>Modified predator-prey <ref type="figure" target="#fig_3">Figure 4</ref> shows the performance of the three algorithms for six settings with different N and P values, where all results demonstrate the superiority of QTRAN to VDN and QMIX. In <ref type="figure" target="#fig_3">Figures 4a and 4d</ref> with low penalties P , all three algorithms learn the policy that catches the prey well and thus obtain high rewards. However, again, the speed in finding the policy in VDN and QMIX is slower than that in QTRAN. As the penalty P grows and exacerbates the non-monotonic characteristic of the environment, we observe a larger performance gap between QTRAN and the two other algorithms. As shown in <ref type="figure" target="#fig_3">Figures 4b and 4c</ref>, even for a large penalty, QTRAN agents still cooperate well to catch the prey. However, in VDN and QMIX, agents learn to work together with somewhat limited coordination, so that they do not actively try to catch the prey and instead attempt only to minimize the penaltyreceiving risk. In <ref type="figure" target="#fig_3">Figures 4e and 4f</ref>, when the number of agents N grows, VDN and QMIX do not cooperate at all and learn only a sub-optimal policy: running away from the prey to minimize the risks of being penalized.</p><p>For the tested values of N and P , we note that the performance gap between QTRAN-base and QTRAN-alt is influenced more strongly by N . When N = 2, the gap in the convergence speed between QTRAN-base and QTRAN-alt increases as the penalty grows. Nevertheless, both algorithms ultimately turn out to train the agents to cooperate well for every penalty value tested. On the other hand, with N = 4, the gap of convergence speed between QTRAN-base and QTRAN-alt becomes larger. With more agents, as shown in <ref type="figure" target="#fig_3">Figures 4d and 4e</ref>, QTRAN-alt achieves higher scores than QTRAN-base does, and QTRAN-base requires longer times to reach positive scores. <ref type="figure" target="#fig_3">Figure 4f</ref> shows that QTRAN-base does not achieve a positive reward until the end, implying that QTRAN-base fails to converge, whereas QTRAN-alt's score increases, with training steps up to 10 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented QTRAN, a learning method to factorize the joint action-value functions of a wide variety of MARL tasks. QTRAN takes advantage of centralized training and fully decentralized execution of the learned policies by appropriately transforming and factorizing the joint action-value function into individual action-value functions. Our theoretical analysis demonstrates that QTRAN handles a richer class of tasks than its predecessors, and our simulation results indicate that QTRAN outperforms VDN and QMIX by a substantial margin, especially so when the game exhibits more severe non-monotonic characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. QTRAN Training Algorithm</head><p>The training algorithms for QTRAN-base and QTRAN-alt are provided in Algorithm 1.</p><p>Algorithm <ref type="formula">1</ref>  for t = 1 to T do 7:</p><p>With probability select a random action u t i 8:</p><formula xml:id="formula_19">Otherwise u t i = arg max u t i Q i (? t i , u t i ) for each agent i 9:</formula><p>Take action u t , and retrieve next observation and reward (o t+1 , r t ) 10:</p><formula xml:id="formula_20">Store transition (? t , u t , r t , ? t+1 ) in D 11:</formula><p>Sample a random minibatch of transitions (? , u, r, ? ) from D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Set y dqn (r, ? ;</p><formula xml:id="formula_21">? ? ) = r + ?Q jt (? ,? ; ? ? ),? = [arg max ui Q i (? i , u i ; ? ? )] N i=1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>If QTRAN-base, update ? by minimizing the loss:</p><formula xml:id="formula_22">L(? , u, r, ? ; ?) = L td + ? opt L opt + ? nopt L nopt , L td (? , u, r, ? ; ?) = Q jt (? , u) ? y dqn (r, ? ; ? ? ) 2 , L opt (? , u, r, ? ; ?) = Q jt (? ,?) ?Q jt (? ,?) + V jt (? ) 2 , L nopt (? , u, r, ? ; ?) = min Q jt (? , u) ?Q jt (? , u) + V jt (? ), 0 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>If QTRAN-alt, update ? by minimizing the loss:</p><formula xml:id="formula_23">L(? , u, r, ? ; ?) = L td + ? opt L opt + ? nopt-min L nopt-min , L td (? , u, r, ? ; ?) = Q jt (? , u) ? y dqn (r, ? ; ? ? ) 2 , L opt (? , u, r, ? ; ?) = Q jt (? ,?) ?Q jt (? ,?) + V jt (? ) 2 , L nopt-min (? , u, r, ? ; ?) = 1 N N i=1 min ui?U Q jt (? , u i , u ?i ) ?Q jt (? , u i , u ?i ) + V jt (? ) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Update target network parameters ? ? = ? with period I 16:</p><p>end for 17: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proofs</head><p>In this section, we provide the proofs of theorems and propositions coming from theorems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Proof of Theorem 1 Theorem 1. A factorizable joint action-value function</head><formula xml:id="formula_24">Q jt (? , u) is factorized by [Q i (? i , u i )], if N i=1 Q i (? i , u i ) ? Q jt (? , u) + V jt (? ) = 0 u =?, ? 0 u =?,<label>(4a)</label></formula><formula xml:id="formula_25">(4b) where V jt (? ) = max u Q jt (? , u) ? N i=1 Q i (? i ,? i ).</formula><p>Proof. Theorem 1 shows that if condition (4) holds, then Q i satisfies IGM. Thus, for some given Q i that satisfies (4), we will show that arg max u Q jt (? , u) =?. <ref type="figure" target="#fig_3">From (4b)</ref>).</p><formula xml:id="formula_26">Recall that? i = arg max ui Q i (? i , u i ) and? = [? i ] N i=1 ,. Q jt (? ,?) = N i=1 Q i (? i ,? i ) + V jt (? ) (From (4a)) ? N i=1 Q i (? i , u i ) + V jt (? ) ? Q jt (? , u) (</formula><p>It means that the set of optimal local actions? maximizes Q jt , showing that Q i satisfies IGM. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Necessity in Theorem 1 Under Affine-transformation</head><p>As mentioned in Section 3.1, the conditions (4) in Theroem 1 are necessary under an affine transformation. The necessary condition shows that for some given factorizable Q jt , there exists Q i that satisfies <ref type="formula">(4)</ref>, which guides us to design the QTRAN neural network. Note that the affine transformation ? is ?</p><formula xml:id="formula_27">(Q) = A ? Q + B, where A = [a ii ] ? R N ?N + is a symmetric diagonal matrix with a ii &gt; 0, ?i and B = [b i ] ? R N . To abuse notation, let ?(Q i (? i , u i )) = a ii Q i (? i , u i ) + b i . Proposition 1. If Q jt (? , u) is factorized by [Q i (? i , u i )], then there exists an affine transformation ?(Q) such that Q jt (? , u) is factorized by [?(Q i (? i , u i ))] and the condition (4) holds by replacing [Q i (? i , u i )] with [?(Q i (? i , u i ))].</formula><p>Proof. To prove, we will show that, for the factors [Q i ] of Q jt , there exists an affine transformation of Q i that also satisfies conditions (4).</p><p>By definition, if Q jt (? , u) is factorized by [Q i (? i , u i )], then the followings hold:</p><formula xml:id="formula_28">(i) Q jt (? ,?) ? max u Q jt (? , u) = 0, (ii) Q jt (? , u) ? Q jt (? ,?) &lt; 0, and (iii) N i=1 (Q i (? i , u i ) ? Q i (? i ,? i )) &lt; 0 if u =?.</formula><p>Now, we consider an affine transformation, in which a ii = ? and b i = 0 ?i, where ? &gt; 0, and ?(Q i ) = ?Q i with this transformation. Since this is a linearly scaled transformation, it satisfies the IGM condition, and thus (4a) holds. We also prove that ?(Q i ) satisfies condition (4a) by showing that there exists a constant ? small enough such that</p><formula xml:id="formula_29">N i=1 ?Q i (? i , u i ) ? Q jt (? , u) + V jt (? , u) = N i=1 ?(Q i (? i , u i ) ? Q i (? i ,? i )) ? (Q jt (? , u) ? Q jt (? ,?)) ? 0,</formula><p>where V jt (? ) is redefined for linearly scaled ?Q i as max u Q jt (? , u) ? N i=1 ?Q i (? i ,? i ). This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Special Case: Theorem 1 in Fully Observable Environments</head><p>If the task is a fully observable case (observation function is bijective for all i), the state-value network is not required and all V jt (? ) values can be set to zero. We show that Theorem 1 holds equally for the case where V jt (? ) = 0 for a fully observable case. This fully observable case is applied to our example of the simple matrix game. The similar necessity under an affine-transformation holds in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1a. (Fully observable case) A factorizable joint action-value function</head><formula xml:id="formula_30">Q jt (? , u) is factorized by [Q i (? i , u i )], if N i=1 Q i (? i , u i )?Q jt (? , u) = 0 u =?, (9a) ? 0 u =?,<label>(9b)</label></formula><p>Proof. We will show arg max u Q jt (? , u) =? (i.e., IGM), if the (9) holds.</p><formula xml:id="formula_31">Q jt (? ,?) = N i=1 Q i (? i ,? i ) ? N i=1 Q i (? i , u i ) ? Q jt (? , u).</formula><p>The first equality comes from (9a), and the last inequality comes from (9b). We note that arg max u Q jt (? , u) = [arg max ui Q i (? i , u i )], so this completes the proof.</p><formula xml:id="formula_32">Proposition 1a. If Q jt (? , u) is factorized by [Q i (? i , u i )]</formula><p>, then there exists an affine transformation ?(Q), such that Q jt (? , u) is factorized by [?(Q i (? i , u i ))] and condition (9) holds by replacing</p><formula xml:id="formula_33">[Q i (? i , u i )] with [?(Q i (? i , u i ))].</formula><p>Proof. From Theorem 1a, if Q jt (? , u) is factorizable, then there exist [Q i ] satisfying both IGM and (9). Now, we define an additive transformation</p><formula xml:id="formula_34">? i (Q i (? i , u i )) = Q i (? i , u i ) + 1 N max u Q jt (? , u) ? Q i (? i ,? i )</formula><p>for a given ? i , which is uniquely defined for fully observable cases. [? i (Q i (? i , u i ))] also satisfy IGM, and the left-hand side of (9) can be rewritten as:</p><formula xml:id="formula_35">N i=1 Q i (? i , u i ) ? Q jt (? , u) ? N i=1 Q i (? i ,? i ) + max u Q jt (? , u) = N i=1 ? i (Q i (? i , u i )) ? Q jt (? , u)</formula><p>So there exist individual action-value functions [? i (Q i (? i , u i ))] that satisfy both IGM and (9), where V jt (? ) is redefined as 0. This completes the proof of the necessity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Proof of Theorem 2</head><p>Theorem 2. The statement presented in Theorem 1 and the necessary condition of Theorem 1 holds by replacing (4b) with the following <ref type="formula" target="#formula_13">(7)</ref>: if u =?,</p><formula xml:id="formula_36">min ui?U Q jt (? , u i , u ?i ) ? Q jt (? , u i , u ?i ) + V jt (? ) = 0, ?i = 1, . . . , N,<label>(7)</label></formula><p>where u ?i = (u 1 , . . . , u i?1 , u i+1 , . . . , u N ), i.e., the action vector except for i's action.</p><p>Proof. (?) Recall that condition <ref type="formula" target="#formula_13">(7)</ref> is stronger than (4b), which is itself sufficient for Theorem 1. Therefore, by transitivity, condition <ref type="formula" target="#formula_13">(7)</ref> is sufficient for Theorem 2. Following paragraphs focus on the other direction, i.e., how condition <ref type="formula" target="#formula_13">(7)</ref> is necessary for Theorem 2.</p><p>(?) We prove that, if there exist individual action-value functions satisfying condition (4), then there exists an individual action-value function Q i that satisfies <ref type="formula" target="#formula_13">(7)</ref>. In order to show the existence of such Q i , we propose a way to construct Q i .</p><p>We first consider the case with N = 2 and then generalize the result for any N . The condition <ref type="formula" target="#formula_13">(7)</ref> for N = 2 is denoted as:</p><formula xml:id="formula_37">min ui?U Q 1 (? 1 , u 1 ) + Q 2 (? 2 , u 2 ) ? Q jt (? , u 1 , u 2 ) + V jt (? ) = 0.</formula><p>Since this way of constructing Q i is symmetric for all i, we present its construction only for u 1 without loss of generality.</p><p>For Q 1 and Q 2 satisfying (4), if ? := min u1?U Q 1 (? 1 , u 1 ) + Q 2 (? 2 , u 2 ) ? Q jt (? , u 1 , u 2 ) + V jt (? ) &gt; 0 for given ? and u 2 , then u 2 =? 2 . This is because Q 1 (? 1 ,? 1 ) + Q 2 (? 2 ,? 2 ) ? Q jt (? ,? 1 ,? 2 ) + V jt (? ) = 0 by condition (4a). Now, we replace Q 2 (? 2 , u 2 ) with Q 2 (? 2 , u 2 ) = Q 2 (? 2 , u 2 ) ? ?. Since Q 2 (? 2 ,? 2 ) &gt; Q 2 (? 2 , u 2 ) &gt; Q 2 (? 2 , u 2 ) ? ?, it does not change the optimal action and other conditions. Then, (7) is satisfied for given ? and u 2 . By repeating this replacement process, we can construct Q i that satisfies condition (7).</p><p>More generally, when N = 2, if min ui?U Q jt (? , u i , u ?i ) ? Q jt (? , u i , u ?i ) + V jt (? ) = ? &gt; 0 for given ? and u ?i , then there exists some j = i that satisfies u j =? j . Therefore, by repeating the same process as when N = 2 through j, we can construct Q i for all i, and this confirms that individual action-value functions satisfying condition <ref type="formula" target="#formula_13">(7)</ref> exist. This completes the proof.</p><p>B.4.1. PROCESS OF CONSTRUCTING Q i IN THE MATRIX GAME USING THEOREM 2</p><p>We now present the process of how we have demonstrated through the example in Section 3.5. In the original matrix shown in <ref type="table" target="#tab_3">Tables 2a and 2d</ref>, the second row does not satisfy the condition <ref type="formula" target="#formula_13">(7)</ref>, and ? = 0.23 for u 1 = B. Then, we replace Q 1 (B) as shown in <ref type="table" target="#tab_3">Table 2e</ref>. <ref type="table" target="#tab_3">Table 2b</ref> shows that its third row does not satisfy the condition <ref type="formula" target="#formula_13">(7)</ref>. Finally, we replace Q 1 (C) as shown in <ref type="table" target="#tab_3">Table 2f</ref>. Then, the resulting <ref type="table" target="#tab_3">Table 2c</ref> satisfies the condition <ref type="formula" target="#formula_13">(7)</ref>.  C. Details of environments and implementation C.1. Environment</p><p>Matrix game In order to see the impact of QTRAN-alt, we train the agents in a single state matrix game where two agents each have 21 actions. Each agent i takes action u i , ranging over ? {0, ..., 20}. The reward value R for a joint action is given as follows:  Multi-domain Gaussian Squeeze We adopt and modify Gaussian Squeeze, where agent numbers (N = 10) and action spaces (u i ? {0, 1, ..., 9}) are much larger than a simple matrix game. In MGS, each of the ten agents has its own amount of unit-level resource s i ? [0, 0.2] given by the environment a priori. This task covers a fully observable case where all agents can see the entire state. We assume that there exist K domains, where the above resource allocation takes place. The joint action u determines the overall resource usage x(u) = i s i ? u i . Reward is given as a function of resource usage as follows:</p><formula xml:id="formula_38">f 1 (u 1 , u 2 ) = 5 ? 15 ? u 1 3 2 ? 5 ? u 2 3 2 f 2 (u 1 , u 2 ) = 10 ? 5 ? u 1 1 2 ? 15 ? u 2 1 2 R(u 1 , u 2 ) = max(f 1 (u 1 , u 2 ), f 2 (u 1 , u 2 ))</formula><formula xml:id="formula_39">G(u) = K k=1 xe ?(x?? k ) 2 /? k 2</formula><p>. We test with two different settings: (i) K = 1, ? 1 = 8, ? 1 = 1 as shown in <ref type="figure" target="#fig_7">Figure 6a</ref>, and (ii) K = 2, ? 1 = 8, ? 1 = 0.5, ? 2 = 5, ? 2 = 1 as shown in <ref type="figure" target="#fig_7">Figure 6b</ref>. In the second setting, there are two local maxima for resource x. The maximum on the left is relatively easy to find through exploration -as manifested in the greater variance of the Gaussian distribution, but the maximum reward -as represented by the lower peak -is relatively low. On the other hand, the maximum on the right is more difficult to find through exploration, but it offers higher reward. Modified predator-prey Predator-prey involves a grid world, in which multiple predators attempt to capture randomly moving prey. Agents have a 5 ? 5 view and select one of five actions ? {Left, Right, Up, Down, Stop} at each time step. Prey move according to selecting a uniformly random action at each time step. We define the "catching" of a prey as when the prey is within the cardinal direction of at least one predator. Each agent's observation includes its own coordinates, agent ID, and the coordinates of the prey relative to itself, if observed. The agents can separate roles even if the parameters of the neural networks are shared by agent ID. We test with two different grid worlds: (i) a 5 ? 5 grid world with two predators and one prey, and (ii) a 7 ? 7 grid world with four predators and two prey. We modify the general predator-prey, such that a positive reward is given only if multiple predators catch a prey simultaneously, requiring a higher degree of cooperation. The predators get a team reward of 1 if two or more catch a prey at the same time, but they are given negative reward ?P , if only one predator catches the prey as shown in <ref type="figure" target="#fig_8">Figure 7</ref>. We experimented with three varying P vales, where P = 0.5, 1.0, 1.5. The terminating condition of this task is when a prey is caught with more than one predator. The prey that has been caught is regenerated at random positions whenever the task terminates, and the game proceeds over fixed 100 steps.  <ref type="table" target="#tab_5">Table 3</ref> shows the values of the hyperparameters for the training in the matrix game environment. Individual action-value networks, which are common in all VDN, QMIX, and QTRAN, each consist of two hidden layers. In addition to the individual Q-networks, QMIX incorporates a monotone network with one hidden layer, and the weights and biases of this network are produced by separate hypernetworks. QTRAN has an additional joint action-value network with two hidden layers. All hidden layer widths are 32, and all activation functions are ReLU. All neural networks are trained using the Adam optimizer. We use -greedy action selection with = 1 independently for exploration.  Multi-domain Gaussian Squeeze <ref type="table" target="#tab_7">Table 4</ref> shows the values of the hyperparameters for the training in the MGS environment. Each individual action-value network consists of three hidden layers. In addition to the individual Q-networks, QMIX incorporates a monotone network with one hidden layer, and the weights and biases of this network are produced by separate hypernetworks. QTRAN has an additional joint action-value network with two hidden layers. All hidden layer widths are 64, and all activation functions are ReLU. All neural networks are trained using the Adam optimizer. We use -greedy action selection with decreasing from 1 to 0.1 for exploration.  Modified predator-prey <ref type="table" target="#tab_9">Table 5</ref> shows the values of the hyperparameters for the training in the modified predator-prey environment. Each individual action-value network consists of three hidden layers. In addition to the individual Q-networks, QMIX incorporates a monotone network with one hidden layer, and the weights and biases of this network are produced by separate hypernetworks. QTRAN has additional joint action-value network with two hidden layers. All hidden layer widths are 64, and all activation functions are ReLU. All neural networks are trained using the Adam optimizer. We use -greedy action selection with decreasing from 1 to 0.1 for exploration. Since history is not very important in this task, our experiments use feed-forward policies, but our method is also applicable with recurrent policies.   <ref type="table">Table 6</ref> shows the comparison between the final performance levels of VDN, QMIX, and QTRAN-base for 310 3 ? 3 random matrix games, where each value of the payoff matrix is given as a random parameter from 0 to 1. Experimental settings are the same as in the previous matrix game. Since matrix games always satisfy IGM conditions, QTRAN-base always trains the optimal action for all 310 cases. On the other hand, VDN and QMIX were shown to be unable to learn an optimal policy for more than half of non-monotonic matrix games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter</head><p>We briefly analyze the nature of the structural constraints assumed by VDN and QMIX, namely, additivity and monotonicity of the joint action-value functions. There have been only 19 cases in which the results of VDN and QMIX differ from each other. Interestingly, for a total of five cases, the performance of VDN is better than QMIX. QMIX was shown to outperform VDN in more cases (14) than the converse case (5). This supports the idea that the additivity assumption imposed by VDN on the joint action-value functions is indeed stronger than the monotonicity assumption imposed by QMIX.</p><p>QTRAN=VDN=QMIX QTRAN&gt;VDN=QMIX VDN&gt;QMIX QMIX&gt;VDN 114 177 5 14 <ref type="table">Table 6</ref>. Final performance comparison with 310 random matrices <ref type="figure">Figures 8-9</ref> show the joint action-value function of VDN and QMIX, and <ref type="figure" target="#fig_0">Figures 10-11</ref> show the transformed joint action-value function of QTRAN-base and QTRAN-alt for a matrix game where two agents each have 20 actions. In the result, VDN and QMIX can not recover joint action-value, and these algorithms learn sub-optimal policy u 1 , u 2 = (15, 5).</p><p>In the other hand, the result shows that QTRAN-base and QTRAN-alt successfully learn the optimal action, but QTRAN-alt has the ability to more accurately distinguish action from non-optimal action as shown in <ref type="figure" target="#fig_0">Figure 11</ref>. E. Comparison with other value-based methods for modified predator-prey Additionally, we have conducted experiments with Dec-HDRQN <ref type="bibr" target="#b16">(Omidshafiei et al., 2017)</ref>. Dec-HDRQN can indeed solve problems similar to ours by changing the learning rate according to TD-error without factorization. However, Dec-HDRQN does not take advantage of centralized training. We implemented Dec-HDRQN (? = 0.001, ? = 0.0002) with modified predator-prey experiments and <ref type="figure" target="#fig_0">Figure 12</ref> shows the performance of algorithms for six settings with different N and P values.</p><p>First, when the complexity of the task is relatively low, Dec-HDRQN shows better performance than VDN and QMIX as shown in the <ref type="figure" target="#fig_0">Figure 12b</ref>. However, QTRAN performs better than Dec-HDRQN in the case. When the penalty and the number of agents are larger, Dec-HDRQN underperforms VDN and QMIX. <ref type="figure" target="#fig_0">Figure 12c</ref> shows Dec-HDRQN scores an average of nearly 0 in the case of N = 2, P = 1.5. There is a limit of Dec-HDRQN since the method is heuristic and does not perform centralized training. Finally, Dec-HDRQN showed slower convergence speed overall.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>QTRAN-base and QTRAN-alt Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>alt: 4000 stepFigure 2. base: QTRAN-base, alt: QTRAN-alt. x-axis and y-axis: agents 1 and 2's actions, respectively. Colored values represent the values of Qjt ((a)-(d)) and Q jt ((e)-(l)) for selected actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Average reward on the GS and GMS tasks with 95% confidence intervals for VDN, QMIX, and QTRAN 4. Experiment4.1. EnvironmentsTo demonstrate the performance of QTRAN, we consider two environments: (i) Multi-domain Gaussian Squeeze and (ii) modified predator-prey. Details on our implementation of QTRAN, the source code of our implementation in TensorFlow, and other experimental scripts are available in the Supplementary and a public repository: https://github.com/Sonkyunghwan/QTRAN.Multi-domain Gaussian Squeeze (MGS) Gaussian Squeeze (GS)<ref type="bibr" target="#b6">(HolmesParker et al., 2014)</ref> is a simple nonmonotonic multi-agent resource allocation problem, used in other papers, e.g.,<ref type="bibr" target="#b29">Yang et al. (2018)</ref> andChen et al.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>, whereas VDN and QMIX significantly deteriorate. Fig-Average reward per episode on the MPP tasks with 95% confidence intervals for VDN, QMIX, and QTRAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>QTRAN-base and QTRAN-alt 1: Initialize replay memory D 2: Initialize [Q i ], Q jt , and V jt with random parameters ? 3: Initialize target parameters ? ? = ? 4: for episode = 1 to M do 5: Observe initial state s 0 and observation o 0 = [O(s 0 , i)] N i=1 for each agent i 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>. x-axis and y-axis: agents 1 and 2's actions, respectively. Colored values represent the reward for selected actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>shows reward for selected action. Colored values represent the values. In the simple matrix game, the reward function has a global maximum point at (u 1 , u 2 ) = (5, 15) and a local maximum point at (u 1 , u 2 ) = (15, 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Gaussian Squeeze and Multi-domain Gaussian Squeeze</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Predator-prey environment C.2. Experiment details Matrix game</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 Figure 9 Figure 10 Figure 11 Figure 12 .</head><label>89101112</label><figDesc>. x-axis and y-axis: agents 1 and 2's actions. Colored values represent the values of Qjt for VDN . x-axis and y-axis: agents 1 and 2's actions. Colored values represent the values of Qjt for QMIX . x-axis and y-axis: agents 1 and 2's actions. Colored values represent the values of Q jt for QTRAN-base . x-axis and y-axis: agents 1 and 2's actions. Colored values represent the values of Q jt for QTRAN-alt QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning Average reward per episode on the MPP tasks with 95% confidence intervals for VDN, QMIX, Dec-HDRQN and QTRAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>School of Electrical Enginerring, KAIST, Daejeon, South Korea. Correspondence to: Yung Yi &lt;yiyung@kaist.edu&gt;, Kyunghwan Son &lt;kevinson9473@kaist.ac.kr&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>all agents. This architectural choice is realized by choosing the loss function to be L nopt-min , replacing L nopt in Payoff matrix of the one-step game and reconstructed Qjt results on the game. Boldface means optimal/greedy actions from the state-action value QTRAN-base as follows:</figDesc><table><row><cell>u1 u2</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell cols="4">Q1 Q2 4.16(A) 2.29(B) 2.29(C)</cell></row><row><cell>A</cell><cell>8</cell><cell>-12</cell><cell>-12</cell><cell>3.84(A)</cell><cell>8.00</cell><cell>6.13</cell><cell>6.12</cell></row><row><cell>B</cell><cell>-12</cell><cell>0</cell><cell>0</cell><cell>-2.06(B)</cell><cell>2.10</cell><cell>0.23</cell><cell>0.23</cell></row><row><cell>C</cell><cell>-12</cell><cell>0</cell><cell>0</cell><cell>-2.25(C)</cell><cell>1.92</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell cols="4">(a) Payoff of matrix game</cell><cell cols="4">(b) QTRAN: Q1, Q2, Q jt</cell></row><row><cell>u1 u2</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>u1 u2</cell><cell>A</cell><cell>B</cell><cell>C</cell></row><row><cell>A</cell><cell>8.00</cell><cell cols="2">-12.02 -12.02</cell><cell>A</cell><cell>0.00</cell><cell>18.14</cell><cell>18.14</cell></row><row><cell>B</cell><cell>-12.00</cell><cell>0.00</cell><cell>0.00</cell><cell>B</cell><cell>14.11</cell><cell>0.23</cell><cell>0.23</cell></row><row><cell>C</cell><cell>-12.00</cell><cell>0.00</cell><cell>-0.01</cell><cell>C</cell><cell>13.93</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell></cell><cell cols="2">(c) QTRAN: Qjt</cell><cell></cell><cell cols="4">(d) QTRAN: Q jt ? Qjt</cell></row><row><cell cols="4">Q1 Q2 -3.14(A) -2.29(B) -2.41(C)</cell><cell cols="4">Q1 Q2 -0.92(A) 0.00(B) 0.01(C)</cell></row><row><cell cols="2">-2.29(A) -5.42</cell><cell>-4.57</cell><cell>-4.70</cell><cell cols="2">-1.02(A) -8.08</cell><cell>-8.08</cell><cell>-8.08</cell></row><row><cell cols="2">-1.22(B) -4.35</cell><cell>-3.51</cell><cell>-3.63</cell><cell>0.11(B)</cell><cell>-8.08</cell><cell>0.01</cell><cell>0.03</cell></row><row><cell cols="2">-0.73(C) -3.87</cell><cell>-3.02</cell><cell>-3.14</cell><cell>0.10(C)</cell><cell>-8.08</cell><cell>0.01</cell><cell>0.02</cell></row><row><cell cols="4">(e) VDN: Q1, Q2, Qjt</cell><cell cols="4">(f) QMIX: Q1, Q2, Qjt</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The process of replacing [Qi] satisfying the condition (9b) with [Qi] satisfying the condition(7)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Weight constants for loss functions L opt , L nopt and L nopt?min</figDesc><table><row><cell></cell><cell>Value</cell><cell>Description</cell></row><row><cell>training step</cell><cell cols="2">20000 Maximum time steps until the end of training</cell></row><row><cell>learning rate</cell><cell cols="2">0.0005 Learning rate used by Adam optimizer</cell></row><row><cell cols="3">replay buffer size 20000 Maximum number of samples to store in memory</cell></row><row><cell>minibatch size</cell><cell>32</cell><cell>Number of samples to use for each update</cell></row><row><cell>? opt , ? nopt</cell><cell>1,1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Hyperparameters for matrix game training</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Weight constants for loss functions L opt , L nopt and L nopt?min</figDesc><table><row><cell></cell><cell>Value</cell><cell>Description</cell></row><row><cell>training step</cell><cell cols="2">1000000 Maximum time steps until the end of training</cell></row><row><cell>learning rate</cell><cell>0.0005</cell><cell>Learning rate used by Adam optimizer</cell></row><row><cell>replay buffer size</cell><cell>200000</cell><cell>Maximum number of samples to store in memory</cell></row><row><cell cols="2">final exploration step 500000</cell><cell>Number of steps over which is annealed to the final value</cell></row><row><cell>minibatch size</cell><cell>32</cell><cell>Number of samples to use for each update</cell></row><row><cell>? opt , ? nopt</cell><cell>1,1</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Hyperparameters for Multi-domain Gaussian Squeeze</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Hyperparameters for predator-prey training D. Additional results for matrix games</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Factorized qlearning for large-scale multi-agent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03738</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2137" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counterfactual multi-agent policy gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiagent planning with factored mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1523" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coordinated reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="227" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cooperative multi-agent control using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting structure and agent-centric rewards to promote coordination in large multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Holmesparker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tumer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Adaptive and Learning Agents Workshop</title>
		<meeting>Adaptive and Learning Agents Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning attentional communication for multi-agent cooperation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7265" to="7275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to schedule communication in multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hostallero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collaborative multiagent reinforcement learning by payoff propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1789" to="1828" />
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computing factored value functions for policies in structured mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1332" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pieter Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6379" to="6390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal and approximate q-value functions for decentralized pomdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Spaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="289" to="353" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A concise introduction to decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep decentralized multi-task multi-agent reinforcement learning under partial observability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monotonic value function factorisation for deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qmix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiagent systems: A survey from a machine learning perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="383" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Valuedecomposition networks for cooperative multi-agent learning based on team reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">F</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AA-MAS</title>
		<meeting>AA-MAS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2085" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiagent cooperation and competition with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tampuu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matiisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kodelja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kuzovkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Korjus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">172395</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Lenient learning in independent-learner stochastic cooperative games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2914" to="2955" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09817</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Multiagent soft q-learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mean field multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5567" to="5576" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
