<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Unifying the Label Space for Aspect-and Sentence-based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
							<email>yimingz@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Wu</surname></persName>
							<email>wusai@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>min_zhang@zju.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
							<email>j.zhao@zju.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename></persName>
							<affiliation key="aff3">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Unifying the Label Space for Aspect-and Sentence-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims to determine the sentiment polarity towards targeted aspect terms occurring in the sentence. The development of the ABSA task is very much hindered by the lack of annotated data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Aspect-based Sentiment Analysis</head><p>The aspect-based sentiment analysis (ABSA) task aims to recognize the sentiment polarities centered on the considered aspect terms occurring in the sentence. The establishment of the ABSA task echoes the long-standing literature of conventional sentence-level sentiment analysis (SA). For instance, as shown in <ref type="figure">Figure 1</ref>, a normal ABSA data annotation tags sentiment score on specific aspect terms in the sentence, like "surroundings" as positive and "food" as negative. Meanwhile, in the conventional sentence-based sentiment analysis, the whole sentence is labeled as negative at a coarser granularity. nega i e nega i e <ref type="figure">Figure 1</ref>: Sentiment Analysis (SA) and Aspect-based Sentiment Analysis (ABSA). The sample on the above is the ABSA task, while the sample on the bottom is the SA task. Both tasks aim at analyzing the sentiments carried by the objects in the box.</p><p>Due to its much finer granularity, the annotation cost is significantly higher than its conventional counterpart. Essentially, many of the existing SA datasets <ref type="bibr" target="#b9">(He et al., 2018)</ref> can be crawled and curated straightforwardly from the review websites such as Amazon 1 or Yelp 2 . The five-star rating system comes in handy to accomplish the annotation. Thus, the SA datasets are often presented at a large scale. By contrast, the ABSA annotation has no such "free lunch". It has to require human annotators to participate. Coupling with its higher complexity on labeling, the ABSA datasets are ubiquitously at considerably smaller scales <ref type="bibr" target="#b30">(Pontiki et al.;</ref><ref type="bibr" target="#b9">He et al., 2018;</ref><ref type="bibr" target="#b47">Yu et al., 2021b)</ref>. To this date, the available datasets for conventional sentiment analysis are generally larger to several orders of magnitude than the ABSA.</p><p>For instance, the commonly used ABSA benchmark SemEval 2014 task 4 has less than 5000 samples <ref type="bibr">(Pontiki et al.)</ref>, while there are 4,000,000 sentences in the Amazon Review dataset 3 for SA. Due to the similarity between the SA task and the ABSA task, it is natural to use SA datasets as auxiliary datasets for the ABSA task <ref type="bibr" target="#b9">(He et al., 2018)</ref>. Most, if not all, previous work has focused on pretraining and multi-task learning methods <ref type="bibr" target="#b9">(He et al., 2018,</ref>  x is the input data, a sentence in the SA dataset, while y is the sentiment carried by a sentence. t i indicates the position of an aspect term in a sentence, and y i is the label for that aspect term. t i and y i are pseudo labels generated by the ABSA model. As we can see, in the PL method, the sentence sentiment labels are dropped, and the SA dataset is regarded as an unlabeled dataset. 2019b). In this paper, we first take the Pseudo-Label method to utilize the SA datasets to solve the challenge faced by the ABSA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Pseudo-Label</head><p>The family of Pseudo-Label methods has had wide success in multiple fields <ref type="bibr" target="#b29">(Pham et al., 2020;</ref><ref type="bibr" target="#b7">Ge et al., 2020;</ref><ref type="bibr" target="#b23">Mallis et al., 2020;</ref><ref type="bibr" target="#b51">Zoph et al., 2020;</ref><ref type="bibr" target="#b8">He et al., 2019a)</ref>. The core of this family is to "trust" the generated fake labels by running the unlabeled samples through a teacher network that is trained by using the limited number of labeled samples. The generated labeled samples are then combined with the original set of supervised datasets and fed to the final model training.</p><p>In this article, our core mission is to incorporate the large-scale datasets into the sentiment analysis with the targeted ABSA task. While there have been works on this line, such as <ref type="bibr" target="#b9">He et al. (2018)</ref> and <ref type="bibr" target="#b10">He et al. (2019b)</ref>, exploring the Pseudo-Label methods has been very much untapped. Indeed, a very straightforward technological solution is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. One can apply the traditional Pseudo-Label method to generate a bunch of pseudo-aspect-based sentiment labels from the SA or even the unlabeled datasets. However, a consequence of this is the total abandonment and waste of the provided coarse-grained labels. While seemingly acceptable, we argue that due to the homogeneous root for the ABSA and SA tasks, the under-exploiting of the sentence-level coarsegrained sentiment labels is sub-optimal. It will be unnecessary if the traditional framework throws away the coarser-grained labels containing finer-grained task-relevant information. We argue that the Pseudo-Label family of approaches is limited to fit a uniform granularity situation. They ought to evolve and further adapt to the discrepancy of granularity in the label space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Dual-granularity Pseudo Labels</head><p>To solve the aforementioned problem, we propose the Dual-granularity Pseudo Labeling framework (DPL). In essence, the DPL augments the original PL framework and is capable of leveraging the labels drawn from both granularities. Briefly, the DPL relies on two teacher models obtained from datasets from both granularities, respectively, then generates pseudo labels for both sides. As a result, datasets from both granularity levels can be merged into a whole, with every sentence sample being tagged by both finer-and coarser-set of labels. To facilitate the employment of both sets of labels, we set a few standard conditions as the design principle of DPL. Slightly more concretely, DPL establishes two separate pathways leading to prediction for both granularities. Together, the two pathways interact in the representation space and ideally may possess controlled information flow that respectively corresponds and only correspond to the considered granularity. We incorporate an adversarial module to accomplish this functionality.</p><p>On the widely used benchmarks, SemEval 2014 task 4 subtask 2 <ref type="bibr">(Pontiki et al.)</ref>, the DPL method significantly surpasses the current state-of-the-art method. We deem our simple but effective framework DPL pioneering a bi-granularity level dataset merging. In what follows, we empirically validate that DPL is a framework that can be seamlessly combined with the previous pre-training or multitask learning methods in terms of ABSA and SA dataset merging.</p><p>To sum up, we make the following contributions in this paper:</p><p>1. Among those works to solve the lack of labeled data in the ABSA task, we pioneer to adopt and enhance a pseudo-label framework to leverage the coarser-grained SA labels.</p><p>2. We propose a novel general framework called Dual-granularity Pseudo Labels (DPL). Just like the vanilla PL method, the DPL is established as a general framework. We validate that DPL is also compatible with previous work on this line, such as pre-training or multitask learning (MTL). DPL has achieved excellent performances on the standardized ABSA benchmarks such as SemEval 2014, which significantly outperforms the prior works.</p><p>2 Related Works 2.1 Aspect-based Sentiment Analysis (ABSA)</p><p>ABSA is a finer-grained task of Sentiment Analysis (SA). It is a pipeline task, including aspect term extraction and aspect term sentiment classification. Aspect term sentiment classification is the true target task in this paper. For convenience, we use ABSA to refer to this task in the remaining parts. Like other application tracks in NLP, the family of neural network models has wide successes in this task <ref type="bibr" target="#b13">(Jiang et al., 2011;</ref><ref type="bibr" target="#b38">Vo and Zhang, 2015;</ref><ref type="bibr" target="#b49">Zhang et al., 2016;</ref><ref type="bibr" target="#b22">Ma et al., 2017;</ref><ref type="bibr" target="#b15">Li et al., 2018;</ref><ref type="bibr" target="#b40">Wang et al., 2018;</ref><ref type="bibr" target="#b12">Huang et al., 2018;</ref>. <ref type="bibr" target="#b41">Wang et al. (2016)</ref> introduce attention mechanism into an LSTM to model the inter-dependence between sentence and aspect term. <ref type="bibr" target="#b37">Tang et al. (2016)</ref> apply Memory Networks in this task.</p><p>Syntax-based models have also been explored widely in this domain <ref type="bibr" target="#b3">(Dong et al., 2014;</ref><ref type="bibr" target="#b36">Tai et al., 2015;</ref><ref type="bibr">Nguyen and Shirai, 2015;</ref><ref type="bibr" target="#b16">Li et al., 2021;</ref><ref type="bibr" target="#b28">Pang et al., 2021)</ref>. <ref type="bibr" target="#b35">Sun et al. (2019)</ref> and  introduced graph convolution networks (GCN) to leverage the structured information from the dependency tree. <ref type="bibr" target="#b11">Huang and Carley (2019)</ref> used graph attention networks (GAT) to improve the performance. <ref type="bibr" target="#b0">Bai et al. (2020)</ref> and <ref type="bibr" target="#b39">Wang et al. (2020)</ref> took the syntax relations as edge features and introduced them into the Relational Graph Attention Network (RGAT).</p><p>In addition, pretrained language models like BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> have greatly promoted the development of ABSA <ref type="bibr" target="#b15">(Li et al., 2018;</ref><ref type="bibr" target="#b6">Gao et al., 2019;</ref><ref type="bibr" target="#b31">Rietzler et al., 2019;</ref><ref type="bibr" target="#b45">Yang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using Extra Dataset for ABSA</head><p>Due to the dataset scale challenge of the ABSA task, there have been some methods exploring how to utilize the auxiliary dataset.</p><p>Some of them <ref type="bibr" target="#b31">Rietzler et al., 2019;</ref><ref type="bibr" target="#b47">Yu et al., 2021b)</ref> achieve decent ABSA performance by post-processing or fine-tuning BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref> with an additional unlabeled dataset. For these methods, we argue that the cost of computation is too high. Moreover, DPL does not conflict with it and can accommodate the results of these works. We take <ref type="bibr" target="#b31">Rietzler et al. (2019)</ref>'s work as an example for comparison in experiments.</p><p>The others <ref type="bibr" target="#b9">(He et al., 2018</ref><ref type="bibr" target="#b10">(He et al., , 2019b</ref><ref type="bibr" target="#b1">Chen and Qian, 2019;</ref><ref type="bibr" target="#b18">Liang et al., 2020;</ref><ref type="bibr" target="#b45">Yang et al., 2019;</ref><ref type="bibr" target="#b27">Oh et al., 2021;</ref><ref type="bibr" target="#b46">Yu et al., 2021a;</ref> utilize some labeled datasets and propose (later extend) a framework applying multitask learning methods. These auxiliary labeled datasets mainly include the sentiment analysis (SA) task and other subtasks of ABSA, such as Aspect Term Extraction, Opinion Term Extraction, and so on . DPL is more similar to these methods, using labeled datasets. However, we argue that the datasets of other subtasks can't solve the problem of the high annotation cost. Thus, DPL utilizes the SA task as auxiliary datasets and is the first to apply the PL method to this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pseudo-Label</head><p>Pseudo-label (PL), often associated with selftraining, is a semi-supervised learning method. PL has been utilized and further developed by many studies <ref type="bibr" target="#b7">(Ge et al., 2020;</ref><ref type="bibr" target="#b23">Mallis et al., 2020;</ref><ref type="bibr" target="#b51">Zoph et al., 2020;</ref><ref type="bibr" target="#b8">He et al., 2019a)</ref>. It has been successfully applied in many tasks, such as image classification <ref type="bibr" target="#b29">(Pham et al., 2020;</ref>, object detection <ref type="bibr" target="#b7">(Ge et al., 2020)</ref>, text classification (Mukherjee and Awadallah, 2020), Etc.</p><p>However, these PL methods are inapplicable under a non-uniform granularity situation; that is, there are massive available coarse-grained datasets for fine-grained tasks. These existing methods can only discard the coarse-grained labels and treat them as unlabeled datasets. Thus, we argue that these PL methods cause loss of information and are definitely unreasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pseudo-Labels</head><p>The traditional PL method generally involves a labeled set denoted by D and an unlabeled set D u . A teacher model is trained on D by cross-entropy loss:</p><formula xml:id="formula_0">L(? T ) = (x,y)?D [? log P ? T (y|x)]<label>(1)</label></formula><p>where ? T denotes the parameters of the teacher model. The cross-entropy loss function is adopted for general classification problems, including image classification, detection, and semantic segmentation <ref type="bibr" target="#b7">(Ge et al., 2020;</ref><ref type="bibr" target="#b29">Pham et al., 2020;</ref><ref type="bibr" target="#b51">Zoph et al., 2020)</ref>.</p><p>In what follows, on the unlabeled dataset D u , one can obtain the corresponding labels via running the unlabeled input through an inference procedure of the teacher model. The yielded label set for D u forms a pseudo-labeled dataset that can later be combined with the original dataset with gold annotations. A student model M S is trained by the newly merged dataset:</p><formula xml:id="formula_1">L(? S ) = (x,y)?D [? log P ? S (y|x)]+ ? (xu,y )?D u [? log P ? S (y |x u )]<label>(2)</label></formula><p>where y indicates the pseudo label corresponding to the sample x u generated by the teacher model. D u are the pseudo-label augmented version of D u . ? is a weighing term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dual-granularity Pseudo Labeling</head><p>In short, our work focuses on expanding the traditional PL method to utilize coarse-grained datasets. To achieve this goal, we draw inspiration from the multi-task learning community and augment the PL method with a different modeling pathway. Consequently, we obtain a framework where two separate pathways are trained synergistically targeted at labels of both granularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Our work is based on two datasets, the fine-grained and the coarse-grained datasets in the same domain.</p><p>Let us use D fine and D coarse to denote two datasets respectively. For the coarse-grained dataset D coarse , the task is to learn a mapping:</p><formula xml:id="formula_2">f coarse (x) ? y,<label>(3)</label></formula><p>For the fine-grained dataset D finer , the target mapping is:</p><formula xml:id="formula_3">f fine (x, t i ) ? y i , i ? {1, ..., m}<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">(x, y) ? D coarse and (x, t i , y i ) ? D fine .</formula><p>x is the input data, and y is the corresponding label for x. t i ? x. m means that x has m sub-parts totally, and y i is the corresponding label for t i . The traditional PL method is limited to fit a uniform granularity situation. The first step to resolve this limitation is to merge the coarse-grained dataset with the fine-grained dataset. Like the traditional PL method, we train a teacher model on one dataset and generate pseudo labels for the other dataset. We repeat this process at two granularities. Here, we suppose that x i for each x in the D coarse have been extracted. After pseudo labels generation, two new datasets are generated, donates as D fine and D coarse , and a new dataset D are merged by these two datasets. Specifically,</p><formula xml:id="formula_5">D = D fine ? D coarse ,<label>(5)</label></formula><p>where (x, t i , y, y i ) ? D coarse and (x, t i , y , y i ) ? D fine . y and y i are the generated pseudo labels.</p><p>Up to now, we get a new dataset with a much larger scale. Our goal translates into obtaining a model trained by the new dataset D with high performance on the fine-grained task. In other words, compared with the traditional PL method, the key problem is: how to utilize the coarse-grained labels to improve the model's performance on the fine-grained task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DPL Skeleton</head><p>As we mentioned, the core challenge for adapting the vanilla PL method is to utilize coarse-grained labels. As displayed in <ref type="figure" target="#fig_3">Figure 3</ref>, we set dual pathways corresponding to each granularity. Both pathways are finished by setting a proper softmaxbased classifier. Using z and h to denote the internal representation vectors for both pathways, we decompose the design philosophy of DPL by the following three conditions:   <ref type="figure">Framework. (x, t i )</ref> is the input data. t i indicates the aspect terms, which are painted by the dark green. We first generate (x, t i ) and (x, 1 ? t i ) as the input of the upper and lower pathways, respectively. In this case, t i = (0, 1, 1, 0, 0, 0) and 1 ? t i = (1, 0, 0, 1, 1, 1). "? enc " is an encoder that outputs z and h. "? + p " is a predictor for the fine-grained task, and "? * p " is a predictor for the coarse-grained task. Correspondingly, y i is the prediction for the fine-grained task, and y is the prediction for the coarse-grained task. "mutual-exclusive" means the information carried by z and h has little overlap.</p><p>? z carries adequate information to determine the label at the fine-grained level. More formally, there exists a function f ? + p in the overall functional space that is able to map the z to y i .</p><p>? The union set of h and z is capable of determining the label at the coarse level. There exists another function f ? * p in the overall functional space that is sufficient to map the [z ? h] to y.</p><p>? h and z are mutually exclusive in terms of the carried information. That means we cannot train a function f ? * p to map h to y i , due to the lack of information contributed from z.</p><p>The main rationale behind these three conditions may include but is not limited to: (i)-the information passing through the pathway with z is only required in the fine-grained task; (ii)-the other information needed by the coarse-grained task passes through the pathway with h; (iii)-the prediction at coarse-grained level is based on the concatenation of h and z, while either of them is insufficient to accomplish the prediction of coarse-grained labels.</p><p>In order to satisfy the model to these three conditions, our loss function consists of three terms. Among them, the two terms are the classification loss terms for the fine-and coarse-grained tasks, respectively, fulfilling conditions 1&amp;2. For condition 3, we draw inspiration from adversarial training <ref type="bibr" target="#b14">(Lample et al., 2017)</ref> to reduce the fine-grained task-relevant information carried by h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Fine-and Coarse-grained Tasks</head><p>As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, the model consists of an encoder, ? enc , together with two predictors, ? * p and ? + p . In particular, ? enc encodes each input data (x, t i ) into two intermediate results, z and h. In the figure, the top line with z is the pathway for the fine-grained task-relevant information flow, while the bottom line with h is the pathway for the fine-grained task-irrelevant information flow.</p><p>The fine-grained predictor ? + p spits out prediction based on z, with a cross-entropy loss:</p><formula xml:id="formula_6">L fine (? enc , ? + p ) = (x,t i ,y,y i )?D [? log P ? + p (y i |z)],<label>(6)</label></formula><p>Another crucial design in the DPL is that the concatenation of h and z, [h ? z], is fed to decide the prediction of the sequence-level prediction:</p><formula xml:id="formula_7">L coarse (? enc , ? * p ) = (x,t i ,y,y i )?D [? log P ? * p (y|h ? z)].<label>(7)</label></formula><p>The gradient of this loss will update the model parameters on both pathways. To prevent the degenerated case where the two pathways act completely separately, we introduce another crucial part to DPL in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Adversarial Training</head><p>The current version of DPL could still work as two separate systems, which is deemed a degenerated case. Therefore, to guarantee the mutual exclusiveness between the h and the z, we introduce an adversarial training loss term to maximally reduce the fine-grained task-relevant information carried by h:</p><formula xml:id="formula_8">L enc (? enc ) = (x,t i ,y,y i )?D [? log P ? + p (1 ? y i |h)],<label>(8)</label></formula><formula xml:id="formula_9">L dis (? + p ) = (x,t i ,y,y i )?D [? log P ? + p (y i |h)], (9) L adv (? enc , ? + p ) = L dis (? + p ) + ?L enc (? enc ),<label>(10)</label></formula><p>where ? weighs the trade-off between ? enc and ? + p . The adversarial training was first introduced in <ref type="bibr" target="#b14">Lample et al. (2017)</ref> and has been widely used <ref type="bibr" target="#b5">Fu et al., 2018;</ref><ref type="bibr" target="#b32">Shen et al., 2017;</ref><ref type="bibr" target="#b24">Melnyk et al., 2017)</ref>. The loss term trains ? enc to fool ? + p by removing fine-grained task relevant information from h. Considering that z is only required by the fine-grained task, the less fine-grained task-relevant information the h has, the less overlap there is between the h and z. As a result, the adversarial training makes h and z more mutually exclusive in terms of the carried information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Loss Function</head><p>The overall loss function to optimzie DPL combines as below:</p><formula xml:id="formula_10">L(? enc , ? * p , ? + p ) =L fine (? enc , ? + p ) +?L coarse (? enc , ? * p ) +?L adv (? enc , ? + p )<label>(11)</label></formula><p>where ? and ? are weighing terms. With this design of the loss functions, we posit that all three philosophies should be satisfied. The ideal result for it is that (i)-z only carries information dedicated at the fine-level; (ii)-h carries the information of the entire coarse level (i.e., the whole sequence) excluding the information of z; (iii)-neither h nor z is sufficient on deciding the whole-sequence coarselevel prediction, but with the concatenation of them, h ? z, the information is just adequate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Grounding</head><formula xml:id="formula_11">D = {(x 0 , y 0 ), (x 1 , y 1 ) . . . (x N , y N )} , where x i</formula><p>donates a sentence and y i donates the sentiment polarity of the sentence. The goal of the task is to learn a mapping function: f sent (x i ) ? y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Aspect-based Sentiment Analysis.</head><p>The ABSA task is to derive the sentiment polarity attached to specific aspect terms in the given sentence. Formally, one can draw a data point (x i , y i ) from the dataset D. We assign a separate variable indicating the aspect terms annotation, {t i,1 , . . . , t i,N i }, where N i denotes the number of total aspect terms in ? i . In addition, the label y is a combination of polarities corresponding to aspect terms, y i = {y i,1 , . . . , y i,N i }. The goal for the ABSA is to learn the mapping f aspect (</p><formula xml:id="formula_12">x i , t i,k ) ? y i,k , where k ? {1, . . . , N i }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Implementation</head><p>Before implementing a specific DPL model, we first map the task objectives of the SA and ABSA tasks to the coarse-and fine-grained tasks in the DPL framework. The coarse-grained task is the SA task, while the fine-grained task is the ABSA task. In another word, the mapping f sent (x i ) ? y i , is considered as the coarsegrained mapping f coarse (x) ? y, and the map-</p><formula xml:id="formula_13">ping f aspect (x i , t i,k ) ? y i,k is considered as f fine (x, t i ) ? y i .</formula><p>Then we choose the model for ? enc , ? + p and ? * p . ? + p and ? * p are simple multilayer perceptron (MLP). It is worth noting that ? enc can be a prior ABSA model. Thus, we argue that the DPL framework can be applied to most ABSA methods. Typically, we choose <ref type="bibr">Bai et al. (2020)'s and</ref><ref type="bibr" target="#b31">Rietzler et al. (2019)</ref>'s works and a multi-task learning baseline as examples to verify. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Dataset</head><p>The experiments of the DPL framework require at least two datasets at different granularities. For the ABSA task, we select the SemEval dataset <ref type="bibr">(Pontiki et al.)</ref> as the fine-grained sentiment task dataset and the Amazon reviews dataset from Kaggle 4 as the coarse-grained sentiment task dataset. The Se-mEval datasets are used as our core task dataset, and the Amazon reviews dataset is used as an auxiliary dataset.</p><p>Dataset SemEval. This dataset is SemEval 2014 task 4 subtask2 <ref type="bibr">(Pontiki et al.)</ref>. It has two sub-datasets, the reviews in the restaurant and laptop domains. We show more details in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Dataset Amazon Reviews. The dataset contains 3.6 million sentences in the training set and 0.4 million sentences in the test set. Considering the huge data volume gap, we only chose the test set as the auxiliary dataset for this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Generation of Pseudo Labels</head><p>Here we provide some details of the pseudo labels generation process.</p><p>As a result of the PL generation, the ABSA dataset has true aspect-level sentimental labels and pseudo-sentence-level sentimental labels, while the SA dataset has true sentence-level sentimental labels and pseudo-aspect-level sentimental labels.</p><p>To get aspect terms from the sentence in the SA dataset, we first performed aspect extraction using the model proposed by  and discarded sentences without aspect terms.</p><p>We train the model proposed by <ref type="bibr" target="#b0">(Bai et al., 2020)</ref> as the teacher models on the aspect-level dataset with the accuracy scores of 86.05% and 79.53% respectively on the domain of Restaurant and Laptop.</p><p>We train a BERT+Linear as the teacher model on the document-level dataset, with a 94.45% accuracy score in the restaurant Domain and a 93.35% accuracy score in the laptop domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Implementation Details</head><p>In addition to the above introduction, some more important details of our experiments need to be clarified for ease of understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluate Matrix</head><p>The model for ABSA is tested on SemEval's test set. Like those who have performed this work before, we use the model classification accuracy (ACC) and macro-F1 (F1) scores as the evaluation criterion.</p><p>Batch  <ref type="table">Table 2</ref>: Results of different methods. "BERT" represents the works that are also based on the BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>, "Auxiliary" represents the methods that also utilize auxiliary datasets to help the ABSA task. "*" means our replication results. The results show that our method achieves state-of-the-art in this benchmark.</p><p>Since the size of the current auxiliary dataset is much larger than the existing dataset. To avoid the large auxiliary dataset changing the original dataset distribution, we adopt two asynchronous loaders and define the step ratio k, i.e., whenever the model is trained on the original dataset by 1 step, it is trained on the auxiliary dataset by k steps. In general, we set k = 1.</p><p>Model Implementation The encoder has three main structures for the ABSA task: BERT <ref type="bibr" target="#b2">(Devlin et al., 2018)</ref>, Relational Graph Attention Networks (RGAT) <ref type="bibr" target="#b39">(Wang et al., 2020)</ref>, and masking embedding module. The BERT and RGAT have been proved to have a good effect on this task. The mask embedding module is used to generate z and h. It is similar to the implementation of "segment_id" in the code of BERT. <ref type="table">Table 2</ref> shows that the DPL has achieved a state-ofthe-art (SOTA) performance in terms of the average accuracy and F1-scores on the SemEval 2014 task 4 subtask 2 dataset. The group denoted as "Auxiliary Dataset is multi-task learning methods based on labeled datasets. Compared with them, our work shows the advantage of the PL method. "BERT-based" are some recently published works with good results. Obviously, our method achieves significant improvements over them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>It should be noted that our design is based on the BERT. Thus the comparison is not made with the methods based on a more powerful pre-trained model, such as Roberta , De-BERTa (Silva and Marcacini), and GPT-3 <ref type="bibr" target="#b4">(Floridi and Chiriatti, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restaurant Laptop</head><p>Acc F1 Acc F1</p><p>RGAT <ref type="formula">(</ref>   <ref type="table">Table 2</ref>. RGAT <ref type="bibr" target="#b0">(Bai et al., 2020)</ref>, Adapter <ref type="bibr" target="#b31">(Rietzler et al., 2019)</ref> are typical ABSA methods. MultiBERT is a multi-task baseline implemented by us. It predicts the SA label based on the "[cls]" and predicts the ABSA task based on the specific word vector. We add the DPL framework to them, denoted as "+DPL", and achieve significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DPL as a General Framework</head><p>As we mentioned, we promote DPL as a general framework capable of combining other methods on the ABSA task. <ref type="table" target="#tab_4">Table 3</ref> shows the performances of some typical methods before and after they combine the DPL framework. On the one hand, RGAT <ref type="bibr" target="#b0">(Bai et al., 2020)</ref> is a model architecture based on GAT and BERT. Thus the improvement shows that the DPL framework fits other architectural designs, even without auxiliary datasets. On the other hand, for those methods involving auxiliary datasets, we take Adapter <ref type="bibr" target="#b31">(Rietzler et al., 2019)</ref> and MultiB-ERT for demonstration. Previous works are mainly divided into two categories, pretraining and multitask learning. Adapter <ref type="bibr" target="#b31">(Rietzler et al., 2019)</ref> can be categorized into the pretraining class while Multi-BERT is a multi-task learning baseline inspired by <ref type="bibr" target="#b9">He et al. (2018)</ref>. Since the previous works using the multi-task method to combine the SA and the ABSA datasets were LSTM based, we implemented a better model based on the BERT. All the improvements verify that the DPL framework does not conflict with these methods and exhibits full compatibility for further performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We set up several sets of ablation experiments and present the results in <ref type="table">Table 4</ref> to explore the role of adversarial training and pseudo labels in the DPL framework.</p><p>The above experiments contain two types of BERT on the SemEval Restaurant dataset. To ensure the fairness of the ablation experiments, we  <ref type="table">Table 4</ref>: Results of ablation study. "Restaurant" takes plain BERT as the initial model while "Restaurant+Pre" takes <ref type="bibr" target="#b31">Rietzler et al. (2019)</ref>'s BERT as the initial model. "DPL" denotes our method. "Traditional Pseudo-Label" represents we take the PL method for fine-grained tasks dropped out the coarse-grained labels. The last three cases named in the form of "-X" means that we deleted the "X" from the original DPL to evaluate the effect of "X".</p><p>use the same parameters when training the same group, and the parameter configurations are shown in Appendix. The comparison with "Traditional Pseudo-Label" shows the advantages of our method. From the item "-adversarial training", the significant decline on F1 reflects that adversarial training plays an important role in the DPL framework. The items, "-coarsegrained pseudo labels" and "-fine-grained pseudo labels", show that only adding adversarial training at one granularity has less effect than adding it both ways.</p><p>Furthermore, we also take Chamfer Distance (CD) between the set of h and the set of z to provide an insight into the effect of the mutual exclusiveness. And the CD of the model with the adversarial training process is 30% larger than that of the model without this process. That means the adversarial training process increases the distance between the variable h and z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose Dual-granularity Pseudo Labeling (DPL). DPL extends from the vanilla Pseudo-Label method and augments it to a dualpathway system. It additionally enforces strong control of information flow directing to the data at different granularities of annotation. The results demonstrate the state-of-the-art performance of DPL on the data-scarce ABSA task. As a pioneering framework design, we also show that the DPL is compatible with pre-training and multi-task learning methods as published before. In the future, we hope to explore the possibility of DPL in other domains, such as computer vision problems where the discrepancy of granularities possesses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, b he food a oo e ible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Dataset Generation in the Pseudo-Label (PL) Method. This figure shows a pipeline of the traditional Pseudo-Label method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The Model for the DPL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of SemEval 2014 task 4 subtask 2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b1">Chen and Qian (2019)</ref> 79.55 71.41 73.87 70.10 He et al. (2019b) 83.89 75.66 75.36 72.02 Liang et al. (2020) 84.93 76.66 77.51 73.42</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="2">Restaurant</cell><cell>Laptop</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell></cell><cell>He et al. (2018)</cell><cell cols="3">78.73 68.63 71.91 68.79</cell></row><row><cell>Auxiliary</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Bai et al. (2020)*</cell><cell cols="3">86.04 80.27 79.53 74.54</cell></row><row><cell>BERT</cell><cell>Pang et al. (2021) Li et al. (2021)</cell><cell cols="3">87.66 82.97 80.22 77.28 87.13 81.16 81.80 78.10</cell></row><row><cell></cell><cell>Rietzler et al. (2019)</cell><cell cols="3">87.89 81.05 80.23 75.77</cell></row><row><cell>Ours</cell><cell>DPL</cell><cell cols="3">89.54 84.86 81.96 78.58</cell></row><row><cell>Loader</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4 www.kaggle.com/bittlingmayer/</cell><cell></cell><cell></cell><cell></cell></row><row><cell>amazonreviews</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b0">Bai et al., 2020)</ref> 86.04 80.27 79.53 74.54 RGAT+DPL 87.22 81.47 81.01 77.52 Improvement +1.18 +1.20 +1.48 +2.98 Adapter(Rietzler et al., 2019) 87.89 81.05 80.23 75.77 Adapter+DPL 89.54 84.86 81.96 78.58</figDesc><table><row><cell>Improvement</cell><cell>+1.65 +3.71 +1.73 +2.81</cell></row><row><cell>MultiBERT</cell><cell>84.54 78.52 78.32 73.87</cell></row><row><cell>MultiBERT+DPL</cell><cell>85.52 79.61 79.75 75.80</cell></row><row><cell>Improvement</cell><cell>+0.98 +1.09 +1.43 +1.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of Combining DPL with Other Methods. Restaurant and Laptop are two benchmarks same as those in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigating typed syntactic dependencies for targeted sentiment classification using graph attention neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transfer capsule network for aspect level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd annual meeting of the association for computational linguistics</title>
		<meeting>the 52nd annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Short papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Floridi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Chiriatti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="681" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Style transfer in text: Exploration and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Target-dependent sentiment classification with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengjie</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="154290" to="154299" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01526</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Revisiting self-training for neural sequence generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13788</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exploiting document knowledge for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04346</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An interactive multi-task learning network for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06906</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Syntax-aware aspect level sentiment classification with graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02606</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with attention-over-attention neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanglan</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction and Behavior Representation in Modeling and Simulation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00409</idno>
		<title level="m">Fader networks: Manipulating images by sliding attributes</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical attention based position-aware network for aspect-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lishuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqiao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd conference on computational natural language learning</title>
		<meeting>the 22nd conference on computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6319" to="6329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting BERT for end-to-end aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</title>
		<meeting>the 5th Workshop on Noisy User-generated Text (W-NUT 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An iterative knowledge transfer network with routing for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01935</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jointly modeling aspect and sentiment with dynamic heterogeneous graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06427</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00893</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised learning of object landmarks via self-training correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Mallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Melnyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahini</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09395</idno>
		<title level="m">Improved neural text attribute transfer with nonparallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-training for few-shot text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep context-and relation-aware learning for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinhyeok</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyub</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesun</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilnam</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaeun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunggyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harksoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03806</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic and multi-channel graph convolutional networks for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguan</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2627" to="2636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<title level="m">Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar. Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adapt or get left behind: Domain adaptation through bert language model finetuning for aspect-target sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rietzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Engl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09655</idno>
		<title level="m">Style transfer from nonparallel text by cross-alignment</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Aspectbased sentiment analysis using bert with disentangled attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcacini</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attentional encoder network for targeted sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09314</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment analysis via convolution over dependency tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mensah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5683" to="5692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<title level="m">Improved semantic representations from tree-structured long short-term memory networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08900</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-fourth international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhou</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Target-sensitive memory networks for aspect sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahisnu</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="957" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference on empirical methods in natural language processing</title>
		<meeting>the 2016 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bert post-training for review reading comprehension and aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A unified generative framework for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04300</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A multi-task learning model for chinese-oriented aspect polarity classification and aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruyang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">419</biblScope>
			<biblScope unit="page" from="344" to="356" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Making flexible use of subtasks: A multiplex interaction network for unified aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2695" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-domain review generation for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4767" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Aspect-based sentiment classification with aspectspecific graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03477</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gated neural networks for targeted sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarially regularized autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5902" to="5911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06882</idno>
		<title level="m">Rethinking pre-training and self-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
