<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anticipative Video Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anticipative Video Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Anticipative Video Transformer (AVT), an end-to-end attention-based video modeling architecture that attends to the previously observed video in order to anticipate future actions. We train the model jointly to predict the next action in a video sequence, while also learning frame feature encoders that are predictive of successive future frames' features. Compared to existing temporal aggregation strategies, AVT has the advantage of both maintaining the sequential progression of observed actions while still capturing long-range dependencies-both critical for the anticipation task. Through extensive experiments, we show that AVT obtains the best reported performance on four popular action anticipation benchmarks: EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads; and it wins first place in the EpicKitchens-100 CVPR'21 challenge. 3 EpicKitchens-55/100 datasets are licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. 4 competitions.codalab.org/competitions/25925 5 competitions.codalab.org/competitions/20071</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Predicting future human actions is an important task for AI systems. Consider an autonomous vehicle at a stop sign that needs to predict whether a pedestrian will cross the street or not. Making this determination requires modeling complex visual signals-the past actions of the pedestrian, such as speed and direction of walking, or usage of devices that may hinder his awareness of the surroundings-and using those to predict what he may do next. Similarly, imagine an augmented reality (AR) device that observes a user's activity from a wearable camera, e.g. as they cook a new dish or assemble a piece of furniture, and needs to anticipate his next steps to provide timely assistance. In many such applications, it is insufficient to recognize what is happening in the video. Rather, the vision system must also anticipate the likely actions that are to follow. Hence, there is a growing interest in formalizing the activity anticipation task <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b83">82]</ref> along with development of multiple challenge benchmarks to support it <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b83">82]</ref>.</p><p>Compared to traditional action recognition, anticipation tends to be significantly more challenging. First of all, it re-  <ref type="figure">Figure 1</ref>: Anticipating future actions using AVT involves encoding video frames with a spatial-attention backbone, followed by a temporal-attention head that attends only to frames before the current one to predict future actions. In this example, it spontaneously learns to attend to hands and objects without being supervised to do so. Moreover, it attends to frames most relevant to predict the next action. For example, to predict 'wash tomato' it attends equally to all previous frames as they determine if any more tomatoes need to be washed, whereas for 'turn-off tap' it focuses most on the current frame for cues whether the person might be done. Please see ? 5.3 for details and additional results.</p><p>quires going beyond classifying current spatiotemporal visual patterns into a single action category-a task nicely suited to today's well-honed discriminative models-to instead predict the multi-modal distribution of future activities. Moreover, while action recognition can often side-step temporal reasoning by leveraging instantaneous contextual cues <ref type="bibr" target="#b31">[31]</ref>, anticipation inherently requires modeling the progression of past actions to predict the future. For instance, the presence of a plate of food with a fork may be sufficient to indicate the action of eating, whereas anticipating that same action would require recognizing and reasoning over the sequence of actions that precede it, such as chopping, cooking, serving, etc. Indeed, recent work <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b78">77]</ref> finds that modeling long temporal context is often critical for anticipation, unlike action recognition where frame-level modeling is often enough <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b82">81]</ref>. These challenges are also borne out in practice. For example, accuracy for one of today's top performing video models <ref type="bibr" target="#b78">[77]</ref> drops from 42% to 17% when treating recognition versus anticipation on the same test clips <ref type="bibr" target="#b13">[13]</ref>-predicting even one second into the future is much harder than declaring the current action.</p><p>The typical approach to solving long-term predictive reasoning tasks involves extracting frame or clip level features using standard architectures <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b87">86,</ref><ref type="bibr" target="#b92">91]</ref>, followed by aggregation using clustering <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b62">62]</ref>, recurrence <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b42">42]</ref>, or attention <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b78">77,</ref><ref type="bibr" target="#b96">95]</ref> based models. Except the recurrent ones, most such models merely aggregate features over the temporal extent, with little regard to modeling the sequential temporal evolution of the video over frames. While recurrent models like LSTMs have been explored for anticipation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b97">96]</ref>, they are known to struggle with modeling long-range temporal dependencies due to their sequential (non-parallel) nature. Recent work mitigates this limitation using attention-based aggregation over different amounts of the context to produce short-term ('recent') and longterm ('spanning') features <ref type="bibr" target="#b78">[77]</ref>. However, it still reduces the video to multiple aggregate representations and loses its sequential nature. Moreover, it relies on careful and datasetspecific tuning of the architecture and the amounts of context used for the different aggregate features.</p><p>In this work, we introduce Anticipative Video Transformer (AVT), an alternate video modeling architecture that replaces "aggregation" based temporal modeling with a anticipative 1 architecture. Aiming to overcome the tradeoffs described above, the proposed model naturally embraces the sequential nature of videos, while minimizing the limitations that arise with recurrent architectures. Similar to recurrent models, AVT can be rolled out indefinitely to predict further into the future (i.e. generate future predictions), yet it does so while processing the input in parallel with long-range attention, which is often lost in recurrent architectures.</p><p>Specifically, AVT leverages the popular transformer architecture <ref type="bibr" target="#b90">[89,</ref><ref type="bibr" target="#b93">92]</ref> with causal 2 masked attention, where each input frame is allowed to attend only to frames that precede it. We train the model to jointly predict the next action while also learning to predict future features that match the true future features and (when available) their intermediate action labels. <ref type="figure">Figure 1</ref> shows examples of how AVT's spatial and temporal attention spreads over previously observed frames for two of its future predictions (wash tomato and turn-off tap). By incorporating intermediate future prediction losses, AVT encourages a predictive video representation that picks up patterns in how the visual activity is likely to unfold into the future. This facet of our model draws an analogy to language, where trans-formers trained with massive text corpora are now powerful tools to anticipate sequences of words (cf. GPT and variants <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b70">70]</ref>). The incremental temporal modeling aspect has been also been explored for action recognition <ref type="bibr" target="#b53">[53]</ref>, albeit with convolutional architectures and without intermediate self-supervised losses.</p><p>While the architecture described so far can be applied on top of various frame or clip encoders (as we will show in experiments), we further propose a purely attention-based video modeling architecture by replacing the backbone with an attention-based frame encoder from the recently introduced Vision Transformer <ref type="bibr" target="#b18">[18]</ref>. This enables AVT to attend not only to specific frames, but also to spatial features within the frames in one unified framework. As we see in <ref type="figure">Figure 1</ref>, when trained on egocentric video, the model spontaneously learns to attend to spatial features corresponding to hands and objects, which tend to be especially important in anticipating future activities <ref type="bibr" target="#b57">[57]</ref>.</p><p>In summary, our contributions are: 1) AVT, a novel end-to-end purely attention based architecture for predictive video modeling; 2) Incorporation of a self-supervised future prediction loss, making the architecture especially applicable to predictive tasks like action anticipation; 3) Extensive analysis and ablations of the model showing its versatility with different backbone architectures, pre-trainings, etc. on the most popular action anticipation benchmarks, both from first and third person viewpoints. Specifically, we outperform all published prior work on EpicKitchens-55 <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b13">[13]</ref>, EpicKitchens-100 <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b14">[14]</ref>, EGTEA Gaze+ <ref type="bibr" target="#b55">[55]</ref>, and 50-Salads <ref type="bibr" target="#b83">[82]</ref>. Most notably, our method outperforms all submissions to the EpicKitchens-100 CVPR'21 challenge 4 , and is ranked #1 on the EpicKitchens-55 leaderboard 5 for seen (S1) and #2 on unseen (S2) test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action anticipation is the task of predicting future actions given a video clip. While well explored in thirdperson video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b83">82,</ref><ref type="bibr" target="#b91">90]</ref>, it has recently gained in popularity for first-person (egocentric) videos <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b78">77]</ref>, due to its applicability on wearable computing platforms. Various approaches have been proposed for this task, such as learning representations by predicting future features <ref type="bibr" target="#b91">[90,</ref><ref type="bibr" target="#b97">96]</ref>, aggregating past features <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b78">77]</ref>, or leveraging affordances and hand motion <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b64">64]</ref>. Our work contributes a new video architecture for anticipation, and we demonstrate its promising advantages on multiple popular anticipation benchmarks. Self-supervised feature learning from video methods learn representations from unlabeled video, often to be fine-tuned for particular downstream tasks. Researchers explore a variety of "free" supervisory signals, such as temporal consistency <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b95">94,</ref><ref type="bibr" target="#b100">99]</ref>, inter-frame predictability <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b84">83]</ref>, and cross-modal correspondence <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b85">84]</ref>. AVT incorporates losses that encourage features predictive of future features (and actions); while this aspect shares motivation with prior <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b79">78,</ref><ref type="bibr" target="#b84">83,</ref><ref type="bibr" target="#b85">84,</ref><ref type="bibr" target="#b91">90]</ref> and concurrent work <ref type="bibr" target="#b97">[96]</ref>, our architecture to achieve predictive features is distinct (transformer based rather than convolutional/recurrent <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b79">78,</ref><ref type="bibr" target="#b97">96]</ref>), it operates over raw frames or continuous video features as opposed to clustered 'visual words' <ref type="bibr" target="#b85">[84]</ref>, assumes only visual data (rather than vision with speech or text <ref type="bibr" target="#b84">[83,</ref><ref type="bibr" target="#b85">84]</ref>), and is jointly trained for action anticipation (rather than pre-trained and then fine-tuned for action recognition <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b84">83]</ref>).</p><p>Language modeling (LM) has been revolutionized with the introduction of self-attention architectures <ref type="bibr" target="#b90">[89]</ref>. LM approaches can generally be classified in three categories: (1) encoder-only <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b67">67]</ref>, which leverage bidirectional attention and are effective for discriminative tasks such as classification; (2) decoder-only <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b69">69]</ref>, which leverage a causal attention <ref type="bibr" target="#b51">[51]</ref> attending on past tokens, and are effective for generative tasks such as text generation; and (3) encoderdecoder <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b71">71]</ref>, which incorporate both a bidirectional encoder and causal decoder, and are effective for tasks such as machine translation. Capitalizing on the analogy between action prediction and generative language tasks, we explore causal decoder-only attention architectures in our model. While language models are typically trained on discrete inputs (words), AVT trains with continuous video features. This distinction naturally influences our design choices, such as an L 2 loss for generative training as opposed to a cross entropy loss for the next word.</p><p>Self-attention and transformers in vision. The general idea of self-attention in vision dates back to non-local means <ref type="bibr" target="#b9">[9]</ref>, and is incorporated into contemporary network architectures as non-local blocks <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b94">93,</ref><ref type="bibr" target="#b96">95]</ref> and gating mechanisms <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b98">97]</ref>. While self-attention approaches like transformers <ref type="bibr" target="#b90">[89,</ref><ref type="bibr" target="#b93">92]</ref> offer strong results for high-level vision reasoning tasks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b102">101]</ref>, more recently, there is growing interest in completely replacing convolutional architectures with transformers for image recognition <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b86">85]</ref>. For video, prior work has mostly leveraged attention architectures <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b94">93,</ref><ref type="bibr" target="#b96">95]</ref> on top of standard spatiotemporal convolutional base architectures <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b87">86,</ref><ref type="bibr" target="#b89">88]</ref>. In contrast, AVT is an end-to-end transformer architecture for video-to our knowledge the first (concurrent with <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b65">65]</ref>). Unlike the concurrent methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b65">65]</ref>, which are bidirectional and address traditional action recognition, AVT has a causal structure and tackles predictive tasks (anticipation). AVT yields the best results to date for several well-studied anticipation benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Segment</head><p>Observed video Unobserved video <ref type="figure">Figure 2</ref>: Action anticipation problem setup. The goal is to use the observed video segment of length ?o to anticipate the future action ?a seconds before it happens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Anticipation Problem Setup</head><p>While multiple anticipation problem setups have been explored in the literature <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b74">73]</ref>, in this work we follow the setup defined in recent challenge benchmarks <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> and illustrated in <ref type="figure">Figure 2</ref>. For each action segment labeled in the dataset starting at time ? s , the goal is to recognize it using a ? o length video segment ? a units before it, i.e. from ? s ? (? a + ? o ) to ? s ? ? a . While methods are typically allowed to use any length of observed segments (? o ), the anticipation time (? a ) is usually fixed for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Anticipative Video Transformer</head><p>We now present the AVT model architecture, as illustrated in <ref type="figure">Figure 3</ref>. It is designed to predict future actions given a video clip as input. To that end, it leverages a twostage architecture, consisting of a backbone network that operates on individual frames or short clips, followed by a head architecture that operates on the frame/clip level features to predict future features and actions. AVT employs causal attention modeling-predicting the future actions based only on the frames observed so far-and is trained using objectives inspired from self-supervised learning. We now describe each model component in detail, followed by the training and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Backbone Network</head><p>Given a video clip with T frames, V = {X 1 , ? ? ? , X T } the backbone network, B, extracts a feature representation for each frame,</p><formula xml:id="formula_0">{z 1 , ? ? ? , z T } where z t = B(X t ).</formula><p>While various video base architectures have been proposed <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b88">87,</ref><ref type="bibr" target="#b92">91]</ref> and can be used with AVT as we demonstrate later, in this work we propose an alternate architecture for video understanding based purely on attention. This backbone, which we refer to as AVT-b, adopts the recently proposed Vision Transformer (ViT) <ref type="bibr" target="#b18">[18]</ref> architecture, which has shown impressive results for static image classification.</p><p>Specifically, we adopt the ViT-B/16 architecture. We split each input frame into 16?16 non-overlapping patches. We flatten each patch into a 256D vector, and linearly project them to 768D, which is the feature dimension used throughout the encoder. While we do not need to classify each frame individually, we still prepend a learnable [class] token embedding to the patch features, whose   <ref type="figure">Figure 3</ref>: (Left) AVT architecture. We split the T input frames into non-overlapping patches that are linearly projected. We add a learned [CLASS] token, along with spatial position embeddings, and the resulting features are passed through multiple layers of multi-head attention, with shared weights across the transformers applied to all frames. We take the resulting features corresponding to the [CLASS] token, append a temporal position encoding and pass it through the Causal Transformer Decoder that predicts the future feature at frame t, after attending to all features from 1 ? ? ? t. The resulting feature is trained to regress to the true future feature (L f eat ) and predict the action at that time point if labeled (L cls ), and the last prediction is trained to predict the future action (Lnext). (Right) Causal Transformer Decoder. It follows the Transformer architecture with pre-norm <ref type="bibr" target="#b93">[92]</ref>, causal masking in attention, and a final LayerNorm <ref type="bibr" target="#b70">[70]</ref>.</p><formula xml:id="formula_1">? !"# ? !"# ? !"# ? $%&amp;' ? (%)' ? (%)' ? (%)' * + ! ! " # $ " " " # " ! " $ LayerNorm</formula><p>output will be used as a frame-level embedding input to the head. Finally, we add learned position embeddings to each patch feature similar to <ref type="bibr" target="#b18">[18]</ref>. We choose to stick to frame-specific spatial position encodings, so that the same backbone model with shared weights can be applied to each frame. We will incorporate the temporal position information in the head architecture (discussed next). The resulting patch embeddings are passed through a standard Transformer Encoder <ref type="bibr" target="#b90">[89]</ref> with pre-norm <ref type="bibr" target="#b93">[92]</ref>. We refer the reader to <ref type="bibr" target="#b18">[18]</ref> for details of the encoder architecture.</p><p>AVT-b is an attractive backbone design because it makes our architecture purely attentional. Nonetheless, in addition to AVT-b, AVT is compatible with other video backbones, including those based on 2D CNNs <ref type="bibr" target="#b81">[80,</ref><ref type="bibr" target="#b92">91]</ref>, 3D CNNs <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b88">87]</ref>, or fixed feature representations based on detected objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> or visual attributes <ref type="bibr" target="#b63">[63]</ref>. In ? 5 we provide experiments testing several such alternatives. For the case of spatiotemporal backbones, which operate on clips as opposed to frames, we extract features as z t = B(X t?L , ? ? ? , X t ), where the model is trained on L-length clips. This ensures the features at frame t do not incorporate any information from the future, which is not allowed in the anticipation problem setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Head Network</head><p>Given the features extracted by the backbone, the head network, referred to as AVT-h, is used to predict the future features for each input frame using a Causal Transformer Decoder, D:?</p><formula xml:id="formula_2">1 , ? ? ? ,? T = D(z 1 , ? ? ? , z T ).<label>(1)</label></formula><p>Here? t is the predicted future feature corresponding to frame feature z t , after attending to all features before and including it. The predicted features are then decoded into a distribution over the semantic action classes using a linear classifier ?, i.e.? t = ?(? t ). The final prediction,? T , is used as the model's output for the next-action anticipation task. Note that since the next action segment (T + 1) is ? a seconds from the last observed frame (T ) as per the problem setup, we typically sample frames at a stride of ? a so that the model learns to predict future features/actions at that frame rate. However, empirically we find the model is robust to other frame rate values as well.</p><p>We implement D using a masked transformer decoder inspired from popular approaches in generative language modeling, such as GPT-2 <ref type="bibr" target="#b70">[70]</ref>. We start by adding a temporal position encoding to the frame features implemented as a learned embedding of the absolute frame position within the clip. The embedded features are then passed through multiple decoder layers, each consisting of masked multihead attention, LayerNorm (LN) and a multi-layer perceptron (MLP), as shown in <ref type="figure">Figure 3</ref> (right). The final output is then passed through another LN, akin to GPT-2 <ref type="bibr" target="#b70">[70]</ref>, to obtain the future frame embeddings.</p><p>Aside from being visual rather than textual, this model differs from the original Transformer Decoder <ref type="bibr" target="#b90">[89]</ref> in terms of the final LN and the masking operation in the multi-head attention. The masking ensures that the model only attends to specific parts of the input, which in the case of predictive tasks like ours, is defined as a 'causal' mask. That is, for the output corresponding to the future after frame t, i.e.? t , we set the mask to only attend to z 1 ? ? ? z t . We refer the reader to <ref type="bibr" target="#b70">[70]</ref> for details on the masking implementation.</p><p>This design differs considerably from previous applications of language modeling architectures to video, such as VideoBERT <ref type="bibr" target="#b85">[84]</ref>. It operates directly on continuous clip embeddings instead of first clustering them into tokens, and it leverages causal attention to allow for anticipative training (discussed next), instead of needing masked language modeling (MLM) as in BERT <ref type="bibr" target="#b17">[17]</ref>. These properties make AVT suited for predictive video tasks while allowing for the long-range reasoning that is often lost in recurrent architectures. While follow-ups to VideoBERT such as CBT <ref type="bibr" target="#b84">[83]</ref> operate on raw clip features, they still leverage a MLM objective with bidirectional attention, with the primary goal of representation learning as opposed to future prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training AVT</head><p>To sample training data, for each labeled action segment in a given dataset, we sample a clip preceding it and ending ? a seconds before the start of the action. We pass the clip through AVT to obtain future predictions, and then supervise the network using three losses.</p><p>First, we supervise the next-action prediction using a cross-entropy loss with the labeled future action, c T +1 :</p><formula xml:id="formula_3">L next = ? log? T [c T +1 ].<label>(2)</label></formula><p>Second, to leverage the causal structure of the model, we supervise the model's intermediate future predictions at the feature level and the action class level. For the former, we predict future features to match the true future features that are present in the clip, i.e.</p><formula xml:id="formula_4">L f eat = T ?1 t=1 ||? t ? z t+1 || 2 2 .<label>(3)</label></formula><p>This loss is inspired from the seminal work by Vondrick et al. <ref type="bibr" target="#b91">[90]</ref> as well as follow ups <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref> that show that anticipating future visual representations is an effective form of self-supervision, though typically for traditional action recognition tasks. Concurrent and recent work adopts similar objectives for anticipation tasks, but with recurrent architectures <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b79">78,</ref><ref type="bibr" target="#b97">96]</ref>. Whereas recent methods <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b97">96]</ref> explore this loss with NCE-style <ref type="bibr" target="#b66">[66]</ref> objectives, in initial experiments we found simple L 2 loss to be equally effective. Since our models are always trained with the final supervised loss, we do not suffer from potential collapse during training that would necessitate the use of contrastive losses. Third, as an action class level anticipative loss, we leverage any action labels available in the dataset to supervise the intermediate predictions, i.e., when the input clip overlaps with any labeled action segments that precede the segment to be anticipated. <ref type="bibr" target="#b5">6</ref> Setting c t = ?1 for any earlier frames for which we do not have labels, we incur the following loss:</p><formula xml:id="formula_5">L cls = T ?1 t=1 L t cls ; L t cls = ? log? t [c t+1 ] if c t+1 ? 0 0 otherwise.<label>(4)</label></formula><p>We train our model with</p><formula xml:id="formula_6">L = L next + L cls + L f eat<label>(5)</label></formula><p>as the objective, and refer to it as the anticipative [a] training setting. As a baseline, we also experiment with a model trained solely with L = L next , and refer to it as the naive [n] setting, as it does not leverage our model's causal attention structure, instead supervising only the final prediction which attends to the full input. As we will show in <ref type="table" target="#tab_9">Table 7</ref>, the anticipative setting leads to significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>We preprocess the input video clips by randomly scaling the height between 248 and 280px, and take 224px crops at training time. We sample 10 frames at 1FPS for most experiments. We adopt network architecture details from <ref type="bibr" target="#b18">[18]</ref> for the AVT-b backbone. Specifically, we use a 12-head, 12-layer transformer encoder model that operates on 768D representations. We initialize the weights from a model pretrained on ImageNet-1K (IN1k), ImageNet-21K (IN21k) or ImageNet-1K finetuned from ImageNet-21K (IN21+1k), and finetune end-to-end for the anticipation tasks. For AVTh, we use a 4-head, 6-layer model that operates on a 2048D representation, initialized from scratch. We employ a linear layer between the backbone and head to project the features to match the feature dimensions used in the head. We train AVT end-to-end with SGD+momentum using 10 ?6 weight decay and 10 ?4 learning rate for 50 epochs, with a 20 epoch warmup <ref type="bibr" target="#b33">[33]</ref>   <ref type="table">Table 1</ref>: Datasets used for evaluation. We use four popular benchmarks, spanning first and third person videos. Class-mean ('cm') =? evaluation is done per-class and averaged over classes. Recall refers to class-mean recall@5 from <ref type="bibr" target="#b22">[22]</ref>. For all, higher is better.</p><p>average the predictions over the corresponding three clips. The default backbone for AVT is AVT-b, based on the ViT-B/16 architecture. However, to enrich our comparisons with some baselines <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b78">77]</ref>, below also we report performance of only our head model operating on fixed features from 1) a frame-level TSN <ref type="bibr" target="#b92">[91]</ref> backbone pre-trained for action classification, or 2) a recent spatiotemporal convolutional architecture irCSN-152 <ref type="bibr" target="#b88">[87]</ref> pre-trained on a large weakly labeled video dataset <ref type="bibr" target="#b27">[27]</ref>, which has shown strong results when finetuned for action recognition. We finetune that model for action classification on the anticipation dataset and extract features that are used by the head for anticipation. In these cases, we only train the AVT-h layers. For all datasets considered, we use the validation set or split 1 to further optimize the hyperparameters, and use that setup over multiple splits or the held out test sets. Code and models will be released for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We empirically evaluate AVT on four popular action anticipation benchmarks covering both first-and third-person videos. We start by describing the datasets and evaluation protocols ( ? 5.1), followed by key results and comparisons to the state of the art ( ? 5.2), and finally ablations and qualitative results ( ? 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Datasets and metrics. We test on four popular action anticipation datasets summarized in <ref type="table">Table 1</ref>. EpicKitchens-100 (EK100) <ref type="bibr" target="#b14">[14]</ref> is the largest egocentric (first-person) video dataset with 700 long unscripted videos of cooking activities totalling 100 hours. EpicKitchens-55 (EK55) <ref type="bibr" target="#b13">[13]</ref> is an earlier version of the same, and allows for comparisons to a larger set of baselines which have not yet been reported on EK100. For both, we use the standard train, val, and test splits from <ref type="bibr" target="#b14">[14]</ref> and <ref type="bibr" target="#b23">[23]</ref> respectively to report performance. The test evaluation is performed on a held-out set through a submission to their challenge server. EGTEA Gaze+ <ref type="bibr" target="#b55">[55]</ref> is another popular egocentric action anticipation dataset. Following recent work <ref type="bibr" target="#b57">[57]</ref>, we report performance on the split 1 <ref type="bibr" target="#b55">[55]</ref> of the dataset at ? a = 0.5s. Finally, 50-Salads (50S) <ref type="bibr" target="#b83">[82]</ref> is a popular third-person anticipation dataset, and  <ref type="table">Table 2</ref>: EK100 (val) using RGB and detected objects (OBJ) modalities separately. AVT outperforms prior work using the exact same features, and further improves with our AVT-b backbone. Performance reported using class-mean recall@5.</p><p>we report top-1 accuracy averaged over the pre-defined 5 splits following prior work <ref type="bibr" target="#b78">[77]</ref>. Some of these datasets employ top-5/recall@5 criterion to account for the multimodality in future predictions, as well as class-mean (cm) metrics to equally weight classes in a long-tail distribution.</p><p>The first three datasets also decompose the action annotations into verb and nouns. While some prior work <ref type="bibr" target="#b78">[77]</ref> supervises the model additionally for nouns and verbs, we train all our model solely to predict actions, and estimate the verb/noun probabilities by marginalizing over the other, similar to <ref type="bibr" target="#b23">[23]</ref>. In all tables, we highlight the columns showing the metric used to rank methods in the official challenge leaderboards. Unless otherwise specified, the reported metrics correspond to future action (act.) prediction, although we do report numbers for verb and nouns separately where applicable. Please see Appendix A for further details.</p><p>Baselines. We compare AVT to its variants with different backbones and pretrained initializations, as well as to the strongest recent approaches for action anticipation, i.e. RULSTM <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>, ActionBanks <ref type="bibr" target="#b78">[77]</ref>, and Forecasting HOI (FHOI) <ref type="bibr" target="#b57">[57]</ref>. Please see Appendix B for details on them.</p><p>While FHOI trains the model end-to-end, RULSTM and ActionBanks operate on top of features from a model pretrained for action classification on that dataset. Hence, we report results both using the exact same features as well as end-to-end trained backbones to facilitate fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to the state-of-the-art</head><p>EK100. We first compare AVT to prior work using individual modalities (RGB and Obj <ref type="bibr" target="#b23">[23]</ref>) in <ref type="table">Table 2</ref> for apples-toapples comparisons and to isolate the performance of each of our contributions. First, we compare to the state-of-theart RULSTM method using only our AVT (head) model applied to the exact same features from TSN <ref type="bibr" target="#b92">[91]</ref> trained for classification on EK100. We note this already improves over RULSTM, particularly in anticipating future objects (nouns). Furthermore, we experiment with backbone fea-  <ref type="table">Table 3</ref>: EK100 val and test sets using all modalities. We split the test comparisons between published work and CVPR'21 challenge submissions. We outperform prior work including all challenge submissions, with especially significant gains on tail classes. Performance is reported using class-mean recall@5. AVT+ and AVT++ late fuse predictions from multiple modalities; please see text for details.</p><p>tures from a recent state-of-the-art video model, irCSN-152 <ref type="bibr" target="#b88">[87]</ref> pretrained on a large weakly supervised dataset, IG65M <ref type="bibr" target="#b27">[27]</ref>. We finetune this backbone for recognition on EK100, extract its features and train AVT-h same as before, but find it to not be particularly effective at the EK100 anticipation task. Next, we replace the backbone with our AVT-b and train the model end-to-end, leading to the best performance so far, and outperforming RULSTM by 1.6%. We make the same comparison over features from an objectdetector <ref type="bibr" target="#b73">[72]</ref> trained on EK100 provided by RULSTM (referred to as OBJ modality, details in Appendix A), and similarly find our method outperforms RULSTM on this modality as well. Note that the fixed features used above can be thought of as a proxy for past recognized actions, as they are trained only for action recognition. Hence, AVT-h on TSN or irCSN152 features is comparable to a baseline that trains a language model over past actions to predict future ones. As the later experiments show, end-to-end trained AVT is significantly more effective, supporting AVT's from-pixels anticipation as opposed to label-space anticipation.</p><p>Finally, we compare models using all modalities on the EK100 val and the held-out test set in <ref type="table">Table 3</ref>. While RUL-STM fuses models trained on RGB, Flow, and OBJ features using an attention based model (MATT <ref type="bibr" target="#b23">[23]</ref>), we simply late fuse predictions from our best RGB and OBJ models (resulting model referred to as AVT+), and outperform all reported work on this benchmark, establishing a new stateof-the-art. Note we get the largest gains on tail classes, suggesting our model is particularly effective at few-shot anticipation. Finally, AVT++ ensembles multiple model variants, and outperforms all submissions on the EK100 CVPR'21    challenge leaderboard. Please refer to the workshop paper <ref type="bibr" target="#b29">[29]</ref> for details on AVT++. EK55. Since EK100 is relatively new and has few baseline methods reported, we also evaluate AVT on EK55. As before, we start by comparing single modality methods (RGBonly) in <ref type="table" target="#tab_5">Table 4</ref>. For AVT-h models, we found a slightly different set of (properly validated) hyperparameters performed better for top-1/5 metrics vs. the recall metric, hence we report our best models for each set of results. Here we find AVT-h performs comparably to RULSTM, and outperforms another attention-based model <ref type="bibr" target="#b78">[77]</ref> (one of the winners of the EK55 2020 challenge) on the top-1 metrics. The gain is more significant on the recall metric, which averages performance over classes, indicating again that AVT-h is especially effective on tail classes which get ignored in top-1/5 metrics. Next, we replace the backbone with AVTb, and find it to perform comparably on top-1/5 metrics, and outperforms on the recall metric. Finally, we experiment with irCSN-152 <ref type="bibr" target="#b88">[87]</ref> pretrained using IG65M <ref type="bibr" target="#b27">[27]</ref> and finetuned on EK55, and find it to outperform all methods by a significant margin on top-1/5. We show further comparisons with the state-of-the-art on EK55 in Appendix C. EGTEA Gaze+. In <ref type="table" target="#tab_6">Table 5</ref> we compare our method at ? a = 0.5s on the split 1 as in recent work <ref type="bibr" target="#b57">[57]</ref>. Even using fixed features with AVT-h on top, AVT outperforms the best reported results, and using the AVT-b backbone further improves performance. Notably, FHOI leverages attention on hand trajectories to obtain strong performance, which, as we see in <ref type="figure">Figure 1</ref>, emerges spontaneously in our model. 50-Salads. Finally, we show that our approach is not lim-   ited to egocentric videos and is also effective in third-person settings. In <ref type="table" target="#tab_7">Table 6</ref>, we report top-1 performance on 50-Salads averaged over standard 5 splits. We observe it outperforms previous RNN <ref type="bibr" target="#b1">[2]</ref> and attention <ref type="bibr" target="#b78">[77]</ref> based approaches by a significant 7.3% absolute improvement, again establishing a new state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablations and Analysis</head><p>We now analyze the AVT architecture, using the RGB modality and EK100 validation set as the test bed.</p><p>Anticipative losses. In <ref type="table" target="#tab_9">Table 7</ref>, we evaluate the contribution of the two intermediate prediction losses that leverage the causal structure of AVT. We find using those objectives leads to significant improvements for both backbones. We find L cls is more effective for TSN, and L f eat for AVT-b. Given that both combined work well in both settings, we use both for all experiments. Note that the naive setting also serves as a baseline with AVT-b backbone followed by simple aggregation on top, and shows our proposed losses encouraging the predictive structure are imperative to obtain strong performance. We analyze per-class gains in Appendix D.1 and find classes like 'cook', which require understanding the sequence of actions so far to anticipate well, obtain the largest gains in the anticipative setting.</p><p>Temporal context. Next, we analyze the effect of temporal context. In <ref type="figure" target="#fig_3">Figure 4</ref>, we train and test the model with different lengths of temporal context, ? o . We notice that the performance improves as we incorporate more frames of context, with more consistent gains for AVT-b. The gains are especially pronounced when trained using the anticipative setting (11.2 ? 14.9 = 3.5 ?) vs. the naive (11.0 ? 13.1 = 2.1 ?). This suggests end-to-end trained AVT using anticipative losses is better suited at modeling sequences of long-range temporal interactions. Attention visualization. To better understand how AVT models videos, we visualize the learned attention in the backbone and head. For the backbone, following prior work <ref type="bibr" target="#b18">[18]</ref>, we use attention rollout <ref type="bibr" target="#b0">[1]</ref> to aggregate attention over heads and layers. For the head, since our causal modeling would bias aggregated attention towards the first AVT can also be used to predict further into the future by rolling out predictions autoregressively. The text on top represents the next action predicted at provided frames, followed by subsequently predicted actions, with the number representing how long that action would repeat.</p><p>few frames, we visualize the last layer attention averaged over heads. As shown in <ref type="figure">Figure 1</ref>, the model spontaneously learns to attend to hands and objects, which has been found beneficial for egocentric anticipation tasks <ref type="bibr" target="#b57">[57]</ref>-but required manual designation in prior work. The temporal attention also varies between focusing on the past or mostly on the current frame depending on the predicted future action. We show additional results in Appendix D.2.</p><p>Long-term anticipation. So far we have shown AVT's applicability in the next-action anticipation task. Thanks to AVT's predictive nature, it can also be rolled out autoregressively to predict a sequence of future actions given the video context. We append the predicted feature and run the model on the resulting sequence, reusing features computed for past frames. As shown in <ref type="figure">Figure 5</ref>, AVT makes reasonable future predictions-'wash spoon' after 'wash knife', followed by 'wash hand' and 'dry hand'-indicating the model has started to learn certain 'action schemas' <ref type="bibr" target="#b68">[68]</ref>, a core capability of our causal attention and anticipative training architecture. We show additional results in Appendix D.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We presented AVT, an end-to-end attention-based architecture for anticipative video modeling. Through extensive experimentation on four popular benchmarks, we show its applicability in anticipating future actions, obtaining stateof-the-art results and demonstrating the importance of its anticipative training objectives. We believe AVT would be a strong candidate for tasks beyond anticipation, such as selfsupervised learning <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b91">90]</ref>, discovering action schemas and boundaries <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b80">79]</ref>, and even for general action recognition in tasks that require modeling temporal ordering <ref type="bibr" target="#b34">[34]</ref>. We plan to explore these directions in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Metrics</head><p>We test on four datasets as described in the main paper. EpicKitchens-100 (EK100) <ref type="bibr" target="#b14">[14]</ref> is the largest egocentric (first-person) video dataset with 700 long unscripted videos of cooking activities totalling 100 hours. It contains 89,977 segments labeled with one of 97 verbs, 300 nouns, and 3807 verb-noun combinations (or "actions"), and uses ? a =1s. The dataset is split in 75:10:15 ratio into train/val/test sets, and the test set evaluation requires submission to the CVPR'21 challenge server. The evaluation metric used is class-mean recall@5 <ref type="bibr" target="#b22">[22]</ref>, which evaluates if the correct future class is within the top-5 predictions, and equally weights all classes by averaging the performance computed individually per class. The top-5 criterion also takes into account the multi-modality in the future predictions. Entries are ranked according to performance on actions.</p><p>EpicKitchens-55 (EK55) <ref type="bibr" target="#b13">[13]</ref> is an earlier version of the EK100, with 39,596 segments labeled with 125 verbs, 352 nouns, and 2,513 combinations (actions), totalling 55 hours, and ? a = 1s. We use the standard splits and metrics from <ref type="bibr" target="#b24">[24]</ref>. For anticipation, <ref type="bibr" target="#b24">[24]</ref> splits the public training set into 23,493 training and 4,979 validation segments from 232 and 40 videos respectively. The test evaluation is similarly performed on the challenge server. The evaluation metrics used are top-1/top-5 accuracies and class-mean recall@5 over verb/noun/action predictions at anticipation time ? a = 1s. Unlike EK100, the recall computation on EK55 is done over a subset of 'many-shot' classes as defined in <ref type="bibr" target="#b23">[23]</ref>. While EK55 is a subset of EK100, we use it to compare to a larger set of baselines, which have not yet been reported on EK100.</p><p>Prior work on the EK datasets <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b78">77]</ref> operate on features from pre-trained models, specifically RGB features extracted using a TSN <ref type="bibr" target="#b92">[91]</ref> architecture trained for action classification on the train set; Flow features using a TSN trained on optical flow; and OBJ features from a Faster R-CNN, whose output is converted into a vector depicting the distribution over object classes for that frame. We refer the reader to <ref type="bibr" target="#b23">[23]</ref> for details. We use these features for some experiments in the paper that use fixed backbones (eg TSN). We use the features as provided in the code release by <ref type="bibr" target="#b23">[23]</ref>. <ref type="bibr" target="#b6">7</ref> EGTEA Gaze+ <ref type="bibr" target="#b55">[55]</ref> is another popular egocentric action anticipation dataset, consisting of 10,325 action annotations with 106 unique actions. To be comparable to prior work <ref type="bibr" target="#b57">[57]</ref>, we report performance on the split 1 <ref type="bibr" target="#b55">[55]</ref> of the dataset at ? a = 0.5s using overall top-1 accuracy and mean over top-1 class accuracies (class mean accuracy).</p><p>Finally, we also experiment with a popular third-person action anticipation dataset: 50-Salads (50S) <ref type="bibr" target="#b83">[82]</ref>. It contains fifty 40s long videos, with 900 segments labeled with 7 https://github.com/fpv-iplab/rulstm  one of 17 action classes. We report top-1 accuracy averaged over the pre-defined 5 splits for an anticipation time ? a = 1s, following prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b78">77]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines Details</head><p>RULSTM leverages a 'rolling' LSTM to encode the past, and an 'unrolling' LSTM to predict the future, from different points in the past. It was ranked first in the EK55 challenge in 2019, and is currently the best reported method on EK100. ActionBanks <ref type="bibr" target="#b78">[77]</ref> improves over RULSTM through a carefully designed architecture leveraging nonlocal <ref type="bibr" target="#b94">[93]</ref> and long-term feature aggregation <ref type="bibr" target="#b96">[95]</ref> blocks over different lengths of past features, and was one of the winners of the CVPR'20 EK55 anticipation challenge. Forecasting HOI <ref type="bibr" target="#b57">[57]</ref> takes an alternate approach, leveraging latest spatio-temporal convnets <ref type="bibr" target="#b88">[87]</ref> jointly with hand motion and interaction hotspot prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EpicKitchens-55 Full Results</head><p>We use the irCSN-152 backbone for comparisons to state-of-the-art on EK55, as that performed the best in <ref type="table" target="#tab_5">Table 4</ref> on top-1, the primary metric used in EK55 leaderboards. For comparisons using all modalities, in <ref type="table" target="#tab_11">Table 8</ref>, we late fuse our best model with the other modalities from <ref type="bibr" target="#b78">[77]</ref> (resulting model referred to as AVT+). We outperform all reported work on the validation set. Finally in <ref type="table" target="#tab_12">Table 9</ref> we train our model on train+val, late fuse other modalities from <ref type="bibr" target="#b78">[77]</ref>, and evaluate on the test sets on the challenge server. Here as well we outperform all prior work. Note that our models are only trained for action prediction, and individual verb/noun predictions are obtained by marginalizing over the other. We outperform all prior work on on seen test set (S1), and are only second to concurrent work <ref type="bibr" target="#b16">[16]</ref> on Seen test set (S1) Unseen test set (S2) Top-1 Accuracy% Top-5 Accuracy% Top-1 Accuracy% Top-5 Accuracy% Verb Noun Act. Verb Noun Act. Verb Noun Act. Verb Noun Act.  unseen (S2) for top-1 actions. It is worth noting that <ref type="bibr" target="#b16">[16]</ref> uses transductive learning, leveraging the test set. AVT is also capable of similarly leveraging the test data with unsupervised objectives (L f eat ), which could potentially further improve in performance. We leave that exploration to future work. In <ref type="table" target="#tab_13">Table 10</ref>, we analyze the effect of different losses on the final performance on EK55, similar to the analysis on EK100 in <ref type="table" target="#tab_9">Table 7</ref>. We see a similar trend: using the anticipative setting performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Per-class Gains</head><p>To better understand the source of these gains, we analyze the class-level gains with anticipative training in <ref type="figure" target="#fig_5">Figure 6</ref>. We notice certain verb classes show particularly large gains across the backbones, such as 'cook' and 'choose'. We posit that is because predicting the person will cook an item would often require understanding the sequence of ac- tions so far, such as preparing ingredients, turning on the stove etc., which the anticipative training setting encourages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Attention Visualizations</head><p>In <ref type="figure" target="#fig_8">Figure 8</ref> we show additional visualizations of the spatial and temporal attention, similar to <ref type="figure">Figure 1</ref>. We also show failure cases, which often involve temporal inaccuracy (i.e. when the model anticipates an action too soon or too late) and object recognition errors (predicting 'wash spoon' instead of 'wash fork'). We also provide attached videos to visualize predicted future classes along with the ground truth (GT) future prediction in a video form for EK100 and EGTEA Gaze+, at each time step (as opposed to only 2 shown in these figures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Long-term Anticipation</head><p>In <ref type="figure">Figure 9</ref> we show additional visualizations of the long-term anticipation, similar to : Different L f eat functions and weights. We found similar or better performance of the simpler L2 metric over NCE and use it for all experiments in the paper. The graph here shows performance on EK100 (validation, RGB) at ?a = 1s, at different scalar weights used on this loss during optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. L f eat Formulation</head><p>In <ref type="figure">Figure 7</ref> we show the performance of AVT with both AVT-b and TSN backbones, using two different loss functions for L f eat : L 2 as used in paper, and InfoNCE <ref type="bibr" target="#b66">[66]</ref> objective as in some recent work <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b97">96]</ref>, at different weights used on that loss during training. We find that L 2 is as effective or better for both backbones, and hence we use it with weight=1.0 for all experiments. While further hyperparameter tuning can potentially lead to further improvements for InfoNCE as observed in some concurrent work <ref type="bibr" target="#b97">[96]</ref>, we leave that exploration to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Computational complexity</head><p>The only additional compute in anticipative training as opposed to naive is for applying the linear layer to classify past frame features for L cls , since L f eat simply matches past features, which anyway need to be computed for self attention to predict the next action. We found GPU memory remains nearly same, and runtime was only 1% higher than a model that only predicts the next action. Also, this additional processing is only for training; inference is exactly same irrespective of additional losses.   <ref type="figure">Figure 1</ref>. For each input frame, we visualize the effective spatial attention by AVT-b using attention rollout <ref type="bibr" target="#b0">[1]</ref>. The red regions represent the regions of highest attention, which we find to often correspond to hands+objects in the egocentric EpicKitchens-100 videos. The text on the top show future predictions at 2 points in the video, along with the temporal attention (last layer of AVT-h averaged over heads) visualized using the width of the lines. The green color of text indicates that it matches the GT action at that future frame (or that nothing is labeled at that frame). As seen in <ref type="figure">Figure 1</ref>, spatial attention focuses on hands and objects. The temporal attention focuses on the last frame when predicting actions like 'turn-off tap', whereas more uniformly on all frames when predicting 'open fridge' (as an action like that usually follows a sequence of actions involving packing up food items and moving towards the fridge).</p><p>wash plate wash bowl turn-on tap put sponge mix pasta filter pasta take glass put glass turn-on tap turn-off tap <ref type="figure" target="#fig_8">Figure 8</ref>: More Qualitative Results. (Continued) Here we also see some failure cases (the text in black-does not match the labeled ground truth). Note that the predictions in those failure cases are still reasonable. For instance in the second example the model predicts 'turn-on tap', while the groundtruth on that frame is 'wash cloth'. As we can see in the frame that the water is running, hence the 'turn-on tap' does happen before the eventual labeled action of 'wash cloth', albeit slightly sooner than when the model predicts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a t e x i t s h a 1 _ b a s e 6 4 = " g C y p q 3 O 8 U F / y H R 3 a I x s d K v Z U O N w = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u 0 i z n u 6 V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 3 N s o Q r Z J J a 2 w n 8 F M M J N S i Y 5 N N S N 7 M 8 p W x E B 7 z j q K I J t + F k f u 2 U n D m l T 2 J t X C k k c / X 3 x I Q m 1 o 6 T y H U m F I d 2 2 Z u J / 3 m d D O O b c C J U m i F X b L E o z i R B T W a v k 7 4 w n K E c O 0 K Z E e 5 W w o b U U I Y u o J I L I V h + e Z U 0 L 6 r B V d W / v 6 z U / D y O I p z A K Z x D A N d Q g z u o Q w M Y P M I z v M K b p 7 0 X 7 9 3 7 W L Q W v H z m G P 7 A + / w B o t 2 P H A = = &lt; / l a t e x i t &gt; ? o &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g C y p q 3 O 8 U F / y H R 3 a I x s d K v Z U O N w = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u 0 i z n u 6 V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 3 N s o Q r Z J J a 2 w n 8 F M M J N S i Y 5 N N S N 7 M 8 p W x E B 7 z j q K I J t + F k f u 2 U n D m l T 2 J t X C k k c / X 3 x I Q m 1 o 6 T y H U m F I d 2 2 Z u J / 3 m d D O O b c C J U m i F X b L E o z i R B T W a v k 7 4 w n K E c O 0 K Z E e 5 W w o b U U I Y u o J I L I V h + e Z U 0 L 6 r B V d W / v 6 z U / D y O I p z A K Z x D A N d Q g z u o Q w M Y P M I z v M K b p 7 0 X 7 9 3 7 W L Q W v H z m G P 7 A + / w B o t 2 P H A = = &lt; / l a t e x i t &gt; ? o) Anticipation Time ( &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u G p q d C Q 2 C P n b H q 0 S b 1 S 7 x C N 2 I A E = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J f R O Z p M x s z P L z K w Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g U 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V p y h p U C a X b E R o m u G Q N y 6 1 g 7 V Q z T C L B W t H o d u a 3 n p g 2 X M k H O 0 5 Z m O B A 8 p h T t E 5 q d i 1 m P e y V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 1 F s 4 R J S w U a 0 w n 8 1 I Y T 1 J Z T w a a l b m Z Y i n S E A 9 Z x V G L C T D i Z X z s l Z 0 7 p k 1 h p V 9 K S u f p 7 Y o K J M e M k c p 0 J 2 q F Z 9 m b i f 1 4 n s / F N O O E y z S y T d L E o z g S x i s x e J 3 2 u G b V i 7 A h S z d 2 t h A 5 R I 7 U u o J I L I V h + e Z U 0 L 6 r B V d W / v 6 z U / D y O I p z A K Z x D A N d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B j a W P D g = = &lt; / l a t e x i t &gt; ? a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u G p q d C Q 2 C P n b H q 0 S b 1 S 7 x C N 2 I A E = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J f R O Z p M x s z P L z K w Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S g U 3 1 v e / v c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q G p V p y h p U C a X b E R o m u G Q N y 6 1 g 7 V Q z T C L B W t H o d u a 3 n p g 2 X M k H O 0 5 Z m O B A 8 p h T t E 5 q d i 1 m P e y V K 3 7 V n 4 O s k i A n F c h R 7 5 W / u n 1 F s 4 R J S w U a 0 w n 8 1 I Y T 1 J Z T w a a l b m Z Y i n S E A 9 Z x V G L C T D i Z X z s l Z 0 7 p k 1 h p V 9 K S u f p 7 Y o K J M e M k c p 0 J 2 q F Z 9 m b i f 1 4 n s / F N O O E y z S y T d L E o z g S x i s x e J 3 2 u G b V i 7 A h S z d 2 t h A 5 R I 7 U u o J I L I V h + e Z U 0 L 6 r B V d W / v 6 z U / D y O I p z A K Z x D A N d Q g z u o Q w M o P M I z v M K b p 7 w X 7 9 3 7 W L Q W v H z m G P 7 A + / w B j a W P D g = = &lt; / l a t e x i t &gt; ? a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a t e x i t s h a 1 _ b a s e 6 4 = " N j G I R + 4 a d D I m g J V P g w B h E g 5 s e 9 g = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h d 2 g 6 D H g x W M E 8 4 B k C b O T 2 W T M 7 M w y 0 y u E k H / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d g p 7 u 7 t H x y W j o 6 b V m e G 8 Q b T U p t 2 R C 2 X Q v E G C p S 8 n R p O k 0 j y V j S 6 n f m t J 2 6 s 0 O o B x y k P E z p Q I h a M o p O a X R Q J t 7 1 S 2 a / 4 c 5 B V E u S k D D n q v d J X t 6 9 Z l n C F T F J r O 4 G f Y j i h B g W T f F r s Z p a n l I 3 o g H c c V d Q t C S f z a 6 f k 3 C l 9 E m v j S i G Z q 7 8 n J j S x d p x E r j O h O L T L 3 k z 8 z + t k G N + E E 6 H S D L l i i 0 V x J g l q M n u d 9 I X h D O X Y E c q M c L c S N q S G M n Q B F V 0 I w f L L q 6 R Z r Q R X F f / + s l y r 5 n E U 4 B T O 4 A I C u I Y a 3 E E d G s D g E Z 7 h F d 4 8 7 b 1 4 7 9 7 H o n X N y 2 d O 4 A + 8 z x + y r Y 8 o &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N j G I R + 4 a d D I m g J V P g w B h E g 5 s e 9 g = " &gt; A A A B 7 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h d 2 g 6 D H g x W M E 8 4 B k C b O T 2 W T M 7 M w y 0 y u E k H / w 4 k E R r / 6 P N / / G S b I H T S x o K K q 6 6 e 6 K U i k s + v 6 3 t 7 a + s b m 1 X d g p 7 u 7 t H x y W j o 6 b V m e G 8 Q b T U p t 2 R C 2 X Q v E G C p S 8 n R p O k 0 j y V j S 6 n f m t J 2 6 s 0 O o B x y k P E z p Q I h a M o p O a X R Q J t 7 1 S 2 a / 4 c 5 B V E u S k D D n q v d J X t 6 9 Z l n C F T F J r O 4 G f Y j i h B g W T f F r s Z p a n l I 3 o g H c c V d Q t C S f z a 6 f k 3 C l 9 E m v j S i G Z q 7 8 n J j S x d p x E r j O h O L T L 3 k z 8 z + t k G N + E E 6 H S D L l i i 0 V x J g l q M n u d 9 I X h D O X Y E c q M c L c S N q S G M n Q B F V 0 I w f L L q 6 R Z r Q R X F f / + s l y r 5 n E U 4 B T O 4 A I C u I Y a 3 E E d G s D g E Z 7 h F d 4 8 7 b 1 4 7 9 7 H o n X N y 2 d O 4 A + 8 z x + y r Y 8 o &lt; / l a t e x i t &gt; ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Temporal context. AVT effectively leverages longer temporal context, especially in the [a] setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 W 1 Dry 2 timeFigure 5 :</head><label>4125</label><figDesc>Spoon &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x O 9 T j P S x C 8 j l w Y 7 H w t q t o S c T A k = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u y g S b n v l i l / 1 5 y C r J M h J B X L U e + W v b l + z L O E K m a T W d g I / x X B C D Q o m + b T U z S x P K R v R A e 8 4 q q h b E k 7 m 1 0 7 J m V P 6 J N b G l U I y V 3 9 P T G h i 7 T i J X G d C c W i X v Z n 4 n 9 f J M L 4 J J 0 K l G X L F F o v i T B L U Z P Y 6 6 Q v D G c q x I 5 Q Z 4 W 4 l b E g N Z e g C K r k Q g u W X V 0 n z o h p c V f 3 7 y 0 r N z + M o w g m c w j k E c A 0 1 u I M 6 N I D B I z z D K 7 x 5 2 n v x 3 r 2 P R W v B y 2 e O 4 Q + 8 z x + y E 4 8 m &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x O 9 T j P S x C 8 j l w Y 7 H w t q t o S c T A k = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u y g S b n v l i l / 1 5 y C r J M h J B X L U e + W v b l + z L O E K m a T W d g I / x X B C D Q o m + b T U z S x P K R v R A e 8 4 q q h b E k 7 m 1 0 7 J m V P 6 J N b G l U I y V 3 9 P T G h i 7 T i J X G d C c W i X v Z n 4 n 9 f J M L 4 J J 0 K l G X L F F o v i T B L U Z P Y 6 6 Q v D G c q x I 5 Q Z 4 W 4 l b E g N Z e g C K r k Q g u W X V 0 n z o h p c V f 3 7 y 0 r N z + M o w g m c w j k E c A 0 1 u I M 6 N I D B I z z D K 7 x 5 2 n v x 3 r 2 P R W v B y 2 e O 4 Q + 8 z x + y E 4 8 m &lt; / l a t e x i t &gt; ? a s h Hand &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x O 9 T j P S x C 8 j l w Y 7 H w t q t o S c T A k = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u y g S b n v l i l / 1 5 y C r J M h J B X L U e + W v b l + z L O E K m a T W d g I / x X B C D Q o m + b T U z S x P K R v R A e 8 4 q q h b E k 7 m 1 0 7 J m V P 6 J N b G l U I y V 3 9 P T G h i 7 T i J X G d C c W i X v Z n 4 n 9 f J M L 4 J J 0 K l G X L F F o v i T B L U Z P Y 6 6 Q v D G c q x I 5 Q Z 4 W 4 l b E g N Z e g C K r k Q g u W X V 0 n z o h p c V f 3 7 y 0 r N z + M o w g m c w j k E c A 0 1 u I M 6 N I D B I z z D K 7 x 5 2 n v x 3 r 2 P R W v B y 2 e O 4 Q + 8 z x + y E 4 8 m &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x O 9 T j P S x C 8 j l w Y 7 H w t q t o S c T A k = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u y g S b n v l i l / 1 5 y C r J M h J B X L U e + W v b l + z L O E K m a T W d g I / x X B C D Q o m + b T U z S x P K R v R A e 8 4 q q h b E k 7 m 1 0 7 J m V P 6 J N b G l U I y V 3 9 P T G h i 7 T i J X G d C c W i X v Z n 4 n 9 f J M L 4 J J 0 K l G X L F F o v i T B L U Z P Y 6 6 Q v D G c q x I 5 Q Z 4 W 4 l b E g N Z e g C K r k Q g u W X V 0 n z o h p c V f 3 7 y 0 r N z + M o w g m c w j k E c A 0 1 u I M 6 N I D B I z z D K 7 x 5 2 n v x 3 r 2 P R W v B y 2 e O 4 Q + 8 z x + y E 4 8 m &lt; / l a t e x i t &gt; ? Hand &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x O 9 T j P S x C 8 j l w Y 7 H w t q t o S c T A k = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u y g S b n v l i l / 1 5 y C r J M h J B X L U e + W v b l + z L O E K m a T W d g I / x X B C D Q o m + b T U z S x P K R v R A e 8 4 q q h b E k 7 m 1 0 7 J m V P 6 J N b G l U I y V 3 9 P T G h i 7 T i J X G d C c W i X v Z n 4 n 9 f J M L 4 J J 0 K l G X L F F o v i T B L U Z P Y 6 6 Q v D G c q x I 5 Q Z 4 W 4 l b E g N Z e g C K r k Q g u W X V 0 n z o h p c V f 3 7 y 0 r N z + M o w g m c w j k E c A 0 1 u I M 6 N I D B I z z D K 7 x 5 2 n v x 3 r 2 P R W v B y 2 e O 4 Q + 8 z x + y E 4 8 m &lt; / l a t e x i t &gt; ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N x O 9 T j P S x C 8 j l w Y 7 H w t q t o S c T A k = " &gt; A A A B 7 X i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e y K o s e A F 4 8 R z A O S J c x O Z p M x s z P L T K 8 Q Q v 7 B i w d F v P o / 3 v w b J 8 k e N L G g o a j q p r s r S q W w 6 P v f X m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 4 1 r c 4 M 4 w 2 m p T b t i F o u h e I N F C h 5 O z W c J p H k r W h 0 O / N b T 9 x Y o d U D j l M e J n S g R C w Y R S c 1 u y g S b n v l i l / 1 5 y C r J M h J B X L U e + W v b l + z L O E K m a T W d g I / x X B C D Q o m + b T U z S x P K R v R A e 8 4 q q h b E k 7 m 1 0 7 J m V P 6 J N b G l U I y V 3 9 P T G h i 7 T i J X G d C c W i X v Z n 4 n 9 f J M L 4 J J 0 K l G X L F F o v i T B L U Z P Y 6 6 Q v D G c q x I 5 Q Z 4 W 4 l b E g N Z e g C K r k Q g u W X V 0 n z o h p c V f 3 7 y 0 r N z + M o w g m c w j k E c A 0 1 u I M 6 N I D B I z z D K 7 x 5 2 n v x 3 r 2 P R W v B y 2 e O 4 Q + 8 z x + y E 4 8 m &lt; / l a t e x i t &gt; ? Long-term anticipation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Verb classes that gain the most with causal modeling, averaged over the TSN and AVT-b backbones. Actions such as 'cook' and 'choose' show particularly significant gains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 7</head><label>57</label><figDesc>Figure 7: Different L f eat functions and weights. We found similar or better performance of the simpler L2 metric over NCE and use it for all experiments in the paper. The graph here shows performance on EK100 (validation, RGB) at ?a = 1s, at different scalar weights used on this loss during optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>More Qualitative Results. The spatial and temporal attention visualization in EK100, similar to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and 30 epochs of cosine annealed decay. At test time, we employ 3-crop testing, where we compute three 224px spatial crops from 248px input frames, and</figDesc><table><row><cell>Dataset</cell><cell cols="4">Viewpoint Segments Classes ? a (s)</cell><cell>Metric(s)</cell></row><row><cell>EK100 [14]</cell><cell>1st</cell><cell>90.0K</cell><cell cols="3">3,807 1.0 [14] recall</cell></row><row><cell>EK55 [13]</cell><cell>1st</cell><cell>39.6K</cell><cell cols="3">2,513 1.0 [13] top-1/5, recall</cell></row><row><cell cols="2">EGTEA Gaze+ [55] 1st</cell><cell>10.3K</cell><cell>106</cell><cell cols="2">0.5 [57] top-1, cm top-1</cell></row><row><cell>50S [82]</cell><cell>3rd</cell><cell>0.9K</cell><cell>17</cell><cell cols="2">1.0 [2] top-1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>30.8 14.0 28.8 27.2 14.2 19.8 22.0 11.1 AVT+ (TSN) 25.5 31.8 14.8 25.5 23.6 11.5 18.5 25.8 12.6 AVT+ 28.2 32.0 15.9 29.5 23.9 11.9 21.1 25.8 14.1</figDesc><table><row><cell></cell><cell></cell><cell>Overall</cell><cell>Unseen Kitchen</cell><cell>Tail Classes</cell></row><row><cell cols="2">Split Method</cell><cell cols="2">Verb Noun Act Verb Noun Act Verb Noun Act</cell></row><row><cell></cell><cell>chance</cell><cell cols="2">6.4 2.0 0.2 14.4 2.9 0.5 1.6 0.2 0.1</cell></row><row><cell cols="4">Val 27.8 Test RULSTM [14] chance 6.2 2.3 0.1 8.1 3.3 0.3 1.9 0.7 0.0 RULSTM [14] 25.3 26.7 11.2 19.4 26.9 9.7 17.6 16.0 7.9</cell></row><row><cell></cell><cell>TBN [100]</cell><cell cols="2">21.5 26.8 11.0 20.8 28.3 12.2 13.2 15.4 7.2</cell></row><row><cell></cell><cell>AVT+</cell><cell cols="2">25.6 28.8 12.6 20.9 22.3 8.8 19.0 22.0 10.1</cell></row><row><cell>Challenge</cell><cell cols="3">IIE MRG NUS CVML [76] 21.8 30.6 12.6 17.9 27.0 10.5 13.6 20.6 8.9 25.3 26.7 11.2 19.4 26.9 9.7 17.6 16.0 7.9 ICL+SJTU [35] 36.2 32.2 13.4 27.6 24.2 10.1 32.1 29.9 11.9 Panasonic [98] 30.4 33.5 14.8 21.1 27.1 10.2 24.6 27.5 12.7</cell></row><row><cell></cell><cell>AVT++</cell><cell cols="2">26.7 32.3 16.7 21.0 27.6 12.9 19.3 24.0 13.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>EK55 using only RGB modality for action anticipation. AVT performs comparably, and outperforms when combined with a backbone pretrained on large weakly labeled dataset.</figDesc><table><row><cell></cell><cell>Top-1 acc.</cell><cell>Class mean acc.</cell></row><row><cell>Method</cell><cell cols="2">Verb Noun Act. Verb Noun Act.</cell></row><row><cell cols="3">I3D-Res50 [12] 48.0 42.1 34.8 31.3 30.0 23.2</cell></row><row><cell>FHOI [57]</cell><cell cols="2">49.0 45.5 36.6 32.5 32.7 25.3</cell></row><row><cell cols="3">AVT-h (+TSN) 51.7 50.3 39.8 41.2 41.4 28.3</cell></row><row><cell>AVT</cell><cell cols="2">54.9 52.2 43.0 49.9 48.3 35.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Head</cell><cell>Top-1</cell></row><row><cell>DMR [90]</cell><cell>6.2</cell></row><row><cell>RNN [2]</cell><cell>30.1</cell></row><row><cell>CNN [2]</cell><cell>29.8</cell></row><row><cell cols="2">ActionBanks [77] 40.7</cell></row><row><cell>AVT</cell><cell>48.0</cell></row><row><cell>EGTEA Gaze+ Split 1 at ?a =</cell><cell></cell></row><row><cell>0.5s. AVT outperforms prior work by sig-</cell><cell></cell></row><row><cell>nificant margins, especially when trained</cell><cell></cell></row><row><cell>end-to-end with the AVT-b backbone.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell>: 50-Salads.</cell></row><row><cell>AVT</cell><cell>outperforms</cell></row><row><cell cols="2">prior work even in</cell></row><row><cell cols="2">3rd person videos.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Anticipative training.</figDesc><table><row><cell>15</cell><cell></cell><cell></cell></row><row><cell>10 11 12 13 14 Class Mean Recall @ 5</cell><cell>TSN [a] TSN [n]</cell><cell cols="2">AVT-b [a] AVT-b [n]</cell></row><row><cell>2</cell><cell cols="2">4 Observed segment ( o) (seconds) 6 8</cell><cell>10</cell></row><row><cell>Employing the anticipative train-</cell><cell></cell><cell></cell></row><row><cell>ing losses are imperative to obtain</cell><cell></cell><cell></cell></row><row><cell>strong performance with AVT.</cell><cell></cell><cell></cell></row><row><cell>Reported on EK100/cm recall@5.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>EK55 (val) results reported in top-1/5 (%) at ?a = 1.0s. The final late-fused model outperforms all prior work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>EK55 test set results obtained from the challenge server. AVT outperforms all published work on this dataset on top-5 metric, and is only second to<ref type="bibr" target="#b16">[16]</ref> on S2 on top-1. Note that<ref type="bibr" target="#b16">[16]</ref> leverages transductive learning (using the test set for initial graph representation learning), whereas AVT only uses the train set.</figDesc><table><row><cell></cell><cell cols="2">Losses</cell><cell cols="2">Backbones</cell></row><row><cell>Setting</cell><cell cols="4">L cls L f eat IG65M AVT-b</cell></row><row><cell>naive [n]</cell><cell>-</cell><cell>-</cell><cell>31.4</cell><cell>25.9</cell></row><row><cell></cell><cell>?</cell><cell>-</cell><cell>31.4</cell><cell>28.8</cell></row><row><cell></cell><cell>-</cell><cell>?</cell><cell>31.7</cell><cell>23.9</cell></row><row><cell cols="2">anticipative [a] ?</cell><cell>?</cell><cell>31.7</cell><cell>30.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Anticipative training on EK55. Employing the anticipative training losses are imperative to obtain strong performance with AVT; similar to as seen inTable 7.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use the term "anticipative" to refer to our model's ability to predict future video features and actions.<ref type="bibr" target="#b1">2</ref> Throughout we use the term "causal" to refer to the constraint that video be processed in a forward, online manner, i.e. functions applied at time t can only reference the frames preceding them, akin to Causal Language Modeling (CLM)<ref type="bibr" target="#b51">[51]</ref>. This is not to be confused with other uses of "causal" in AI where the connotation is instead cause-and-effect.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">For example, this would be true for each frame for densely labeled datasets like 50-Salads, and a subset of frames for sparsely labeled datasets like EpicKitchens-55.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: Authors would like to thank Antonino Furnari, Fadime Sener and Miao Liu for help with prior work; Naman Goyal and Myle Ott for help with language models; and Tushar Nagarajan, Gedas Bertasius and Laurens van der Maaten for feedback on the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>On top of each frame, we show the future prediction at that frame (not the action that is happening in the frame, but what the model predicts will happen next). The following text boxes show the future predictions made by the model by rolling out autoregressively, using the predicted future feature. The number next to the rolled out predictions denotes for how many time steps that specific action would repeat, according to the model. For example, 'wash spoon: 4' means the model anticipates the 'wash spoon' action to continue for next 4 time steps. On top of the predictions we show the labeled ground truth future actions. As we can observe, AVT makes reasonable future predictions, such as 'put pan' would follow 'wash pan'; 'dry hand' would follow 'wash hand' etc. This suggests the model has picked up on action schemas <ref type="bibr" target="#b68">[68]</ref>.</p><p>Ground truth future:  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yazan Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ViVit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cobe: Contextualized object embeddings from narrated instructional video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
		</author>
		<editor>Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<meeting><address><addrLine>Benjamin Chess, Jack Clark, Christopher Berner, Sam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A nonlocal algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epickitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rescaling egocentric vision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling temporal structure with lstm for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roeland</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Forecasting action through contact representations from first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eadom</forename><surname>Dessalene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maynord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinmaya</forename><surname>Devaraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty to rethink loss functions and evaluation measures for egocentric action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Battiato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rollingunrolling lstms for action anticipation from first-person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting the future: A jointly learnt model for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshala</forename><surname>Gammulle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Largescale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video Action Transformer Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Anticipative Video Transformer @ EPIC-Kitchens Action Anticipation Challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatiotemporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fr?nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transaction: Icl-sjtu submission to epickitchens action anticipation challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benny</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Zhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Memoryaugmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action-reaction: Forecasting the dynamics of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for driver activity anticipation via sensory-fusion architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Slow and steady feature analysis: higher order temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8545" to="8552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from selfsupervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Directional temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11746</idno>
		<title level="m">VidTr: Video transformer without convolutions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">In the eye of beholder: Joint learning of gaze and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Forecasting human object interaction: Joint prediction of motor attention and actions in first person video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning activity progression in lstms for activity detection and early detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">EGO-TOPO: Environment affordances from egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719</idno>
		<title level="m">Maya Zohar, and Dotan Asselmann. Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. In NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">La naissance de l&apos;intelligence chez l&apos;enfant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Piaget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<biblScope unit="page">216</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<pubPlace>Adam Roberts, Katherine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">First-person activity forecasting with online inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-tocoarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Action anticipation by predicting future dynamic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dibyadip</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03152</idno>
		<title level="m">Temporal aggregate representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Temporal aggregate representations for long-range video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Action anticipation with rbf kernelized feature mapping rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuge</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Generic event boundary detection: A benchmark for event segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10511</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UbiComp</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">VideoBERT: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Learning to anticipate egocentric actions by imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Submission to epic-kitchens action anticipation challenge 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Yamamuro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Shida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuyoshi</forename><surname>Kodake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Takenaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Fujimatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Video representation learning with visual tempo consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15489</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Multi-modal temporal convolutional network for anticipating actions in egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Zatsarynna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Philip Torr, and Vladlen Koltun. Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fhoi+obj</surname></persName>
		</author>
		<idno>57] 36.25 23.83 15.42 79.15 51.98 34.29 29.87 16.80 9.94 71.77 38.96 23.69</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
