<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 MASTERING ATARI WITH DISCRETE WORLD MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 MASTERING ATARI WITH DISCRETE WORLD MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DreamerV2 is the first agent that learns purely within a world model to achieve human-level Atari performance, demonstrating the high accuracy of its learned world model. DreamerV2 further outperforms the top single-GPU agents Rainbow and IQN, whose scores are provided by <ref type="bibr">Dopamine (Castro et al., 2018)</ref>. According to its authors, SimPLe <ref type="bibr">(Kaiser et al., 2019)</ref> was only evaluated on an easier subset of 36 games and trained for fewer steps and additional training does not further increase its performance.</p><p>To successfully operate in unknown environments, reinforcement learning agents need to learn about their environments over time. World models are an explicit way to represent an agent's knowledge about its environment. Compared to model-free reinforcement learning that learns through trial and error, world models facilitate generalization and can predict the outcomes of potential actions to enable planning <ref type="bibr">(Sutton, 1991)</ref>. Capturing general aspects of the environment, world models have been shown to be effective for transfer to novel tasks <ref type="bibr">(Byravan et al., 2019)</ref>, directed exploration <ref type="bibr">(Sekar et al., 2020)</ref>, and generalization from offline datasets <ref type="bibr">(Yu et al., 2020)</ref>. When the inputs are high-dimensional images, latent dynamics models predict ahead in an abstract latent space <ref type="bibr">(Watter et al., 2015;</ref><ref type="bibr">Ha and Schmidhuber, 2018;</ref><ref type="bibr">Hafner et al., 2018;</ref><ref type="bibr">Zhang et al., 2019)</ref>. Predicting compact representations instead of images has been hypothesized to reduce accumulating errors and their small memory footprint enables thousands of parallel predictions on a single <ref type="bibr">GPU (Hafner et al., 2018;</ref><ref type="bibr">2019)</ref>. Leveraging this approach, the recent Dreamer agent <ref type="bibr">(Hafner et al., 2019)</ref> has solved a wide range of continuous control tasks from image inputs.</p><p>Despite their intriguing properties, world models have so far not been accurate enough to compete with the stateof-the-art model-free algorithms on the most competitive benchmarks. The well-established Atari benchmark</p><p>Published as a conference paper at ICLR 2021 <ref type="bibr">(Bellemare et al., 2013)</ref> historically required model-free algorithms to achieve human-level performance, such as <ref type="bibr">DQN (Mnih et al., 2015)</ref>, <ref type="bibr">A3C (Mnih et al., 2016</ref><ref type="bibr">), or Rainbow (Hessel et al., 2018</ref>. Several attempts at learning accurate world models of Atari games have been made, without achieving competitive performance <ref type="bibr">(Oh et al., 2015;</ref><ref type="bibr">Chiappa et al., 2017;</ref><ref type="bibr">Kaiser et al., 2019)</ref>. On the other hand, the recently proposed MuZero agent <ref type="bibr">(Schrittwieser et al., 2019)</ref> shows that planning can achieve impressive performance on board games and deterministic Atari games given extensive engineering effort and a vast computational budget. However, its implementation is not available to the public and it would require over 2 months of computation to train even one agent on a GPU, rendering it impractical for most research groups.</p><p>In this paper, we introduce DreamerV2, the first reinforcement learning agent that achieves humanlevel performance on the Atari benchmark by learning behaviors purely within a separately trained world model, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Learning successful behaviors purely within the world model demonstrates that the world model learns to accurately represent the environment. To achieve this, we apply small modifications to the Dreamer agent <ref type="bibr">(Hafner et al., 2019)</ref>, such as using discrete latents and balancing terms within the KL loss. Using a single GPU and a single environment instance, DreamerV2 outperforms top single-GPU Atari agents <ref type="bibr">Rainbow (Hessel et al., 2018)</ref> and <ref type="bibr">IQN (Dabney et al., 2018)</ref>, which rest upon years of model-free reinforcement learning research <ref type="bibr">(Van Hasselt et al., 2015;</ref><ref type="bibr">Schaul et al., 2015;</ref><ref type="bibr">Wang et al., 2016;</ref><ref type="bibr">Bellemare et al., 2017;</ref><ref type="bibr">Fortunato et al., 2017)</ref>. Moreover, aspects of these algorithms are complementary to our world model and could be integrated into the Dreamer framework in the future. To rigorously compare the algorithms, we report scores normalized by both a human gamer <ref type="bibr">(Mnih et al., 2015)</ref> and the human world record <ref type="bibr">(Toromanoff et al., 2019)</ref> and make a suggestion for reporting scores going forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DREAMERV2</head><p>We present DreamerV2, an evolution of the Dreamer agent <ref type="bibr">(Hafner et al., 2019)</ref>. We refer to the original Dreamer agent as DreamerV1 throughout this paper. This section describes the complete DreamerV2 algorithm, consisting of the three typical components of a model-based agent <ref type="bibr">(Sutton, 1991)</ref>. We learn the world model from a dataset of past experience, learn an actor and critic from imagined sequences of compact model states, and execute the actor in the environment to grow the experience dataset. In Appendix C, we include a list of changes that we applied to DreamerV1 and which of them we found to increase empirical performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WORLD MODEL LEARNING</head><p>World models summarize an agent's experience into a predictive model that can be used in place of the environment to learn behaviors. When inputs are high-dimensional images, it is beneficial to learn compact state representations of the inputs to predict ahead in this learned latent space <ref type="bibr">(Watter et al., 2015;</ref><ref type="bibr">Karl et al., 2016;</ref><ref type="bibr">Ha and Schmidhuber, 2018)</ref>. These models are called latent dynamics models. Predicting ahead in latent space not only facilitates long-term predictions, it also allows to efficiently predict thousands of compact state sequences in parallel in a single batch, without having to generate images. DreamerV2 builds upon the world model that was introduced by PlaNet (Hafner et al., 2018) and used in DreamerV1, by replacing its Gaussian latents with categorical variables.</p><p>Experience dataset The world model is trained from the agent's growing dataset of past experience that contains sequences of images x 1:T , actions a 1:T , rewards r 1:T , and discount factors ? 1:T . The discount factors equal a fixed hyper parameter ? = 0.999 for time steps within an episode and are set to zero for terminal time steps. For training, we use batches of B = 50 sequences of fixed length L = 50 that are sampled randomly within the stored episodes. To observe enough episode ends during training, we sample the start index of each training sequence uniformly within the episode and then clip it to not exceed the episode length minus the training sequence length.</p><p>Model components The world model consists of an image encoder, a Recurrent State-Space Model <ref type="bibr">(RSSM;</ref><ref type="bibr">Hafner et al., 2018)</ref> to learn the dynamics, and predictors for the image, reward, and discount factor. The world model is summarized in <ref type="figure">Figure 2</ref>. The RSSM uses a sequence of deterministic recurrent states h t , from which it computes two distributions over stochastic states at each step. The posterior state z t incorporates information about the current image x t , while the prior state? t aims to predict the posterior without access to the current image. The concatenation of deterministic and  <ref type="figure">Figure 2</ref>: World Model Learning. The training sequence of images x t is encoded using the CNN. The RSSM uses a sequence of deterministic recurrent states h t . At each step, it computes a posterior stochastic state z t that incorporates information about the current image x t , as well as a prior stochastic state? t that tries to predict the posterior without access to the current image. Unlike in PlaNet and DreamerV1, the stochastic state of DreamerV2 is a vector of multiple categorical variables. The learned prior is used for imagination, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The KL loss both trains the prior and regularizes how much information the posterior incorporates from the image. The regularization increases robustness to novel inputs. It also encourages reusing existing information from past steps to predict rewards and reconstruct images, thus learning long-term dependencies.</p><p>stochastic states forms the compact model state. From the posterior model state, we reconstruct the current image x t and predict the reward r t and discount factor ? t . The model components are:</p><formula xml:id="formula_0">RSSM ? ? ? Recurrent model: h t = f ? (h t?1 , z t?1 , a t?1 ) Representation model: z t ? q ? (z t | h t , x t ) Transition predictor:? t ? p ? (? t | h t ) Image predictor:x t ? p ? (x t | h t , z t ) Reward predictor:r t ? p ? (r t | h t , z t ) Discount predictor:? t ? p ? (? t | h t , z t ).</formula><p>(1) All components are implemented as neural networks and ? describes their combined parameter vector. The transition predictor guesses the next model state only from the current model state and the action but without using the next image, so that we can later learn behaviors by predicting sequences of model states without having to observe or generate images. The discount predictor lets us estimate the probability of an episode ending when learning behaviors from model predictions.</p><p>Neural networks The representation model is implemented as a Convolutional Neural Network (CNN; <ref type="bibr">LeCun et al., 1989)</ref> followed by a Multi-Layer Perceptron (MLP) that receives the image embedding and the deterministic recurrent state. The RSSM uses a Gated Recurrent Unit <ref type="bibr">(GRU;</ref><ref type="bibr">Cho et al., 2014)</ref> to compute the deterministic recurrent states. The model state is the concatenation of deterministic GRU state and a sample of the stochastic state. The image predictor is a transposed CNN and the transition, reward, and discount predictors are MLPs. We down-scale the 84 ? 84 grayscale images to 64 ? 64 pixels so that we can apply the convolutional architecture of DreamerV1.</p><p>Algorithm 1: Straight-Through Gradients with Automatic Differentiation sample = one_hot(draw(logits)) # sample has no gradient probs = softmax(logits) # want gradient of this sample = sample + probs -stop_grad(probs) # has gradient of probs</p><p>We use the ELU activation function for all components of the model <ref type="bibr">(Clevert et al., 2015)</ref>. The world model uses a total of 20M trainable parameters.</p><p>Distributions The image predictor outputs the mean of a diagonal Gaussian likelihood with unit variance, the reward predictor outputs a univariate Gaussian with unit variance, and the discount predictor outputs a Bernoulli likelihood. In prior work, the latent variable in the model state was a diagonal Gaussian that used reparameterization gradients during backpropagation <ref type="bibr">(Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014)</ref>. In DreamerV2, we instead use a vector of several categorical variables and optimize them using straight-through gradients <ref type="bibr">(Bengio et al., 2013)</ref>, which are easy to implement using automatic differentiation as shown in Algorithm 1. We discuss possible benefits of categorical over Gaussian latents in the experiments section.</p><p>Loss function All components of the world model are optimized jointly. The distributions produced by the image predictor, reward predictor, discount predictor, and transition predictor are trained to maximize the log-likelihood of their corresponding targets. The representation model is trained to produce model states that facilitates these prediction tasks, through the expectation below. Moreover, it is regularized to produce model states with high entropy, such that the model becomes robust to many different model states during training. The loss function for learning the world model is:</p><formula xml:id="formula_1">L(?) . = E q ? (z 1:T | a 1:T ,x 1:T ) T t=1 ? ln p ? (x t | h t , z t ) image log loss ? ln p ? (r t | h t , z t ) reward log loss ? ln p ? (? t | h t , z t ) discount log loss +? KL q ? (z t | h t , x t ) p ? (z t | h t ) KL loss .<label>(2)</label></formula><p>We jointly minimize the loss function with respect to the vector ? that contains all parameters of the world model using the Adam optimizer (Kingma and Ba, 2014). We scale the KL loss by ? = 0.1 for Atari and by ? = 1.0 for continuous control <ref type="bibr">(Higgins et al., 2016)</ref>.</p><p>KL balancing The world model loss function in Equation 2 is the ELBO or variational free energy of a hidden Markov model that is conditioned on the action sequence. The world model can thus be interpreted as a sequential VAE, where the representation model is the approximate posterior and the transition predictor is the temporal prior. In the ELBO objective, the KL loss serves two purposes: it trains the prior toward the representations, and it regularizes the representations toward the prior. However, learning the transition function is difficult and we want to avoid regularizing the representations toward a poorly trained prior. To solve this problem, we minimize the KL loss faster with respect to the prior than the representations by using different learning rates, ? = 0.8 for the prior and 1 ? ? for the approximate posterior. We implement this technique as shown in Algorithm 2 and refer to it as KL balancing. KL balancing encourages learning an accurate prior over increasing posterior entropy, so that the prior better approximates the aggregate posterior. KL balancing is different from and orthogonal to beta-VAEs (Higgins et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BEHAVIOR LEARNING</head><p>DreamerV2 learns long-horizon behaviors purely within its world model using an actor and a critic. The actor chooses actions for predicting imagined sequences of compact model states. The critic accumulates the future predicted rewards to take into account rewards beyond the planning horizon. Both the actor and critic operate on top of the learned model states and thus benefit from the representations learned by the world model. The world model is fixed during behavior learning, so the actor and value gradients do not affect its representations. Not predicting images during behavior learning lets us efficiently simulate 2500 latent trajectories in parallel on a single GPU.</p><p>Imagination MDP To learn behaviors within the latent space of the world model, we define the imagination MPD as follows. The distribution of initial states? 0 in the imagination MDP is the distribution of compact model states encountered during world model training. From there, the transition predictor p ? (? t |? t?1 ,? t?1 ) outputs sequences?  <ref type="figure">Figure 2</ref> is used for learning a policy from trajectories imagined in the compact latent space. The trajectories start from posterior states computed during model training and predict forward by sampling actions from the actor network. The critic network predicts the expected sum of future rewards for each state. The critic uses temporal difference learning on the imagined rewards. The actor is trained to maximize the critic prediction, via reinforce gradients, straight-through gradients of the world model, or a combination of them.</p><p>imagination horizon H = 15. The mean of the reward predictor p ? (r t |? t ) is used as reward sequencer 1:H . The discount predictor p ? (? t |? t ) outputs the discount sequence? 1:H that is used to down-weight rewards. Moreover, we weigh the loss terms of the actor and critic by the cumulative predicted discount factors to softly account for the possibility of episode ends.</p><p>Model components To learn long-horizon behaviors in the imagination MDP, we leverage a stochastic actor that chooses actions and a deterministic critic. The actor and critic are trained cooperatively, where the actor aims to output actions that lead to states that maximize the critic output, while the critic aims to accurately estimate the sum of future rewards achieved by the actor from each imagined state. The actor and critic use the parameter vectors ? and ?, respectively:</p><formula xml:id="formula_2">Actor:? t ? p ? (? t |? t ) Critic: v ? (? t ) ? E p ? ,p ? ? ?t? ? ?tr ? .<label>(3)</label></formula><p>In contrast to the actual environment, the latent state sequence is Markovian, so that there is no need for the actor and critic to condition on more than the current model state. The actor and critic are both MLPs with ELU activations <ref type="bibr">(Clevert et al., 2015)</ref> and use 1M trainable parameters each. The actor outputs a categorical distribution over actions and the critic has a deterministic output. The two components are trained from the same imagined trajectories but optimize separate loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Critic loss function</head><p>The critic aims to predict the discounted sum of future rewards that the actor achieves in a given model state, known as the state value. For this, we leverage temporal-difference learning, where the critic is trained toward a value target that is constructed from intermediate rewards and critic outputs for later states. A common choice is the 1-step target that sums the current reward and the critic output for the following state. However, the imagination MDP lets us generate on-policy trajectories of multiple steps, suggesting the use of n-step targets that incorporate reward information into the critic more quickly. We follow DreamerV1 in using the more general ?-target (Sutton and <ref type="bibr">Barto, 2018;</ref><ref type="bibr">Schulman et al., 2015)</ref> that is defined recursively as follows:</p><formula xml:id="formula_3">V ? t . =r t +? t (1 ? ?)v ? (? t+1 ) + ?V ? t+1 if t &lt; H, v ? (? H ) if t = H.<label>(4)</label></formula><p>Intuitively, the ?-target is a weighted average of n-step returns for different horizons, where longer horizons are weighted exponentially less. We set ? = 0.95 in practice, to focus more on long horizon targets than on short horizon targets. Given a trajectory of model states, rewards, and discount factors, we train the critic to regress the ?-return using a squared loss:</p><formula xml:id="formula_4">L(?) . = E p ? ,p ? H?1 t=1 1 2 v ? (? t ) ? sg(V ? t ) 2 .<label>(5)</label></formula><p>We optimize the critic loss with respect to the critic parameters ? using the Adam optimizer. There is no loss term for the last time step because the target equals the critic at that step. We stop the gradients around the targets, denoted by the sg(?) function, as typical in the literature. We stabilize value learning using a target network <ref type="bibr">(Mnih et al., 2015)</ref>, namely, we compute the targets using a copy of the critic that is updated every 100 gradient steps.</p><p>Actor loss function The actor aims to output actions that maximize the prediction of long-term future rewards made by the critic. To incorporate intermediate rewards more directly, we train the actor to maximize the same ?-return that was computed for training the critic. There are different gradient estimators for maximizing the targets with respect to the actor parameters. DreamerV2 combines unbiased but high-variance Reinforce gradients with biased but low-variance straightthrough gradients. Moreover, we regularize the entropy of the actor to encourage exploration where feasible while allowing the actor to choose precise actions when necessary.</p><p>Learning by Reinforce (Williams, 1992) maximizes the actor's probability of its own sampled actions weighted by the values of those actions. The variance of this estimator can be reduced by subtracting the state value as baseline, which does not depend on the current action. Intuitively, subtracting the baseline centers the weights and leads to faster learning. The benefit of Reinforce is that it produced unbiased gradients and the downside is that it can have high variance, even with baseline.</p><p>DreamerV1 relied entirely on reparameterization gradients <ref type="bibr">(Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014)</ref> to train the actor directly by backpropagating value gradients through the sequence of sampled model states and actions. DreamerV2 uses both discrete latents and discrete actions.</p><p>To backpropagate through the sampled actions and state sequences, we leverage straight-through gradients <ref type="bibr">(Bengio et al., 2013)</ref>. This results in a biased gradient estimate with low variance. The combined actor loss function is:</p><formula xml:id="formula_5">L(?) . = E p ? ,p ? H?1 t=1 ?? ln p ? (? t |? t ) sg(V ? t ? v ? (? t )) reinforce ?(1 ? ?)V ? t dynamics backprop ?? H[a t |? t ] entropy regularizer .<label>(6)</label></formula><p>We optimize the actor loss with respect to the actor parameters ? using the Adam optimizer. We consider both Reinforce gradients and straight-through gradients, which backpropagate directly through the learned dynamics. Intuitively, the low-variance but biased dynamics backpropagation could learn faster initially and the unbiased but high-variance could to converge to a better solution.</p><p>For Atari, we find Reinforce gradients to work substantially better and use ? = 1 and ? = 10 ?3 . For continuous control, we find dynamics backpropagation to work substantially better and use ? = 0 and ? = 10 ?4 . Annealing these hyper parameters can improve performance slightly but to avoid the added complexity we report the scores without annealing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>We evaluate DreamerV2 on the well-established Atari benchmark with sticky actions, comparing to four strong model-free algorithms. DreamerV2 outperforms the four model-free algorithms in all scenarios. For an extensive comparison, we report four scores according to four aggregation protocols and give a recommendation for meaningfully aggregating scores across games going forward. We also ablate the importance of discrete representations in the world model. Our implementation of DreamerV2 reaches 200M environment steps in under 10 days, while using only a single NVIDIA V100 GPU and a single environment instance. During the 200M environment steps, DreamerV2 learns its policy from 468B compact states imagined under the model, which is 10,000? more than the 50M inputs received from the real environment after action repeat.  <ref type="table">Table 1</ref> for numeric scores. The standards in the literature to aggregate over tasks are shown in the left two plots. These normalize scores by a professional gamer and compute the median or mean over tasks <ref type="bibr">(Mnih et al., 2015;</ref><ref type="bibr">2016)</ref>. In Section 3, we point out limitations of this methodology. As a robust measure of performance, we recommend the metric in the right-most plot. We normalize scores by the human world record <ref type="bibr">(Toromanoff et al., 2019)</ref> and then clip them, such that exceeding the record does not further increase the score, before averaging over tasks.</p><p>Experimental setup We select the 55 games that prior works in the literature from different research labs tend to agree on <ref type="bibr">(Mnih et al., 2016;</ref><ref type="bibr">Brockman et al., 2016;</ref><ref type="bibr">Hessel et al., 2018;</ref><ref type="bibr">Castro et al., 2018;</ref><ref type="bibr">Badia et al., 2020)</ref> and recommend this set of games for evaluation going forward. We follow the evaluation protocol of Machado et al. <ref type="formula" target="#formula_1">(2018)</ref>   <ref type="bibr">, 2015)</ref>. We use the scores of these agents provided by the Dopamine framework (Castro et al., 2018) that use sticky actions. These may differ from the reported results in the papers that introduce these algorithms in the deterministic Atari setup. The training time of Rainbow was reported at 10 days on a single GPU and using one environment instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ATARI PERFORMANCE</head><p>The performance curves of DreamerV2 and four standard model-free algorithms are visualized in <ref type="figure" target="#fig_2">Figure 4</ref>. The final scores at 200M environment steps are shown in <ref type="table">Table 1</ref> and the scores on individual games are included in   <ref type="figure">Figure H</ref>.1. The ablations highlight the benefit of using categorical over Gaussian latent variables and of using KL balancing. Moreover, they show that the world model relies on image gradients for learning its representations. Stopping reward gradients even improves performance on some tasks, suggesting that representations that are not specifically trained to predict previously experienced rewards may generalize better to new situations.</p><p>? Gamer Median Atari scores are commonly normalized based on a random policy and a professional gamer, averaged over seeds, and the median over tasks is reported <ref type="bibr">(Mnih et al., 2015;</ref><ref type="bibr">2016)</ref>. However, if almost half of the scores would be zero, the median would not be affected.</p><p>Thus, we argue that median scores are not reflective of the robustness of an algorithm and results in wasted computational resources for games that will not affect the score. ? Gamer Mean Compared to the task median, the task mean considers all tasks. However, the gamer performed poorly on a small number of games, such as Crazy Climber, James Bond, and Video Pinball. This makes it easy for algorithms to achieve a high normalized score on these few games, which then dominate the task mean so it is not informative of overall performance. ? Record Mean Instead of normalizing based on the professional gamer, Toromanoff et al.</p><p>(2019) suggest to normalize based on the registered human world record of each game. This partially addresses the outlier problem but the mean is still dominated by games where the algorithms easily achieve superhuman performance. ? Clipped Record Mean To overcome these limitations, we recommend normalizing by the human world record and then clipping the scores to not exceed a value of 1, so that performance above the record does not further increase the score. The result is a robust measure of algorithm performance on the Atari suite that considers performance across all games.</p><p>From <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="table">Table 1</ref>, we see that the different aggregation approaches let us examine agent performance from different angles. Interestingly, Rainbow clearly outperforms IQN in the first aggregation method but IQN clearly outperforms Rainbow in the remaining setups. DreamerV2 outperforms the model-free agents in all four metrics, with the largest margin in record normalized mean performance. Despite this, we recommend clipped record normalized mean as the most meaningful aggregation method, as it considers all tasks to a similar degree without being dominated by a small number of outlier scores. In <ref type="table">Table 1</ref>, we also include DreamerV2 with schedules that anneal the actor entropy loss scale and actor gradient mixing over the course of training, which further increases the gamer median score of DreamerV2.</p><p>Individual games The scores on individual Atari games at 200M environment steps are included in <ref type="table">Table K</ref>.1, alongside the model-free algorithms and the baselines of random play, human gamer, and human world record. We filled in reasonable values for the 2 out of 55 games that have no registered world record. <ref type="figure">Figure E</ref>.1 compares the score differences between DreamerV2 and each model-free algorithm for the individual games. DreamerV2 achieves comparable or higher performance on most games except for Video Pinball. We hypothesize that the reconstruction loss of the world model does not encourage learning a meaningful latent representation because the most important object in the game, the ball, occupies only a single pixel. One the other hand, DreamerV2 achieves the strongest improvements over the model-free agents on the games James Bond, Up N Down, and Assault.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ABLATION STUDY</head><p>To understand which ingredients of DreamerV2 are responsible for its success, we conduct an extensive ablation study. We compare equipping the world model with categorical latents, as in DreamerV2, to Gaussian latents, as in DreamerV1. Moreover, we study the importance of KL balancing. Finally, we investigate the importance of gradients from image reconstruction and reward prediction for learning the model representations, by stopping one of the two gradient signals before entering the model states. The results of the ablation study are summarized in <ref type="figure" target="#fig_3">Figure 5</ref> and <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Refer to the appendix for the score curves of the individual tasks.</p><p>Categorical latents Categorical latent variables outperform than Gaussian latent variables on 42 tasks, achieve lower performance on 8 tasks, and are tied on 5 tasks. We define a tie as being within 5% of another. While we do not know the reason why the categorical variables are beneficial, we state several hypotheses that can be investigated in future work:</p><p>? A categorical prior can perfectly fit the aggregate posterior, because a mixture of categoricals is again a categorical. In contrast, a Gaussian prior cannot match a mixture of Gaussian posteriors, which could make it difficult to predict multi-modal changes between one image and the next. ? The level of sparsity enforced by a vector of categorical latent variables could be beneficial for generalization. Flattening the sample from the 32 categorical with 32 classes each results in a sparse binary vector of length 1024 with 32 active bits. ? Despite common intuition, categorical variables may be easier to optimize than Gaussian variables, possibly because the straight-through gradient estimator ignores a term that would otherwise scale the gradient. This could reduce exploding and vanishing gradients. ? Categorical variables could be a better inductive bias than unimodal continuous latent variables for modeling the non-smooth aspects of Atari games, such as when entering a new room, or when collected items or defeated enemies disappear from the image.</p><p>KL balancing KL balancing outperforms the standard KL regularizer on 44 tasks, achieves lower performance on 6 tasks, and is tied on 5 tasks. Learning accurate prior dynamics of the world model is critical because it is used for imagining latent state trajectories using policy optimization. By scaling up the prior cross entropy relative to the posterior entropy, the world model is encouraged to minimize the KL by improving its prior dynamics toward the more informed posteriors, as opposed to reducing the KL by increasing the posterior entropy. KL balancing may also be beneficial for probabilistic models with learned priors beyond world models.</p><p>Model gradients Stopping the image gradients increases performance on 3 tasks, decreases performance on 51 tasks, and is tied on 1 task. The world model of DreamerV2 thus heavily relies on the learning signal provided by the high-dimensional images. Stopping the reward gradients increases performance on 15 tasks, decreases performance on 22 tasks, and is tied on 18 tasks. <ref type="figure">Figure H</ref>.1 further shows that the difference in scores is small. In contrast to MuZero, DreamerV2 thus learns general representations of the environment state from image information alone. Stopping reward gradients improved performance on a number of tasks, suggesting that the representations that are not specific to previously experienced rewards may generalize better to unseen situations.  Policy gradients Using only Reinforce gradients to optimize the policy increases performance on 18 tasks, decreases performance on 24 tasks, and is tied on 13 tasks. This shows that DreamerV2 relies mostly on Reinforce gradients to learn the policy. However, mixing Reinforce and straight-through gradients yields a substantial improvement on James Bond and Seaquest, leading to a higher gamer normalized task mean score. Using only straight-through gradients to optimize the policy increases performance on 5 tasks, decreases performance on 44 tasks, and is tied on 6 tasks. We conjecture that straight-through gradients alone are not well suited for policy optimization because of their bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Model-free Atari The majority of agents applied to the Atari benchmark have been trained using model-free algorithms. <ref type="bibr">DQN (Mnih et al., 2015)</ref> showed that deep neural network policies can be trained using Q-learning by incorporating experience replay and target networks.   <ref type="bibr">, 2017)</ref>. The sequence model is trained purely by predicting task-specific information and does not incorporate explicit representation learning using the images, as shown in <ref type="table" target="#tab_6">Table 3</ref>. MuZero shows that with significant engineering effort and a vast computational budget, planning can achieve impressive performance on several board games and deterministic Atari games. However, MuZero is not publicly available, and it would require over 2 months to train an Atari agent on one GPU. By comparison, DreamerV2 is a simple algorithm that achieves human-level performance on Atari on a single GPU in 10 days, making it reproducible for many researchers. Moreover, the advanced planning components of MuZero are complementary and could be applied to the accurate world models learned by DreamerV2. DreamerV2 leverages the additional learning signal provided by the input images, analogous to recent successes by semi-supervised image classification <ref type="bibr">(Chen et al., 2020;</ref><ref type="bibr">He et al., 2020;</ref><ref type="bibr">Grill et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>We A HUMANOID FROM PIXELS <ref type="figure">Figure</ref>  While the main experiments of this paper focus on the Atari benchmark with discrete actions, Dream-erV2 is also applicable to control tasks with continuous actions. For this, we the actor outputs a truncated normal distribution instead of a categorical distribution. To demonstrate the abilities of DreamerV2 for continuous control, we choose the challenging humanoid environment with only image inputs, shown in <ref type="figure">Figure A.</ref>1. We find that for continuous control tasks, dynamics backpropagation substantially outperforms reinforce gradients and thus set ? = 0. We also set ? = 10 ?5 and ? = 2 to further accelerate learning. We find that DreamerV2 reliably solves both the stand-up motion required at the beginning of the episode and the subsequent walking. The score is shown in <ref type="figure">Figure A</ref>  <ref type="bibr">(2019)</ref>. This suggests that the world model may help with solving sparse reward tasks, for example due to improved generalization, efficient policy optimization in the compact latent space enabling more actor critic updates, or because the reward predictor generalizes and thus smooths out the sparse rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C SUMMARY OF MODIFICATIONS</head><p>To develop DreamerV2, we used the Dreamer agent (Hafner et al., 2019) as a starting point. This subsection describes the changes that we applied to the agent to achieve high performance on the Atari benchmark, as well as the changes that were tried but not found to increase performance and thus were not not included in DreamerV2.</p><p>Summary of changes that were tried and were found to help:</p><p>? Categorical latents Using categorical latent states using straight-through gradients in the world model instead of Gaussian latents with reparameterized gradients. ? KL balancing Separately scaling the prior cross entropy and the posterior entropy in the KL loss to encourage learning an accurate temporal prior, instead of using free nats. ? Reinforce only Reinforce gradients worked substantially better for Atari than dynamics backpropagation. For continuous control, dynamics backpropagation worked substantially better. ? Model size Increasing the number of units or feature maps per layer of all model components, resulting in a change from 13M parameters to 22M parameters. ? Policy entropy Regularizing the policy entropy for exploration both in imagination and during data collection, instead of using external action noise during data collection.</p><p>Summary of changes that were tried but were found to not help substantially:</p><p>? Binary latents Using a larger number of binary latents for the world model instead of categorical latents, which could have encouraged a more disentangled representation, was worse. ? Long-term entropy Including the policy entropy into temporal-difference loss of the value function, so that the actor seeks out states with high action entropy beyond the planning horizon. ? Mixed actor gradients Combining Reinforce and dynamics backpropagation gradients for learning the actor instead of Reinforce provided marginal or no benefits. ? Scheduling Scheduling the learning rates, KL scale, actor entropy loss scale, and actor gradient mixing (from 0.1 to 0) provided marginal or no benefits. ? Layer norm Using layer normalization in the GRU that is used as part of the RSSM latent transition model, instead of no normalization, provided no or marginal benefits.</p><p>Due to the large computational requirements, a comprehensive ablation study on this list of all changes is unfortunately infeasible for us. This would require 55 tasks times 5 seeds for 10 days per change to run, resulting in over 60,000 GPU hours per change. However, we include ablations for the most important design choices in the main text of the paper.   No Straight-Through No Reinforce <ref type="figure">Figure I</ref>.1: Comparison of leveraging Reinforce gradients, straight-through gradients, or both for training the actor. While Reinforce gradients are crucial, straight-through gradients are not important for most of the tasks. Nonetheless, combining both gradients yields substantial improvements on a small number of games, most notably on Seaquest. We conjecture that straight-through gradients have low variance and thus help the agent start learning, whereas Reinforce gradients are unbiased and help converging to a better solution. No Layer Norm Random Data <ref type="figure">Figure J</ref>.1: Comparison of DreamerV2 to a version without layer norm in the GRU and to training from experience collected over time by a uniform random policy. We find that the benefit of layer norm depends on the task at hand, increasing and decreasing performance on a roughly equal number of tasks. The comparison to random data collection highlights which of the tasks require non-trivial exploration, which can help guide future work on directed exploration using world models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D HYPER PARAMETERS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Gamer normalized median score on the Atari benchmark of 55 games with sticky actions at 200M steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>1:H of compact model states up to the Algorithm 2: KL Balancing with Automatic Differentiation kl_loss = alpha * compute_kl(stop_grad(approx_posterior), prior) + (1 -alpha) *compute_kl(approx_posterior, stop_grad(prior)) Actor Critic Learning. The world model learned in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>0Figure 4 :</head><label>4</label><figDesc>Refer to the project website for videos, the source code, and training curves in JSON format. 1 Atari performance over 200M steps. See</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Clipped record normalized scores of various ablations of the DreamerV2 agent. This experiment uses a slightly earlier version of DreamerV2. The score curves for individual tasks are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Several works have extended DQN to incorporate bias correction as inDDQN (Van Hasselt et al., 2015), prioritized experience replay(Schaul et al., 2015), architectural improvements(Wang et al., 2016), and distributional value learning(Bellemare et al., 2017; Dabney et al., 2017; 2018). Besides value learning, agents based on policy gradients have targeted the Atari benchmark, such as ACER(Schulman et al., 2017a), PPO (Schulman et al., 2017a), ACKTR (Wu et al., 2017), and Reactor (Gruslys et al., 2017). Another line of work has focused on improving performance by distributing data collection, often while increasing the budget of environment steps beyond 200M (Mnih et al., 2016; Schulman et al., 2017b; Horgan et al., 2018; Kapturowski et al., 2018; Badia et al., 2020).World models Several model-based agents focus on proprioceptive inputs(Watter et al., 2015; Gal  et al., 2016; Higuera et al., 2018; Henaff et al., 2018; Chua et al., 2018; Wang et al., 2019; Wang  and Ba, 2019), model images without using them for planning(Oh et al., 2015; Krishnan et al.,  2015; Karl et al., 2016; Chiappa et al., 2017;<ref type="bibr" target="#b0">Babaeizadeh et al., 2017;</ref> Gemici et al., 2017; Denton  and Fergus, 2018; Buesing et al., 2018; Doerr et al., 2018; Gregor and Besse, 2018), or combine the benefits of model-based and model-free approaches(Kalweit and Boedecker, 2017; Nagabandi  et al., 2017; Weber et al., 2017; Kurutach et al., 2018; Buckman et al., 2018; Ha and Schmidhuber,  2018; Wayne et al., 2018; Igl et al., 2018; Srinivas et al., 2018; Lee et al., 2019). Risi and Stanley (2019) optimize discrete latents using evolutionary search. Parmas et al. (2019) combine reinforce and reparameterization gradients. Most world model agents with image inputs have thus far been limited to relatively simple control tasks(Watter et al., 2015; Ebert et al., 2017; Ha and Schmidhuber,  2018; Hafner et al., 2018; Zhang et al., 2019; Hafner et al., 2019). We explain the two model-based approaches that were applied to Atari in detail below.SimPLe The SimPLe agent(Kaiser et al., 2019)  learns a video prediction model in pixel-space and uses its predictions to train a PPO agent(Schulman et al., 2017a), as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>A.1: Behavior learned by DreamerV2 on the Humanoid Walk task from pixel inputs only. The task is provided by the DeepMind Control Suite and uses a continuous action space with 21 dimensions. The frames show the agent inputs. Performance on the humanoid walking task from only pixel inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>.2. To the best of our knowledge, this constitutes the first published result of solving the humanoid environment from only pixel inputs. B MONTEZUMA'S REVENGE Figure B.1: Behavior learned by DreamerV2 on the Atari game Montezuma's Revenge, that poses a hard exploration challenge. Without any explicit exploration mechanism, DreamerV2 reaches about the same performance as the exploration method ICM. Performance on the Atari game Montezuma's Revenge. While our main experiments use the same hyper parameters across all tasks, we find that DreamerV2 achieves higher performance on Montezuma's Revenge by using a lower discount factor of ? = 0.99, possibly to stabilize value learning under sparse rewards. Figure B.2 shows the resulting performance, with all other hyper parameters left at their defaults. DreamerV2 outperforms existing modelfree approaches on the hard-exploration game Montezuma's Revenge and matches the performance of the explicit exploration algorithm ICM (Pathak et al., 2017) that was applied on top of Rainbow by Taiga et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FFigure</head><label></label><figDesc>Figure E.1: Atari agent comparison. The bars show the difference in gamer normalized scores at 200M steps. DreamerV2 outperforms the four model-free algorithms IQN, Rainbow, C51, and DQN while learning behaviors purely by planning within a separately learned world model. DreamerV2 achieves higher or similar performance on all tasks besides Video Pinball, where we hypothesize that the reconstruction loss does not focus on the ball that makes up only one pixel on the screen. F.1: Comparison of DreamerV2 to the top model-free RL methods IQN and Rainbow. The curves show mean and standard deviation over 5 seeds. IQN and Rainbow additionally average each point over 10 evaluation episodes, explaining the smoother curves. DreamerV2 outperforms IQN and Rainbow in all four aggregated scores. While IQN and Rainbow tend to succeed on the same tasks, DreamerV2 shows a different performance profile.Figure G.1: Comparison of DreamerV2, Gaussian instead of categorical latent variables, and no KL balancing. The ablation experiments use a slightly earlier version of the agent. The curves show mean and standard deviation across two seeds. Categorical latent variables and KL balancing both substantially improve performance across many of the tasks. The importance of the two techniques is reflected in all four aggregated scores.Figure H.1: Comparison of leveraging image prediction, reward prediction, or both for learning the model representations.While image gradients are crucial, reward gradients are not necessary for our world model to succeed and their gradients can be stopped. Representations learned purely from images are not biased toward previously encountered rewards and outperform reward-specific representations on a number of tasks, suggesting that they may generalize better to unseen situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>with 200M environment steps, action repeat of 4, a time limit of 108,000 steps per episode that correspond to 30 minutes of game play, no access to life information, full action space, and sticky actions. Because the world model integrates information over time, DreamerV2 does not use frame stacking. The experiments use a single-task setup where a separate agent is trained for each game. Moreover, each agent uses only a single environment instance. We compare the algorithms based on both human gamer and human world record normalization(Toromanoff et al., 2019).Model-free baselinesWe compare the learning curves and final scores of DreamerV2 to four model-free algorithms, IQN (Dabney et al., 2018), Rainbow (Hessel et al., 2018), C51 (Bellemare et al., 2017), and DQN (Mnih et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table K.1. There are different approaches for aggregating the scores across the 55 games and we show that this choice can have a substantial impact on the relative performance between algorithms. To extensively compare DreamerV2 to the model-free algorithms, we consider the following four aggregation approaches:</figDesc><table><row><cell>Agent</cell><cell cols="4">Gamer Median Gamer Mean Record Mean Clipped Record Mean</cell></row><row><cell>DreamerV2</cell><cell>2.15</cell><cell>11.33</cell><cell>0.44</cell><cell>0.28</cell></row><row><cell>DreamerV2 (schedules)</cell><cell>2.64</cell><cell>10.45</cell><cell>0.43</cell><cell>0.28</cell></row><row><cell>IQN</cell><cell>1.29</cell><cell>8.85</cell><cell>0.21</cell><cell>0.21</cell></row><row><cell>Rainbow</cell><cell>1.47</cell><cell>9.12</cell><cell>0.17</cell><cell>0.17</cell></row><row><cell>C51</cell><cell>1.09</cell><cell>7.70</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell>DQN</cell><cell>0.65</cell><cell>2.84</cell><cell>0.12</cell><cell>0.12</cell></row></table><note>Table 1: Atari performance at 200M steps. The scores of the 55 games are aggregated using the four different protocols described in Section 3. To overcome limitations of the previous metrics, we recommend the task mean of clipped record normalized scores as a robust measure of algorithm performance, shown in the right-most column. DreamerV2 outperforms previous single-GPU agents across all metrics. The baseline scores are taken from Dopamine Baselines (Castro et al., 2018).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablations to DreamerV2 measured by their Atari performance at 200M frames, sorted by the last column. The this experiment uses a slightly earlier version of DreamerV2 compared toTable 1. Each ablation only removes one part of the DreamerV2 agent. Discrete latent variables and KL balancing substantially contribute to the success of DreamerV2. Moreover, the world model relies on image gradients to learn general representations that lead to successful behaviors, even if the representations are not specifically learned for predicting past rewards.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Conceptual comparison of recent RL algorithms that leverage planning with a learned model. DreamerV2 and SimPLe learn complete models of the environment by leveraging the learning signal provided by the image inputs, while MuZero learns its model through value gradients that are specific to an individual task. The Monte-Carlo tree search used by MuZero is effective but adds complexity and is challenging to parallelize. This component is orthogonal to the world model proposed here.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>The model directly predicts each frame from the previous four frames and receives an additional discrete latent variable as input. The authors evaluate SimPLe on a subset of Atari games for 400k and 2M environment steps, after which they report diminishing returns. Some recent model-free methodsMuZero The MuZero agent (Schrittwieser et al., 2019) learns a sequence model of rewards and values (Oh et al., 2017) to solve reinforcement learning tasks via Monte-Carlo Tree Search (MCTS; Coulom, 2006; Silver et al.</figDesc><table /><note>have followed the comparison at 400k steps (Srinivas et al., 2020; Kostrikov et al., 2020). However, the highest performance achieved in this data-efficient regime is a gamer normalized median score of 0.28 (Kostrikov et al., 2020) that is far from human-level performance. Instead, we focus on the well-established and competitive evaluation after 200M frames, where many successful model-free algorithms are available for comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>present DreamerV2, a model-based agent that achieves human-level performance on the Atari 200M benchmark by learning behaviors purely from the latent-space predictions of a separately trained world model. Using a single GPU and a single environment instance, DreamerV2 outperforms top model-free single-GPU agents Rainbow and IQN using the same computational budget and training time. To develop DreamerV2, we apply several small modifications to the Dreamer agent(Hafner et al., 2019). We confirm experimentally that learning a categorical latent space and using KL balancing improves the performance of the agent. Moreover, we find the DreamerV2 relies on image information for learning generally useful representations -its performance is not impacted by whether the representations are especially learned for predicting rewards.DreamerV2 serves as proof of concept, showing that model-based RL can outperform top model-free algorithms on the most competitive RL benchmarks, despite the years of research and engineering effort that modern model-free agents rest upon. Beyond achieving strong performance on individual tasks, world models open avenues for efficient transfer and multi-task learning, sample-efficient learning on physical robots, and global exploration based on uncertainty estimates.F Ebert, C Finn, AX Lee, S Levine. Self-Supervised Visual Planning With Temporal Skip Connections. ArXiv Preprint ArXiv:1710.05268, 2017. M Fortunato, MG Azar, B Piot, J Menick, I Osband, A Graves, V Mnih, R Munos, D Hassabis, O Pietquin, et al. Noisy Networks for Exploration. ArXiv Preprint ArXiv:1706.10295, 2017. Karl, M Soelch, J Bayer, P van der Smagt. Deep Variational Bayes Filters: Unsupervised Learning of State Space Models From Raw Data. ArXiv Preprint ArXiv:1605.06432, 2016. DP Kingma J Ba. Adam: A Method for Stochastic Optimization. ArXiv Preprint ArXiv:1412.6980, 2014. DP Kingma M Welling. Auto-Encoding Variational Bayes. ArXiv Preprint ArXiv:1312.6114, 2013.</figDesc><table><row><cell>Y Gal, R McAllister, CE Rasmussen. Improving Pilco With Bayesian Neural Network Dynamics</cell></row><row><cell>Models. Data-Efficient Machine Learning Workshop, ICML, 2016.</cell></row><row><cell>M Gemici, CC Hung, A Santoro, G Wayne, S Mohamed, DJ Rezende, D Amos, T Lillicrap.</cell></row><row><cell>Generative Temporal Models With Memory. ArXiv Preprint ArXiv:1702.04649, 2017.</cell></row><row><cell>K Gregor F Besse. Temporal Difference Variational Auto-Encoder. ArXiv Preprint ArXiv:1806.03107,</cell></row><row><cell>2018.</cell></row><row><cell>JB Grill, F Strub, F Altch?, C Tallec, PH Richemond, E Buchatskaya, C Doersch, BA Pires, ZD Guo,</cell></row><row><cell>MG Azar, et al. Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. ArXiv</cell></row><row><cell>Preprint ArXiv:2006.07733, 2020.</cell></row><row><cell>ArXiv Preprint ArXiv:1705.07177, 2018.</cell></row><row><cell>M Hessel, J Modayil, H Van Hasselt, T Schaul, G Ostrovski, W Dabney, D Horgan, B Piot, M Azar,</cell></row><row><cell>D Silver. Rainbow: Combining Improvements in Deep Reinforcement Learning. Thirty-Second</cell></row><row><cell>AAAI Conference on Artificial Intelligence, 2018.</cell></row><row><cell>I Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner. Beta-</cell></row><row><cell>Vae: Learning Basic Visual Concepts With a Constrained Variational Framework. International</cell></row><row><cell>Conference on Learning Representations, 2016.</cell></row><row><cell>JCG Higuera, D Meger, G Dudek. Synthesizing Neural Network Controllers With Probabilistic</cell></row><row><cell>Model Based Reinforcement Learning. ArXiv Preprint ArXiv:1803.02291, 2018.</cell></row><row><cell>D Horgan, J Quan, D Budden, G Barth-Maron, M Hessel, H Van Hasselt, D Silver. Distributed</cell></row><row><cell>Prioritized Experience Replay. ArXiv Preprint ArXiv:1803.00933, 2018.</cell></row><row><cell>M Igl, L Zintgraf, TA Le, F Wood, S Whiteson. Deep Variational Reinforcement Learning for</cell></row><row><cell>Pomdps. ArXiv Preprint ArXiv:1806.02426, 2018.</cell></row><row><cell>L Kaiser, M Babaeizadeh, P Milos, B Osinski, RH Campbell, K Czechowski, D Erhan, C Finn,</cell></row><row><cell>P Kozakowski, S Levine, et al. Model-Based Reinforcement Learning for Atari. ArXiv Preprint</cell></row><row><cell>ArXiv:1903.00374, 2019.</cell></row></table><note>A Gruslys, W Dabney, MG Azar, B Piot, M Bellemare, R Munos. The Reactor: A Fast and Sample- Efficient Actor-Critic Agent for Reinforcement Learning. ArXiv Preprint ArXiv:1704.04651, 2017.D Ha J Schmidhuber. World Models. ArXiv Preprint ArXiv:1803.10122, 2018.D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson. Learning Latent Dynamics for Planning From Pixels. ArXiv Preprint ArXiv:1811.04551, 2018.D Hafner, T Lillicrap, J Ba, M Norouzi. Dream to Control: Learning Behaviors by Latent Imagination. ArXiv Preprint ArXiv:1912.01603, 2019.K He, H Fan, Y Wu, S Xie, R Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.M Henaff, WF Whitney, Y LeCun. Model-Based Planning With Discrete and Continuous Actions.G Kalweit J Boedecker. Uncertainty-Driven Imagination for Continuous Deep Reinforcement Learning. Conference on Robot Learning, 2017.S Kapturowski, G Ostrovski, J Quan, R Munos, W Dabney. Recurrent Experience Replay in Distributed Reinforcement Learning. International Conference on Learning Representations, 2018.M</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Table D.1: Atari hyper parameters of DreamerV2. When tuning the agent for a new task, we recommend searching over the KL loss scale ? ? {0.1, 0.3, 1, 3}, actor entropy loss scale ? ? {3 ? 10 ?5 , 10 ?4 , 3 ? 10 ?4 , 10 ?3 }, and the discount factor ? ? {0.99, 0.999}. The training frequency update should be increased when aiming for higher data-efficiency.</figDesc><table><row><cell cols="2">E AGENT COMPARISON</cell></row><row><cell>10 100</cell><cell>DreamerV2 vs IQN</cell></row><row><cell>0</cell><cell></cell></row><row><cell>10</cell><cell></cell></row><row><cell>100</cell><cell>James Bond Up N Down Krull Gopher Demon Attack Assault Road Runner Time Pilot Breakout Asterix Phoenix Qbert Atlantis Zaxxon Ice Hockey Wizard Of Wor Yars Revenge Name This Game Kung Fu Master Robotank Crazy Climber Asteroids Frostbite Centipede Gravitar Beam Rider Kangaroo Fishing Derby Skiing Amidar Bank Heist Tutankham Riverraid Bowling Ms Pacman Berzerk Pitfall Freeway Pong Battle Zone Private Eye Solaris Montezuma Rev. Alien Seaquest Boxing Hero Tennis Enduro Chopper Com. Venture Space Invaders Double Dunk Star Gunner Video Pinball</cell></row><row><cell>10 100</cell><cell>DreamerV2 vs Rainbow</cell></row><row><cell>0</cell><cell></cell></row><row><cell>10</cell><cell></cell></row><row><cell>100</cell><cell>James Bond Up N Down Krull Assault Gopher Demon Attack Road Runner Time Pilot Atlantis Breakout Asterix Phoenix Qbert Zaxxon Ice Hockey Yars Revenge Kung Fu Master Skiing Robotank Wizard Of Wor Name This Game Tennis Asteroids Gravitar Beam Rider Frostbite Crazy Climber Centipede Kangaroo Fishing Derby Ms Pacman Tutankham Alien Bank Heist Bowling Amidar Battle Zone Pitfall Freeway Berzerk Pong Seaquest Solaris Montezuma Rev. Private Eye Riverraid Boxing Enduro Hero Space Invaders Venture Chopper Com. Double Dunk Star Gunner Video Pinball</cell></row><row><cell>10 100</cell><cell>DreamerV2 vs C51</cell></row><row><cell>0</cell><cell></cell></row><row><cell>10</cell><cell></cell></row><row><cell>100</cell><cell>James Bond Up N Down Assault Demon Attack Krull Gopher Road Runner Time Pilot Atlantis Double Dunk Asterix Phoenix Qbert Zaxxon Breakout Yars Revenge Ice Hockey Wizard Of Wor Kangaroo Robotank Kung Fu Master Frostbite Crazy Climber Fishing Derby Skiing Gravitar Boxing Asteroids Beam Rider Amidar Bank Heist Enduro Centipede Battle Zone Name This Game Ms Pacman Alien Riverraid Tutankham Bowling Berzerk Pong Pitfall Freeway Private Eye Solaris Montezuma Rev. Hero Chopper Com. Tennis Seaquest Space Invaders Venture Star Gunner Video Pinball</cell></row><row><cell></cell><cell>James Bond Up N Down Assault Demon Attack Krull Gopher Road Runner Time Pilot Double Dunk Asterix Atlantis Breakout Phoenix Qbert Zaxxon Ice Hockey Frostbite Yars Revenge Wizard Of Wor Crazy Climber Robotank Kung Fu Master Name This Game Fishing Derby Enduro Boxing Gravitar Tutankham Tennis Centipede Asteroids Kangaroo Amidar Beam Rider Bank Heist Battle Zone Skiing Space Invaders Ms Pacman Riverraid Freeway Alien Hero Seaquest Bowling Berzerk Pong Private Eye Chopper Com. Montezuma Rev. Pitfall Venture Solaris Star Gunner Video Pinball</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://danijar.com/dreamerv2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank our anonymous reviewers for their feedback and Nick Rhinehart for an insightful discussion about the potential benefits of categorical latent variables.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table K</ref><p>.1: Atari individual scores. We select the 55 games that are common among most papers in the literature. We compare the algorithms DreamerV2, IQN, and Rainbow to the baselines of random actions, DeepMind's human gamer, and the human world record. Algorithm scores are highlighted in bold when they fall within 5% of the best algorithm. Note that these scores are already averaged across seeds, whereas any aggregated scores must be computed before averaging across seeds.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Stochastic Variational Video Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<idno>ArXiv:1710.11252</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
