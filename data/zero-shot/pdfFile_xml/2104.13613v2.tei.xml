<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptive Semantic Segmentation with Self-Supervised Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MPI for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">KU Lueven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Fink</surname></persName>
							<email>ofink@ethz.chdai</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptive Semantic Segmentation with Self-Supervised Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation for semantic segmentation aims to improve the model performance in the presence of a distribution shift between source and target domain. Leveraging the supervision from auxiliary tasks (such as depth estimation) has the potential to heal this shift because many visual tasks are closely related to each other. However, such a supervision is not always available. In this work, we leverage the guidance from self-supervised depth estimation, which is available on both domains, to bridge the domain gap. On the one hand, we propose to explicitly learn the task feature correlation to strengthen the target semantic predictions with the help of target depth estimation. On the other hand, we use the depth prediction discrepancy from source and target depth decoders to approximate the pixel-wise adaptation difficulty. The adaptation difficulty, inferred from depth, is then used to refine the target semantic segmentation pseudo-labels. The proposed method can be easily implemented into existing segmentation frameworks. We demonstrate the effectiveness of our approach on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes, on which we achieve the new stateof-the-art performance of 55.0% and 56.6%, respectively. Our code is available at https://qin.ee/corda.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of semantic segmentation requires models to assign pixel-level category labels to given scenes. While deep learning models have achieved good performance on benchmark datasets with the help of a large amount of high quality annotated training data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b47">48]</ref>, they still face the real-world challenge of the domain shift between training and test data because of the variance in illumination, appearance, viewpoints, backgrounds, etc. Unsupervised domain adaptation (UDA) can potentially heal this domain gap by aligning the domain distributions <ref type="bibr" target="#b39">[40]</ref>, or recursively refining the target pseudo-labels <ref type="bibr" target="#b54">[55]</ref>. <ref type="bibr">*</ref> The corresponding author <ref type="figure">Figure 1</ref>. We propose to use self-supervised depth estimation (green) to improve semantic segmentation performance under the unsupervised domain adaptation setup. We explicitly learn the task feature correlation (orange) between semantics and depth and use it to improve the target semantics. We use the adaptation difficulty (blue) approximated by depth prediction discrepancy of the target image from two domain-specific depth decoders to refine our target semantic pseudo-label. The proposed correlation-aware domain adaptation method can largely improve the segmentation performance in the target domain.</p><p>In recent years, motivated by the success of multi-task learning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b43">44]</ref>, auxiliary tasks (such as depth estimation) have been increasingly used to help the adaptation. As auxiliary tasks are often coupled with the semantics, they have been proved to be beneficial for the main segmentation task <ref type="bibr" target="#b18">[19]</ref>. Existing works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3]</ref> typically utilize the easyto-access depth information from a synthetic source domain to train an auxiliary depth network but do not take target depth into account because of its inaccessibility. Inspired by recent progress on self-supervised depth estimation, where depth can be trained from stereo pairs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> or video sequences <ref type="bibr" target="#b53">[54]</ref>, we propose to make use of self-supervised depth estimates for the domains (the source domain and/or target domain) on which ground-truth depth is not available.</p><p>The additional self-supervised depth estimation can facilitate us to explicitly learn the correlation between tasks to improve the final semantic segmentation performance. The learning of the correlation is motivated by the fact that the correlation between tasks is more invariant across domains than the individual modalities. As mentioned by previous works <ref type="bibr" target="#b2">[3]</ref>, sky is always faraway, roads and sideways are always flat. These domain-robust correlations between semantics and depth have the potential to largely improve the target semantic segmentation performance in the presence of a domain shift.</p><p>To this end, we propose to exploit such a correlation in two ways. On the one hand, we propose to explicitly learn the task feature correlation between depth and semantics. This is achieved by using domain-shared multi-modal distillation modules to model the interaction and complementarity between semantics and depth features. The correlation learned from the source domain can be shared and transferred to the target domain to improve target segmentation performance. On the other hand, we make use of the correlation to refine the target semantic pseudo-labels. We approximate the adaptation difficulty by calculating the discrepancy between the predictions of the domain-specific depth decoders. As depth and semantics are coupled, we make the assumption that the estimated adaptation difficulty can be transferred from depth to semantics. We propose to use this relation to guide the semantic segmentation pseudo-label refinement on the target domain. Combining the two ways of correlation exploitation leads to our proposed Correlation-Aware Domain Adaptation (CorDA) approach. We illustrate the two ways to utilize the correlation in <ref type="figure">Figure 1</ref>.</p><p>It is also worth mentioning that our strategies can be implemented easily. The self-supervised depth estimation can be learned from easy-to-access image sequences or stereo images and the proposed correlation learning module can be readily incorporated into existing UDA networks for semantic segmentation. We demonstrate the effectiveness of our proposed approach on the benchmark tasks SYNTHIAto-Cityscapes and GTA-to-Cityscapes, on which we achieve new state-of-the-art segmentation performance.</p><p>Our contributions are summarized as follows:</p><p>? We propose a novel UDA framework which effectively utilizes self-supervised depth estimation available on both domains to improve semantic segmentation.</p><p>? Specifically, we explicitly learn the correlation between modalities and share it across domains. Furthermore, we refine the semantic pseudo-labels by using the adaptation difficulty approximated by depth prediction discrepancy.</p><p>? Despite of the simplicity, our proposed approach achieves new state-of-the-art segmentation performance on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised domain adaptation Unsupervised domain adaptation (UDA) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> aims to improve the target model performance in the presence of a domain shift between the labeled source and unlabeled target domain. Many UDA methods have been proposed to alleviate the domain shift. One common motivation is to align the source and target distribution <ref type="bibr" target="#b5">[6]</ref>. This can be achieved in several different ways. AdaptSegNet <ref type="bibr" target="#b36">[37]</ref> and Advent <ref type="bibr" target="#b39">[40]</ref> alleviates the domain shift by adversarially aligning the distributions in the output space or feature space. Another popular direction is to align the input pixels of source and target images via generative adversarial networks <ref type="bibr" target="#b12">[13]</ref> or Fourier transforms <ref type="bibr" target="#b46">[47]</ref>. In recent years, especially in the field of UDA for semantic segmentation, pseudo-label refinement under a self-training frameworks has achieved competitive results. By iteratively using gradually-improving target pseudo-labels to train the network, the performance on the target domain can be further improved. Following this motivation, CBST <ref type="bibr" target="#b54">[55]</ref> improved the self-training performance by using class-specific thresholds. PyCDA <ref type="bibr" target="#b20">[21]</ref> found that including pseudo-labels in different scales can further improve model performance. <ref type="bibr" target="#b52">[53]</ref> used the uncertainty of semantic predictions to refine the pseudo-labels. Using prototypes <ref type="bibr" target="#b49">[50]</ref> to refine pseudo-labels has also shown promising results. Recently, DACS <ref type="bibr" target="#b35">[36]</ref> demonstrated strong results by combining self-training with ClassMix <ref type="bibr" target="#b24">[25]</ref>, which mixes source and target images during the training.</p><p>Use of geometric information in semantic segmentation Additional geometric information has been recently increasingly used to help learning the semantics <ref type="bibr" target="#b28">[29]</ref> because geometric and semantic information are highly correlated. In the UDA setup, there are several works which pioneered this direction. SPIGAN <ref type="bibr" target="#b18">[19]</ref> translates source images into the style of targets to reduce the domain gap. An auxiliary depth regression task is used in SPIGAN to regularize the generator, and better capture the semantics for the translated image. DADA <ref type="bibr" target="#b40">[41]</ref> uses an auxiliary depth prediction branch to predict the depth for both domains. The predictions are later fused together with semantic predictions and fed into the domain discriminator. GIO-Ada <ref type="bibr" target="#b2">[3]</ref> makes use of the depth information in both input-space translation and output-level adaptation, where a discriminator is applied on the concatenation of depth and semantic predictions. Existing works often use the additional depth information from the synthetic data in the source domain. The supervision from target geometric information is largely unexplored.</p><p>Multi-task distillation Our work is also closely related to multi-task learning (MTL) <ref type="bibr" target="#b37">[38]</ref>, where multiple tasks are predicted by a single network. Modern multi-task learning methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b38">39]</ref> aim at distilling the information from different tasks. This is often achieved by using a shared backbone network and task-specific heads. Initial task predictions are first made to learn task-specific intermediate features. These task-specific feature representations are then combined via a multi-modal distillation unit, before performing the final task predictions. Most multi-task learning works focus on the fully-supervised case where there exist no domain shift and the ground truths for all tasks are directly provided. We focus on the UDA setup where target ground truth is not provided for both main and auxiliary tasks. MTL under such a setup is understudied. Motivated by the success of these methods we modify and generalize the PAD-Net <ref type="bibr" target="#b43">[44]</ref> to capture the correlation between modalities across domains in order to facilitate the efficient joint learning of semantics and depth in the UDA setup. The idea of the multi-modality learning was also explored in other related areas such as object detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Self-supervised learning Our work is also related to self-supervised learning in a broad sense. Self-supervised learning has recently achieved strong performance in learning meaningful representations in various vision tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref>. In the UDA for classification context, self-supervised learning has been shown to be able to improve generalization ability in the target domain by learning to predict auxiliary tasks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. However, The auxiliary tasks used by these works are relatively arbitrary (such as rotation prediction) and do not exploit the correlation between the main task and auxiliary task. In this work, we exploit the possibility of using depth estimation to improve the semantic segmentation performance under the UDA framework. In contrast to works combining (semi-)supervised semantic segmentation with self-supervised depth estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref>, we explicitly deal with the challenge of the domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In the UDA setup, we are given labeled data from the source domain and unlabeled training samples from the target domain. As annotations for synthetic data are comparably easy to generate, labeled synthetic data is often used as source S and unlabeled target data is treated as target T . Formally, in the source domain, we have</p><formula xml:id="formula_0">D S = {(x S 1 , y S 1 , d S 1 ), . . . , (x S n , y S n , d S n )} as the set of la- beled training data, where x S i is the i-th sample, y S i</formula><p>is the corresponding label for semantic segmentation, d S i is the label for an optional auxiliary task (such as depth estimation), and n is the total number of labeled source samples. The optional auxiliary task is not used in the classic UDA training setup. Similarly, target real training data can be represented as</p><formula xml:id="formula_1">D T = {(x T 1 , d T 1 ), . . . , (x T m d T m )} where x T i</formula><p>is the i-th unlabeled training sample, d T i is the label for an optional auxiliary task, and m is the number of unlabeled samples. The task of UDA for semantic segmentation is to train a model which performs well on test images</p><formula xml:id="formula_2">D test = {x test 1 , . . . , x test t</formula><p>} from the target domain T . We consider depth estimation as the auxiliary task. Precise depth information is often not provided in the real-world target dataset. Existing works therefore often only use the source depth information from the virtual environment. Unfortunately, this limits the possibility of learning the comprehensive correlation between modalities and domains. To overcome this limitation, in this work, we propose to use self-supervised depth estimates as pseudo ground truth on the target domain d T i . The use of selfsupervised depth enables us to exploit the correlation between modalities to further improve the UDA performance as shown in <ref type="figure">Figure 1</ref>. First, we learn the domain-robust task feature correlation between semantics and depth features on the source domain and transfer it with the target domain as described in Section 3.2. In our implementation, in order to avoid a two-stage training, we used a continuous transfer by having a shared module during the learning process. Second, we approximate the adaptation difficulty by calculating the discrepancy between the predictions of source and target depth decoder. As depth and semantics are naturally coupled, we use the adaptation difficulty to refine the semantic pseudo labels as described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Supervised Depth Estimation</head><p>The self-supervised depth estimation can be trained from stereo pairs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> or video sequences <ref type="bibr" target="#b53">[54]</ref>. Both are relatively easy to obtain and, therefore, often already part of real-world datasets. By using off-the-shelf solutions such as Semi-Global Matching <ref type="bibr" target="#b11">[12]</ref> and MonoDepth2 <ref type="bibr" target="#b8">[9]</ref>, pseudo depth information can be easily generated. The generated depth is used as the fixed pseudo depth ground truth for the training of our proposed model. A detailed explanation on the generation process is provided in Section 4 and more extensively in the supplementary. If depth information is unavailable in the source domain, such as for GTA5 <ref type="bibr" target="#b29">[30]</ref>, the same generation procedure can be applied as well. These additional depth estimates can now facilitate the learning of correlation between semantics and depth in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Correlation-Aware Architecture</head><p>In order to exploit the domain-robust correlation between the depth and the semantic information, we adapt recent developments of multi-task learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b38">39]</ref> to our correlation-aware UDA framework. <ref type="figure">Figure 2</ref> depicts the framework of the proposed approach. Both domains share a common convolutional backbone network to encode images into deep features. This can be achieved by any modern deep CNN model. Then, domain-specific depth heads and a shared semantic prediction head are used to generate intermediate multi-modal predictions. In the next step, a domain-shared task feature correlation module is used to explicitly learn the correlation between depth and semantics and incorporate the complementary information from the other task to strengthen final segmentation predictions. <ref type="figure">Figure 2</ref>. The network architecture of our proposed Correlation-Aware Domain Adaptation (CorDA), in which we combine the proposed task feature correlation module and the pseudo-label refinement based on adaptation difficulty transfer. The semantic and depth features are processed by the domain-shared feature correlation module to explicitly learn the domain-robust correlation between them and provide complementary information for the other modality. In addition, as shown in the right-most side of the figure, during the training process, the semantic pseudo-labels are re-weighted based on the adaptation difficulty approximated by the depth prediction discrepancy.</p><p>Domain-specific intermediate predictions Intermediate predictions are first generated to enable the later learning of the correlation between semantic and depth information. By applying convolution bottlenecks on the backbone features, we acquire semantic features and depth features of 256 channels. Semantic and depth prediction heads are applied to provide the intermediate predictions. We use two separate depth heads for source and target domains as depth supervision from both domains are available with the help of self-supervised depth estimation. Since there is no strong supervision available for target semantic predictions, we share the semantic heads for both domains. Predictions are re-scaled to the input resolution by bilinear interpolation. Following <ref type="bibr" target="#b40">[41]</ref>, we use the reverse Huber loss for depth:</p><formula xml:id="formula_3">berHu(e z ) = |e z | , if |e z | ? c, e 2 z +c 2 2c otherwise,<label>(1)</label></formula><p>where c is the threshold which is set to <ref type="bibr">1 5</ref> of the maximum depth difference. We use the cross entropy loss for the semantic loss calculation. This leads us to the following loss components for the intermediate prediction losses:  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref>, inverse depth is adopted for the depth learning losses. For all our experiments, the ground-truth depth is either from the simulator or from pre-calculated depth estimations. The tilde inL indicates that this is the loss function for the intermediate predictions.</p><formula xml:id="formula_4">L S seg (x S , y S ) = ? H h=1 W w=1 y S log? S init ,<label>(2)</label></formula><formula xml:id="formula_5">L T seg (x T , y T ) = ? H h=1 W w=1 w? T log? T init ,<label>(3)</label></formula><formula xml:id="formula_6">L S depth (x S , d S ) = H h=1 W w=1 berHu(d S init ? d S ),<label>(4)</label></formula><formula xml:id="formula_7">L T depth (x T , d T ) = H h=1 W w=1 berHu(d T init ? d T ),<label>(5)</label></formula><p>Shared task feature correlation module The semantic and depth features from the last step are then fed into a domain-shared task feature correlation module to learn the correlation between semantics and depth. This is achieved by incorporating two spatial attentions, which capture the mutual relationship between depth and semantics. The design of the feature correlation module is largely inspired by works in the field of multi-task learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>, where similar attention modules were used to help the joint learning of multiple tasks. Existing works extract the correlation from multiples scales and different modalities. We build our module based on PAD-Net <ref type="bibr" target="#b43">[44]</ref> because of its simplicity and effectiveness. Specifically, given the semantic features F seg and the depth features F depth , the distilled features F o seg , F o depth , are calculated by:</p><formula xml:id="formula_8">F o seg = F seg + (W 1 d ? F depth ) ?(W 2 d ? F depth ) (6) F o depth = F depth + (W 1 s ? F seg ) ?(W 2 s ? F seg ),<label>(7)</label></formula><p>where ? denotes the convolution operation and denotes the element-wise multiplication. ? is the sigmoid function for the normalization of the attention map. W denotes the learnable weights for the convolution. We notice that this self-attention variant performs better in our experiments. The benefits of the task feature correlation module are twofold. On the one hand, the attention captures the complementary information from the other modality and ignores the irrelevant information. Thus, we explicitly learn the correlation between the two modalities. On the other hand, by designing to share the attentions from the source domain to the target domain, we aim to learn a more robust and more generalizable correlation.</p><p>Domain-specific final decoders Given the distilled semantic features F o seg and the distilled depth features F o depth , we can now provide the final predictions for the entire network. Similar to the intermediate predictions, we use a shared semantic decoder for both domains to perform final predictions using F o seg as input. The depth decoders of source and target domain remain independent. The overall loss function for the entire network, thus, results in:</p><formula xml:id="formula_9">L =L S seg +L T seg + ? SLS depth + ? TLT depth + L S seg + L T seg + ? S L S depth + ? T L T depth ,<label>(8)</label></formula><p>where ? S and ? T are the hyperparameters for the depth loss. The loss functions for the final predictions have the same formulations as their intermediate counterparts.</p><p>Summary of the architecture The architecture of the proposed framework, thus, contains domain-specific depth decoders and a shared task feature correlation module to explicitly learn the correlation between the depth and semantics. The final semantic predictions? T for the target images can then be generated by the semantic decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pseudo-Label Refinement with Adaptation Difficulty</head><p>As target semantic supervision is unavailable in the UDA setup, it is common for self-training approaches <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21]</ref> to use target semantic predictions? T as semantic pseudolabels? T for the training. However, pseudo-labels can be noisy and over-confident <ref type="bibr" target="#b55">[56]</ref>, thus it is important to filter out unreliable ones. Existing works refine pseudo-labels by exploiting prediction uncertainty <ref type="bibr" target="#b52">[53]</ref> and class-wise confidence <ref type="bibr" target="#b55">[56]</ref>. Our method is complementary to them. We leverage the availability of self-supervised depth and task correlations to refine the semantic pseudo-labels.</p><p>With domain-specific depth decoders, we can approximate the difficulty of domain adaptation by calculating the discrepancy between the predictions of source and target depth decoders on the target image. As depth and semantics are naturally coupled, we assume that the estimated adaptation difficulty can be transferred from depth to semantics. We exploit this relation to refine the semantic pseudo labels.</p><p>Specifically, given a target image input x T , we calculate the final depth predictions of the target image using both the source depth decoder f S and the target depth decoder f T . We compare the pixel-wise prediction discrepancy between the depth estimated by both the source and target decoder. The discrepancy is then used as an indicator for the pixelwise adaptation difficulty. We hypothesize that the adaptation difficulty can be transferred from depth to semantics because of the coupled relationship of semantic and depth. Pixels where the depth prediction discrepancy is high indicate a larger domain gap for this region, thus, should be assigned a lower weight for the semantic pseudo-labels. The following equation is used to assign weights for the semantic pseudo-labels on the target domain:</p><formula xml:id="formula_10">? = abs(f S (x T ) ? f T (x T )) w = relu(1 ? ? d T ),<label>(9)</label></formula><p>where d T is the pseudo ground truth of the target depth and the pixel-wise weight w is applied on target semantic pseudo labels inL T seg and L T seg , as shown in Equation 3. The prediction difference is normalized by the pseudo ground truth d T in order to make the prediction difference more comparable across pixels with different distances with respect to the camera. The pixel-wise weight w is designed to be in the range 0 to 1. If the source and target depth decoders give identical predictions for a pixel in the target image, this indicates that the domain gap in this region is very small, and the predicted semantic pseudo-label is likely to be correct. Thus, we assign 1 to the semantic pseudo-label for this pixel. If the depth prediction difference is large, then the domain gap is large, thus, it is hard to predict the semantics correctly. In this case, the weight w becomes closer to 0, and the semantic pseudo-label for this region has little contribution to the semantic training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Summary</head><p>Combining the proposed correlation-aware architecture with the task feature correlation transfer and the pseudolabel refinement with adaptation difficulty leads to our Correlation-Aware Domain Adaptation (CorDA) framework. As shown in <ref type="figure">Figure 2</ref>, we use a correlation-aware architecture, which incorporates a shared feature correlation module and domain-specific depth decoders. During the entire training process, the semantic pseudo-labels are re-weighted using the pixel-wise domain gap indicator introduced in our depth-guided difficulty refinement.</p><p>The proposed method can be readily integrated into any UDA framework for semantic segmentation. To show that our method is complementary to existing frameworks, we use DACS <ref type="bibr" target="#b35">[36]</ref> as our base framework as it offers a simple but strong baseline. DACS mixes source and target images and uses a fixed threshold to filter the pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed approach on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes.</p><p>Cityscapes The Cityscapes dataset <ref type="bibr" target="#b4">[5]</ref> is a real-world dataset containing driving scenarios of European cities. It contains fine semantic segmentations with 19 classes and consists of 2,975 training images as well as 500 validation images. Following the experimental protocol used by <ref type="bibr" target="#b2">[3]</ref>, the original images which have a fixed spatial resolution of 2048 ? 1024 pixels are down-sized to 1024 ? 512. We use the publicly-available stereo depth estimation from <ref type="bibr" target="#b33">[34]</ref>. These depth estimations were originally generated using the Semi-Global Matching <ref type="bibr" target="#b11">[12]</ref> with stereoscopic inpainting <ref type="bibr" target="#b41">[42]</ref>. In the ablation study, we also evaluate the possibility of using self-supervised monocular depth estimation as pseudo ground truth for Cityscapes. It is provided by a Monodepth2 <ref type="bibr" target="#b8">[9]</ref> model trained on the Cityscapes training image sequences. We use the Cityscapes training set without labels as target domain for the adaptation and report our results on the validation set. We always report the Intersection Over Union (IoU) for per class performance as well as the mean Intersection over Union (mIoU) over all classes.</p><p>SYNTHIA The SYNTHIA dataset <ref type="bibr" target="#b30">[31]</ref> is a synthetic dataset of road scenes collected from a virtual environment. Following the setup used by <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3]</ref>, we adopt the SYNTHIA-RAND-CITYSCAPES split using Cityscapesstyle annotations (16 overlapping classes). The dataset consists of 9,400 synthetic images. We use the simulated depth provided by the dataset as our source depth supervision.</p><p>GTA5 The GTA5 dataset <ref type="bibr" target="#b29">[30]</ref> is generated from a game environment. It contains 24,966 images which are labeled using Cityscapes-style annotation (19 classes). We use Monodepth2 <ref type="bibr" target="#b8">[9]</ref> to generate the depth information for the GTA5 dataset. The monodepth2 model is trained solely on the image sequences from GTA5 dataset. We will release our monocular depth estimation datasets. <ref type="table">Table 1</ref>. Ablation study of different components in our proposed framework on the SYNTHIA-to-Cityscapes adaptation task. Stereo depth estimation is used for the target data. mIoU* denotes performance over 13 classes excluding wall, fence, and pole as it is also widely used in the literature. Implementation details For our correlation-aware architecture, we adopt ResNet-101 <ref type="bibr" target="#b10">[11]</ref> as the shared encoder and DeepLabv2 <ref type="bibr" target="#b0">[1]</ref> as task decoder. The semantic and depth feature bottlenecks are residual blocks with two 3x3 and four 1x1 convolution operations. Our training procedure is based on DACS <ref type="bibr" target="#b35">[36]</ref> and enhanced by our pseudo-label refinement with adaptation difficulty. Following <ref type="bibr" target="#b35">[36]</ref>, batch size is set as 2. The learning rate starts from 2.5 ? 10 ?4 and follows a polynomial decay with exponent of 0.9. Images from the source domain are scaled to 1280 ? 760. The resolution of 1024 ? 512 is used for the target domain as input for training. Random crops of size 512 ? 512 are used as an additional augmentation. We set the weights for source depth loss to ? S = 0.01 and target depth loss weight to ? T = 0.001. For the GTA-to-Cityscapes task, we apply the initial semantic decoder after the ResNet features, and use the initial semantic prediction as pseudo labels for the first 10% training iterations. This helps the model to learn in early stages. All models are trained for 250,000 iterations. We report our performance at the end of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on SYNTHIA? ?Cityscapes</head><p>We first evaluate the effectiveness of the proposed model on the SYNTHIA-to-Cityscapes task. We report the mIoU performance on the common 16 classes.</p><p>Ablation study: individual modules The main contribution of our proposed framework is to utilize the selfsupervised depth to effectively learn the shared correlation between tasks and domains. To validate our motivation, we conduct an ablation study on each of these components. We first include DACS as a strong baseline, which is already able to capture semantics relatively well without the help of geometric information. We then additionally use the selfsupervised depth and add the depth prediction auxiliary task (without using the task feature correlation module) for both source and target depth to check whether a naive approach can provide improvement to the DACS baseline. Then, we evaluate our proposed domain-shared task feature correlation module to verify the contribution of explicitly learning the correlation between modalities. Finally, we add our pseudo-label refinement based on the adaptation difficulty. This final setup corresponds to our proposed framework.</p><p>As shown in <ref type="table">Table 1</ref>, directly using source and target depth information as auxiliary tasks (denoted as Sim-pleAux in the <ref type="table">Table)</ref> without making any modifications on the architecture and training process can already lead to a small improvement over the DACS baseline and gives us 49.6% mIoU. This verifies the common belief that additional depth information can be helpful for learning semantics. However, the improvement is not significant, most likely because this naive way of simultaneously learning two tasks can not guarantee a good generalization ability for both tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>. By explicitly modelling the correlation between depth and semantics using the correlationaware architecture with task feature correlation CorDA (F), we can make better use of the depth information and significantly reduce the domain gap. This leads to an 4.6% absolute improvement, yielding 54.2% mIoU on the target domain. If we remove the correlation learning modules and keep the extra feature and semantic bottlenecks, the performance drops back to 51.7% mIoU. This clearly demonstrates the importance of learning the correlation between the two modalities. In addition, by comparing the prediction discrepancy between source and target depth decoders, we integrate our pseudo-label refinement with adaptation difficulty module into the network, which leads to our final proposed framework CorDA (FD). This gives us further 0.8% of absolute performance improvement. From <ref type="table">Table 1</ref>, we can observe that both the correlation-aware architecture with task feature correlation and pseudo-label refinement with adaptation difficulty are beneficial for improving the semantic segmentation performance. The results clearly validate contributions of each of the proposed components.</p><p>Ablation study: choice of pseudo depth ground truth As mentioned in earlier sections, the depth information used as pseudo ground truth can come from a variety of sources, such as self-supervised monocular depth estimation or stereoscopic depth estimation. In this ablation study, we compare the impact of the choice of the source of depth information and investigate the robustness of our proposed method against different types of depth estimation. We again use SYNTHIA-to-Cityscapes as our evaluation task. We change the pseudo depth ground truth of Cityscapes from the before stereoscopic estimation to monocular depth estimation from Monodepth2. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the performance of our complete model CorDA is relatively similar with the two depth options. The use of monocu- <ref type="figure">Figure 3</ref>. Semantic segmentation results on GTA-to-Cityscapes. lar depth yields 54.5% mIoU, while the stereoscopic depth yields 55.0% mIoU. Model performance with monocular depth is slightly lower because the stereo depth usually has higher estimation quality. In both cases, the performance is very competitive and much stronger than the baseline. This indicates that the proposed method is relatively robust to the choice of pseudo depth ground truth and is able to capture the correlation between semantics and depth information, regardless of whether it is a monocular or stereo estimation. We would like to highlight that for both stereo and monocular depth estimations, only stereo pairs or image sequences from the same dataset are used to train and generate the pseudo depth estimation model. As no data from external datasets is used, and stereo pairs and image sequences are relatively easy to obtain, our proposal of using self-supervised depth have the potential to be effectively realized in real-world applications.</p><p>Comparison to the state-of-the-art approaches We compare the performance of our final proposed model to state-of-the-art methods on the SYNTHIA-to-Cityscapes unsupervised domain adaptation task in <ref type="table" target="#tab_2">Table 2</ref>. By exploit- ing the supervision from self-supervised depth estimation and learning the correlation between semantics and depth, the proposed method achieves 55.0% mIoU (stereo depth) on this task. This yields a large margin of 6.7% absolute improvement compared to the previous state-of-the-art published work DACS <ref type="bibr" target="#b35">[36]</ref>. We would like to highlight that, by using either monocular or stereo depth estimations, our proposed method steadily outperforms the other approaches by a large margin. This again shows the importance of learning the correlation between semantic and depth. We additionally compare our method to four existing works which also utilizes available depth information during training. Unlike these works which use adversarial training to make use of the additional depth from source domain, we explicitly learn the correlation between modalities in both domains without any adversarial component. This makes the training more stable and exploits the correlation more effectively. As shown in the table, the proposed CorDA outperforms these methods by a large margin. Nevertheless, our method is complementary and can be potentially combined with these existing adversarial methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on GTA? ?Cityscapes</head><p>To further demonstrate the effectiveness of the proposed CorDA framework and the importance of explicitly learning the correlation between depth and semantics, we compare our method to a wide range of 12 competitive works on the GTA5-to-Cityscapes task. The experimental results are summarized in <ref type="table" target="#tab_3">Table 3</ref>. We use monocular depth estimation as pseudo depth ground truth for GTA5 (as no stereo pairs are available due to the limitation of the dataset) and stereo depth estimation for Cityscapes. The results demonstrate that our framework is robust to different sources of depth estimations and a competitive CorDA model can be successfully trained using different types of depth estimations for the two domains. Our method yields an absolute improvement of 4.5% mIoU over DACS, and achieves 56.6% mIoU. This outperforms competing methods with a significant margin. As shown by sample predictions in <ref type="figure">Figure 3</ref>, the prediction quality is largely improved on easily confusable classes such as sidewalk and road.</p><p>Choice of Pre-trained Weight To ensure a fair comparison with DACS, the same pretrained weights (Ima-geNet+COCO) was used in previous experiments. An alternative is to use ImageNet-only pretrained weights. To evaluate the impact of the pretrained weight choice on CorDA, we reran the benchmark experiments with stereo Cityscapes depth estimation using the ImageNet-only weights. In this setup, CorDA achieves 54.6% (16 classes) and 56.4% mIoU for SYNTHIA-and GTA-to-Cityscapes, respectively. This performance is very similar to the results with Im-ageNet+COCO weight, and still outperforms competing methods with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we introduced a new domain adaptation framework for semantic segmentation which effectively leverages the guidance from self-supervision of auxiliary task to bridge domain gaps. The proposed method explicitly learns the correlation between semantics and auxiliary tasks to better transfer this domain-shared knowledge to the target domain. To achieve this, a domain-shared task feature correlation module is used. We further made use of the adaptation difficulty, approximated by the prediction discrepancy from the domain depth decoders, to refine our segmentation predictions. By integrating our approach into an existing self-training framework, we achieved state-of-theart performance on the two benchmark tasks SYNTHIAto-Cityscapes and GTA-to-Cityscapes. The results verified our motivation and demonstrated the importance of capturing the correlation between modalities to improve semantic segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where? init are the semantic intermediate predictions.? T is the one-hot semantic pseudo-label for target domain. Bot? y S init and? T init are the intermediate semantic predictions from the same shared semantic decoder.d S andd T are the intermediate depth predictions from the separate source and target depth decoders. w is a pixel-wise pseudo-label weight which we will introduce in Section 3.3. Following</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Semantic segmentation results for the SYNTHIA-to-Cityscapes adaptation task. mIoU* denotes performance over 13 classes excluding those marked with *.</figDesc><table><row><cell>Method</cell><cell>Depth</cell><cell>road</cell><cell>s.walk</cell><cell>build.</cell><cell>wall*</cell><cell>fence*</cell><cell>pole*</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>moto.</cell><cell>bike</cell><cell cols="2">mIoU* mIoU</cell></row><row><cell>Source [36]</cell><cell></cell><cell cols="3">36.3 14.6 68.8</cell><cell>9.2</cell><cell cols="2">0.2 24.4</cell><cell>5.6</cell><cell>9.1</cell><cell cols="5">69.0 79.4 52.5 11.3 49.8</cell><cell>9.5</cell><cell cols="2">11.0 20.7</cell><cell>33.7</cell><cell>29.5</cell></row><row><cell>OutputAdapt [37]</cell><cell></cell><cell cols="3">84.3 42.7 77.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.7</cell><cell>7.0</cell><cell cols="8">77.9 82.5 54.3 21.0 72.3 32.2 18.9 32.3</cell><cell>46.7</cell><cell>-</cell></row><row><cell>ADVENT [40]</cell><cell></cell><cell cols="3">85.6 42.2 79.7</cell><cell>8.7</cell><cell cols="2">0.4 25.9</cell><cell>5.4</cell><cell>8.1</cell><cell cols="8">80.4 84.1 57.9 23.8 73.3 36.4 14.2 33.0</cell><cell>48.0</cell><cell>41.2</cell></row><row><cell>CBST [55]</cell><cell></cell><cell cols="16">68.0 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6 78.3 60.6 28.3 81.6 23.5 18.8 39.8</cell><cell>48.9</cell><cell>42.6</cell></row><row><cell>R-MRNet [53]</cell><cell></cell><cell cols="16">87.6 41.9 83.1 14.7 1.7 36.2 31.3 19.9 81.6 80.6 63.0 21.8 86.2 40.7 23.6 53.1</cell><cell>54.9</cell><cell>47.9</cell></row><row><cell>SIM [43]</cell><cell></cell><cell cols="3">83.0 44.0 80.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">17.1 15.8 80.5 81.8 59.9 33.1 70.2 37.3 28.5 45.8</cell><cell>52.1</cell><cell>-</cell></row><row><cell>FDA [47]</cell><cell></cell><cell cols="3">79.3 35.0 73.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">19.9 24.0 61.7 82.6 61.4 31.1 83.9 40.8 38.4 51.1</cell><cell>52.5</cell><cell>-</cell></row><row><cell>Yang et al. [46]</cell><cell></cell><cell cols="3">85.1 44.5 81.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="10">16.4 15.2 80.1 84.8 59.4 31.9 73.2 41.0 32.6 44.7</cell><cell>53.1</cell><cell>-</cell></row><row><cell>IAST [24]</cell><cell></cell><cell cols="16">81.9 41.5 83.3 17.7 4.6 32.3 30.9 28.8 83.4 85.0 65.5 30.8 86.5 38.2 33.1 52.7</cell><cell>57.0</cell><cell>49.8</cell></row><row><cell>DACS [36]</cell><cell></cell><cell cols="16">80.6 25.1 81.9 21.5 2.9 37.2 22.7 24.0 83.7 90.8 67.6 38.3 82.9 38.9 28.5 47.6</cell><cell>54.8</cell><cell>48.3</cell></row><row><cell>SPIGAN [19]</cell><cell></cell><cell cols="3">71.1 29.8 71.4</cell><cell>3.7</cell><cell cols="2">0.3 33.2</cell><cell>6.4</cell><cell cols="9">15.6 81.2 78.9 52.7 13.1 75.9 25.5 10.0 20.5</cell><cell>42.4</cell><cell>36.8</cell></row><row><cell>GIO-Ada [3]</cell><cell></cell><cell cols="14">78.3 29.2 76.9 11.4 0.3 26.5 10.8 17.2 81.7 81.9 45.8 15.4 68.0 15.9</cell><cell>7.5</cell><cell>30.4</cell><cell>43.0</cell><cell>37.3</cell></row><row><cell>DADA [41]</cell><cell></cell><cell cols="3">89.2 44.8 81.4</cell><cell>6.8</cell><cell cols="2">0.3 26.2</cell><cell>8.6</cell><cell cols="9">11.1 81.8 84.0 54.7 19.3 79.7 40.7 14.0 38.8</cell><cell>49.8</cell><cell>42.6</cell></row><row><cell>CTRL [32]</cell><cell></cell><cell cols="16">86.4 42.5 80.4 20.0 1.0 27.7 10.5 13.3 80.6 82.6 61.0 23.7 81.8 42.9 21.0 44.7</cell><cell>51.5</cell><cell>45.0</cell></row><row><cell>CorDA (mono)</cell><cell></cell><cell cols="16">90.2 47.5 85.6 24.5 3.0 38.2 41.6 36.5 85.9 91.7 70.3 42.4 86.0 42.9 34.7 50.4</cell><cell>62.0</cell><cell>54.5</cell></row><row><cell>CorDA (stereo)</cell><cell></cell><cell cols="16">93.3 61.6 85.3 19.6 5.1 37.8 36.6 42.8 84.9 90.4 69.7 41.8 85.6 38.4 32.6 53.9</cell><cell>62.8</cell><cell>55.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experiment results (mIoU in %) on the GTA5-to-Cityscapes task. Our method CorDA uses monocular depth estimation for GTA5 and stereo depth estimation for Cityscapes.Source only<ref type="bibr" target="#b36">[37]</ref> 75.<ref type="bibr" target="#b7">8</ref> 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6 ROAD [4] 76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6 39.4 OutputAdapt [37] 86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1 42.4 ADVENT [40] 87.6 21.4 82.0 34.8 26.2 28.5 35.6 23.0 84.5 35.1 76.2 58.6 30.7 84.8 34.2 43.4 0.4 28.4 35.3 44.8 CBST [55] 91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9 BDL [20] 91.0 44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 MRKLD-SP [56] 90.8 46.0 79.9 27.4 23.3 42.3 46.2 40.9 83.5 19.2 59.1 63.5 30.8 83.5 36.8 52.0 28.0 36.8 46.4 49.2 Kim et al. [16] 92.9 55.0 85.3 34.2 31.1 34.9 40.7 34.0 85.2 40.1 87.1 61.0 31.1 82.5 32.3 42.9 0.3 36.4 46.1 50.2 CAG-UDA [51] 90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2 0.0 27.3 34.0 52.1 CorDA 94.7 63.1 87.6 30.7 40.6 40.2 47.8 51.6 87.6 47.0 89.7 66.7 35.9 90.2 48.9 57.5 0.0 39.8 56.0 56.6</figDesc><table><row><cell>Method</cell><cell>road</cell><cell>s.walk</cell><cell>build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>moto.</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2 50.2</cell></row><row><cell>FDA [47]</cell><cell cols="20">92.5 53.3 82.4 26.5 27.6 36.4 40.6 38.9 82.3 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.9 27.7 46.4 50.5</cell></row><row><cell>PIT [23]</cell><cell cols="20">87.5 43.4 78.8 31.2 30.2 36.3 39.9 42.0 79.2 37.1 79.3 65.4 37.5 83.2 46.0 45.6 25.7 23.5 49.9 50.6</cell></row><row><cell>IAST [24]</cell><cell cols="20">93.8 57.8 85.1 39.5 26.7 26.2 43.1 34.7 84.9 32.9 88.0 62.6 29.0 87.3 39.2 49.6 23.2 34.7 39.6 51.5</cell></row><row><cell>DACS [36]</cell><cell cols="2">89.9 39.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The contributions of Qin Wang and Olga Fink were funded by the Swiss National Science Foundation (SNSF) Grant no. PP00P2 176878. This work is also funded by Toyota Motor Europe via the research project TRACE-Zurich.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with leftright consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR))</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR))</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate and efficient stereo processing by semi-global matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Three ways to improve semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>K?ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10782</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised relative depth learning for urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Maire</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12972" to="12981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6129" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent scene parsing with perspective understanding in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="956" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spigan: Privileged adversarial learning from simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-domain semantic segmentation via domain-invariant interactive relation transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic depth fusion and transformation for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning across tasks and domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Tonioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to relate depth and semantics for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menelaos</forename><surname>Danda Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8197" to="8207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universal domain adaptation through self supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16282" to="16292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="687" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via crossdomain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mti-net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dada: Depth-aware domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7364" to="7373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stereoscopic inpainting: Joint color and depth completion from stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12635" to="12644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selfsupervised domain adaptation for computer vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="156694" to="156706" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Label-driven reconstruction for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinliang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaochao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno>abs/2003.04614</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10979</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4106" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5981" to="5990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
