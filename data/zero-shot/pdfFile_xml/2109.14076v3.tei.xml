<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RAFT: A Real-World Few-Shot Text Classification Benchmark</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Alex</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Lifland</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Maham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stiftung</forename><surname>Neue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verantwortung</forename><forename type="middle">C Jess</forename><surname>Riedel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmie</forename><surname>Hine</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Ashurst</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Sedille</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harvard</forename><forename type="middle">Kennedy</forename><surname>School</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Carlier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noetel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">Stuhlm?ller</forename><surname>Ought</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Ought</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Ought</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">NTT Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Oxford Internet Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Alan Turing Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Australian Catholic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RAFT: A Real-World Few-Shot Text Classification Benchmark</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for human research assistants? Existing benchmarks are not designed to measure progress in applied settings, and so don't directly answer this question. The RAFT benchmark (Realworld Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment. Baseline evaluations on RAFT reveal areas current techniques struggle with: reasoning over long texts and tasks with many classes. Human baselines show that some classification tasks are difficult for non-expert humans, reflecting that real-world value sometimes depends on domain expertise. Yet even non-expert human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets and leaderboard will track which model improvements translate into real-world benefits at https://raft.elicit.org.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Few-shot learning, the capacity to complete a task given a small number of demonstrations <ref type="bibr" target="#b10">[11]</ref>, is one of the hallmarks of human intelligence <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b16">17]</ref>. As researchers, we leverage this capacity when we delegate work on crowdsourcing platforms or give a task with examples to a human research assistant.</p><p>Brown et al. <ref type="bibr" target="#b5">[6]</ref> show that large pre-trained language models exhibit few-shot learning capabilities for a wide range of natural language tasks. If those capabilities were comparable to people on economically relevant tasks, this would be important to know: a single model could be used across multiple real-world tasks, with low per-task data labeling cost. However, these models have also been shown to have inconsistent few-shot performance depending on the exact setup and task being solved [e.g. <ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>. The mixed evidence suggests that it would be valuable to measure and track few-shot performance on a set of tasks that is representative of what appears in practice. <ref type="figure">Figure 1</ref>: RAFT includes naturally occurring classification datasets, mimicking work that is usually given to human research assistants. Each task comes with natural language instructions and labels in addition to 50 training examples.</p><p>Natural language tasks coarsely split into generation, classification, and retrieval. We focus on classification tasks because they support high-quality automated evaluation, cover a wide range of economically valuable tasks, and yet don't have existing real-world benchmarks.</p><p>Existing few-shot classification benchmarks are typically designed to highlight areas where models fall short <ref type="bibr" target="#b28">[29]</ref> or to study particular model abilities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21]</ref>. The tasks and evaluation setup aren't optimized to measure progress in applied settings:</p><p>? Tasks that are generated or chosen specifically to test language models may not represent some of the challenges found when applying these models in real-world settings. For example, SuperGLUE <ref type="bibr" target="#b31">[32]</ref> and the few-shot equivalent FewGLUE <ref type="bibr" target="#b28">[29]</ref> mainly include short texts. Doing well on applied tasks sometimes requires reasoning over long texts. Existing systems struggle with long texts due to a limited context window, especially in the few-shot setting where some systems learn from examples presented in context.</p><p>? The evaluation does not closely mirror deployment, and may both under-and overestimate models' capabilities. It may underestimate model capability by restricting models to the closed-book setting (e.g., no retrieval from online sources) and using uninformative labels (e.g., 0/1 instead of "about literature" vs. "about movies"). It may overestimate model capability by using many more than a few examples for setting hyperparameters during validation <ref type="bibr" target="#b23">[24]</ref>.</p><p>RAFT is a real-world few-shot text classification benchmark designed to measure how much recent and upcoming NLP advances benefit applications:</p><p>? The tasks are naturally occurring tasks. Their labeling is inherently valuable to someone, and they may have challenges that are not reflected in synthetic tasks. Inherent value means that, if it were sufficiently fast and cheap, it would be desirable to outsource the task to human research assistants or crowd workers. Challenges refers to the need for information retrieval, domain expertise, parsing long documents, and making use of instructions. <ref type="table" target="#tab_2">Table 1</ref> shows the real-world challenges presented by RAFT, including 4 datasets with long input texts.</p><p>? The evaluation closely mirrors deployment. For each task, we release a public training set with 50 examples and a larger unlabeled test set <ref type="bibr" target="#b1">2</ref> . We encourage unsupervised pre-training on the unlabelled examples and open-domain information retrieval. We keep the test-set labels private and provide automated evaluation through a Hugging Face leaderboard <ref type="bibr" target="#b2">3</ref> .</p><p>In addition to the gold-standard human labels, we collect automatic and crowdsourced baselines. The automatic baselines reveal areas where current techniques struggle, such as reasoning over long texts and tasks with many classes. The crowdsourced baseline reveals that RAFT includes a mix of moderate to difficult tasks. We also observe difficulties in collecting human crowdsourced baselines on some datasets, particularly when domain expertise is important, which suggests that real-world value often depends on domain knowledge.</p><p>The RAFT datasets and leaderboard can be viewed and submitted to at https://raft.elicit.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We briefly review few-shot learning in NLP, then the benchmarks that are most similar to RAFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot learning in NLP</head><p>Pre-trained language models (PLMs) such as BERT <ref type="bibr" target="#b9">[10]</ref> and GPT-3 <ref type="bibr" target="#b5">[6]</ref> can learn to do some NLP tasks when prompted with a few demonstrations, including some classification tasks. The two primary approaches to few-shot classification using PLMs are in-context learning and prompt-based fine-tuning.</p><p>In-context learning. A PLM is primed with labeled examples in its prompt. It classifies the example included at the end of the prompt by predicting the classification conditioned on the priming. GPT-3 <ref type="bibr" target="#b5">[6]</ref> used in-context learning to achieve promising results on a variety of classification tasks. UniFew <ref type="bibr" target="#b4">[5]</ref> similarly achieved strong results on classification tasks via in-context learning, converting classification tasks into a multiple-choice question answer format for prompting.</p><p>Prompt-based fine-tuning. A PLM is fine-tuned with masked-language modeling objectives to learn from few examples. This is also known as Pattern-exploiting training (PET) <ref type="bibr" target="#b27">[28]</ref>. While PET requires task-specific prompts, it achieves better performance than GPT-3 in-context with smaller models <ref type="bibr" target="#b28">[29]</ref>. LM-BFF <ref type="bibr" target="#b12">[13]</ref> improves prompt-based fine-tuning by dynamically constructing prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot NLP benchmarks</head><p>The most closely related few-shot NLP benchmarks are FLEX <ref type="bibr" target="#b4">[5]</ref>, FewGLUE <ref type="bibr" target="#b28">[29]</ref>, CrossFit <ref type="bibr" target="#b36">[37]</ref>, and NaturalInstructions <ref type="bibr" target="#b20">[21]</ref>. Each of these benchmarks includes at least some classification tasks with meaningful textual labels.</p><p>These benchmarks are designed to study transfer between tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref>, pinpoint where NLP models fall short <ref type="bibr" target="#b28">[29]</ref>, and evaluate ability of models to follow instructions <ref type="bibr" target="#b20">[21]</ref>, whereas RAFT is designed to be representative of real-world classification tasks. This difference in goals is reflected in selection of tasks and evaluation: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmark Description</head><p>RAFT is a few-shot classification benchmark. We focus on classification primarily because automatic evaluation is more reliable than for generation tasks. We believe (as our results will later confirm) that there still is a substantial gap between even non-expert humans and automated systems in the few-shot classification setting.</p><p>Both tasks (datasets and metadata) and evaluation (rules for submission, metrics) are chosen to mirror real-world classification problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tasks</head><p>A classification task is a dataset with labeled natural language entries. Each label corresponds one-to-one with a natural language class name. Each task has instructions for labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Dataset selection criteria</head><p>We selected datasets based on the following criteria ("non-trivial real world tasks"):</p><p>Naturally occurring. We focus on data that are naturally occurring, rather than being synthetically generated to test and improve language models.</p><p>Intrinsic value. We select datasets for which the correct labeling inherently provides real-world value. RAFT includes tasks like hate-speech detection, medical case report parsing, and literature review automation, where better performance translates into practical benefits. This criterion involves subjectivity, but we aimed to select tasks that approximate the distribution of valuable classification tasks well.</p><p>Realistic class distribution. We did not exclude datasets with heavily imbalanced classes.</p><p>Open-domain feasibility. As we provide an open-domain setting where information retrieved from the web may be used to augment predictions, we excluded tasks for which the correct label is extremely easily discoverable through a Google search. For example, we considered including the LIAR <ref type="bibr" target="#b34">[35]</ref> dataset which includes Politifact statements and their veracity. We decided against including it since it would be trivial to get 100% accuracy by running a site search on https://www.politifact.com/.</p><p>In order to gather datasets meeting the above requirements, we put out a collaboration request. We also reached out to users of classification on Elicit <ref type="bibr" target="#b22">[23]</ref>. Lastly, we conducted a search of existing datasets on the Hugging Face Hub 5 and PapersWithCode 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Dataset preparation</head><p>In cases where the test set was over 5,000 data points, we randomly selected 5,000 to serve as a test set in order to keep the test set sizes manageable. When the dataset didn't already have textual labels, we added textual labels according to our best understanding of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Selected RAFT datasets</head><p>We selected 11 datasets, accessible at https://raft.elicit.org/datasets. <ref type="table" target="#tab_2">Table 1</ref> presents an overview of the datasets. More details are available in the Appendix.</p><p>ADE Corpus V2 (ADE). The ADE corpus V2 <ref type="bibr" target="#b14">[15]</ref> contains sentences from medical case reports annotated for relation to adverse drug effects. We focus on the binary classification task of whether a sentence is related to an adverse drug effect (ADE).</p><p>Banking77 (B77). Banking77 <ref type="bibr" target="#b6">[7]</ref> contains online banking customer service queries annotated with their intents.</p><p>NeurIPS impact statement risks (NIS). We include the broader impact statements from NeurIPS 2020 papers collected in the dataset from Ashurst et al. <ref type="bibr" target="#b0">[1]</ref>. We annotate these based on whether they mention possibly harmful applications of the research done in the paper . 7 5 https://huggingface.co/datasets 6 https://paperswithcode.com <ref type="bibr" target="#b6">7</ref> The raw scraped NeurIPS impact statements can be found at https://raft.elicit.org/neurips-impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Name</head><p>Long inputs  OneStopEnglish (OSE). OneStopEnglish <ref type="bibr" target="#b30">[31]</ref> contains articles sourced from The Guardian newspaper and rewritten by teachers to suit three levels of adult English as a Second Language (ESL) learners.</p><p>Overruling (Over). Overruling <ref type="bibr" target="#b38">[39]</ref> contains statements from a law corpus annotated based on whether they are overruling, defined as nullifying a previous case decision as a precedent.</p><p>Semiconductor org types (SOT). We collect a dataset of institutions that have contributed to semiconductor conferences in the last 25 years, then classify these institutions into organization types: "university", "company", and "research institute".</p><p>Systematic review inclusion (SRI). We use data from a systematic meta-review studying interventions to increase charitable donations <ref type="bibr" target="#b21">[22]</ref>. The task is to predict whether a paper advances past the screening stage.</p><p>TAI safety research (TAI). We include data from the formation of a bibliographic database for research on the safety of transformative artificial intelligence (TAI) <ref type="bibr" target="#b26">[27]</ref>. We choose the binary task of predicting whether a work is classified as TAI safety research.</p><p>Terms of Service (ToS). The Terms of Service dataset <ref type="bibr" target="#b18">[19]</ref> contains clauses from Terms of Services, annotated by whether they are potentially unfair to consumers.</p><p>TweetEval Hate (TEH). We include the hate-speech detection task from the TweetEval dataset <ref type="bibr" target="#b1">[2]</ref>, which was curated from Basile et al. <ref type="bibr" target="#b2">[3]</ref>.</p><p>Twitter complaints (TC). We include a dataset of tweets annotated by whether they contain a complaint <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Setting and rules</head><p>The RAFT evaluation replicates real-world few-shot classification problems by restricting to 50 labeled examples without validation set, providing meaningful instructions and labels, and using a no-holds-barred setting:</p><p>50 labeled examples. We provide 50 labeled examples per task (not per class). In the authors' experience with users of the classification tool Elicit <ref type="bibr" target="#b22">[23]</ref> Task-specific instructions. As an important replacement for large amounts of labeled data, instructions can specify how a task should be done. Therefore, we provide the instructions we give to human labelers so that they can be used in instructing automatic systems. The level of detail of the instructions varies. We write the instructions based on information from publications (for datasets published elsewhere) or in consultation with the dataset creator (for new datasets).</p><p>Meaningful label names. Similar to instructions, textual labels are an important aspect of few-shot and especially zero-shot learning. We create default textual labels for each dataset as recommended by FLEX <ref type="bibr" target="#b4">[5]</ref>.</p><p>Transfer learning permitted. Transfer and meta-learning using other datasets is permitted, including further pre-training on other corpora.</p><p>Unlabeled data permitted. Use of the unlabeled RAFT test sets is permitted, as unlabeled data are usually available in the applied setting.</p><p>Open-domain retrieval permitted. Models may be augmented with information retrieved from the internet, e.g. via automated web searches. <ref type="bibr" target="#b8">9</ref> Submission requires only labels. Submission on the test set is open to the public and only requires upload of test set labels. This is in line with benchmarks like GLUE <ref type="bibr" target="#b32">[33]</ref> and SuperGLUE <ref type="bibr" target="#b31">[32]</ref>, but is in contrast to the few-shot benchmark FLEX <ref type="bibr" target="#b4">[5]</ref>. By only requiring labels, we give submission creators maximal flexibility in what models to set up.</p><p>Weekly evaluation. Evaluation is run on a weekly basis to minimize information gained from frequent repeated submissions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metrics</head><p>Since some RAFT datasets have substantial class imbalances, we use F1 as our evaluation metric. We compute macro-averaged F1 scores, even for binary datasets. To get an overall score, we average across all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Baselines</head><p>The code for all automatic baselines is open-sourced at https://raft.elicit.org/baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GPT-3 baseline</head><p>We provide a simple automatic baseline using GPT-3 <ref type="bibr" target="#b5">[6]</ref>, accessed through the OpenAI API <ref type="bibr" target="#b9">10</ref> . As in Brown et al. <ref type="bibr" target="#b5">[6]</ref>, we use in-context learning, adding labeled examples to the prompt to prime GPT-3. We also run a zero-shot version with no training examples included in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Prompt construction</head><p>We build a prompt consisting of: We select N training examples to include in a given prompt and then truncate the examples. In a given task, the instructions take up I tokens. Separators between the instructions and each example take up S tokens. No more than T = 2, 048 tokens can be used. Field selection and sorting. We exclude data fields that are unlikely to contribute substantially to GPT-3's performance. These fields either deal with the authors of the textual example or are URLs. Additionally, we sort the order in which the text fields occur to put the most important fields first. When examples are truncated, the most important information is preserved.</p><p>Semantic selection. To select training examples to include in the prompt for a given test example, we selected the most similar training examples as in Liu et al. <ref type="bibr" target="#b19">[20]</ref>. To perform semantic search, we use the OpenAI API search endpoint with the ada engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Classification</head><p>With the prompt formed, we retrieve GPT-3's 100 most likely next tokens using the davinci engine. For each class, we assign the probability that its first token is generated. We then normalize the probabilities to sum to 1. For the B77 dataset, multiple labels share the same first token so we prepend a numerical prefix such as "1. " to each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Parameter selection</head><p>We tune the GPT-3 baseline on the training set using leave-one-out cross validation (LOOCV): k-fold cross validation with k = n so that only one test example is used at a time for validation. While LOOCV isn't robust with as few as 50 examples as discussed in Perez et al. <ref type="bibr" target="#b23">[24]</ref>, it is one of the best options for parameter selection in the few-shot setting. Detailed LOOCV results are in Section A.5.</p><p>Instructions. We test two modes of instruction: (a) a generic classification prompt: "Possible labels:" followed by a list of textual labels. (b) instructions similar to the ones given to human labelers, plus the list of textual labels. The instructions are taken whole when possible, and otherwise shortened and summarized manually to limit usage of the GPT-3 context window. Task-specific instructions outperform generic instructions by an .04 on averaged F1 score, thus we include task-specific instructions in the baseline.</p><p>Semantic training example selection. To select training examples for inclusion in the prompt from a larger set, we consider (a) selecting examples randomly and (b) using semantic search to identify the training examples most similar to the test example. Semantic selection outperforms random selection by 0.03 on averaged F1, thus we include semantic selection in the baseline.</p><p>Number of examples in the prompt. We select the number of examples to include in the prompt on a per-dataset basis, as our truncation strategy induces a quality-quantity trade-off. For each dataset, we test performance with 5, 10, 25, and 50 <ref type="bibr" target="#b10">11</ref> training examples and choose the number that performs best by F1. For datasets with long inputs, smaller numbers of more detailed samples often produce better performance, while datasets with smaller inputs can fit more complete labeled examples in the prompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other automatic baselines</head><p>In-context baselines. We run further in-context baselines GPT-Neo <ref type="bibr" target="#b3">[4]</ref> and GPT-2 <ref type="bibr" target="#b25">[26]</ref>. We provide code <ref type="bibr" target="#b11">12</ref> for generating predictions on RAFT using these models and any other causal language model available on the HuggingFace Hub. For semantic search, we use a MiniLM <ref type="bibr" target="#b33">[34]</ref> fine-tuned on sentence pairs via the sentence-transformers package <ref type="bibr" target="#b12">13</ref> .</p><p>Zero-shot baselines. We run two transformers in the zero-shot setting:</p><p>? GPT-3, to judge to what extent training examples in the prompt aid performance ? BART <ref type="bibr" target="#b17">[18]</ref> trained on MNLI <ref type="bibr" target="#b35">[36]</ref>, as suggested by Yin et al. <ref type="bibr" target="#b37">[38]</ref> and Davison <ref type="bibr" target="#b8">[9]</ref> as an effective zero-shot classification approach</p><p>Non-neural baselines. We run AdaBoost <ref type="bibr" target="#b11">[12]</ref> to establish a strong non-neural baseline. We construct feature vectors for each example based on the counts of n-grams of 1-5 words as the input to a weighted ensemble of 100 depth-3 decision trees. These decision trees and weights are trained with AdaBoost with learning rate 1, and evaluated through weighted voting. We also include a plurality (most frequent) class baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human baseline</head><p>To collect human baselines, we use the Surge 14 crowdsourcing platform. Following Wang et al. <ref type="bibr" target="#b31">[32]</ref>, we randomly select 100 data points from each test set and use a 2-step labeling process: qualification then annotation. The crowdsourced label is the plurality vote of 5 labelers.</p><p>We put crowd workers in a similar situation to automated systems. We link to a sheet with the same 50 labeled examples, use the same textual labels, and give the same task-specific instructions that we are providing to practitioners to adapt for instructing language models. <ref type="bibr" target="#b14">15</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Humans generally outperform GPT-3. Humans outperform GPT-3 on 8 out of 11 tasks, demonstrating room for improvement for models on real-world few-shot tasks. We expect that exceeding the crowdsourced baseline will require substantial advances in model performance, and even more so for a future expert human baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weaknesses of GPT-3 include:</head><p>? labeled examples. Additionally, just listing out the possible classes takes up a large portion of GPT-3's context window. ? Long inputs: GPT-3 performs poorly on some tasks requiring reasoning over long inputs, such as NIS and OSE. GPT-3's context window may be a contributing factor.</p><p>Crowd-sourced baselines struggle on domain-specific tasks. Crowd-sourced humans substantially outperform GPT-3 on only 1 of 4 tasks we identified as requiring domain expertise:</p><p>? Humans substantially outperform GPT-3 on ADE, which requires medical expertise.</p><p>? Humans outperformed GPT-3 by just .053 on ToS, which requires parsing legal language.</p><p>? GPT-3 outperforms humans on Over, which requires greater legal expertise than ToS <ref type="bibr" target="#b38">[39]</ref>, and TAI, which requires expertise in AI safety research.</p><p>Zero-shot performance is weak. GPT-3 zero-shot does poorly on RAFT, performing worse than the plurality class baseline. BART zero-shot exceeds the plurality class baseline but does not do so in every dataset, and it is not competitive with few-shot language models. We encourage future research on improving performance in the zero-shot setting, perhaps through improved prompt construction and transfer learning.</p><p>Neural baselines besides few-shot GPT-3 perform worse than AdaBoost. Generative language models smaller than GPT-3 comfortably outperform the plurality class baseline but remain below AdaBoost. We use the same amount of labelled examples in the prompt as with GPT-3 despite the context window being smaller; performance may improve with fewer (but longer) examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Limitations</head><p>Linguistic diversity. The benchmark only includes English tasks. Dealing with multilingual corpora is a real-world challenge for many NLP systems, especially for those deployed in countries where there are multiple national languages. To fully capture the distribution of real-world tasks, additional languages will be needed.</p><p>Possible biases in data collection. While we attempted to execute our dataset selection process as described in Section 3.1.3 in an unbiased manner, the datasets we ended up selecting are part of a subjective human process that may be subject to biases. For example, the organizations we work with are disproportionately in technology and policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact</head><p>Offensive content. By including a hate-speech detection dataset, we include offensive content and may harm readers of the dataset. We believe the advantages from studying hate-speech detection are likely greater than the disadvantages of publicizing hate-speech datasets.</p><p>Prohibitive costs. The models best equipped to perform well on RAFT will often be the massive transformer models trained by private corporations. In advancing this benchmark as a means of evaluating models, we risk further widening the gap between what a dedicated individual or team can do, and what can only be done by industry research labs with sufficient funding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Future Work</head><p>Stronger human baselines. Human baselines are intended to tell us how well the dataset would be labeled in the absence of automated systems. For many RAFT datasets, this process would involve a stronger baseline than is easily available via a crowd-worker platform: for example, the Over dataset would be labeled by someone with law expertise. In addition to ML submissions, we welcome efforts to collect stronger human baselines for RAFT.</p><p>Additional automatic baselines. We expect that systems that use prompt-based fine-tuning rather than in-context learning may provide an even stronger automatic baseline. We further expect that models that leverage the open-domain information retrieval option can surpass models that don't.</p><p>Application-specific metrics. Different applications care about different metrics; e.g., in some applications it is more important to minimize false positives, whereas in others the focus is on false negatives. An ideal measure of real-world value would take that into account.</p><p>Learning from natural language In this work, we focused on instructions as a supplement to labeled examples. Similarly to Mishra et al. <ref type="bibr" target="#b20">[21]</ref>, we found that including task-specific instructions improved performance. Like humans, NLP systems could also learn from other types of natural language. For example, could including explanations with each labeled example be used to further improve few-shot performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>RAFT is a benchmark that tests language models across multiple domains on economically valuable classification tasks in the true few-shot setting. To our knowledge, this is the first multi-task benchmark designed to closely mirror how models are applied in both the task distribution and the evaluation setup. By complementing existing synthetic benchmarks designed to highlight where models fall short, it helps measure the gap between research and practice, incentivizes work that is valuable for deployed systems, and provides a template for future benchmarks that mirror deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A. Twitter complaints (TC). Unlicensed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset examples</head><p>See <ref type="table" target="#tab_9">Table 3</ref> for one training example from each dataset. <ref type="table" target="#tab_10">Table 4</ref> contains an excerpt from the instructions for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Task-specific instructions</head><p>Below we provide the full instructions given to human annotators and adapted for automatic baselines for each RAFT task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE Corpus V2 (ADE)</head><p>Label the sentence based on whether it is related to an adverse drug effect (ADE). Details are described below:</p><p>Drugs: Names of drugs and chemicals that include brand names, trivial names, abbreviations and systematic names were annotated. Mentions of drugs or chemicals should strictly be in a therapeutic context. This category does not include the names of metabolites, reaction byproducts, or hospital chemicals (e.g. surgical equipment disinfectants).</p><p>Adverse effect: Mentions of adverse effects include signs, symptoms, diseases, disorders, acquired abnormalities, deficiencies, organ damage or death that strictly occur as a consequence of drug intake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Banking77 (B77)</head><p>The following is a banking customer service query. Classify the query into one of the 77 categories available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NeurIPS impact statement risks (NIS)</head><p>Label the impact statement based on whether it mentions a harmful application of the research done in the paper. Make sure the statement is sufficient to conclude there are harmful applications of the research being done, not a past risk that this research is solving.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overruling (Over)</head><p>In law, an overruling sentence is a statement that nullifies a previous case decision as a precedent, by a constitutionally valid statute or a decision by the same or higher ranking court which establishes a different rule on the point of law involved. Label the sentence based on whether it is overruling or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Name</head><p>Instructions excerpt</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE Corpus V2 (ADE)</head><p>Label the sentence based on whether it is related to an adverse drug effect (ADE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Banking77 (B77)</head><p>The following is a banking customer service query.</p><p>NeurIPS impact statement risks (NIS) Label the impact statement as "mentions a harmful application" or "doesn't mention a harmful application" based on whether it mentions a harmful application of the research done in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OneStopEnglish (OSE)</head><p>The following is an article sourced from The Guardian newspaper, and rewritten by teachers to suit three levels of adult English as Second Language (ESL) learners: elementary, intermediate, and advanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overruling (Over)</head><p>In law, an overruling sentence is a statement that nullifies a previous case decision as a precedent, by a constitutionally valid statute or a decision by the same or higher ranking court which establishes a different rule on the point of law involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semiconductor org types (SOT)</head><p>The dataset is a list of institutions that have contributed papers to semiconductor conferences in the last 25 years, as catalogued by IEEE and sampled randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systematic review inclusion (SRI)</head><p>Identify whether this paper should be included in a meta-review which includes the findings of systematic reviews on interventions designed to promote charitable donations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAI safety research (TAI)</head><p>The contents of the paper are directly motivated by, and substantively inform, the challenge of ensuring good outcomes for Transformative AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Terms of Service (ToS)</head><p>According to art. 3 of the Directive 93/13 on Unfair Terms in Consumer Contracts, a contractual term is unfair if: 1) it has not been individually negotiated; and 2) contrary to the requirement of good faith, it causes a significant imbalance in the parties rights and obligations, to the detriment of the consumer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TweetEval</head><p>Hate (TEH) Label whether the following tweet contains hate speech against either immigrants or women.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Twitter complaints (TC)</head><p>A complaint presents a state of affairs which breaches the writer's favorable expectation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semiconductor org types (SOT)</head><p>The dataset is a list of institutions that have contributed papers to semiconductor conferences in the last 25 years, as catalogued by IEEE and sampled randomly. The goal is to classify the institutions into one of three categories: "university", "company" or "research institute".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systematic review inclusion (SRI)</head><p>Identify whether this paper should be included in a meta-review which includes the findings of systematic reviews on interventions designed to promote charitable donations.</p><p>Papers should be included if they meet all of these criteria:</p><p>1. systematic reviews, scoping reviews, or similar reproducible reviews; 2. reviews describing monetary charitable donations; 3. reviews assessing any population of participants in any context; and 4. peer reviewed and written in English (due to logistical constraints).</p><p>They shouldn't be included if they meet any of these criteria:</p><p>1. primary research reporting new data (e.g., randomised experiments); 2. non-systematic reviews, theory papers, or narrative reviews; 3. reviews on cause-related marketing; and 4. reviews of other kinds of prosocial behaviour (e.g., honesty, non-financial donations like volunteering, blood, or organ donations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAI safety research (TAI)</head><p>Transformative AI (TAI) is defined as AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.</p><p>Label a paper as "TAI safety research" if:</p><p>1. The contents of the paper are directly motivated by, and substantively inform, the challenge of ensuring good outcomes for TAI. The paper need not mention TAI explicitly, but it must be motivated by it, since there are far too many papers that are merely relevant to safety. Judging motivation is, unfortunately, inherently subjective, but this is necessary to avoid penalizing papers that do not explicitly mention TAI for appearance reasons, while also not including every paper on, e.g., adversarial examples (which are motivated by capabilities and near-term safety). If the paper would likely have been written even in the absence of TAI-safety concerns, it is excluded. Ultimately, we want to support researchers who are motivated by TAI safety and allow them to find each other's work 2. There is substantive content on AI safety, not just AI capabilities. That said, for more speculative papers it is harder to distinguish between safety vs. not safety, and between technical vs. meta, and we err on the side of inclusion. Articles on the safety of autonomous vehicles are generally excluded, but articles on the foundations of decision theory for AGI are generally included. 3. The intended audience is the community of researchers. Popular articles and books are excluded. Papers that are widely released but nevertheless have substantial research content (e.g., Bostrom's Superintelligence) are included, but papers that merely try to recruit researchers are excluded. 4. It meets a subjective threshold of seriousness/quality. This is intended to be a very low threshold, and would, for instance, include anything that was accepted to be placed on the ArXiv. Web content not intended for review (e.g., blog posts) is only accepted if it has reached some (inevitably subjective) threshold of notability in the community. It is of course infeasible for us to document all blog posts that are about TAI safety, but we do not want to exclude some posts that have been influential but have never been published formally. 5. Peer review is not required. White papers, preprints, and book chapters are all included.</p><p>Otherwise, label it as "not TAI safety research".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Terms of Service (ToS)</head><p>Label the sentence from a Terms of Service based on whether it is potentially unfair. If it seems clearly unfair, mark it as potentially unfair.</p><p>According to art. 3 of the Directive 93/13 on Unfair Terms in Consumer Contracts, a contractual term is unfair if: 1) it has not been individually negotiated; and 2) contrary to the requirement of good faith, it causes a significant imbalance in the parties rights and obligations, to the detriment of the consumer.</p><p>Details on types of potentially unfair clauses are found below:</p><p>The jurisdiction clause stipulates what courts will have the competence to adjudicate disputes under the contract. Jurisdiction clauses giving consumers a right to bring disputes in their place of residence were marked as clearly fair, whereas clauses stating that any judicial proceeding takes a residence away (i.e. in a different city, different country) were marked as clearly unfair.</p><p>The choice of law clause specifies what law will govern the contract, meaning also what law will be applied in potential adjudication of a dispute arising under the contract. Clauses defining the applicable law as the law of the consumer's country of residence were marked as clearly fair. In every other case, the choice of law clause was considered as potentially unfair.</p><p>The limitation of liability clause stipulates that the duty to pay damages is limited or excluded, for certain kind of losses, under certain conditions. Clauses that explicitly affirm non-excludable providers' liabilities were marked as clearly fair. Clauses that reduce, limit, or exclude the liability of the service provider were marked as potentially unfair when concerning broad categories of losses or causes of them, such as any harm to the computer system because of malware or loss of data or the suspension, modification, discontinuance or lack of the availability of the service. Also those liability limitation clauses containing a blanket phrase like "to the fullest extent permissible by law", were considered potentially unfair. Clause meant to reduce, limit, or exclude the liability of the service provider for physical injuries, intentional damages as well as in case of gross negligence were marked as clearly unfair.</p><p>The unilateral change clause specifies the conditions under which the service provider could amend and modify the terms of service and/or the service itself. Such clause was always considered as potentially unfair.</p><p>The unilateral termination clause gives provider the right to suspend and/or terminate the service and/or the contract, and sometimes details the circumstances under which the provider claims to have a right to do so.</p><p>The contract by using clause stipulates that the consumer is bound by the terms of use of a specific service, simply by using the service, without even being required to mark that he or she has read and accepted them. We always marked such clauses as potentially unfair.</p><p>The content removal gives the provider a right to modify/delete user's content, including in-app purchases, and sometimes specifies the conditions under which the service provider may do so.</p><p>The arbitration clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court. It is therefore considered a kind of forum selection clause. However, such a clause may or may not specify that arbitration should occur within a specific jurisdiction. Clauses stipulating that the arbitration should (1) take place in a state other then the state of consumer's residence and/or (2) be based not on law but on arbiter's discretion were marked as clearly unfair. Clauses defining arbitration as fully optional would have to be marked as clearly fair.</p><p>TweetEval Hate (TEH) WARNING: This task involves labeling offensive and hateful content, particularly toward immigrants and women.</p><p>Label whether the following tweet contains hate speech against either immigrants or women.</p><p>Label whether the following tweet contains hate speech against either immigrants or women. Hate Speech (HS) is commonly defined as any communication that disparages a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics. Detailed guidelines are provided below, please read before labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hate speech against immigrants</head><p>More specifically, HS against immigrants may include:</p><p>? insults, threats, denigrating or hateful expressions ? incitement to hatred, violence or violation of rights to individuals or groups perceived as different for somatic traits (e.g. skin color), origin, cultural traits, language, etc.</p><p>? presumed association of origin/ethnicity with cognitive abilities, propensity to crime, laziness or other vices ? references to the alleged inferiority (or superiority) of some ethnic groups with respect to others ? delegitimization of social position or credibility based on origin/ethnicity ? references to certain backgrounds/ethnicities as a threat to the national security or welfare or as competitors in the distribution of government resources ? dehumanization or association with animals or entities considered inferior While answering the question "Is this tweet hateful?", you must take into account the following aspects:</p><p>1. the tweet content MUST have IMMIGRANTS/REFUGEES as main TARGET, or even a single individual, but considered for his/her membership in that category (and NOT for the individual characteristics) 2. we must deal with a message that spreads, incites, promotes or justifies HATRED OR VIOLENCE TOWARDS THE TARGET, or a message that aims at dehumanizing, hurting or intimidating the target</p><p>The joint presence of both elements in a tweet is considered essential to determine whether the tweet has hateful contents, therefore if both of them occur, your answer will be 'Yes'.</p><p>In case even just one of these conditions is not detected, HS (at least against immigrants) is assumed not to occur, then your answer will be 'No'.</p><p>Here a list of other aspects that are NOT considered hate speech for our purposes:</p><p>? HATE SPEECH AGAINST OTHER TARGETS ? offensive language ? blasphemy ? historical denial ? overt incitement to terrorism ? offense towards public servants and police officers ? defamation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hate speech against women</head><p>Label the tweet as hate speech if it is misogynous against women. A tweet is misogynous if it expresses hating towards women in particular (in the form of insulting, sexual harassment, threats of violence, stereotype, objectification and negation of male responsibility).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Twitter complaints (TC)</head><p>A complaint presents a state of affairs which breaches the writer's favorable expectation. Label the tweet text based on whether it contains a complaint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Dataset documentation</head><p>We provide documentation using applicable questions from the datasheets framework <ref type="bibr" target="#b13">[14]</ref> for the NIS, SOT, and TAI datasets. For documentation on other datasets we refer readers to the works in which the datasets were originally introduced as cited in Section 3.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 NeurIPS impact statement risks</head><p>The labeling section of this documentation contains information on how the impact statements were annotated based on whether they mention a harmful application. The other sections largely contain information on how the original dataset of NeurIPS impact statements <ref type="bibr" target="#b0">[1]</ref> was collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>? For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The original dataset was created to evaluate the then new requirement for authors to include an "impact statement" in their 2020 NeurIPS papers. Had it been successful? What kind of things did authors mention the most? How long were impact statements on average? See <ref type="bibr" target="#b0">[1]</ref> for more details.</p><p>? Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The original dataset was created as part of a project based at the Centre for the Governance of AI, which involved individual researchers and developers from the University of Oxford, Oxford Internet Institute, Harvard Kennedy School and the Alan Turing Institute.</p><p>? Who funded the creation of dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. The project was based at the Centre for the Governance of AI. There was no grant associated with the project. Individuals were funded by their respective organisations, or as contractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition</head><p>? Is any information missing from individual instances in the dataset? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. No.</p><p>? Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. This dataset has limitations that should be taken into consideration when using it. In particular, the method used to collect broader impact statements involved automated downloads, conversions and scraping and was not error-proof (see https://github.com/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/maindataset/notes-on-data.md for details). Although care has been taken to identify and correct as many errors as possible, not all texts have been reviewed by a human. This means it is possible some of the broader impact statements contained in the dataset are truncated or otherwise incorrectly extracted from their original article. The original dataset also contains labels describing whether authors chose to effectively "opt-out" of the requirement (for example by stating that a broader impact section is "Not Applicable"). Several statements were ambiguous in this respect, and so this label represents a subjective judgement on what constituted an opt-out. The labeling performed for this paper (whether a harmful application is mentioned) also constitutes a subjective judgment, and will contain human biases. Please see the section on Preprocessing, Cleaning, Labeling for more details.</p><p>? Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description. The dataset contains authors' names. These were scraped from publicly available scientific papers submitted to NeurIPS 2020.</p><p>? Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No.</p><p>? Does the dataset relate to people? The dataset does not relate to people directly, although it does contain authors' names. These were scraped from publicly available scientific papers submitted to NeurIPS 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection</head><p>? How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The data was directly observable (raw text scraped) for the most part; although some data was taken from previous datasets (which themselves had taken it from raw text). The data was validated, but only in part, by human reviewers. Further details can be found here: https://github.com/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/maindataset/notes-on-data.md</p><p>? What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? The main dataset was collected using software, and a combination of code iteration and human review was used to validate the results. Further details may be found here: https://github.com/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/main-dataset/notes-on-data.md.</p><p>? If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? The subset annotated based on harmful applications was sampled randomly.</p><p>? Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</p><p>The original dataset was created as part of a project based at the Centre for the Governance of AI, which involved individual researchers and developers as described above. The labeling for this paper (whether a harmful application is mentioned) was performed by Ought contractors.</p><p>? Does the dataset relate to people? The dataset does not relate to people directly, although it does contain authors' names. These were scraped from publicly available scientific papers submitted to NeurIPS 2020.</p><p>? Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? The impact statements were collected from the NeurIPS websites. Metadata included in the original dataset was collected from the NeurIPS chairs, and websites (for example where affiliated institutions are geographically based). See <ref type="bibr" target="#b0">[1]</ref> for further details. The labeling for this paper (whether a harmful application is mentioned) was collected from the contractors directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing, Cleaning, Labeling</head><p>? Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? For the original dataset <ref type="bibr" target="#b0">[1]</ref>, the manuscript pdfs for accepted papers were obtained from the NeurIPS 2020 proceedings website. The pdfs were converted to XML, and the title and impact statement section were extracted. The dataset was appended with information about paper subject area, author names, affiliations, affiliation type and affiliation institution locations, as follows. Primary and secondary subject area, as selected by authors on submission, were supplied to us by the NeurIPS programme chairs. Author names and affiliations were obtained from separate scrapes of the NeurIPS papers. Each affiliation was tagged with a location and type (industry or academia) based on <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b7">[8]</ref> respectively. Further details on the generation of the original dataset, and its assumptions and limitations, can be found at https://github.com/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/main-dataset/notes-on-data.md. Contractors paid by Ought performed the labeling of whether impact statements mention harmful applications. A majority vote was taken from three annotators.</p><p>? Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?</p><p>The original NeurIPS impact statements data is available at https://github.com/paulsedille/NeurIPS-Broader-Impact-Statements. The accepted papers containing the statements can also be found at https://proceedings.neurips.cc/paper/2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uses</head><p>? Has the dataset been used for any tasks already? If so, please provide a description.</p><p>An analysis of the original dataset has been prepared by the dataset authors, which can be found in Ashurst et al. <ref type="bibr" target="#b0">[1]</ref>.</p><p>? What (other) tasks could the dataset be used for? Other researchers are encouraged to use the dataset to provide further analysis on the outcomes of the NeurIPS broader impact requirement. The dataset could also be used for additional meta-analysis of NeurIPS 2020 accepted papers.</p><p>? Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? This dataset has limitations that should be taken into consideration when using it. In particular, the method used to collect broader impact statements involved automated downloads, conversions and scraping and was not error-proof. Although care has been taken to identify and correct as many errors as possible, not all texts have been reviewed by a human. This means it is possible some of the broader impact statements contained in the dataset are truncated or otherwise incorrectly extracted from their original article. More details may be found at https://github.com/paulsedille/NeurIPS-Broader-Impact-Statements/blob/main/main-dataset/notes-on-data.md. For this paper, individual labelers were asked whether harmful applications were mentioned in the statement, but what constitutes a harmful application is of course highly subjective, and will depend on the particular views and experiences of the labeler. For example, many applications will provide some benefits to some individuals and groups, while creating risks and harms to others. The intention was to capture a rough measure of whether the authors had intended to point out potential negative effects that could arise from the use of their work, or whether they chose to limit to potential positive impacts only. This will likely exclude applications that are typically viewed as beneficial or neutral, despite the fact that such applications can cause harm to individuals or subgroups in society. We therefore urge caution in how such labels are interpreted for future tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Semiconductor org types</head><p>This Labeling section of this documentation contains information on how the semiconductor organizations were annotated by type. The other sections mainly contain information describing how the unlabeled dataset of semiconductor organizations was collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>? For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The data set was originally created to understand better which countries' organisations have contributed most to semiconductor R&amp;D over the past 25 years using three main conferences. Moreover, to estimate the share of academic and private sector contributions, the organisations were classified as "university", "research institute" or "company".</p><p>? Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The data science unit of Stiftung Neue Verantwortung (Berlin).</p><p>? Who funded the creation of dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. The Stiftung Mercator is funding the data science unit in general</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composition</head><p>? Is any information missing from individual instances in the dataset? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. This data set is a sample of 500 out of many more organisations. Examples where the institution names contain "universit" were deleted because all language models can classify this as "university" and no discrimination is gained.</p><p>? Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. The human-created labels could be wrong.</p><p>? Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Collection</head><p>? What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? We used the IEEE API to obtain institutions that contributed papers to semiconductor conferences in the last 25 years. This is a random sample of 500 of them with a corresponding conference paper title. The three conferences were the International Solid-State Circuits Conference (ISSCC), the Symposia on VLSI Technology and Circuits (VLSI) and the International Electron Devices Meeting (IEDM). ? If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? It was probabilistic. Duplicate entries (by organisation name) were deleted. ? Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</p><p>A student was involved and paid according to German law. ? Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. March 2021</p><p>Preprocessing, Cleaning, Labeling</p><p>? Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? Yes. Contractors paid by Ought performed the labeling of organization types. A majority vote was taken from 3 annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution</head><p>? Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. It can only be used for non-commercial research purposes. See here and here. The annotated data is licensed under Creative Commons Attribution-NonCommercial 4.0 International.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 TAI Safety Research</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motivation</head><p>? For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The primary motivations for assembling this database were to: (1) Aid potential donors in assessing organizations focusing on TAI safety by collecting and analyzing their research output. (2) Assemble a comprehensive bibliographic database that can be used as a base for future projects, such as a living review of the field. ? Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? Angelica Deibel and myself (Jess Riedel). We did not do it on behalf of any entity. ? Who funded the creation of dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. I volunteered my own time and paid Angelica Deibel for her time from my personal funds.</p><p>? Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. No.</p><p>? Does the dataset relate to people? It's a database of papers, which have authors.</p><p>? Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? Both.</p><p>? Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. We asked authors to suggest papers that should be included in the database. ? Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis)been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing, Cleaning, Labeling</head><p>? Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? Yes. See the LessWrong post for more details on our labels, which was done largely by hand, on citation numbers, collected from Google Scholar by automated API call, and on the basic bibliographic information, which was collected with the automated tools from Zotero: https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographicdatabase ? Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? No. There was no clean distinction between raw and processed data. We used several automated tools that interacted, plus corrections and additions by hand. ? Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point. See link to the Citation numbers API called for Google Scholar in the the LessWrong post for more details: https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographicdatabase Uses ? Has the dataset been used for any tasks already? If so, please provide a description.</p><p>Yes, for the report we posted on LessWrong here. It was also used by "Larks" in his review. ? Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. No, this hasn't been used in any academic papers yet. ? Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? No.</p><p>? Are there tasks for which the dataset should not be used? If so, please provide a description. Don't use it to create a dangerous AI that could bring the end of days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution</head><p>? Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</p><p>As mentioned in the LessWrong post: "We release the Zotero database under the Creative Commons Attribution-ShareAlike 4.0 International License. In short, the means you are free to use, modify, and reproduce the database for anything so long as you cite us and release any derivative works under the same license." ? Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. No. The CC-SA-BY license is the only restriction</p><p>? Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 GPT-3 baseline details</head><p>The code for the GPT-3 baseline is available at https://raft.elicit.org/baselines under an MIT license.</p><p>Running the automatic baseline of GPT-3 davinci on the test sets cost approximately $2,600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 Parameter selection</head><p>Tables 5, 6 and 7 detail the results of parameter selection runs. All runs were done using GPT-3.</p><p>We mistakenly use 50 rather than 25 training examples in the prompt for TEH when running in-context baselines, despite 25 performing better in LOOCV.</p><p>When running in-context baselines besides GPT-3, we use the same number of training examples in the prompt. Note that this may be suboptimal due to other models having smaller context windows; we leave improving upon these baselines to future work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 AdaBoost baseline details</head><p>We concatenated all non-label data in every training example into a single string, separated by periods, then constructed n-grams from all words and adjacent sets of n words in the dataset for n ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> after removing letter cases and certain special symbols. Each training or test example was vectorized as the count of each n-gram in the example. For the base estimator, we used decision trees with a maximum depth of 3. We ensembled 100 estimators with a learning rate of 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head><p>Avg ADE B77 NIS OSE Over SOT SRI TAI ToS TEH TC  We tuned several hyperparameters in our AdaBoost implementation. First, we tested the learning rate of AdaBoost, the rate at which the weights of the ensembled classifiers are changed, finding that it didn't change results substantially from within a reasonable range. We then tested a number of different depths of decision trees in the ensemble, finding that low depths were ideal. Finally, we tested the number of trees to ensemble, finding that around 50 to 100 trees perform the best. All hyperparameters were tuned with leave-one-out cross validation.  <ref type="table" target="#tab_2">Table 10</ref>: LOO Cross Validation performance for number of trees, F1 scores from an AdaBoost ensemble classifier with learning rate 1.0 trained on n-grams of the dataset for n ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learn Rate</head><p>collect 5 labels for each of the 100 data points. We then take the plurality vote for each data point, breaking ties randomly.</p><p>Due to extreme class imbalance, we conduct only an annotation phase of 200 data points for the SRI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.2 Instructions</head><p>We attempted to mimic annotation instructions reported by the works introducing datasets whenever possible. The instructions we gave to annotators was as follows (parts enclosed in brackets denote variations in the instructions depending on the task or phase):</p><p>[If qualification phase: This task will serve as a qualification stage for annotation on a larger set. Label at least 10 examples to be considered for qualification for the annotation task. Please only complete this qualification if you're available to label 100 more data points in the next day.] You may use info on the internet (e.g. Google searches) to help you.</p><p>We know that labeling accuracy will (a) vary some based on level of background knowledge and (b) have some inherent subjectivity. Please select your best guess for each data point.</p><p>Task-specific instructions are detailed in Section A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.3 Costs</head><p>We spent $2,030 compensating crowdworkers for human baselines. We conservatively estimate that workers were paid $15/hr.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 . 4 E</head><label>14</label><figDesc>E = T ? I ? S tokens are allotted for the training examples and classification target. 2. The classification target is truncated to 1 4 E tokens. 3. Each of the N remaining training examples is truncated to 3 N tokens. We truncate from a training example's data fields first, leaving the label intact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>{</head><label></label><figDesc>' Sentence ': ' No regional side effects were noted . ' , ' Label ': ' not ADE -related '} Banking77 (B77) { ' Query ': ' Is it possible for me to change my P ... ' , ' Label ': ' change_pin '} NeurIPS impact statement risks (NIS) { ' Paper title ': ' Auto -Panoptic : Cooperative Mul ... ' , ' Paper link ': ' https :// proceedings . neurips . cc /... ' , ' Impact statement ': ' This work makes the first ... ' , 'ID ': '0 ' , ' Label ': " doesn ' t mention a harmful application "} OneStopEnglish (OSE) { ' Article ': ' For 85 years , it was just a grey b ... ' , ' Label ': ' intermediate '} Overruling (Over) { ' Sentence ': ' in light of both our holding toda ... ' , ' Label ': ' overruling '} Semiconductor org types (SOT) { ' Paper title ': '3 Gb / s AC -coupled chip -to -chip ... ' , ' Organization name ': ' North Carolina State Uni ... ' , ' Label ': ' university '} Systematic review inclusion (SRI) { ' Title ': ' Prototyping and transforming facial ... ' , ' Abstract ': ' Wavelet based methods for prototy ... ' , ' Authors ': ' Tiddeman , B .; Burt , M .; Perrett , D . ' , ' Journal ': ' IEEE Comput Graphics Appl ' , ' Label ': ' not included '} TAI safety research (TAI) { ' Title ': ' Malign generalization without intern ... ' , ' Abstract Note ': " In my last post , I challenge ..." , 'Url ': ' https :// www . alignmentforum . org / posts / y ... ' , ' Publication Year ': '2020 ' , ' Item Type ': ' blogPost ' , ' Author ': ' Barnett , Matthew ' , ' Publication Title ': ' AI Alignment Forum ' , ' Label ': ' TAI safety research '} Terms of Service (ToS) { ' Sentence ': ' Crowdtangle may change these term ... ' , ' Label ': ' potentially unfair '} TweetEval Hate (TEH) { ' Tweet ': ' New to Twitter --any men on here kno ... ' , ' Label ': ' not hate speech '} Twitter complaints (TC) { ' Tweet text ': ' @HMRCcustomers No this is my fi ... ' , ' Label ': ' no complaint '}OneStopEnglish (OSE)The following is an article sourced from The Guardian newspaper, and rewritten by teachers to suit three levels of adult English as Second Language (ESL) learners: elementary, intermediate, and advanced. Predict the level of the article.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Avg ADE B77 NIS OSE Over SOT SRI TAI ToS TEH TC Task-Specific 0.593 0.752 0.081 0.566 0.231 0.940 0.777 0.495 0.480 0.691 0.731 0.780 Generic 0.551 0.797 0.052 0.476 0.184 0.899 0.717 0.495 0.462 0.438 0.708 0.830</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 0 .</head><label>0</label><figDesc>611 0.696 0.076 0.571 0.528 0.860 0.745 0.474 0.672 0.789 0.618 0.688 10 0.593 0.667 0.096 0.559 0.456 0.920 0.623 0.479 0.642 0.610 0.735 0.733 25 0.617 0.714 0.090 0.740 0.445 0.960 0.591 0.412 0.643 0.548 0.778 0.862 49 0.598 0.653 0.074 0.643 0.394 0.960 0.692 0.375 0.586 0.643 0.718 0.842</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Avg ADE B77 NIS OSE Over SOT SRI TAI ToS TEH TC 0.03 0.547 0.587 0.012 0.760 0.344 0.900 0.538 0.495 0.720 0.621 0.475 00.643 0.000 0.392 0.432 0.597 0.284 0.495 0.653 0.368 0.333 0.319Table 8: LOO Cross Validation performance for learning rate, F1 scores from an AdaBoost ensemble classifier of 50 depth-1 decision trees trained on n-grams of the dataset for n ? [1, 5]. Depth Avg ADE B77 NIS OSE Over SOT SRI TAI ToS TEH TC1 0.546 0.636 0.000 0.718 0.466 0.919 0.385 0.495 0.699 0.621 0.494 0.569 2 0.511 0.592 0.000 0.716 0.405 0.900 0.366 0.495 0.466 0.527 0.626 0.524 3 0.549 0.721 0.004 0.735 0.463 0.919 0.366 0.495 0.507 0.621 0.626 0.586 4 0.531 0.556 0.000 0.672 0.410 0.880 0.438 0.495 0.607 0.602 0.524 0.653 5 0.506 0.619 0.004 0.583 0.318 0.860 0.360 0.495 0.451 0.602 0.592 0.684Table 9: LOO Cross Validation performance for depth of trees, F1 scores from an AdaBoost ensemble classifier of 50 decision trees with learning rate 1.0 trained on n-grams of the dataset for n ? [1, 5]. A.7 Human baseline details A.7.1 Labeling process For each dataset, we first conduct a qualification phase with 20 data points from the training set, showing labelers the other 30 as reference examples. Labelers who label at least 10 data points and achieved at least median accuracy advance to the annotation phase. In the annotation round, we # Trees Avg ADE B77 NIS OSE Over SOT SRI TAI ToS TEH TC 10 0.547 0.649 0.000 0.616 0.394 0.860 0.516 0.495 0.660 0.691 0.582 0.554 50 0.544 0.636 0.000 0.697 0.482 0.919 0.381 0.495 0.699 0.621 0.491 0.569 100 0.554 0.667 0.000 0.756 0.318 0.919 0.390 0.495 0.697 0.642 0.592 0.616 500 0.537 0.649 0.000 0.814 0.305 0.919 0.385 0.495 0.557 0.642 0.592 0.548</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>[</head><label></label><figDesc>Task-specific instructions] There are 50 [30 if qualification phase] labeled examples here to help you. If it seems that the instructions and examples are in conflict, use the examples as a guide.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Overview of the tasks in RAFT. Long inputs, Domain expertise, and Detailed instructions are some of the real-world challenges posed by RAFT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, this is approximately the number of examples people are willing to label for a task with a few thousand unlabeled examples. The 50 examples are chosen randomly, mirroring the applied setting in which one can't easily choose a balanced set. No examples beyond the chosen 50 are available for validation. 8</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Performance of RAFT baselines (F1)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Many classes: Humans most outperform GPT-3 on B77, which has by far the most classes in RAFT. With 77 classes and 50 labeled examples, many classes have no corresponding</figDesc><table /><note>11 49 rather than 50 training examples for LOO experiments 12 https://raft.elicit.org/baselines 13 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 14 https://www.surgehq.ai/ 15 For details on the human baseline gathering process, see Section A.7.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>1 Dataset licensingThe curating authors (NA, EL, and AS) bear all responsibility in the case of violation of rights. Below we provide information on the license for each dataset:ADE Corpus V2 (ADE). Unlicensed.Banking77 (B77). Creative Commons Attribution 4.0 International.NeurIPS impact statement risks (NIS). The NeurIPS impact statement dataset has an MIT License. We license the derivative NeurIPS impact statement risks dataset under Creative Commons Attribution 4.0 International.</figDesc><table><row><cell>OneStopEnglish (OSE). Creative Commons Attribution-ShareAlike 4.0 International.</cell></row><row><cell>Overruling (Over). Unlicensed.</cell></row><row><cell>Semiconductor org types (SOT). We license it under Creative Commons Attribution-</cell></row><row><cell>NonCommercial 4.0 International.</cell></row></table><note>Systematic review inclusion (SRI). Creative Commons Attribution 4.0 International TAI safety research (TAI). Creative Commons Attribution-ShareAlike 4.0 International Terms of Service (ToS). Unlicensed. TweetEval Hate (TEH). Unlicensed.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>A training example from every dataset, with textual label</figDesc><table><row><cell>Dataset</cell><cell>Training Sample</cell></row><row><cell>ADE Corpus V2</cell><cell></cell></row><row><cell>(ADE)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Instructions excerpt for each dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>content of individuals' non-public communications)? If so, please provide a description. No. ? Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No. ? Does the dataset relate to people? No.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>LOO Cross Validation performance for task-specific versus generic instructions, F1 scores. The experiment was run with 20 training examples for all datasets and no semantic selection.SelectionAvg ADE B77 NIS OSE Over SOT SRI TAI ToS TEH TCSemantic 0.622 0.696 0.098 0.635 0.454 0.940 0.716 0.419 0.696 0.578 0.778 0.836 Random 0.593 0.752 0.081 0.566 0.231 0.94 0.777 0.495 0.480 0.691 0.731 0.780</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>LOO Cross Validation performance for semantic versus random training example selection, F1 scores. The experiment was run with 20 training examples for all datasets and task-specific instructions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>LOO Cross Validation performance for number of training examples, F1 scores. The experiment was run with task-specific instructions and semantic selection of training examples.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Datasets are at https://raft.elicit.org/datasets 3 Instructions for submission are at https://raft.elicit.org/submit 4 https://huggingface.co/datasets</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">By only releasing 50 labelled examples, we make it difficult to cheat by using more than 50 examples for validation. For the NeurIPS impact statement risks and Semiconductor org type datasets, the test set labels aren't available publicly. For other datasets, the test set labels are available publicly but it is non-trivial and discouraged to seek them out.<ref type="bibr" target="#b8">9</ref> Ideally, we'd only allow information from before the time each dataset needed to be labeled. This isn't currently feasible, so we settle for a fully open-domain setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://beta.openai.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation. None of the existing benchmarks allow open-domain information retrieval. Like FLEX,</head><p>RAFT provides no extra validation data beyond the training examples. Perez et al. <ref type="bibr" target="#b23">[24]</ref> argue that the performance of state-of-the-art few-shot methods has been overestimated by most other existing benchmarks because they use labeled examples beyond the few training instances provided for model and parameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>Our automatic baseline collection was subsidized by compute credits generously provided by OpenAI. Ethan Perez, Samuel Bowman, and Long Ouyang gave feedback on early versions of the RAFT concept and dataset lists. Douwe Kiela and Stella Biderman offered helpful advice on the project direction. Ross Gruetzemacher suggested inclusion of the Twitter Complaints dataset. We thank Thomas Wolf and Simon Brandeis for discussions and advice around the design of the benchmark's infrastructure.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ai ethics statements -analysis and lessons learnt from neurips broader impact statements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Ashurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmie</forename><surname>Hine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Sedille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Carlier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01705</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tweet-Eval: Unified benchmark and comparative evaluation for tweet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.148</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.148" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="1644" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabetta</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viviana</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco Manuel Rangel</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2007</idno>
		<ptr target="https://aclanthology.org/S19-2007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5297715" />
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
	<note>If you use this software, please cite it using these metadata</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Flex: Unifying evaluation for few-shot nlp. arXiv: Computation and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno>doi: null</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Efficient intent detection with dual sentence encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Tem?inas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dataset of affiliation types</title>
		<ptr target="https://docs.google.com/spreadsheets/d/1CT3hCvbKxyJeS1FdrZtlK5MTuvWsfXR_" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Zero-shot learning in modern nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<ptr target="https://joeddav.github.io/blog/2020/05/29/ZSL.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="DOI">10.1006/jcss.1997.1504.URLhttps:/www.sciencedirect.com/science/article/pii/S002200009791504X</idno>
		<idno>0022-0000</idno>
		<ptr target="https://doi.org/10.1006/jcss.1997.1504.URLhttps://www.sciencedirect.com/science/article/pii/S002200009791504X" />
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Hal Daum? III au2, and Kate Crawford. Datasheets for datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Briana</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><forename type="middle">Mateen</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008.URLhttps:/www.sciencedirect.com/science/article/pii/</idno>
		<idno>1532-0464</idno>
		<ptr target="https://doi.org/10.1016/j.jbi.2012.04.008.URLhttps://www.sciencedirect.com/science/article/pii/" />
	</analytic>
	<monogr>
		<title level="m">S1532046412000615. Text Mining and Natural Language Processing in Pharmacogenomics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="885" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neurips affiliation locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<ptr target="https://github.com/nd7141/icml2020/blob/master/university2.csv" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.13461" />
		<imprint>
			<date type="published" when="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Claudette: an automated detector of potentially unfair clauses in online terms of service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemys?aw</forename><surname>Pa?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Contissa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Lagioia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Wolfgang</forename><surname>Micklitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10506-019-09243-2</idno>
		<ptr target="http://dx.doi.org/10.1007/s10506-019-09243-2" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="139" />
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What makes good in-context examples for gpt-3?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<title level="m">Natural instructions: Benchmarking generalization to new tasks from natural language instructions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What works to increase charitable donations? a meta-review with meta-meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noetel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Slattery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">K</forename><surname>Saeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joannie</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Houlden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Farr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romy</forename><surname>Gelber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Huuskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Timmons</surname></persName>
		</author>
		<ptr target="URLpsyarxiv.com/yxmva" />
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<ptr target="https://elicit.org" />
		<title level="m">The ai research assistant</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatically identifying complaints in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preo?iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Gaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1495</idno>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="19" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tai safety bibliographic database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jess</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelica</forename><surname>Deibel</surname></persName>
		</author>
		<ptr target="https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database#Inclusion___categorization1" />
		<imprint>
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploiting cloze questions for few shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How to grow a mind: Statistics, structure, and abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Lu?i?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-0535</idno>
		<ptr target="https://aclanthology.org/W18-0535" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>1905.00537</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new benchmark dataset for fake news detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2067</idno>
		<ptr target="https://aclanthology.org/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="17" to="2067" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference. CoRR, abs/1704.05426</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1704.05426" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Crossfit: A few-shot learning challenge for cross-task generalization in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.00161" />
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">This does not include intentionally removed information, but might include, e.g., redacted text. Not really sure what this means for our case. There&apos;s no redacted information, but there are undoubtedly tons of papers we failed to find in our literature search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Composition ? Is any information missing from individual instances in the dataset? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). and we undoubtedly made mistakes applying this criteria</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">? Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. See above. No redundancies that I know of</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">? Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals&apos; non-public communications)? If so</title>
		<imprint/>
	</monogr>
	<note>please provide a description</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">? Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">? Does the dataset relate to people? If not, you may skip the remaining questions in this section. Sort of. It&apos;s a database of papers, and those papers have authors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. It&apos;s a database of papers</title>
	</analytic>
	<monogr>
		<title level="m">? Does the dataset identify any subpopulations</title>
		<imprint/>
	</monogr>
	<note>and those papers have authors. This information is already public</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">? Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. We asked TAI safety organizations for what their employees had written, emailed some individual authors, and searched Google Scholar. See the LessWrong post for more details</title>
		<ptr target="https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database" />
	</analytic>
	<monogr>
		<title level="m">Collection ? How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? Mostly be hand. We collected citation information using an automated API call to Google Scholar. See the LessWrong post for more details</title>
		<ptr target="https://www.lesswrong.com/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">? Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? It was Angelica Deibel and me. I volunteered and paid Angelica $20/hour</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
