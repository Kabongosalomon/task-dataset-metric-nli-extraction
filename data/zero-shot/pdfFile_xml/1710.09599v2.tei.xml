<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Watch Your Step: Learning Node Embeddings via Graph Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alemi</surname></persName>
							<email>alemi@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google AI New</orgName>
								<address>
									<settlement>York City</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Google AI Mountain View</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Watch Your Step: Learning Node Embeddings via Graph Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph embedding methods represent nodes in a continuous vector space, preserving different types of relational information from the graph. There are many hyperparameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph. In this paper, we replace previously fixed hyperparameters with trainable ones that we automatically learn via backpropagation. In particular, we propose a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective. Unlike previous approaches to attention models, the method that we propose utilizes attention parameters exclusively on the data traversal (e.g. on the random walk), and are not used by the model for inference. We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. We improve state-of-the-art results on a comprehensive suite of real-world graph datasets including social, collaboration, and biological networks, where we observe that our graph attention model can reduce the error by 20% to 40%. We show that our automatically-learned attention parameters can vary significantly per graph, and correspond to the optimal choice of hyper-parameter if we manually tune existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised graph embedding methods seek to learn representations that encode the graph structure. These embeddings have demonstrated outstanding performance on a number of tasks including node classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13]</ref>, knowledge-base completion <ref type="bibr" target="#b22">[23]</ref>, semi-supervised learning <ref type="bibr" target="#b34">[35]</ref>, and link prediction <ref type="bibr" target="#b1">[2]</ref>. In general, as introduced by Perozzi et al <ref type="bibr" target="#b27">[28]</ref>, these methods operate in two discrete steps: First, they sample pair-wise relationships from the graph through random walks and counting node co-occurances. Second, they train an embedding model e.g. using Skipgram of word2vec <ref type="bibr" target="#b23">[24]</ref>, to learn representations that encode pairwise node similarities.</p><p>While such methods have demonstrated positive results on a number of tasks, their performance can significantly vary based on the setting of their hyper-parameters. For example, <ref type="bibr" target="#b27">[28]</ref> observed that the quality of learned representations is dependent on the length of the random walk (C). In practice, DeepWalk <ref type="bibr" target="#b27">[28]</ref> and many of its extensions [e.g. 13] use word2vec implementations <ref type="bibr" target="#b23">[24]</ref>. Accordingly, it has been revealed by <ref type="bibr" target="#b19">[20]</ref> that the hyper-parameter C, refered to as training window length in word2vec <ref type="bibr" target="#b23">[24]</ref>, actually controls more than a fixed length of the random walk. Instead, it parameterizes a function, we term the context distribution and denote Q, which controls the probability of sampling a node-pair when visited within a specific distance 2 . Implicitly, the choices of C and Q, create a weight mass on every node's neighborhood. In general, the weight is higher on nearby nodes, but the specific form of the of the mass function is determined by the aforementioned hyper-parameters. In this work, we aim to replace these hyper-parameters with trainable parameters, so that they can be automatically learned for each graph. To do so, we pose graph embedding as end-to-end learning, where the (discrete) two steps of random walk co-occurance sampling, followed by representation learning, are joint using a closed-form expectation over the graph adjacency matrix.</p><p>Our inspiration comes from the successful application of attention models in domains such as Natural Language Processing (NLP) [e.g. <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>, image recognition <ref type="bibr" target="#b24">[25]</ref>, and detecting rare events in videos <ref type="bibr" target="#b28">[29]</ref>. To the best of our knowledge, the approach we propose is significantly different from the standard application of attention models. Instead of using attention parameters to guide the model where to look when making a prediction, we use attention parameters to guide our learning algorithm to focus on parts of the data that are most helpful for optimizing upstream objective.</p><p>We show mathematical equivalence between the context distribution and the co-efficients of power series of the transition matrix. This allows us to learn the context distribution by learning an attention model on the power series. The attention parameters "guide" the random walk, by allowing it to focus more on short-or long-term dependencies, as best suited for the graph, while optimizing an upstream objective. To the best of our knowledge, this work is the first application of attention methods to graph embedding.</p><p>Specifically, our contributions are the following:</p><p>1. We propose an extendible family of graph attention models that can learn arbitrary (e.g. non-monotonic) context distributions.</p><p>2. We show that the optimal choice of context distribution hyper-parameters for competing methods, found by manual tuning, agrees with our automatically-found attention parameters.</p><p>3. We evaluate on a number of challenging link prediction tasks comprised of real world datasets, including social, collaboration, and biological networks. Experiments show we substantially improve on our baselines, reducing link-prediction error by 20%-40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Embeddings</head><p>Given an unweighted graph G = (V, E), its (sparse) adjacency matrix A ? {0, 1} |V |?|V | can be constructed according to</p><formula xml:id="formula_0">A vu = 1[(v, u) ? E],</formula><p>where the indicator function 1[.] evaluates to 1 iff its boolean argument is true. In general, graph embedding methods minimize an objective: ||A ? L ? R || F 2 Specifically, rather than using C as constant and assuming all nodes visited within distance C are related, a desired context distance ci is sampled from uniform (ci ? U{1, C}) for each node pair i in training. If the node pair i was visited more than ci-steps apart, it is not used for training. This was revealed to us by Levy et al <ref type="bibr" target="#b19">[20]</ref>, see their Section 3.1. Many DeepWalk-style methods inherited this, as they utilize word2vec implementation. <ref type="bibr" target="#b2">3</ref> Also known in NLP <ref type="bibr" target="#b23">[24]</ref> as the "input" and "output" embedding representations.</p><formula xml:id="formula_1">min Y L(f (A), g(Y)); (1) where Y ? R |V |?d is a d-dimensional node embedding dictionary; f : R |V |?|V | ? R |V |?|V | is</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Embeddings via Random Walks</head><p>Introduced by <ref type="bibr" target="#b27">[28]</ref>, this family of methods [incl. <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref> induce random walks along E by starting from a random node v 0 ? sample(V ), and repeatedly sampling an edge to transition to next node</p><formula xml:id="formula_2">as v i+1 := sample(E[v i ]), where E[v i ] are the outgoing edges from v i . The transition sequences v 0 ? v 1 ? v 2 ? .</formula><p>. . (i.e. random walks) can then be passed to word2vec algorithm, which learns embeddings by stochastically taking every node along the sequence v i , and the embedding representation of this anchor node v i is brought closer to the embeddings of its next neighbors, {v i+1 , v i+2 , . . . , v i+c }, the context nodes. In practice, the context window size c is sampled from a distribution e.g. uniform U{1, C} as explained in <ref type="bibr" target="#b19">[20]</ref>.</p><p>Let D ? R |V |?|V | be the co-occurrence matrix from random walks, with each entry D vu containing the number of times nodes v and u are co-visited within context distance c ? U {1, C}, in all simulated random walks. Embedding methods utilizing random walks, can also be viewed using the framework of Eq. (1). For example, to get Node2vec <ref type="bibr" target="#b12">[13]</ref>, we can set f (A) = D, set the edge function to the embeddings outer product g(Y) = Y ? Y , and set the loss function to negative log likelihood of softmax, yielding:</p><formula xml:id="formula_3">min Y ? ? log Z ? v,u?V D vu (Y v Y u ) ? ? ,<label>(2)</label></formula><p>where partition function Z = v,u exp(Y v Y u ) can be estimated with negative sampling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Graph Likelihood</head><p>A recently-proposed objective for learning embeddings is the graph likelihood <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_4">v,u?V ?(g(Y) v,u ) Dvu (1 ? ?(g(Y) v,u )) 1[(v,u)/ ?E] ,<label>(3)</label></formula><p>where g(Y) v,u is the output of the model evaluated at edge (v, u), given node embedings Y; the activation function ?(.) is the logistic; Maximizing the graph likelihood pushes the model score g(Y) v,u towards 1 if value D vu is large and pushes it towards 0 if (v, u) / ? E.</p><p>In our work, we minimize the negative log of Equation 3, written in our matrix notation as:</p><formula xml:id="formula_5">min Y ||?D ? log (?(g(Y))) ? 1[A = 0] ? log (1 ? ?(g(Y)))|| 1 ,<label>(4)</label></formula><p>which we minimize w.r.t node embeddings Y ? R |V |?d , where ? is the Hadamard product; and the L1-norm ||.|| 1 of a matrix is the sum of its entries. The entries of this matrix are positive because 0 &lt; ?(.) &lt; 1. Matrix D ? R |V |?|V | can be created similar to <ref type="bibr" target="#b1">[2]</ref>, by counting node co-occurances in simulated random walks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention Models</head><p>We mention attention models that are most similar to ours [e.g. <ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> , where an attention function is employed to suggest positions within the input example that the classification function should pay attention to, when making inference. This function is used during the training phase in the forward pass and in the testing phase for prediction. The attention function and the classifier are jointly trained on an upstream objective e.g. cross entropy. In our case, the attention mechanism is only guides the learning procedure, and not used by the model for inference. Our mechanism suggests parts of the data to focus on, during training, as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>Following our general framework (Eq 1), we set g(</p><formula xml:id="formula_6">Y) = g ([L | R]) = L ? R T and f (A) = E[D]</formula><p>, the expectation on co-occurrence matrix produced from simulated random walk. Using this closed form, we extend the NLGL loss (Eq. 4) to include attention parameters on the random walk sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Expectation on the co-occurance matrix: E[D]</head><p>Rather than obtaining D by simulation of random walks and sampling co-occurances, we formulate an expectation of this sampling, as E[D]. In general. this allows us to tune sampling parameters living inside of the random walk procedure including number of steps C.</p><p>Let T be the transition matrix for a graph, which can be calculated by normalizing the rows of A to sum to one. This can be written as:</p><formula xml:id="formula_7">T = diag(A ? 1 n ) ?1 ? A.<label>(5)</label></formula><p>Given an initial probability distribution p (0) ? R |V | of a random surfer, it is possible to find the distribution of the surfer after one step conditioned on p (0) as p (1) = p (0) T T and after k steps as</p><formula xml:id="formula_8">p (k) = p (0) T (T ) k , where (T ) k multiplies matrix T with itself k-times.</formula><p>We are interested in an analytical expression for E[D], the expectation over co-occurrence matrix produced by simulated random walks. A closed form expression for this matrix will allow us to perform end-to-end learning.</p><p>In practice, random walk methods based on DeepWalk <ref type="bibr" target="#b27">[28]</ref> do not use C as a hard limit; instead, given walk sequence (v 1 , v 2 , . . . ), they sample c i ? U{1, C} separately for each anchor node v i and potential context nodes, and only keep context nodes that are within c i -steps of v i . In expectation then, nodes v i+1 , v i+2 , v i+3 , . . . , will appear as context for anchor node v i , respectively with probabilities</p><formula xml:id="formula_9">1, 1 ? 1 C , 1 ? 2 C , . . . . We can write an expectation on D ? R |V |?|V | : E D DEEPWALK ; C = C k=1 Pr(c ? k)P (0) (T ) k ,<label>(6)</label></formula><p>which is parametrized by the (discrete) walk length C; where Pr(c ? k) indicates the probability of node with distance k from anchor to be selected; andP (0) ? R |V |?|V | is a diagonal matrix (the initial positions matrix), withP</p><formula xml:id="formula_10">(0)</formula><p>vv set to the number of walks starting at node v. Since Pr(c = k) = 1 C for all k = {1, 2, . . . , C}, we can expand Pr(c ? k) = C j=k P (c = j), and re-write the expectation as:</p><formula xml:id="formula_11">E D DEEPWALK ; C =P (0) C k=1 1 ? k ? 1 C (T ) k .<label>(7)</label></formula><p>Eq. (3.1) is derived, step-by-step, in the Appendix. We are not concerned by the exact definition of the scalar coefficient, 1 ? k?1 C , but we note that the coefficient decreases with k. Instead of keeping C a hyper-parameter, we want to analytically optimize it on an upstream objective. Further, we are interested to learn the co-efficients to (T ) k instead of hand-engineering a formula.</p><p>As an aside, running the GloVe embedding algorithm <ref type="bibr" target="#b26">[27]</ref> over the random walk sequences, in expectation, is equivalent to factorizing the co-occurance matrix:</p><formula xml:id="formula_12">E D GloVe ; C =P (0) C k=1 1 k (T ) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning the Context Distribution</head><p>We want to learn the co-efficients to (T ) k . Let the context distribution Q be a C-dimensional vector</p><formula xml:id="formula_13">as Q = (Q 1 , Q 2 , ? ? ? , Q C ) with Q k ? 0 and k Q k = 1.</formula><p>We assign co-efficient Q k to (T ) k . Formally, our expectation on D is parameterized with, and is differentiable w.r.t., Q:</p><formula xml:id="formula_14">E [D; Q 1 , Q 2 , . . . Q C ] =P (0) C k=1 Q k (T ) k =P (0) E k?Q [(T ) k ],<label>(8)</label></formula><p>Training embeddings over random walk sequences, using word2vec or GloVe, respectively, are special cases of <ref type="bibr">Equation 8</ref>, with Q fixed apriori as</p><formula xml:id="formula_15">Q k = 1 ? k?1 C or Q k ? 1 k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Attention Models</head><p>To learn Q automatically, we propose an attention model which guides the random surfer on "where to attend to" as a function of distance from the source node. Specifically, we define a Graph Attention Model as a process which models a node's context distribution Q as the output of softmax:    where the variables q k are trained via backpropagation, jointly while learning node embeddings. Our hypothesis is as follows. If we don't impose a specific formula on Q = (Q 1 , Q 2 , . . . Q C ), other than (regularized) softmax, then we can use very large values of C and allow every graph to learn its own form of Q with its preferred sparsity and own decay form. Should the graph structure require a small C, then the optimization would discover a left-skewed Q with all of probability mass on {Q 1 , Q 2 } and k&gt;2 Q k ? 0. However, if according to the objective, a graph is more accurately encoded by making longer walks, then they can learn to use a large C (e.g. using uniform or even right-skewed Q distribution), focusing more attention on longer distance connections in the random walk.</p><formula xml:id="formula_16">(Q 1 , Q 2 , Q 3 , . . . ) = softmax((q 1 , q 2 , q 3 , . . . )),<label>(9)</label></formula><p>To this end, we propose to train softmax attention model on the infinite power series of the transition matrix. We define an expectation on our proposed random walk matrix D softmax[?] as <ref type="bibr" target="#b3">4</ref> :</p><formula xml:id="formula_17">E D softmax[?] ; q 1 , q 2 , q 3 , . . . =P (0) lim C?? C k=1 softmax(q 1 , q 2 , q 3 , . . . ) k (T ) k ,<label>(10)</label></formula><p>where q 1 , q 2 , . . . are jointly trained with the embeddings to minimize our objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objective</head><p>The final training objective for the Softmax attention mechanism, coming from the NLGL Eq. (4), min L,R,q</p><formula xml:id="formula_18">?||q|| 2 2 + ?E[D; q] ? log ?(L ? R ) ? 1[A = 0] ? log 1 ? ?(L ? R ) 1 (11)</formula><p>is minimized w.r.t attention parameter vector q = (q 1 , q 2 , . . . ) and node embeddings L, R ? R |V |? d 2 . Hyper-parameter ? ? R applies L2 regularization on the attention parameters. We emphasize that our attention parameters q live within the expectation over data D, and are not part of the model (L, R) and are therefore not required for inference. The constraint k Q k = 1, through the softmax activation, prevents E[D softmax ] from collapsing into a trivial solution (zero matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Algorithmic Complexity</head><p>The naive computation of (T ) k requires k matrix multiplications and so is O(|V | 3 k). However, as most real-world adjacency matrices have an inherent low rank structure, a number of fast approximations to computing the random walk transition matrix raised to a power k have been proposed [e.g. <ref type="bibr" target="#b31">32]</ref>. Alternatively SVD can decompose T as T = U?V T and then the k th power can be calculated by raising the diagonal matrix of singular values to k as (T ) k = U(?) k V T since V T U = I. Furthermore, the SVD can be approximated in time linear to the number of non-zero entries <ref type="bibr" target="#b13">[14]</ref>. Therefore, we can calculate (T ) k in O(|E|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Extensions</head><p>As presented, our proposed method can learn the weights of the context distribution C. However, we briefly note that such a model can be trivially extended to learn the weight of any other type of pair-wise node similarity (e.g. Personalized PageRank, Adamic-Adar, etc). In order to do this, we can extend the definition of the context Q with an additional dimension Q k+1 for the new type of similarity, and an additional element in the softmax q k+1 to learn a joint importance function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Link Prediction Experiments</head><p>We evaluate the quality of embeddings produced when random walks are augmented with attention, through experiments on link prediction <ref type="bibr" target="#b21">[22]</ref>. Link prediction is a challenging task, with many real world applications in information retrieval, recommendation systems and social networks. As such, it has been used to study the properties of graph embeddings <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13]</ref>. Such an intrinsic evaluation emphasizes the structure-preserving properties of embedding.</p><p>Our experimental setup is designed to determine how well the embeddings produced by a method captures the topology of the graph. We measure this in the manner of <ref type="bibr" target="#b12">[13]</ref>: remove a fraction (=50%) of graph edges, learn embeddings from the remaining edges, and measure how well the embeddings can recover those edges which have been removed. More formally, we split the graph edges E into two partitions of equal size E train and E test such that the training graph is connected. We also sample non existent edges ((u, v) / ? E) to make E ? train and E ? test . We use (E train , E ? train ) for training and model selection, and use (E test , E ? test ) to compute evaluation metrics. Training: We train our models using TensorFlow, with PercentDelta optimizer <ref type="bibr" target="#b0">[1]</ref>. For the results <ref type="table">Table 1</ref>, we use ? = 0.5, C = 10, andP (0) = diag(80), which corresponds to 80 walks per node. We analyze our model's sensitivity in Section 4.2. To ensure repeatability of results, we have released our model and evaluation scripts. <ref type="bibr" target="#b4">5</ref> . <ref type="table">Table 1a</ref> describes the datasets used in our experiments. Datasets available from SNAP https://snap.stanford.edu/data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We evaluate against many baselines. For all methods, we calculate g(Y) ? R |V |?|V | , and extract entries from g(Y) corresponding to positive and negative test edges, then use them to compute ROC AUC. We compare against following baselines: -EigenMaps <ref type="bibr" target="#b4">[5]</ref>. Minimizes Euclidean distance of adjacent nodes of A.</p><p>-SVD. Singular value decomposition of A.</p><p>-DNGR <ref type="bibr" target="#b7">[8]</ref>. Non-linear (i.e. deep) embedding of nodes, using an auto-encoder on A. We use author's code to learn the deep embeddings Y and use for inference g(Y) = YY T . -node2vec <ref type="bibr" target="#b12">[13]</ref>. Simulates random walks and uses word2vec to learn node embeddings. Minimizes We note that both methods are unsupervised, and we have colored the learned representations by node labels. <ref type="figure">Fig. (c)</ref> However, quantitatively, our embeddings achieves better separation.</p><p>objective in Eq. (2). For <ref type="table">Table 1</ref>, we use author's code to learn embeddings Y then use g(Y) = YY .</p><p>We run with C = 2 and C = 5. 6 -AsymProj <ref type="bibr" target="#b1">[2]</ref>. Learns edges as asymmetric projections in a deep embedding space, trained by maximizing the graph likelihood (Eq. 3). We take results from authors.</p><p>Results: Our results, summarized in <ref type="table">Table 1</ref>, show that our proposed methods substantially outperform all baseline methods. Specifically, we see that the error is reduced by up to 45% over baseline methods which have fixed context definitions. This shows that by parameterizing the context distribution and allowing each graph to learn its own distribution, we can better preserve the graph structure (and thereby better predict missing edges).</p><p>Discussion: <ref type="figure" target="#fig_2">Figure 2a</ref> shows how the learned attention weights Q vary across datasets. Each dataset learns its own attention form, and the highest weights generally correspond to the highest weights when doing a grid search over C for node2vec (as in <ref type="figure">Figure 1b</ref>).</p><p>The hyper-parameter C determines the highest power of the transition matrix, and hence the maximum context size available to the attention model. We suggest using large values for C, since the attention weights can effectively use a subset of the transition matrix powers. For example, if a network needs only 2 hops to be accurately represented, then it is possible for the softmax attention model to learn Q 3 , Q 4 , ? ? ? ? 0. <ref type="figure" target="#fig_2">Figure 2b</ref> shows how varying the regularization term ? allows the softmax attention model to "attend to" only what each dataset requires. We observe that for most graphs, the majority of the mass gets assigned to Q 1 , Q 2 . This shows that shorter walks are more beneficial for most graphs. However, on wiki-vote, better embeddings are produced by paying attention to longer walks, as its softmax Q is uniform-like, with a slight right-skew.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sensitivity Analysis</head><p>So far, we have removed two hyper-parameters, the maximum window size C, and the form of the context distribution U. In exchange, we have introduced other hyper-parameters -specifically walk length (also C) and a regularization term ? for the softmax attention model. Nonetheless, we show that our method is robust to various choices of these two. <ref type="figure" target="#fig_2">Figures 2a and 2b</ref> both show that the softmax attention weights drop to almost zero if the graph can be preserved using shorter walks, which is not possible with fixed-form distributions (e.g. U). <ref type="figure">Figure 4</ref> examines this relationship in more detail for d = 128 dimensional embeddings, sweeping our hyper-parameters C and ?, and comparing results to the best and worst node2vec embeddings for C ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>. (Note that node2vec lines are horizontal, as they do not depend on ?.) We observe that all the accuracy metrics are within 1% to 2%, when varying these hyper-parameters, and are all still well-above our baseline (which sample from a fixed-form context distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node Classification Experiments</head><p>We conduct node classification experiments, on two citation datasets, Cora and Citeseer, with the following statistics: Cora contains (2, 708 nodes, 5, 429 edges and K = 7 classes); and Citeseer contains (3, 327 nodes, 4, 732 edges and K = 6 classes). We learn embeddings from only the graph structure (nodes and edges), without observing node features nor labels during training. <ref type="figure" target="#fig_3">Figure 3</ref> shows t-SNE visualization of the Cora dataset, comparing our method with node2vec <ref type="bibr" target="#b12">[13]</ref>. For classification, we follow the data splits of <ref type="bibr" target="#b34">[35]</ref>. We predict labels L ? R |V |?K as:  <ref type="figure">Figure 4</ref>: Sensitivity Analysis of softmax attention model. Our method is robust to choices of both ? and C. We note that it consistently outperforms even an optimally set node2vec.</p><formula xml:id="formula_19">L = exp (?g(Y)) ? L train ,</formula><p>nodes in training set and zeros elsewhere. The scalar ? ? R is manually tuned on the validation set.</p><p>The classification results, summarized in <ref type="table">Table 3c</ref>, show that our model learns a better unsupervised representation than previous methods, that can then be used for supervised tasks. We do not compare against other semi-supervised methods that utilize node features during training and inference [incl. <ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19]</ref>, as our method is unsupervised.</p><p>Our classification prediciton function contains one scalar parameter ?. It can be thought of a "smooth" k-nearest-neighbors, as it takes a weighted average of known labels, where the weights are exponential of the dot-product similarity. Such a simple function should introduce no model bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The field of learning on graphs has attracted much attention lately. Here we summarize two broad classes of algorithms, and point the reader to several recent reviews <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> for more context.</p><p>The first class of algorithms are semi-supervised and concerned with predicting labels over a graph, its edges, and/or its nodes. Typically, these algorithms process a graph (nodes and edges) as well as per-node features. These include recent graph convolution methods [e.g. <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33]</ref> with spectral variants <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>, diffusion methods [e.g. <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9]</ref>, including ones trained until fixed-point convergence <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b20">21]</ref> and semi-supervised node classification <ref type="bibr" target="#b34">[35]</ref> with low-rank approximation of convolution <ref type="bibr" target="#b18">[19]</ref>. We differ from these methods as (1) our algorithm is unsupervised (trained exclusively from the graph structure itself) without utilizing labels during training, and (2) we explicitly model the relationship between all node pairs.</p><p>The second class of algorithms consist of unsupervised graph embedding methods. Their primary goal is to preserve the graph structure, to create task independent representations. They explicitly model the relationship of all node pairs (e.g. as dot product of node embeddings). Some methods directly use the adjacency matrix <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b33">34]</ref>, and others incorporate higher order structure (e.g. from simulated random walks) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b1">2]</ref>. Our work falls under this class of algorithms, where inference is a scoring function V ? V ? R, trained to score positive edges higher than negative ones. Unlike existing methods, we do not specify a fixed context distribution apriori, whereas we push gradients through the random walk to those parameters, which we jointly train while learning the embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an attention mechanism for learning the context distribution used in graph embedding methods. We derive the closed-form expectation of the DeepWalk <ref type="bibr" target="#b27">[28]</ref> co-occurrence statistics, showing an equivalence between the context distribution hyper-parameters, and the coefficients of the power series of the graph transition matrix. Then, we propose to replace the context hyper-parameters with trainable models, that we learn jointly with the embeddings on an objective that preserves the graph structure (the Negative Log Graph Likelihood, NLGL). Specifically, we propose Graph Attention Models, using a softmax to learn a free-form contexts distribution with a parameter for each type of context similarity (e.g. distance in a random walk).</p><p>We show significant improvements on link prediction and node classification over state-of-theart baselines (that use a fixed-form context distribution), reducing error on link prediction and classification, respectively by up to 40% and 10%. In addition to improved performance (by learning distributions of arbitrary forms), our method can obviate the manual grid search over hyper-parameters:</p><p>walk length and form of context distribution, which can drastically fluctuate the quality of the learned embeddings and are different for every graph. On the datasets we consider, we show that our method is robust to its hyper-parameters, as described in Section 4.2. Our visualizations of converged attention weights convey to us that some graphs (e.g. voting graphs) can be better preserved by using longer walks, while other graphs (e.g. protein-protein interaction graphs) contain more information in short dependencies and require shorter walks.</p><p>We believe that our contribution in replacing these sampling hyperparameters with a learnable context distribution is general and can be applied to many domains and modeling techniques in graph representation learning. Using Markov Chain, we can write:</p><formula xml:id="formula_20">Pr (x(i + k) = u | x(i) = v) = Pr (x(k) = u | x(0) = v) = T k uv<label>(12)</label></formula><p>Now, if node u was visited k steps after node v, then the probabilitiy of it being sampled is given by:</p><formula xml:id="formula_21">Pr (u ? W v (k) | x(k) = u, x(0) = v).<label>(13)</label></formula><p>In case of DeepWalk <ref type="bibr" target="#b27">[28]</ref>, probability above equals:</p><formula xml:id="formula_22">Pr (c ? k | x(k) = u, x(0) = v) where c ? U{1, C},<label>(14)</label></formula><p>and </p><p>= (C ? k + 1)</p><formula xml:id="formula_24">1 C = 1 ? k ? 1 C ,<label>(17)</label></formula><p>where second line is trivial since the events c = j are disjoint. We can now use Bayes' rule to derive the probability of u being visited k steps after v and being selected in v's sampled context, as:</p><formula xml:id="formula_25">Pr (u ? W v (k), x(k) = u | x(0) = v) = Pr (u ? W v (k) | x(k) = u, x(0) = v) Pr (x(k) = u | x(0) = v) = 1 ? k ? 1 C T k uv<label>(18)</label></formula><p>Now, let E vku be the event that a walker visits v and after k steps, visits u and selects it part of its context. This event happens with the probability indicated in Equation <ref type="bibr" target="#b17">18</ref>. Concretely,</p><formula xml:id="formula_26">E [E vku | x(0) = v] = 1 ? k ? 1 C T k uv .<label>(19)</label></formula><p>Let E v * u count the events {E vku : k ? [1, C]}, then:</p><formula xml:id="formula_27">E [E v * u | x(0) = v] = E C k=1 E vku x(0) = v (20) = C k=1 E [E vku | x(0) = v] = C k=1 1 ? k ? 1 C T k uv .<label>(21)</label></formula><p>Suppose we run DeepWalk, starting m random walks from each node v, then the expected number of times that u is present in the context of v is given by:</p><formula xml:id="formula_28">E D DEEPWALK uv = mE [E v * u | x(0) = v] = m C k=1 1 ? k ? 1 C T k uv .</formula><p>Finally, we can write down the expectation over the square matrix D: The github code of DeepWalk and node2vec start a fixed number (m) walks from every graph node v ? V . For node2vec, m defaults to 10, see num-walks flag in https://github.com/ aditya-grover/node2vec/blob/master/src/main.py. Therefore, in our experiments, we set P (0) := diag(m, m, . . . , m). This initial condition yields D vu to be the expected number of times that u is visited if we started m walks from v. There can be other reasonable choices. Nonetheless, we use what worked well in practice for <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>. We leave the search for a betterP (0) as future work.</p><formula xml:id="formula_29">E D DEEPWALK = diag(m,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Depiction of Learned Context Distribution</head><p>DeepWalk Ours social graph voting graph <ref type="figure">Figure:</ref> Depiction of how our model assigns context distributions (shaded red) compared to earlier work. We depict the graph from the perspective of anchor node (yellow). Given a social graph (top), where friends of friends are usually friends, our algorithm learns a leftskewed distribution. Given a voting graph (bottom), with general transitivity: a ? b ? c =? a ? c, it learns a long-tail distribution. Earlier methods (e.g. DeepWalk) use word2vec, which internally uses a linear decay context distribution, treating all graphs the same.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Learned Attention weights Q (log scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 (</head><label>7</label><figDesc>b) Q with varying the regularization ? (linear scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) shows learned attention weights Q, which agree with grid-search of node2vec (Figure 1b). (b) shows how varying ? affects the learned Q. Note that distributions can quickly tail off to zero (ego-Facebook and PPI), while other graphs (wiki-vote) contain information across distant nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Node Classification. Fig. (a)/(b): t-SNE visualization of node embeddings for Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>event k ? c is independant of the condition (x(k) = u ? x(0) = v). Further, event k ? c can be partitioned and Eq. (14) can be written as Pr (c = k ? c = k + 1 ? ? ? ? ? c = C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>; and finally setting L to the Frobenius norm of the error, yielding:min</figDesc><table><row><cell>L,R</cell></row></table><note>a transformation of the adjacency matrix; g : R |V |?d ? R |V |?|V | is a pairwise edge function; and L : R |V |?|V | ? R |V |?|V | ? R is a loss function. For instance, a stochastic version of Singular Value Decomposition (SVD) is an embedding method, and can be casted into our framework by setting f (A) = A; decomposing Y into two halves, the left-and right-embedding representations 3 as Y = [L|R] with L, R ? R |V |? d 2 then setting g to their outer product g(Y) = g([L|R]) = L ? R</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Test ROC-AUC as a function of C using node2vec.Figure 1: In 1a we present statistics of our datasets. In 1b, we motivate our work by showing the necessity of setting the parameter C for node2vec (d=128, each point is the average of 7 runs).</figDesc><table><row><cell>Dataset wiki-vote ego-Facebook ca-AstroPh ca-HepTh PPI [31]</cell><cell cols="5">|V | 7, 066 103, 663 |E| 4, 039 88, 234 17, 903 197, 031 researchers nodes users users 8, 638 24, 827 researchers 3, 852 20, 881 proteins</cell><cell cols="2">votes friendship co-authorship co-authorship chemical interaction edges</cell><cell>ROC-AUC</cell><cell>0.9900 0.9905 0.9910 0.9915 0.9920 0.9925</cell><cell>facebook</cell><cell>0.70 0.75 0.80 0.85</cell><cell>ppi</cell><cell>0.60 0.61 0.62 0.63 0.64</cell><cell>wiki-vote</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 2 3 4 5 6 7 8 9 10 C</cell><cell>1 2 3 4 5 6 7 8 9 10 C</cell><cell>1 2 3 4 5 6 7 8 9 10 C</cell></row><row><cell cols="11">(a) Datasets used in our experiments. Adjacency Matrix (b) Dataset dim D by Simulation Eigen Maps SVD DNGR node2vec C = 2 node2vec C = 5 Asym Proj</cell><cell>Graph Attention Error Reduction (ours) (10)</cell></row><row><cell cols="7">64 61.3 86.0 59.8 wiki-vote 128 62.2 80.8 55.4</cell><cell>64.4 63.7</cell><cell></cell><cell></cell><cell>63.6 64.6</cell><cell>91.7 91.7</cell><cell>93.8 ? 0.13 93.8 ? 0.05</cell><cell>25.2% 25.2%</cell></row><row><cell cols="2">ego-Facebook</cell><cell cols="5">64 96.4 96.7 98.1 128 95.4 94.5 98.4</cell><cell>99.1 99.3</cell><cell></cell><cell></cell><cell>99.0 99.2</cell><cell>97.4 97.3</cell><cell>99.4 ? 0.10 99.5 ? 0.03</cell><cell>33.3% 28.6%</cell></row><row><cell cols="7">64 82.4 91.1 93.9 ca-AstroPh 128 82.9 92.4 96.8</cell><cell>97.4 97.7</cell><cell></cell><cell></cell><cell>96.9 97.5</cell><cell>95.7 95.7</cell><cell>97.9 ? 0.21 98.1 ? 0.49</cell><cell>19.2% 24.0%</cell></row><row><cell cols="2">ca-HepTh</cell><cell cols="5">64 80.2 79.3 86.8 128 81.2 78.0 89.7</cell><cell>90.6 90.1</cell><cell></cell><cell></cell><cell>91.8 92.0</cell><cell>90.3 90.3</cell><cell>93.6 ? 0.06 93.9 ? 0.05</cell><cell>22.0% 23.8%</cell></row><row><cell></cell><cell cols="6">64 70.7 75.4 76.7 PPI 128 73.7 71.2 76.9</cell><cell>79.7 81.8</cell><cell></cell><cell></cell><cell>70.6 74.4</cell><cell>82.4 83.9</cell><cell>89.8 ? 1.05 91.0 ? 0.28</cell><cell>43.5% 44.2%</cell></row><row><cell></cell><cell cols="2">e g o -F a ce b o o k ca -H e p T h</cell><cell>ca -A st ro P h</cell><cell>P P I</cell><cell cols="2">w ik i-vo te</cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 1: Results on Link Prediction Evaluation. Shown is the ROC-AUC.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Step-by-step Derivation of Equation (3.1) Let x(k) be the position of random surfer at time k. Speficically, x : Z + ? V . We assume a Markov chain: The value of x(k) only depends on previous step: x(k ? 1). To calulate the expectation E[D], the square node-to-node co-occurence matrix, we start by calculating one entry at a time: E[D uv ], the expected number of times that u is selected in v's context. Let W v (k) be the context set that gets sampled if v is visited at the k th step. Concretely, if x(k) = v, and the random walker continues the sequence, x(k + 1) = v 1 then x(k + 2) = v 2 then x(k + 3) = v 3 . . . , the context set of DeepWalk can be defined as W v (k) = {v 1 , v 2 . . . , v c , where c ? U{1, C}. We would like to count the event u ? W v (k) for every k ? {1, 2, . . . , C}.</figDesc><table><row><cell>7 Appendix</cell></row><row><cell>7.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We do not actually unroll the summation in Eq. (10) an infinite number of times. Our experiments show that unrolling it 10 or 20 times is sufficient to obtain state-of-the-art results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Available at http://sami.haija.org/graph/attention.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We sweep C inFigure 1b, showing that there are no good default for C that works best across datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Proportionate gradient updates with percentdelta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning edge representations via low-rank asymmetric projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for the Advancement of Artificial Intelligence</title>
		<meeting>the Association for the Advancement of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martinsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Review</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Data Engineering BulletinIEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Author2vec: Learning author representations by combining content and link information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Companion on World Wide Web (WWW), WWW &apos;16 Companion</title>
		<meeting>the International Conference Companion on World Wide Web (WWW), WWW &apos;16 Companion</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liben-Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of American Society for Information Science and Technology</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Context-dependent knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Emperical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Trans. on Neural Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Biogrid: A general repository for interaction datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Breitkreutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyers</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pubmed/16381927" />
	</analytic>
	<monogr>
		<title level="m">Nucleic Acids Research</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fast random walk with restart and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
