<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Adversarial Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Lau</surname></persName>
							<email>felixlaumon@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scale</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
							<email>nishants@allenai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Harrison</surname></persName>
							<email>sasha.harrison@scale.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aerin</forename><surname>Kim</surname></persName>
							<email>aerin.kim@scale.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Branson</surname></persName>
							<email>elliot.branson@scale.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
							<email>rosanne@mlcollective.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Masakhane</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Scale AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Scale AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Scale AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">ML Collective</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Adversarial Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although state-of-the-art object detection methods have shown compelling performance, models often are not robust to adversarial attacks and out-of-distribution data. We introduce a new dataset, Natural Adversarial Objects (NAO), to evaluate the robustness of object detection models. NAO contains 7,934 images and 9,943 objects that are unmodified and representative of real-world scenarios, but cause state-of-the-art detection models to misclassify with high confidence. The mean average precision (mAP) of EfficientDet-D7 drops 74.5% when evaluated on NAO compared to the standard MSCOCO validation set.</p><p>Moreover, by comparing a variety of object detection architectures, we find that better performance on MSCOCO validation set does not necessarily translate to better performance on NAO, suggesting that robustness cannot be simply achieved by training a more accurate model.</p><p>We further investigate why examples in NAO are difficult to detect and classify. Experiments of shuffling image patches reveal that models are overly sensitive to local texture. Additionally, using integrated gradients and background replacement, we find that the detection model is reliant on pixel information within the bounding box, and insensitive to the background context when predicting class labels. NAO can be downloaded here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It is no longer surprising to have machine learning vision models perform well on large scale training sets and also generalize on canonical test sets coming from the same distribution. However, generalization towards difficult, out-ofdistribution samples still poses difficulty. Recht et al. <ref type="bibr" target="#b19">[20]</ref> showed that model performance on canonical test sets is an overestimate of how they will perform on new data. Moreover, recent research on adversarial attacks has shown that <ref type="bibr">Figure 1</ref>. Mean average precision (mAP) of various detection models evaluated on NAO and MSCOCO val or test-dev set. All models show significant reduction in performance on NAO despite their accuracy improvement in MSCOCO in recent years. NAO is a challenging test set for detection models trained on MSCOCO and future work is required to close the performance gap. deep neural networks are surprisingly vulnerable to artificially manipulated images, casting new doubt on the efficacy and security of such models.</p><p>The vulnerability of neural networks to adversarial attacks that are deliberately generated to fool the system is unsurprising, and well studied. However, this type of attack represents a narrow threat model because it necessitates that the adversary has control over the raw input, or has access to the model weights. It is often overlooked that real-world, unmodified images can also be used adversarially to cause models to fail. These "natural" adversarial attacks represent a less restricted threat model, where an attacker can easily create black-box attacks without carefully constructing input perturbations <ref type="bibr" target="#b5">[6]</ref>, but only by using naturally occurring images that are easily obtainable. Such images are called natural adversarial examples <ref type="bibr" target="#b8">[9]</ref>: unmodified, real-world images that cause modern image classification models to make egregious, high-confidence errors.</p><p>In <ref type="bibr" target="#b8">[9]</ref> natural adversarial examples are only constructed for image classification models. In this work, we seek to create an evaluation set analogous to <ref type="bibr" target="#b8">[9]</ref>, but instead targeted at object detection tasks. We name such a dataset Natural Adversarial Objects (NAO). The goal of NAO is to benchmark the worst case performance of state-of-theart object detection models, while requiring that examples included in the benchmark are unmodified and naturally occurring in the real world.</p><p>We present a method to identify natural adversarial objects using a combination of existing object detection models and human annotators. First, we compare the predictions from various off-the-shelf detection models against a dataset already annotated with ground truth bounding boxes. We consider images containing high confidence false positives and misclassified objects as candidates for NAO. Then, we use a human annotation pipeline to filter out mislabeled images and non-obvious objects (e.g. occluded or blurry objects). Finally, we re-annotate the images using the object categories of the Microsoft Common Objects in Context (MSCOCO) dataset <ref type="bibr" target="#b15">[16]</ref>.</p><p>We perform extensive analyses to understand why objects in NAO are naturally adversarial. We visualize the embedding space common to MSCOCO, OpenImages, and NAO, and show that NAO images exist in the "blind spots" of the MSCOCO dataset. Next, by comparing integrated gradients <ref type="bibr" target="#b27">[28]</ref> with predicted bounding boxes and replacing object backgrounds, we show that the detection model seldom makes use of object contexts. Lastly, by shuffling patches within the bounding box, we show that models relies on object subparts and texture to detect and classify the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Natural Adversarial Examples Hendrycks et al. <ref type="bibr" target="#b8">[9]</ref> construct two datasets, namely ImageNet-A and ImageNet-O, to measure the robustness of image classifiers against out of distribution examples. To construct these two datasets, they choose images on which a pretrained ResNet model failed to make a correct prediction. We adopt a similar approach for selecting adversarial examples but use an object detection model and take extra steps to ensure high quality annotations by using human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Examples</head><p>Adversarial examples are inputs that are specifically designed to cause the target model to produce erroneous outputs. Since the findings of Szegedy et al. <ref type="bibr" target="#b28">[29]</ref>, there has been a flurry of research addressing adversarial attacks. Defensive distillation <ref type="bibr" target="#b18">[19]</ref>, one of the promising defense mechanisms against adversarial examples, was invented but then defeated within a year by a novel attack method proposed by Carlini et al. <ref type="bibr" target="#b2">[3]</ref>. Similarly, defenses using adversarial training <ref type="bibr" target="#b13">[14]</ref>, once considered to be robust to whitebox attacks, along with other methods that rely on gradient masking, such as Defense GAN <ref type="bibr" target="#b24">[25]</ref>, stochastic activation pruning <ref type="bibr" target="#b3">[4]</ref>, Pixeldefend <ref type="bibr" target="#b26">[27]</ref>, etc., are now each proven to be vulnerable to other types of attacks. These developments demonstrate how difficult it is to successfully defend against adversarial examples.</p><p>There are many hypotheses as to why adversarial examples pose such a challenge for classifiers. Papernot et al. <ref type="bibr" target="#b17">[18]</ref> suggested that adversarial examples cause models to fail because they lie in the low probability region of the data manifold. Goodfellow et al. <ref type="bibr" target="#b6">[7]</ref> showed that deep neural networks are especially vulnerable to adversarial examples due to the local linearity property, and proposed FGSM to fool the deep neural networks. Although deep neural networks use non-linear activation functions, we often observe that models operate in the linear regions of the activation functions to avoid the vanishing gradient problem <ref type="bibr" target="#b9">[10]</ref>.</p><p>Arpit et al. <ref type="bibr" target="#b0">[1]</ref> analyzed the capacity of neural networks to memorize training data, and found that models with a high degree of memorization are more vulnerable to adversarial examples. Jo et al. <ref type="bibr" target="#b11">[12]</ref> have shown that convolutional neural networks tend to learn the statistical regularities in the training dataset, rather than the high level abstract concepts. Since adversarial examples are transferable between models that are trained on the same dataset, these different models may have learned the same statistics and therefore are vulnerable to similar adversarial attacks. Brendel et al. <ref type="bibr" target="#b1">[2]</ref> show that small local image features are sufficient for deep learning model to achieve high accuracy. Geirhos et al. <ref type="bibr" target="#b4">[5]</ref> show that ImageNet-trained CNNs are biased toward texture and created Stylized-ImageNet to reveal the severity of such bias. Similarly, Ilyas et al. <ref type="bibr" target="#b10">[11]</ref> showed that adversarial examples are a byproduct of exploiting non-robust features that exist in a dataset. Non-robust features are derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. Undoubtedly, the reasons behind the existence and pervasiveness of adversarial examples still remains an open research problem.</p><p>Model Interpretability While the interpretability of deep neural networks remains an open research question, there exist attribution methods that help explain the relationships between the input and output of such models. In simpler terms, they can be used to understand why a model makes mistakes. Sundararajan et al. <ref type="bibr" target="#b27">[28]</ref> suggests that attribution methods should satisfy two axioms: sensitivity and im- High confidence misclassified objects where the ground truth label is in-distribution and among the MSCOCO object categories. Right: High confidence false positives where the ground truth object is out-of-distribution (i.e. not part of MSCOCO object categories). The misclassified objects and false positives are superficially similar to the predicted classes -for example, the fin of the shark is visually similar to the airplane tail and the yellow petals of the flower is similar to a bunch of bananas. plementation invariance, and proposes a new method, Integrated Gradient, to understand which parts of an image influence the prediction the most.</p><p>Object Detection Architectures Detection models fall into two categories: one-stage ( <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref>) and twostage models ( <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>), differentiated by whether the model has a region pooling stage. Single-stage model are more computationally efficient, but usually less accurate than the 2-stage models. In this paper, we evaluate both single and two-stage models using the NAO dataset. Tan et al. <ref type="bibr" target="#b30">[31]</ref> introduced EfficientDet, which uses Efficient-Net <ref type="bibr" target="#b29">[30]</ref> as backbone and uses BiFPN such that the model is more efficient while more accurate, achieving state-of-theart results in MSCOCO at 54.4 on the val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Creating Natural Adversarial Objects (NAO) Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Limitations of MSCOCO</head><p>MSCOCO <ref type="bibr" target="#b15">[16]</ref> is a common benchmark dataset for object detection models. It contains 118,287 images in the training set, 5,000 images in the val set and 20,288 images in the test-dev set. MSCOCO contains 80 object categories consisting of common objects such as horse, clock, and car. The goal of MSCOCO is to introduce a large-scale dataset that contains objects in non-iconic or non-canonical views. The images in MSCOCO were originally sourced from Flickr, then filtered down in order to limit the scope of the benchmark to a set of 80 categories. These 80 categories were chosen from a list of the most commonplace visually identifiable objects. Still, this category list represents only a small subset of object categories in real life. For example, 'fish' is not among the 80 categories, and as a result there are only a few photos taken underwater. This leads to a biased benchmark with limitations for generalizability and robustness. As a result, in this work, we ensure more diverse sourcing -choosing images from OpenImages v6 <ref type="bibr" target="#b14">[15]</ref>, a dataset with 600 object categories, in order to create a more representative dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sourcing Images for NAO from OpenImages</head><p>To create NAO, we first sourced images from the training set of OpenImages <ref type="bibr" target="#b14">[15]</ref>, a large, annotated image dataset containing approximately 1.9 million images and 15.8 million bounding boxes across 600 object classes.</p><p>One challenge of using OpenImages is that the bounding boxes are not exhaustively annotated. Each image is first annotated with positive and negative labels which indicate the presence or absence of an object in the image. Only objects belonging to the positive label categories are annotated with bounding boxes. As a result, some objects that belong to the OpenImages object categories are not labeled with a bounding box. For example, imagine both horse and pig are represented in the 600 object classes. If an image contains a horse and a pig, and only the category of horse is included in preliminary round of positive labels, then the image would be labeled with a bounding box for the horse but not the pig. This non-exhaustive annotation approach makes it difficult to produce and compare precision and recall to other exhaustively annotated dataset such as MSCOCO. This is because false positives and false negatives can only be evaluated accurately if the ground truth bounding boxes are exhaustive.</p><p>One other challenge that arises when sourcing images from OpenImages is that the object categories of OpenImages and MSCOCO are not the same. Therefore, after obtaining a set of natural adversarial images, we exhaustively annotate the images with all 80 MSCOCO object classes to facilitate straightforward comparison between NAO and the MSCOCO val and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Candidate Generation</head><p>To generate object candidates, we perform inference on OpenImages using an EfficientDet-D7 model <ref type="bibr" target="#b30">[31]</ref> pretrained on MSCOCO, which yields predicted object bounding boxes for each candidate image. Our goal is to find two types of errors: (i) hard false positives (i.e. false positives with high confidence) and (ii) egregiously misclassified objects. For a detection to be a hard false positive, we require the prediction to have no matching ground truth box with intersection over union (IoU) greater than 0.5, but to have a class confidence greater than 0.8. We define egregiously misclassified objects as predictions that have a matching ground truth bounding box with an IoU greater than 0.5, but have an incorrect classification with a confidence greater than 0.8. We do not consider false negatives with high confidence because we observe that these are commonplace especially in crowded scenes. There are 43,860 images containing at least one hard false positive or egregiously misclassified object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Annotation Process</head><p>Our annotation process has two annotation stages: classification and bounding box annotation.</p><p>Classification Stage In the classification stage, annotators identify whether the object described by the bounding box shown indeed belongs to the ground truth class as defined by the annotation in OpenImages or as predicted by the EfficientDet-D7. The purpose of this stage is to remove the possibility that the model prediction is "incorrect" due to the ground truth label being incorrect.</p><p>In addition, we ask the annotators to confirm whether the object can be "obviously classified" according to the following criteria:</p><p>1. Is the bounding box around the object correctly sized and positioned such that it is not too big or too small?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Does the object appear blurry?</head><p>3. Is the object occluded (i.e. are there other objects in front of this one)?</p><p>4. Is the object a depiction of the correct class (such as a drawing or an image on a billboard)?</p><p>We ask these additional questions to filter out ambiguous objects, such that a human can easily identify what class an object belongs to. After this filtering, 18.1% of the images (7,934) remain; each of the remaining images are confirmed to fulfill the 4 criteria, and represent true misclassifivations by the model. In this first annotation stage (classification), 5 different annotators are asked to annotate the same image and we use their consensus to produce an aggregated response by majority vote.</p><p>Bounding Box Stage In the second annotation stage (bounding box), annotators exhaustively identify and put boxes around all objects that belong to the MSCOCO object categories. We are unable to directly use the annotations from OpenImages because there is not a one-to-one mapping between the OpenImages and MSCOCO object categories, and because the bounding box annotations from OpenImages are not exhaustively annotated. However, the bounding box annotations from OpenImages are provided to the annotators as a starting point.</p><p>These bounding box annotation tasks are completed by 2 sets of annotators. The first set of annotators complete the bulk of the task by placing bounding boxes around objects that belong to the MSCOCO object categories. The second set of annotators review the work of the first set of annotators, sometimes adding missing bounding boxes or editing the existing ones.</p><p>To ensure the quality of the annotation is high, in both of these stages, the annotators have to pass multiple quizzes before they can start working tasks to ensure they understand the instructions well. If the annotator fails to maintain a good score, they are no longer eligible to continue to annotate the images. This process of vetting annotators is consistent with the methodology used to construct MSCOCO <ref type="bibr" target="#b15">[16]</ref>.</p><p>When the annotators from the 2 different stages disagree, we tie break by choosing second annotator who is positioned as the reviewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation Protocol</head><p>The goal of NAO is to test the robustness of object detection models against edge cases and out-of-distribution images. We propose two main evaluation metrics: overall mAP and mAP without out-of-distribution objects. mAP without out-of-distribution objects evaluates against edge cases of object categories that the detection models are trained on, while the overall mAP evaluates robustness against out-of-distribution objects. For calculating mAP without out-of-distribution objects, any detection matched to an object not belong to the 80 MSCOCO object categories is not considered a false positive.</p><p>NAO should be mainly used as a test set to evaluate detection models trained on MSCOCO. However, a split of train, validation and test set is also provided for robustness approaches that require training. <ref type="figure">Figure 1</ref> and <ref type="table">Table 2</ref> show the mean average precision (mAP) of several state-of-the-art detection models evaluated on MSCOCO and NAO. Despite the fact that the images in NAO were chosen using an EfficientDet-D7 model, we observe that other object-detection architectures show a similar reduction in mAP when evaluated on NAO. Concretely, when using NAO the mAP of EfficientDet-D7 is reduced by 74.5%, while Faster RCNN is reduced by 36.3% when compared to MSCOCO. Even though EfficientDet-D7 was developed more recently than Faster RCNN, the mAP on NAO is similar. This indicates that latest models are not more robust on NAO, despite their superior performance on MSCOCO evaluation sets. This in turn suggests that modeling improvements from recent years do not address the issue of high confidence misclassification in outof-distribution samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation of Detection Models</head><p>We also calculate the mAP without out-of-distribution objects. That is, if a detection matches a bounding box that does not belong the MSCOCO object categories, the detection is not counted as a false positive. We can see that, this exclusion improves mAPs on NAO, but overall, the results are still considerably worse than those from the MSCOCO val and test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Common Failure Modes</head><p>In <ref type="figure" target="#fig_2">Figure 4</ref>, we visualize some failure modes of the detection models on NAO. In most of the misclassified objects, the predicted class is superficially similar to the ground truth class, but obviously different in terms of function. For example, clocks and coins are similar in shape (circular), texture (metallic in some cases) and both have characters near the perimeters. However, they are very different in function and in scale, such that any human can easily tell the difference between the two. Similarly, airplanes and sharks are similar in overall shape, color, and texture, but exist in rather different scenes.</p><p>Another common failure mode is differentiating different animal species. For example, elephant and rhinoceros both have somewhat similar skin color and texture but they are very different in size and rhinoceros do not have the distinctive elephant trunk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dataset Blind Spot</head><p>As mentioned in Section 3.1, MSCOCO sourced images from Flickr search queries related to the 80 object categories. This process can be seen as a biased sampling process of all captured photos, resulting in "blind spots" in MSCOCO. For example, because there is not any "fish" category, the frequency of photos taken underwater in MSCOCO is much lower than all captured photos. In this section, we investigate this sampling bias by comparing the image embeddings of BiT ResNet-50 <ref type="bibr" target="#b12">[13]</ref> pretrained on ImageNet-21k <ref type="bibr" target="#b23">[24]</ref>   <ref type="table">Table 2</ref>. mAP of various detection models evaluated on MSCOCO val and test-dev set and NAO. Accuracy of all models were significantly lower on NAO than on MSCOCO. There is a slight increase in mAP when out-of-distribution objects are excluded. as a proxy for all captured images and with MSCOCO and NAO being a subset of the captured images. The image embedding is the output of the global average pooling layer, resulting in a vector of size 2,048. We then use UMAP <ref type="bibr" target="#b16">[17]</ref> to reduce the dimension to 2 for visualization as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. When comparing the embedding space of MSCOCO with OpenImages, we found that there are regions where the density is significantly lower in MSCOCO than in Open-Images. Some of these low-density regions are indicated by the black circles in <ref type="figure" target="#fig_3">Figure 5</ref>. When cross-referencing with the embedding space of NAO, we can see that these low-density regions of MSCOCO are in fact high-density in NAO, indicating that the examples in NAO are exploiting the under-represented regions that arise from MSCOCO's biased sampling process. We visualize 3 of such lowdensity clusters and they each reveal a common failure mode (i.e. fish misclassified as bird, insects misclassified as umbrella and van misclassified as truck.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Background Cues</head><p>Hendrycks et al. <ref type="bibr" target="#b8">[9]</ref> suggest that classification models are vulnerable to natural adversarial examples because classifiers are trained to associate the entire image with an object class, resulting in frequently appearing background elements being associated with a class. Object detection models are different from image classifiers in that they re-ceive additional supervision about the object position and size. We instead argue that the primary cause of detection models being vulnerable to NAO is their tendency to focus too much on the information within the predicted bounding boxes.</p><p>In this section, we study the effect of object background on classification probabilities. Specifically, we quantify the change in probability of the detected object when its background is replaced. We use a MSCOCO-pretrained Mask-RCNN <ref type="bibr" target="#b7">[8]</ref> with a ResNet 50 backbone to obtain instance segmentation masks on MSCOCO val and NAO. Then, we use the instance segmentation masks to retain only the most confident object and replace the rest of the image with a new background. There are 6 new backgrounds -underwater, beach, forest, road, mountain and sky -where Mask-RCNN detects no objects of probability higher than 0.1 from the backgrounds themselves. We measure the change of probability by matching the bounding box detected on the original image and the bounding box detected on the new image with the background replaced. We repeat this process for all images in NAO and MSCOCO val set and all 6 backgrounds.</p><p>As show in <ref type="figure" target="#fig_4">Figure 6</ref>, in both NAO and MSCOCO, the change in probabilities is low, indicating that the model does not make use of the background when detecting the object. While this robustness against background change is favorable in most cases, this also shows that the model also  does not account for unlikely combinations of background and foreground objects. For example, when the model misclassifies a shark as an airplane, the network could have noticed that the detected "airplane" is underwater and assigned a lower probability to the class airplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Integrated Gradients Analysis</head><p>We further try to understand the source of the egregious misclassifications by computing the integrated gradients <ref type="bibr" target="#b27">[28]</ref> of the network classification head output with respect to the input image. We aim to find the proportion of integrated output within the bounding box to understand if the network makes use the context of the object for detec-tion and classification.</p><p>Specifically, we computed the gradients of the classification output of highest-scored bounding box with respect to the input image and measure the proportion of the sum of attribution inside the bounding box with respect to the total attribution. When there are multiple same-class objects to detect, we make sure to attribute each object separately. For example, when there are 2 people, we calculate the attribution of one person, ensuring the attribution of the other person is not counted towards the background. We used EfficientDet-D4 and randomly sampled 1000 images for this experiment. We found that for most classes, the majority of the attributions come from inside the bounding box.</p><p>Both <ref type="figure" target="#fig_4">Figure 6</ref> and <ref type="figure">Figure 7</ref> suggest that the detection model do not make use of background enough and instead mainly focus on the information within the predicted bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Patch Shuffling and Local Texture Bias</head><p>Geirhos et al. <ref type="bibr" target="#b4">[5]</ref> demonstrated that classification networks are biased towards recognizing texture instead of shape. Brendel et al. <ref type="bibr" target="#b1">[2]</ref> showed that a classification network can still reach a high level of accuracy using just small patches extracted from images. In this work, we show that detection models also show strong local texture bias, making them susceptible to adversarial objects with similar object subparts but are of another object category. For each prediction from EfficientDet-D7 on MSCOCO and NAO, we randomly sample a patch from within the bounding box and swap it with another patch from inside the bounding box. We swap these random patches 3 times such that object is barely recognizable by just the shape. We then match <ref type="bibr">Figure 7</ref>. Proportion of attributed gradients within the bounding box by object category. In many classes, the detection model seldom make use of the object surroundings for classification and detection. <ref type="figure">Figure 8</ref>. Integrated gradients of a ceiling fan misclassified as an airplane. From the attribution heatmap in the right, we can see that the model focus on the fins of the fan but not the lights in the middle or the fact that the fan is indoors. <ref type="figure">Figure 9</ref>. Left: Original image with the detected "airplane" bounding box. Right: Image after random patches within the detected bounding box are swapped. The shark is still misclassified as airplane, which indicates that the model does not make full use of the shape of the object but relies on texture and small subparts of the objects the detected bounding box from the shuffled images to the original bounding box with the highest overlap. We repeat this shuffling process independently 10 times and record the absolute change in probability of the detected object. <ref type="figure">Figure  10</ref> shows that there is only a modest reduction in probability after the shuffles. In <ref type="figure">Figure 9</ref>, we show an example image where the detection model still misclassifies a shark as an airplane despite patch shuffling. <ref type="figure">Figure 10</ref>. Mean absolute change in probability when patches inside the bounding box are swapped randomly. The blue and orange dotted line represent the mean average change in probability across all object categories for MSCOCO and NAO respectively. This confirms the texture bias hypothesis because even if the shape of the objects are heavily distorted while the local texture is intact, the network is still able to make the same prediction with similar confidence in most object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduce "Natural Adversarial Objects" (NAO), a challenging robustness evaluation dataset for detection models trained on MSCOCO. We evaluated seven state-ofthe-art detection models from various families, and show that they consistently fail to perform accurately on NAO, comparing to MSCOCO val and test-dev set, including on both in-distribution and out-of-distribution objects. We explained the procedure of creating such a dataset which can be useful for creating similar datasets in the future.</p><p>We expose that these naturally adversarial objects are difficult to classify correctly due to the "blind-spots" in the MSCOCO dataset. We also utilize integrated gradients, background replacement, and patch shuffling to demonstrate that detection models are overly sensitive to local texture but insensitive to background change, leading such models to be susceptible to natural adversarial objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Sample images from NAO where EfficientDet-D7 produces high confidence false positives and egregious classification. Left:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Top: Annotation interface for the first annotation stage (classification) where the annotator confirms that the object belongs to the correct category, not occluded, not blurry and not a depiction. Bottom: Annotation interface for second annotation stage (bounding box) where the annotators locate and classify all objects in the images using the MSCOCO object categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Selected samples to showcase common failure modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>BiT ResNet-50 embeddings projected by UMAP on OpenImages train, MSCOCO train and NAO. NAO images are underrepresented in MSCOCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Average change in probability of objects when the backgrounds are replaced. The orange and blue dotted lines indicate average change in probabilities across all classes in MSCOCO and NAO. The small change in probability indicates that the detection model did not make use of background to classify the objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics of MSCOCO val, test-dev and NAO.</figDesc><table><row><cell></cell><cell></cell><cell>Statistics</cell><cell cols="3">Top 3 Objects (Count)</cell></row><row><cell></cell><cell cols="3">Number of Images Number of Objects 1st</cell><cell>2nd</cell><cell>3rd</cell></row><row><cell>MSCOCO val</cell><cell>5,000</cell><cell>36,781</cell><cell cols="3">Person (11,004) Car (1,932) Chair (1,791)</cell></row><row><cell cols="2">MSCOCO test-dev 20,288</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NAO</cell><cell>7,934</cell><cell>9,943</cell><cell>Person (3,551)</cell><cell cols="2">Cup (1,366) Car (707)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>across the 3 datasets -OpenImages train, MSCOCO train and NAO. We consider OpenImages MSCOCO val MSCOCO test-dev</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NAO</cell></row><row><cell></cell><cell cols="2">Params mAP</cell><cell>mAP</cell><cell cols="3">mAP mAR mAP w/o out-of-distribution</cell></row><row><cell>Faster RCNN</cell><cell>42M</cell><cell>21.2</cell><cell>21.5</cell><cell>13.5</cell><cell>41.4</cell><cell>22.8</cell></row><row><cell>RetinaNet-R50</cell><cell>34M</cell><cell>39.2</cell><cell>39.2</cell><cell>11.1</cell><cell>37.2</cell><cell>19.5</cell></row><row><cell>YOLOv3</cell><cell>62M</cell><cell>-</cell><cell>33.0</cell><cell>10.0</cell><cell>28.4</cell><cell>17.5</cell></row><row><cell cols="2">Mask RCNN R50 44M</cell><cell>37.9</cell><cell>-</cell><cell>15.2</cell><cell>43.8</cell><cell>24.6</cell></row><row><cell>EfficientDet-D2</cell><cell>8.1M</cell><cell>43.5</cell><cell>43.9</cell><cell>12.8</cell><cell>40.2</cell><cell>25.4</cell></row><row><cell>EfficientDet-D4</cell><cell>21M</cell><cell>49.3</cell><cell>49.7</cell><cell>15.0</cell><cell>42.7</cell><cell>29.6</cell></row><row><cell>EfficientDet-D7</cell><cell>52M</cell><cell>53.4</cell><cell>53.7</cell><cell>13.6</cell><cell>40.8</cell><cell>26.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxinder</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno>PMLR. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
			<date type="published" when="2017-08" />
			<publisher>International Convention Centre</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximating CNNs with bag-of-local-features models works surprisingly well on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">D</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<title level="m">Motivating the rules of the game for adversarial example research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Jacob Steinhardt, and Dawn Song. Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Measuring the tendency of cnns to learn surface statistical regularities. CoRR, abs/1711.11561</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">UMAP: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Goodfellow. Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>abs/1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno>abs/1511.04508</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Do ImageNet classifiers generalize to Im-ageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouya</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
