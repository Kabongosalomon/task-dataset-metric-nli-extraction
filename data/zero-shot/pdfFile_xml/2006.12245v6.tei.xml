<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Few-Shot Image Classification with Unlabelled Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
							<email>pbateni@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarred</forename><surname>Barber</surname></persName>
							<email>jarred.barber@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
							<email>j.vandemeent@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
							<email>fwood@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Few-Shot Image Classification with Unlabelled Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">3</biblScope>
							<biblScope unit="issue">2</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve state of the art performance on the Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have been made publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have transformed machine learning and computer vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b45">46]</ref>, enabled in part by the development of large and diverse sets of curated training data <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>. However, in many image classification tasks, millions of labelled examples are not available; therefore, techniques that can achieve sufficient classification performance with few labels are required. This has motivated research on few-shot learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b2">3]</ref>, which seeks to develop methods for developing classifiers with much smaller datasets. Given a few labelled "support" images per class, a few-shot image classifier is expected to produce labels for a given set of unlabelled "query" images. Typical approaches to few-shot learning adapt a base classifier network to a new support set through various means, such as learning new class embeddings <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b50">51]</ref>, amortized <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b33">34]</ref> or iterative <ref type="bibr" target="#b58">[58]</ref> partial adaptation of the feature extractor, and complete fine-tuning of the entire network end-to-end <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>In addition to the standard fully supervised setting, techniques have been developed to exploit additional unlabeled * Authors contributed equally ? Work performed while at Inverted AI ? Work performed while at Charles River Analytics 1 Code available at github.com/plai-group/simple-cnaps</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft-K Assignment Initialization</head><p>Cluster Updates support data (semi-supervision) <ref type="bibr" target="#b38">[39]</ref> as well as information present in the query set (transduction) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18]</ref>. In our work, we focus on the transductive paradigm, where the entire query set is labeled at the same time. This allows us to exploit the additional unlabeled data, with the hopes of improving classification performance. Existing transductive few-shot methods reason about unlabelled examples by performing k-means clustering with Euclidean distance <ref type="bibr" target="#b38">[39]</ref> or message passing in graph convolutional networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18]</ref>. Since few-shot classification requires handling a varying number of classes, an important architectural choice is the final feature to class mapping. Previous methods have used the Euclidean distance <ref type="bibr" target="#b38">[39]</ref>, the absolute difference <ref type="bibr" target="#b18">[19]</ref>, cosine similarity <ref type="bibr" target="#b54">[54]</ref>, linear classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> or additional neural network layers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51]</ref>. Bateni et al. <ref type="bibr" target="#b1">[2]</ref> introduced a class-adaptive Mahalanobis distance. Their method, Simple CNAPS, uses a conditional neural-adaptive feature extractor, along with a regularized Mahalanobisdistance-based classifier. This modification to CNAPS <ref type="bibr" target="#b40">[41]</ref> achieved improved performance on the Meta-Dataset benchmark <ref type="bibr" target="#b52">[53]</ref>, only recently surpassed by SUR <ref type="bibr" target="#b4">[5]</ref> and URT <ref type="bibr" target="#b26">[27]</ref>. However, its performance suffers when there are five or fewer support examples available per class.</p><p>Motivated by these observations, we explore the use of unlabelled examples through transductive learning within the same framework as Simple CNAPS. Our contributions are as follows. (1) We propose a transductive few-shot learner, namely Transductive CNAPS, that extends Simple CNAPS with a transductive two-step task encoder, as  well as an iterative soft k-means procedure for refining class parameter estimates (mean and covariance) using both labelled and unlabelled examples. <ref type="bibr" target="#b1">(2)</ref> We demonstrate the efficacy of our approach by achieving new state of the art performance on Meta-Dataset <ref type="bibr" target="#b52">[53]</ref>. (3) When deployed with a feature extractor trained on their respective training sets, Transductive CNAPS achieves state of the art performance on 4 out of 8 settings on mini-ImageNet <ref type="bibr" target="#b47">[48]</ref> and tiered-Imagenet <ref type="bibr" target="#b38">[39]</ref>, while matching state of the art on another 2. (4) When additional non-overlapping classes from Im-ageNet <ref type="bibr" target="#b41">[42]</ref> are used to train the feature extractor, Transductive CNAPS is able to leverage this example-rich feature extractor to achieve state of the art across the board on mini-ImageNet and tiered-ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q q F o s H m C b U i / B I s J A R D F r y z b o b Y R g T z L P 7 3 H d h T A H 7 Z s N u 2 j N Y y 8 Q p S Q O V a P v m l z u M S R p R A Y R j p Q a O n Y C X Y Q m M c J r X 3 F T R B J M J H t G B p g J H V H n Z L H t u H W t l a I W x 1 E + A N V N / b 2 Q 4 U m o a B X q y S K o W v U L 8 z x u k E F 5 5 G R N J C l S Q + a E w 5 R b E V l G E N W S S E u B T T T C R T G e 1 y B h L T E D X V d M l O I t f X</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Few-Shot Learning using Labelled Data</head><p>Early work on few-shot visual classification has focused on improving classification accuracy through the use of better classification metrics with a meta-learned non-adaptive feature extractor. Matching networks <ref type="bibr" target="#b54">[54]</ref> use cosine similarities over feature vectors produced by independently learned feature extractors. Siamese networks <ref type="bibr" target="#b18">[19]</ref> classify query images based on the nearest support example in feature space, under the L 1 metric. Relation networks <ref type="bibr" target="#b50">[51]</ref> and variants <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b43">44]</ref> learn their own similarity metric, parameterised through a Multi-Layer Perceptron. More recently, Prototypical Networks <ref type="bibr" target="#b47">[48]</ref> learn a shared feature extractor that is used to produce class means in a feature space where the Euclidean distance is used for classification. ReMP 2 <ref type="bibr" target="#b59">[59]</ref> extends this framework by incorporating self-attention for learning of prototypes in a rectified metric space, maintaining metric consistency between training and testing tasks.</p><p>Other work has focused on adapting the feature extractor for new few-shot tasks. Transfer learning by fine-tuning pretrained visual classifiers <ref type="bibr" target="#b58">[58]</ref> was an early approach that proved limited in success due to issues arising from overfitting. MAML <ref type="bibr" target="#b6">[7]</ref> and its variants <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref> learn metaparameters that allow fast task-adaptation with only a few gradient updates. Work has also been done on partial adap-2 Note that we do not directly compare to these methods as they are either unpublished (ArXiv) or were developed concurrent to our work. tation of feature extractors using conditional neural adaptive processes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b1">2]</ref>. These methods rely on channelwise adaptation of pretrained convolutional layers by adjusting parameters of FiLM layers <ref type="bibr" target="#b34">[35]</ref> inserted throughout the network. Our work builds on the most recent of these neural adaptive approaches, specifically Simple CNAPS <ref type="bibr" target="#b1">[2]</ref>. SUR <ref type="bibr" target="#b4">[5]</ref> and URT <ref type="bibr" target="#b26">[27]</ref> are two very recent methods that employ universal representations stemming from multiple domain-specific feature extraction heads. URT <ref type="bibr" target="#b26">[27]</ref>, which was developed and released publicly in parallel to this work, achieves state of the art performance by using a universal transformation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Few-Shot Learning using Unlabelled Data</head><p>Several approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref> have also explored the use of unlabelled instances for few-shot visual classification. EGNN <ref type="bibr" target="#b17">[18]</ref> employs a graph convolutional edgelabelling network for iterative propagation of labels from support to query instances. Similarly, TPN <ref type="bibr" target="#b27">[28]</ref> learns a graph construction module for neural propagation of soft labels between elements of the query set. These methods rely on a neural parameterization of distance within the feature space. TEAM <ref type="bibr" target="#b35">[36]</ref> uses an episodic-wise transductive adaptable metric for performing inference on query examples using a task-specific metric. Song et al. <ref type="bibr" target="#b48">[49]</ref> use a cross attention network with a transductive iterative approach for augmenting the support set using the query examples. TAFSSL 2 <ref type="bibr" target="#b24">[25]</ref> improves few-shot learning accuracy in transductive and semi-supervised settings by performing a search for a compact feature sub-space that is discriminative for a given few-shot test-task.</p><p>The closest approach to our work is that of Ren et al. <ref type="bibr" target="#b38">[39]</ref>. Their method extends prototypical networks <ref type="bibr" target="#b47">[48]</ref> by performing a single additional soft-label weighted estimation of class prototypes. Our work, on the other hand, differs in three ways. First, we produce soft-labelled estimates of both class mean and covariance. Second, we use an expectation-maximization (EM) algorithm that performs a dynamic number of soft-label updates, depending on the task at hand. Lastly, we employ a neural-adaptive procedure for feature extraction that is conditioned on a two-step learned transductive task representation, as opposed to a fixed feature-extractor. As we discuss in Section 4.2, this novel task-representation encoder is responsible for substantial performance gains on out-of-domain tasks.   </p><formula xml:id="formula_0">Z s f C l N B T E z y A s i A K 2 R G T C y h T H G b l b A R V Z Q Z W 1 P F l u A t f n m Z P J 7 V v f P 6 5 f 1 F r X F T 1 F G G I z i G U / D g C h p w B 0 1 o A Y M E n u E V 3 p z U e X H</formula><formula xml:id="formula_1">t Z f x K E k 1 R m x + K E g F 0 T H J C y A D L p F p M T G E M s l N V s J G V F K m T U 0 V U 4 K z + O V l 0 j 6 r O + f 1 y / u L W u O m q K M M R 3 A M p + D A F T T g D p r Q A g Y J P M M r v F m p 9 W K 9 W x / z 0 Z J V 7 B z C H 1 i f P 3 C f k f Q = &lt; / l a t e x i t &gt; 0 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g H V L P + 6 a B Y B l K w q Q T Y m a d S 9 M e u Q = " &gt; A A A B 6 H i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 f 6 D H o x W M C 5 g H J E m Y n v c m Y 2 d l l Z l Y I S 7 7 A i w d F v P p J 3 v w b J 8 k e N L G g o a j q p r s r S A T X x n W / n Z X V t f W N z c J W c X t n d 2 + / d H D Y 1 H G q G D Z Y L G L V D q h G w S U 2 D D c C 2 4 l C G g U C W 8 H o b u q 3 n l B p H s s H M 0 7 Q j + h A 8 p A z a q x U d 3 u l s l t x Z y D L x M t J G X L U e q W v b j 9 m a Y T S M E G 1 7 n h u Y v y M K s O Z w E m x m 2 p M K B v R A X Y s l T R C 7 W e z Q y f k 1 C p 9 E s b K l j R k p v 6 e y G i k 9 T g K b G d E z V A v e l P x P 6 + T m v D G z 7 h M U o O S z R e F q S A m J t O v S Z 8 r Z E a M L a F M c X s r Y U O q K D M 2 m 6 I N w V t 8 e Z k 0 z y v e R e W q f l m u 3 u Z x F O A Y T u A M P L i G K t x D D R r A A O E Z X u H N e X R e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition</head><p>Following <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7]</ref>, we focus on a few-shot classification setting where a distribution D over image classification tasks (S, Q) is provided for training. Each task</p><formula xml:id="formula_2">(S, Q) ? D consists of a support set S = {(x i , y i )} n i=1 of labelled images and a query set Q = {x * i } m i=1</formula><p>of unlabelled images; the goal is to predict labels for these query examples, given the (typically small) support set. Each query image x * i ? Q has a corresponding ground truth label y * i available at training time. A model will be trained by minimizing, over some parameters ? (which are shared across tasks), the expected query set classification loss over tasks:</p><formula xml:id="formula_3">E (S,Q)?D [ x * i ?Q ? log p ? (y * i |x * i , S, Q)]</formula><p>; the inclusion of the dependence on all of Q here allows for the model to be transductive. At test time, a separate distribution of tasks generated from previously unseen images and classes is used to evaluate performance. Let us also define shot as the number of support examples per class, and way as the number of classes within the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simple CNAPS</head><p>Our method extends the Simple CNAPS <ref type="bibr" target="#b1">[2]</ref> architecture for few-shot visual classification. Simple CNAPS performs few-shot classification in two steps. First, it computes task-adapted features for every support and query example. This part of the architecture is the same as that in CNAPS <ref type="bibr" target="#b40">[41]</ref>, and is based on the FiLM metalearning framework <ref type="bibr" target="#b34">[35]</ref>. Second, it uses the support set to estimate a per-class Mahalanobis metric, which is used to assign query examples to classes. The architecture uses a ResNet18 <ref type="bibr" target="#b12">[13]</ref> feature extractor. Within each residual block, Feature-wise Linear Modulation (FiLM) layers compute a scale factor ? and shift ? for each output channel, using block-specific adaptation networks ? ? that are conditioned on a task encoding. The task encoding g ? (S) consists of the mean-pooled feature vectors of support examples produced by d ? , a separate but end-to-end learned Convolution Neural Network (CNN). This produces an adapted feature extractor f ? (which implicitly depends on the support set S) that maps support/query images onto the corresponding adapted feature space. We will denote by S ? , Q ? versions of the support/query sets where each image is mapped into its feature representation z = f ? (x).</p><p>Simple CNAPS then computes a Mahalanobis distance relative to each class k by estimating a mean ? k and regularized covariance Q k in the adapted feature space, using the support instances:</p><formula xml:id="formula_4">? k = 1 n k i I[y i = k] z i (1) Q k = ? k ? k + (1 ? ? k ) ? + ?I, ? k = n k n k + 1<label>(2)</label></formula><p>Here I[y i = k] is the indicator function and n k = i I[y i = k] is the number of examples from class k in the support set S. The ratio ? k balances a task-conditional sample covariance ? and a class-conditional sample covariance ? k :</p><formula xml:id="formula_5">? = 1 n i z i ?? z i ?? T (3) ? k = 1 n k i I[y i = k] z i ?? k z i ?? k T<label>(4)</label></formula><p>where ? = 1 n i z i is the task-level mean. When few support examples are available for a particular class, ? k is small, and the estimate is regularized towards the tasklevel covariance ?. As the number of support examples for the class increases, the estimate tends towards the classconditional covariance ? k . Additionally, a regularizer ?I (we set ? = 1 in our experiments) is added to ensure invertibility. Given the class means and covariances, Simple CNAPS computes class probabilities for each query feature vector z * i through a softmax over the squared Mahalanobis distances with respect to each class:</p><formula xml:id="formula_6">p(y * = k | z * ) ? exp ? (z ? ? k ) T Q ?1 k (z ? ? k ) (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transductive CNAPS</head><p>Transductive CNAPS extends Simple CNAPS by taking advantage of the query set, both in the feature adaptation step and the classification step. First, the task encoder g ? is extended to incorporate both a support-set embedding e s and a query-set embedding e q such that,</p><formula xml:id="formula_7">e s = 1 K k 1 n k i I[y i = k] d ? (x i ),<label>(6)</label></formula><formula xml:id="formula_8">e q = 1 n q i * d ? (x * i ),<label>(7)</label></formula><p>where d ? is a learned CNN. The support embedding e s is formed by an average of (encoded) support examples, with weighting inversely proportional to their class counts to prevent bias from class imbalance. The query embedding e q uses simple mean-pooling; both e s and e q are invariant to   permutations of the respective support/query instances. We then process e s and e q through two steps of a Long Short Term Memory (LSTM) network in the same order to generate the final transductive task-embedding g ? (S, Q) used for adaptation. This process is visualized in <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>Second, we can interpret Simple CNAPS as a form of "supervised clustering" in feature space; each cluster (corresponding to a class k) is parameterized with a centroid ? k and a metric Q ?1 k , and we interpret (5) as class assignment probabilities based on the distance to each centroid. With this viewpoint in mind, a natural extension to consider is to use the estimates of the class assignment probabilities on unlabelled data to refine the class parameters ? k , Q k in a soft k-means framework based on per-cluster Mahalanobis distances <ref type="bibr" target="#b29">[30]</ref>. In this framework, as shown in <ref type="figure" target="#fig_9">Figure 1</ref>, we alternate between computing updated assignment probabilities using (5) on the query set and using those assignment probabilities to compute updated class parameters.</p><p>We define R ? = S ? ?Q ? as the disjoint union of the support set and the query set. For each element of R ? , which we index by j, we define responsibilities w jk in terms of their class predictions when it is part of the query set and in terms of the label when it is part of the support set,</p><formula xml:id="formula_9">w jk = p y ? j = k | z ? j z ? j ? Q ? , I[y ? j = k] (z ? j , y ? j ) ? S ? .<label>(8)</label></formula><p>Using these responsibilities we can incorporate unlabelled samples from the support set by defining weighted estimates ? ? k and Q ? k :  </p><formula xml:id="formula_10">? ? k = 1 n ? k j w jk z ? j ,<label>(9)</label></formula><formula xml:id="formula_11">Q ? k = ? ? k ? ? k + (1 ? ? ? k )? ? + ?I,<label>(10)</label></formula><formula xml:id="formula_12">? ? = 1 k n ? k jk w jk z ? j ?? ? z ? j ?? ? T<label>(11)</label></formula><formula xml:id="formula_13">? ? k = 1 n ? k j w jk z ? j ?? ? k z ? j ?? ? k T<label>(12)</label></formula><p>where ? ? = ( k n ? k ) ?1 jk w jk z ? j is the task mean. These update equations are weighted versions of the original Simple CNAPS estimators from Section 3.2, and reduce to them exactly in the case of an empty query set.</p><p>Algorithm 1 summarizes the soft k-means procedure based on these updates. We initialize our weights using only the labelled support set. We use those weights to compute class parameters, then compute updated weights using both the support and query sets. At this point, the weights associated with the query set Q are the same class probabilities as estimated by Simple CNAPS. However, we continue this procedure iteratively until we reach either reach a maximum number of iterations, or until class assignments argmax k w jk stop changing.</p><p>Unlike the transductive task-encoder, this second extension, namely the soft k-mean iterative estimation of class parameters, is used at test time only. During training, a single estimation is produced for both mean and covariance using only the support examples. This, as we discuss more in Section 4.2, was shown to empirically perform better. See <ref type="figure" target="#fig_6">Figure 4</ref> for a high-level visual comparison of classification in Simple CNAPS vs. Transductive CNAPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Relationship to Bregman Soft Clustering</head><p>The procedure in Algorithm 1 resembles the Bregman clustering algorithms proposed by Banerjee et al. <ref type="bibr" target="#b0">[1]</ref>. Specifically, the updates to soft assignments w jk in Equation 5 are the semi-supervised equivalent of those in Bregman soft clustering, in which the divergence is based on the</p><formula xml:id="formula_14">Mahalanobis distance F (z) = z T Q ?1 z, D F (z, z ? ) = F (z) ? F (z ? ) ? ?F (z ? ) T (z ? z ? ).</formula><p>(13) However, Algorithm 1 differs in that it updates both ? ? k and Q ? k at each iteration, rather than just ? ? k . In general, any (regular) exponential family can be associated with a Bregman divergence and vice versa, which gives rise to a correspondence between EM-based clustering and Bregman soft clustering algorithms <ref type="bibr" target="#b0">[1]</ref>. Standard Bregman soft clustering corresponds to EM in which the likelihood is a Gaussian with unknown mean and a known covariance Q that is shared across clusters. The case where the covariance is unknown corresponds to Gaussian mixture models (GMMs), but the function F (z) is not simply the Mahalanobis distance in this case.</p><p>The updates for ? ? k and Q ? k in Algorithm 1 are equivalent to those in a GMM that incorporates regularization for the covariances. However, GMM clustering differs in the calculation of the assignment probabilities</p><formula xml:id="formula_15">p(y * = k | z * ) ? ? k exp ? 1 2 (z ? ? k ) T Q ?1 k (z ? ? k ) ? 1 2 log |Q k | .</formula><p>These probabilities incorporate a term ? k = p(y * = k), which defines a prior probability of assignments to a cluster, and a term exp(? log |Q k |), which reflects the fact that GMMs employ a likelihood with unknown covariance. In short, our clustering procedure employs an update to soft assignments w jk that is similar to the one in soft Bregman clustering, but employs updates to ? ? k and Q ? k that are similar to those in a (regularized) GMM. In Section 4.2 we demonstrate through ablations that this combination of updates improves emperical performance relative to baselines that perform GMM-based clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Benchmarks</head><p>Meta-Dataset <ref type="bibr" target="#b52">[53]</ref> is a few-shot visual classification benchmark that consists of 10 widely used datasets: ILSVRC-2012 (ImageNet) <ref type="bibr" target="#b41">[42]</ref>, Omniglot <ref type="bibr" target="#b22">[23]</ref>, FGVC-Aircraft (Aircraft) <ref type="bibr" target="#b28">[29]</ref>, CUB-200-2011 (Birds) <ref type="bibr" target="#b55">[55]</ref>, Describable Textures (DTD) <ref type="bibr" target="#b3">[4]</ref>, QuickDraw <ref type="bibr" target="#b16">[17]</ref>, FGVCx Fungi (Fungi) <ref type="bibr" target="#b44">[45]</ref>, VGG Flower (Flower) <ref type="bibr" target="#b32">[33]</ref>, Traffic Signs (Signs) <ref type="bibr" target="#b14">[15]</ref> and MSCOCO <ref type="bibr" target="#b25">[26]</ref>. Consistent with past work <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2]</ref>, we train our model on the official training splits of the first 8 datasets and use the test splits to evaluate in-domain performance. We use the remaining two datasets as well as three external benchmarks, namely MNIST <ref type="bibr" target="#b23">[24]</ref>, CIFAR10 <ref type="bibr" target="#b19">[20]</ref> and CIFAR100 <ref type="bibr" target="#b19">[20]</ref>, for out-of-domain evaluation.</p><p>Task generation in Meta-Dataset follows a complex procedure where tasks can be of different ways and individual classes can be of varying shots even within the same task. Specifically, for each task, the task way is first sampled uniformly between 5 and 50 and way classes are selected at random from the corresponding class/dataset split. Then, for each class, 10 instances are sampled at random and used as query examples for the class, while of the remaining images for the class, a shot is sampled uniformly from <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">100]</ref> and shot number of images are selected at random as support examples with total support set size of 500.</p><p>Additional dataset-specific constraints are enforced, as discussed in Section 3.2 of <ref type="bibr" target="#b52">[53]</ref>, and since some datasets For j ranging over support and query sets, w jk ? 1 if (z ? j , y ? j ) ? S ? and y j = k 0 otherwise <ref type="bibr">3:</ref> for iter = 0 ? ? ? N iter do ? The first iteration is equivalent to Simple CNAPS; <ref type="bibr">4:</ref> Compute class parameters ? k , Q k according to update equations (9)-(11) 5:</p><p>Compute class weights using class parameters according to <ref type="bibr" target="#b7">(8)</ref> 6:</p><p>break if the most probable class for each query example hasn't changed <ref type="bibr">7:</ref> end for <ref type="bibr">8:</ref> return class probabilities w jk for j corresponding to Q ? 9: end procedure have fewer than 50 classes and fewer than 100 images per class, the overall way and shot distributions resemble Poisson distributions where most tasks have fewer than 10 classes and most classes have fewer than 10 support examples (see Appendix-A.1). Following <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b40">[41]</ref>, we first train our ResNet18 feature extractor on the Meta-Dataset defined training split of ImageNet following the procedure in Appendix-A.3. The ResNet18 parameters are then kept fixed while we train the adaptation network for a total of 110K sampled tasks using Episodic Training <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b6">7]</ref> (see Appendix-A.3 for details). Mini/tiered-ImageNet <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b38">39]</ref> are two benchmarks for few-shot learning. Both datasets employ subsets of Ima-geNet <ref type="bibr" target="#b41">[42]</ref> with a total of 100 classes and 60K images in mini-ImageNet and 608 classes and 779K images in tiered-ImageNet. Unlike Meta-Dataset, tasks across these datasets have pre-defined shots and ways that are uniform across every task generated in the specified setting.</p><p>Following <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>, we report performance on the 1/5-shot 5/10-way settings across both datasets with 10 query examples per class. We first train the ResNet18 on the training set of the corresponding benchmark at hand following the procedure noted in Appendix-A.4. We also consider a more feature-rich ResNet18 trained on the larger ImageNet dataset. However, we exclude classes and examples from test sets of mini/tiered-ImageNet to address potential class/example overlap issues, resulting in 825 classes and 1,055,494 images remaining. Then, with the ResNet18 parameters fixed, we train episodically for 20K tasks (see Appendix-A.2 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>Evaluation on Meta-Dataset: In-domain, out-of-domain and overall rankings on Meta-Dataset are shown in <ref type="table">Table 1</ref>. Following <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b40">[41]</ref>, we pretrain the feature extractor on the training split of the ImageNet subset of Meta-Dataset. Transductive CNAPS sets new state of the art accuracy on 2 out of the 8 in-domain datasets, while matching other methods on 2 of the remaining domains. On out-of-domain tasks, it performs better with new state of the art performance on 4 out of the 5 out-of-domain datasets, Overall, it produces an average rank of 1.9 among all datasets, the best among the methods, with an average rank of 2.1 on in-domain tasks, only second to URT which was developed parallel to Transductive CNAPS, and 1.6 on out-of-domain tasks, the best among even the most recent methods.</p><p>Evaluation on mini/tiered-ImageNet: We consider two feature extractor training settings on these benchmarks. First, we use the feature extractor trained on the corresponding training split of the mini/tiered-ImageNet. As shown in <ref type="table">Table 2</ref>, on tiered-ImageNet, Transductive CNAPS achieves state of art accuracy on both 10-way settings while matching state of the art performance of LEO <ref type="bibr" target="#b42">[43]</ref> on the 5-way settings. On the mini-ImageNet, Transductive CNAPS outperforms other methods on 10-way settings while coming second to LEO <ref type="bibr" target="#b42">[43]</ref> and TADAM <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b33">34]</ref> on 5-way tasks.</p><p>We attribute this difference in performance between mini-ImageNet and tiered-ImageNet to the fact that mini-ImageNet only provides 38,400 training examples, compared to 448,695 examples provided by tiered-ImageNet. This results in a lower performing ResNet-18 feature extractor (which is trained in a traditional supervised manner). This hypothesis is further supported by the results provided in our second model (denoted by "FETI", for "Feature Extractor Trained with ImageNet", in <ref type="table">Table 2</ref>). In this model, we train the feature extractor with a much larger subset of ImageNet, which has been carefully selected to prevent any possible overlap (in examples or classes) with the test sets of mini/tiered-ImageNet. Transductive CNAPS is able to take advantage of the more example-rich feature extractor, resulting in state-of-the-art performance across the board. Additionally, it outperforms the Simple CNAPS baseline by a large margin, even when using the same example-rich feature extractor; this demonstrates that leveraging additional query set information yields empirical gains.</p><p>Performance vs. Class Shot: In <ref type="figure" target="#fig_7">Figure 5</ref>, we examine the relationship between class recall (i.e. accuracy among query examples belonging to the class itself) and the num-   <ref type="table">Table 1</ref>: Few-shot classification on Meta-Dataset, MNIST, and CIFAR10/100. Error intervals correspond to 95% confidence intervals, and bold values indicate statistically significant state of the art performance. Average rank is obtained by ranking methods on each dataset and averaging the ranks. <ref type="table">Table 2</ref>: Few-shot visual classification results on 1/5-shot 5/10-way few-shot on mini/tiered-ImageNet. For CNAP-based models, "FETI" indicates that the feature extractor used has been trained on ImageNet <ref type="bibr" target="#b41">[42]</ref> exluding classes within the test splits of mini/tiered-ImageNet (for more details see Appendix-A.4). "BN" indicates implicit transductive conditioning on the query set through the use of batch normalization. Error intervals denote 95% confidence interval.</p><p>ber of support examples in the class (shot). As shown, Transductive CNAPS is very effective when class shot is below 10, showing large average recall improvements, especially at the 1-shot level. However, as the class shot increases beyond 10, performance drops compared to Simple CNAPS. This suggests that soft k-means learning of cluster parameters can be effective when very few support examples are available. Conversely, in high-shot classes, transductive updates can act as distractors. Classification-Time Soft K-means Clustering: We use soft k-means iterative updates of means and covariance at test-time only. It is natural to consider training the feature adaptation network end-to-end through the soft k-means transduction procedure. We provide this comparison in the bottom-half of <ref type="table" target="#tab_4">Table 3</ref>, with "Transductive+ CNAPS" denoting this variation. Iterative updates during training result in an average accuracy decrease of 2.5%, which we conjecture to be due to training instabilities caused by applying this iterative algorithm early in training on noisy features.</p><p>Transductive Feature Extraction vs. Classification: Our approach extends Simple CNAPS in two ways: improved adaptation of the feature extractor using a transductive taskencoding, and the soft k-means iterative estimation of class means and covariances. We perform two ablations, "Feature Extraction Only Transductive" (FEOT) and "Classification Only Transductive" (COT), to independently assess the impact of these extensions. The results are presented in <ref type="table" target="#tab_4">Table 3</ref>; both extensions outperform Simple CNAPS. The transductive task-encoding is especially effective on out-ofdomain tasks, whereas the soft k-mean learning of class parameters boosts accuracy on in-domain tasks. Transductive CNAPS is able to leverage the best of both worlds, allowing it to achieve significant gains over Simple CNAPS.</p><p>Comparison to Gaussian Mixture Models: We consider five GMM-based ablations of our method where the logdeterminant is introduced into the weight updates (using a uniform class prior ? k = 1/K). Results in <ref type="table" target="#tab_4">Table 3</ref> correspond to their soft k-means counterparts in the same order shown. The GMM-based variations of our method and Sim-      In our experiments, we use a minimum number of 2 refinement steps of class parameters, with the maximum set to 4 on the Meta-Dataset and 10 on the mini/tiered-ImageNet benchmarks. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the refinement criteria itself, without any step constraints, results in a significant performance gain as compared to performing no refinements. In fact, it accounts for the majority of the accuracy gain for Transductive CNAPS. We further explore the impact of these step-hyperparameters on the performance on Transductive CNAPS on the Meta-Dataset in <ref type="figure" target="#fig_8">Figure 6</ref>. As shown, requiring the same number of refinement steps for every task results in sub-optimal performance. This is demonstrated by the fact that the peak performance for each minimum number of steps is achieved with a larger number of maximum steps, showcasing the importance of allowing different numbers of refinement steps depending on the task. In addition, we observe that as the number of minimum refinement steps increases, the performance improves up to two steps while declining after. This suggests that, unlike <ref type="bibr" target="#b38">[39]</ref> where only a single refinement step leads to the best performance, our Mahalanobis-based approach can leverage extra steps to further refine the class parameters.</p><p>We do see a decline in performance with a higher number of steps; this suggests that while our refinement criteria can be effective at performing different number of steps depending on the task, it can potentially lead to over-fitting, justifying the need for a well chosen maximum number of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this paper, we have presented a few-shot visual classification method that achieves new state of the art performance via a transductive clustering procedure for refining class parameters derived from a previous neural adaptive Mahalanobis-distance based approach. The resulting architecture, Transductive CNAPS, is more effective at producing useful estimates of class mean and covariance especially in low-shot settings, when used at test time. Even though we demonstrate the efficacy of our approach in the transductive domain where query examples themselves are used as unlabelled data, our soft k-means clustering procedure naturally extends to use other sources of unlabelled examples in a semi-supervised fashion.</p><p>Transductive CNAPS superficially resembles a transductive GMM stacked on top of a learned feature representation; however, when we try to make this connection exact (by including the log-determinant of the class covariances), we suffer substantial performance hits. Explaining why this happens will be the subject of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Benchmarks and Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Meta-Dataset</head><p>A brief description of the sampling procedure used in the Meta-Dataset setting is already provided in Section 4.1. This sampling procedure, however, comes with additional specifications that are uniform across all tasks (such as count enforcing) and dataset specific details such as considering the class hierarchy in ImageNet tasks. The full algorithm for sampling is outlined in <ref type="bibr" target="#b52">[53]</ref>, and we refer the interested reader to Section 3.2 in <ref type="bibr" target="#b52">[53]</ref> for complete details. This procedure results in a task distribution where most tasks have fewer than 10 classes and each class has fewer than 20 support examples. The task frequency relative to the number of classes is presented in <ref type="figure">Figure 7a</ref>, and the class frequency as compared to the class shot is presented in <ref type="figure">Figure 7b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. mini/tiered-ImageNet</head><p>Task sampling across both mini-ImageNet and tiered-ImageNet first starts by defining a constant number of ways and shots that will be used for each generated task. For a L-shot K-way problem setting, first K classes are sampled from the dataset with uniform probability. Then, for each sampled class, L of the class images are sampled with uniform probability and used as the support examples for the class. In addition, 10 query images (distinct from the support images) are sampled per class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Meta-Dataset Training/Testing</head><p>Following <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b40">[41]</ref>, we train our ResNet18 feature extractor as a supervised multi-class image classifier on the training split of the ImageNet subset of the Meta-Dataset. We directly follow the procedure described in C.1.1 of <ref type="bibr" target="#b40">[41]</ref> and use their released checkpoint. Images from the 712 ImageNet classes designated for training by Meta-Dataset <ref type="bibr" target="#b52">[53]</ref> are first resized to 84x84. The ResNet18 in Transductive CNAPS is then trained as a 712-class image recognition task using this data. Training is done using the crossentropy loss for 125 epochs using SGD with momentum of 0.9, weight decay of 0.0001, batch size of 256, and a learning rate of 0.1 that is reduced by a factor of 10 every 25 epochs. During training, the dataset was augmented with random crops, horizontal flips, and color jitter.</p><p>Once the ResNet18 is trained, we freeze the parameters and proceed to train the adaptation network using Episodic training <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b6">7]</ref> where tasks themselves are used as train-ing examples. For each iteration of Episodic training, a task (with additional ground truth query labels) is generated, and the adaptation network is trained to minimize classification error (cross entropy) of the query set given the task. We train for a total of 110K tasks, with 16 tasks per batch, resulting in 6875 gradient updates. We train using Adam optimizer with learning rate of 5 ? 10 ?4 . We evaluate on the validation splits of all 8 in-domain and 1 outof-domain (MSCOCO) datasets, saving the best performing checkpoint for test-time evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. mini/tiered-ImageNet Training/Testing</head><p>Similar to Meta-Dataset, we first train the ResNet18 feature extractor. This is done with respect to two settings: first, we directly use the training data from mini-ImageNet and tiered-ImageNet, with 38,400 and 448,695 images respectively. Second, we consider a larger training set of 825 classes from ImageNet <ref type="bibr" target="#b41">[42]</ref> that don't overlap with the test sets of the benchmarks. In both cases, we follow the same procedure as the one described for Meta-Dataset in A.3 with the exception of training for 90 epochs and reducing the learning rate every 30 epochs. After training the ResNet18, the weights are frozen while we train the adaptation network using Episodic training: at each iteration, a task is generated, and we backpropagate the query set classification loss through the adaptation network. For mini/tiered-ImageNet, we train for a total of 20K tasks, validating performance every 2K tasks and saving the best checkpoint for test-time evaluation. We, similarly, use the Adam optimiser with learning rate of 5 ? 10 ?4 , and use a batch size of 16, for a total of 1250 gradient steps. <ref type="figure">Figure 8</ref> shows the performance of Transductive CNAPS on 1/5-shot 5-way Mini-ImageNet tasks as the number of query examples per category increases from 10 to 50. As shown, with a greater number of query examples, performance improves as our method is able to exploit more unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Performance vs. Number of Query Examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>? 0 &lt;Figure 1 :</head><label>01</label><figDesc>l a t e x i t s h a 1 _ b a s e 6 4 = " Z w w / K C X B T N C L 7 j 3 2 B T g b O 9 P E g U o = " &gt; A A A B 9 X i c d V D L S g M x F M 3 4 r P V V d e k m W A R X Q 6 b t 0 C 6 L b l x W s A / o T E s m z b S h m c y Q Z J Q y 9 D / c u F D E r f / i z r 8 x 0 1 Z Q 0 Q O B w z n 3 c k 9 O k H C m N E I f 1 t r 6 x u b W d m G n u L u 3 f 3 B Y O j r u qD i V h L Z J z G P Z C 7 C i n A n a 1 k x z 2 k s k x V H A a T e Y X u V + 9 4 5 K x W J x q 2 c J 9 S M 8 F i x k B G s j D b w I 6 0 k Q Z l 6 U z g d o W C o j G 9 U q q F q H y K 4 6 r t v I C U I u q j r Q M S R H G a z Q G p b e v V F M 0 o g K T T h W q u + g R P s Z l p o R T u d F L 1 U 0 w W S K x 7 R v q M A R V X 6 2 S D 2 H 5 0 Y Z w T C W 5 g k N F + r 3 j Q x H S s 2 i w E z m K d V v L x f / 8 vq p D h t + x k S S a i r I 8 l C Y c q h j m F c A R 0 x S o v n M E E w k M 1 k h m W C J i T Z F F U 0 J X z + F / 5 N O x X a q t n t T K z c v V 3 U U w C k 4 A x f A A X X Q B N e g B d q A AA k e w B N 4 t u 6 t R + v F e l 2 O r l m r n R P w A 9 b b J / f / k t U = &lt; / l a t e x i t &gt; Transductive CNAPS' soft k-means Mahalanobisdistance based clustering procedure. First, cluster parameters are initialized using the support examples. Then, during cluster update iterations, query examples are assigned class probabilities as soft labels and subsequently, both softlabelled query examples and labelled support examples are used to estimate new cluster parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " a J m 2 f n H 3 I B b + Q / s 3 S h E A 6 4 K z E W g = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o s g L k r i A 1 0 W 3 b i s Y B / Q p m U y n b R D J 5 M w M 1 F L y H + 4 c a G I W / / F n X / j p M 1 C W w 8 M H M 6 5 l 3 v m e B F n S t v 2 t 1 V Y W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 T R X G k t A G C X k o 2 x 5 W l D N B G 5 p p T t u R p D j w O G 1 5 4 5 v M b z 1 Q q V g o 7 v U k o m 6 A h 4 L 5 j G B t p F 4 3 w H r k + c l T 2 m e 9 k 3 6 5 Y l f t K d A i c X J S g R z 1 f v m r O w h J H F C h C c d K d R w 7 0 m 6 C p W a E 0 7 T U j R W N M B n j I e 0 Y K n B A l Z t M U 6 f o y C g D 5 I f S P K H R V P 2 9 k e B A q U n g m c k s p Z r 3 M v E / r x N r / 8 p N m I h i T Q W Z H f J j j n S I s g r Q g E l K N J 8 Y g o l k J i s i I y w x 0 a a o k i n B m f / y I m m e V p 2 z 6 s X d e a V 2 n d d R h A M 4 h G N w 4 B J q c A t 1 a A A B C c / w C m / W o / V i v V s f s 9 G C l e / s w x 9 Y n z + k E 5 K b &lt; / l a t e x i t &gt; S ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T 6 2 j f q 8 + v e E G g r n 6 9 X e n w U I j 7 C Y = " &gt; A A A B + 3 i c b V D L S s N A F J 3 U V 6 2 v W J d u g k V w V R I f 6 L L o x m V F + 4 A m h M l 0 0 g 6 d T M L M j V h C f s W N C 0 X c + i P u / B s n b R b a e m D g c M 6 9 3 D M n S D h T Y N v f R m V l d W 1 9 o 7 p Z 2 9 r e 2 d 0 z 9 + t d F a e S 0 A 6 J e S z 7 A V a U M 0 E 7 w I D T f i I p j g J O e 8 H k p v B 7 j 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>i b d 0 6 Z z 1 r y 4 O 2 + 0 r s s 6 q u g Q H a E T 5 K B L 1 E K 3 q I 0 6 i K A n 9 I x e 0 Z u R G y / G u / E x H 6 0 Y 5 c 4 B + g P j 8 w d 1 4 Z S 4 &lt; / l a t e x i t &gt; z ? i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L r n P S t U + U W r O 4 + m i 0 J t 6 v + 3 3 0 d 8 = " &gt; A A A B 9 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o s g L k r i A 1 0 W 3 b i s Y B / Q p m U y n b R D J 5 M w M 1 F q y H + 4 c a G I W / / F n X / j p M 1 C W w 8 M H M 6 5 l 3 v m e B F n S t v 2 t 1 V Y W l 5 Z X S u u l z Y 2 t 7 Z 3 y r t 7 T R X G k t A G C X k o 2 x 5 W l D N B G 5 p p T t u R p D j w O G 1 5 4 5 v M b z 1 Q q V g o 7 v U k o m 6 A h 4 L 5 j G B t p F 4 3 w H r k + c l T 2 m e 9 k 3 6 5 Y l f t K d A i c X J S g R z 1 f v m r O w h J H F C h C c d K d R w 7 0 m 6 C p W a E 0 7 T U j R W N M B n j I e 0 Y K n B A l Z t M U 6 f o y C g D 5 I f S P K H R V P 2 9 k e B A q U n g m c k s p Z r 3 M v E / r x N r / 8 p N m I h i T Q W Z H f J j j n S I s g r Q g E l K N J 8 Y g o l k J i s i I y w x 0 a a o k i n B m f / y I m m e V p 2 z 6 s X d e a V 2 n d d R h A M 4 h G N w 4 B J q c A t 1 a A A B C c / w C m / W o / V i v V s f s 9 G C l e / s w x 9 Y n z + n J Z K d &lt; / l a t e x i t &gt; Overview of neural adaptive feature extraction in Transductive and Simple CNAPS. Figure adapted from [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " p f E d s + / k i 9 H 0 w V g V w J x t W M s t V P 0 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q + 0 G X R j c s K 1 h a a U i b T m 3 b o Z B J m J k I J / Q 0 3 L h R x 6 8 + 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I B F c G 9 f 9 d k o r q 2 v r G + X N y t b 2 z u 5 e d f / g U c e p Y t h i s Y h V J 6 A a B Z f Y M t w I 7 C Q K a R Q I b A f j 2 9 x v P 6 H S P J Y P Z p J g L 6 J D y U P O q L G S 7 0 f U j I I w w 2 l f 9 6 s 1 t + 7 O Q J a J V 5 A a F G j 2 q 1 / + I G Z p h N I w Q b X u e m 5 i e h l V h j O B 0 4 q f a k w o G 9 M h d i 2 V N E L d y 2 a Z p + T E K g M S x s o + a c h M / b 2 R 0 U j r S R T Y y T y j X v R y 8 T + v m 5 r w u p d x m a Q G J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e n Y / 5 a M k p d g 7 h D 5 z P H 3 O n k f Y = &lt; / l a t e x i t &gt; e q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 g v O n v U t h l h p c H g a A s + B 7 q + Q w O 4 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q + 0 G X R j c s K 9 g F N K J P p T T t 0 8 n B m I p T Q 3 3 D j Q h G 3 / o w 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 z x E 8 G V t u 1 v q 7 S y u r a + U d 6 s b G 3 v 7 O 5 V 9 w / a K k 4 l w x a L R S y 7 P l U o e I Q t z b X A b i K R h r 7 A j j + + z f 3 O E 0 r F 4 + h B T x L 0 Q j q M e M A Z 1 U Z y 3 Z D q k R 9 k O O 0 / 9 q s 1 u 2 7 P Q J a J U 5 A a F G j 2 q 1 / u I G Z p i J F m g i r V c + x E e x m V m j O B 0 4 q b K k w o G 9 M h 9 g y N a I j K y 2 a Z p + T E K A M S x N K 8 S J O Z + n s j o 6 F S k 9 A 3 k 3 l G t e j l 4 n 9 e L 9 X B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>n H f n Y 9 6 6 4 u Q z R / A H z u c P e 1 2 M u w = = &lt; / l a t e x i t &gt; Overview of the transductive task-encoding procedure, g ? (S, Q), used in Transductive CNAPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Transductive CNAPS (right) extends the Mahalanobis-distance based classifier in Simple CNAPS (left) through transductive soft k-means clustering of the visual space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Class recall (otherwise noted as in-class query accuracy) averaged between classes across all tasks and (a: In-Domain, b: Out-of-domain, c: all) Meta-Dataset sub-datasets. Class recalls have been grouped together, averaged and plotted according to class shot in (a), (b), and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Evaluating Transductive CNAPS on Meta-Dataset with different minimum and maximum number of steps. Performances reported stem from five run averages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1</head><label>1</label><figDesc>Iterative Refinement in Transductive CNAPS 1: procedure COMPUTE QUERY LABELS(S ? , Q ? , N iter ) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>. The query set contains between 1 and 10 (inclusive) examples per class for all tasks; fewer than 10 query examples occur only when there are not enough total images to support 10 query examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>(a) Number of Tasks vs. Ways (b) Number of Classes vs. Shots Test-time way and shot frequency graphs. Figure is directly from Bateni et al. [2]. As shown, most tasks have fewer than 10 classes (way) and most classes have less than 20 support examples (shot). Number of Query Examples per Class in Task Classification Accuracy Mini-ImageNet performance as the number of query examples per class increases on 1/5-shot 5-way tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>9?0.9 86.6?0.8 69.7?0.8 54.1?1.0 56.6?0.7 61.8?1.0 32.6?1.1 76.1?0.8 37.5?0.9 27.4?0.9 NA NA NA 10.5 11.0 10.6 MatchingNet 36.1?1.0 78.3?1.0 69.2?1.0 56.4?1.0 61.8?0.7 60.8?1.0 33.7?1.0 81.9?0.7 55.6?1.1 28.8?1.0 6?1.1 90.7?0.6 83.8?0.6 74.6?0.8 62.1?0.7 74.8?0.7 48.7?1.0 89.6?0.5 67.0?0.7 43.4?1.0 92.3?0.4 69.3?0.8 54.6?1.1 4.7 4.8 4.8 Simple CNAPS 58.6?1.1 91.7?0.6 82.4?0.7 74.9?0.8 67.8?0.8 77.7?0.7 46.9?1.0 90.7?0.5 73.5?0.7 46.2?1.1 93.9?0.4 74.3?0.7 60.5?1.0 3.4 3.0 3.2 SUR 56.3?1.1 93.1?0.5 85.4?0.7 71.4?1.0 71.5?0.8 81.3?0.6 63.1?1.0 82.8?0.7 70.4?0.8 52.4?1.1 94.3?0.4 66.8?0.9 56.6?1.0 3.1 2.6 2.9 URT 55.7?1.0 94.4?0.4 85.8?0.6 76.3?0.8 71.8?0.7 82.5?0.6 63.5?1.0 88.2?0.6 69.4?0.8 52.2?1.1 94.8?0.4 67.3?0.8 56.9?1.0 1.7 2.8 2.2 Our Method 58.8?1.1 93.9?0.4 84.1?0.6 76.8?0.8 69.0?0.8 78.6?0.7 48.8?1.1 91.6?0.4 76.1?0.7 48.7?1.0 95.7?0.3 75.7?0.7 62.9?1.0 2.1 1.6 1.9</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>1 0</cell><cell>R 1 0 0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>C I F A</cell><cell>In Out All</cell></row><row><cell>RelationNet</cell><cell cols="2">30.NA</cell><cell>NA</cell><cell>NA</cell><cell>10.1 8.5 9.8</cell></row><row><cell>MAML</cell><cell>37.8?1.0 83.9?1.0 76.4?0.7 62.4?1.1 64.1?0.8 59.7?1.1 33.5?1.1 79.9?0.8 42.9?1.3 29.4?1.1</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>9.2 10.5 9.5</cell></row><row><cell>ProtoNet</cell><cell>44.5?1.1 79.6?1.1 71.1?0.9 67.0?1.0 65.2?0.8 64.9?0.9 40.3?1.1 86.9?0.7 46.5?1.0 39.9?1.1</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>8.2 9.5 8.5</cell></row><row><cell>ProtoMAML</cell><cell>46.5?1.1 82.7?1.0 75.2?0.8 69.9?1.0 68.3?0.8 66.8?0.9 42.0?1.2 88.7?0.7 52.4?1.1 41.7?1.1</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>7.1 8.0 7.3</cell></row><row><cell>CNAPS</cell><cell cols="5">52.3?1.0 88.4?0.7 80.5?0.6 72.2?0.9 58.3?0.7 72.5?0.8 47.4?1.0 86.0?0.5 60.2?0.9 42.6?1.1 92.7?0.4 61.5?0.7 50.1?1.0 6.6 6.0 6.4</cell></row><row><cell>BOHB-E</cell><cell>55.4?1.1 77.5?1.1 60.9?0.9 73.6?0.8 72.8?0.7 61.2?0.9 44.5?1.1 90.6?0.6 57.5?1.0 51.9?1.0</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell><cell>6.4 4.0 5.9</cell></row><row><cell>TaskNorm</cell><cell>50.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>GMM-EM+ 53.3?1.0 91.8?0.6 81.2?0.6 75.8?0.7 71.8?0.6 72.9?0.7 42.8?0.9 91.0?0.4 66.1?0.8 40.3?1.0 94.2?0.4 69.0?0.7 51.3?0.9 72.6 64.2 69.3 GMM 45.3?1.0 88.0?0.9 80.8?0.8 71.4?0.8 61.1?0.7 70.7?0.8 42.9?1.0 88.1?0.6 68.9?0.7 37.2?0.9 91.4?0.5 64.5?0.7 46.6?0.9 68.5 61.7 65.9 FEOT GMM 52.6?1.1 89.6?0.7 84.0?0.6 76.2?0.8 66.5?0.8 73.4?0.8 45.7?1.0 89.8?0.6 74.4?0.7 44.2?1.0 93.1?0.4 71.1?0.8 56.9?1.0 72.2 67.9 70.6 COT GMM 48.7?1.0 92.3?0.5 80.0?0.7 72.4?0.7 59.8?0.7 71.1?0.7 41.4?0.9 87.7?0.5 63.6?0.8 39.2?0.8 89.8?0.5 66.9?0.7 50.5?0.8 69.2 62.0 66.4 GMM-EM 52.3?1.0 92.0?0.5 84.3?0.6 75.2?0.8 64.3?0.7 72.6?0.8 44.6?1.0 90.8?0.5 71.4?0.7 44.7?0.9 93.0?0.4 71.1?0.7 56.4?0.9 72.0 67.3 70.2 Transductive+ 53.3?1.1 92.3?0.5 81.2?0.7 75.0?0.8 72.0?0.7 74.8?0.8 45.1?1.0 92.1?0.4 71.0?0.8 44.0?1.1 95.9?0.3 71.1?0.7 57.3?1.1 73.2 67.9 71.2 Simple 58.6?1.1 91.7?0.6 82.4?0.7 74.9?0.8 67.8?0.8 77.7?0.7 46.9?1.0 90.7?0.5 73.5?0.7 46.2?1.1 93.9?0.4 74.3?0.7 60.5?1.0 73.8 69.7 72.2 FEOT 57.3?1.1 90.5?0.7 82.9?0.7 74.8?0.8 67.3?0.8 76.3?0.8 47.7?1.0 90.5?0.5 75.8?0.7 47.1?1.1 94.9?0.4 74.3?0.8 61.2?1.0 73.4 70.7 72.4 COT 58.8?1.1 95.2?0.3 84.0?0.6 76.4?0.7 68.5?0.8 77.8?0.7 49.7?1.0 92.7?0.4 70.8?0.7 47.3?1.0 94.2?0.4 75.2?0.7 61.2?1.0 75.4 69.7 73.2 Transductive 58.8?1.1 93.9?0.4 84.1?0.6 76.8?0.8 69.0?0.8 78.6?0.7 48.8?1.1 91.6?0.4 76.1?0.7 48.7?1.0 95.7?0.3 75.7?0.7 62.9?1.0 75.2 71.8 73.9</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance of various ablations of Tranductive and Simple CNAPS on Meta-Dataset. Error intervals indicate 95% confidence intervals, and bold values indicate statistically significant state of the art performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Evaluating min/max refinement restrictions in Transductive CNAPS on Meta-Dataset, MNIST, and CIFAR10/100. ple CNAPS result in a notable 4-8% loss in overall accuracy. It is also surprising to observe that the FEOT variation matches the performance of the full GMM-EM model.</figDesc><table /><note>Maximum and Minimum Number of Refinements:</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">mini-ImageNet Accuracy (%)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgments</head><p>We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada Research Chairs (CRC) Program, the Canada CI-FAR AI Chairs Program, Compute Canada, Intel, Inverted AI under their MITAC Accelerate internship program and DARPA under its D3M and LWLL programs. Additionally, this material is based upon work supported by the United States Air Force under Contract No. FA8750-19-C-0515.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering with bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srujana</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1705" to="1749" />
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaden</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey on metric learning for feature vectors and structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.6709</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Selecting relevant features from a multi-domain representation for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised fewshot learning for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Abdur R Feyjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Kauffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<title level="m">Conditional neural processes. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic fewshot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1804.09458</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural RST-based evaluation of discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorii</forename><surname>Guz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Muglich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="664" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehensive survey of deep learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Md. Zakir Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conf. on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A survey of deep learningbased object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Jongmin Kim, and Nick Fox-Gieg. The quick, draw!-ai experiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Jongejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Kawashima</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TAFSSL: task-adaptive feature sub-space learning for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Lichtenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rog?rio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12352</biblScope>
			<biblScope unit="page" from="522" to="539" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A universal representation transformer layer for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On k-means algorithm with the use of mahalanobis distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Melnykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Melnykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="88" to="95" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Meta-learning with temporal convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1707.03141</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On firstorder meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transductive episodic-wise adaptive metric for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast and flexible multi-task classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Fgvcx fungi classification challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Imagining the road ahead: Multi-agent trajectory prediction via differentiable simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Scibior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lioutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv 1409.1556</idno>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Channel attention based iterative residual learning for depth map super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey on image classification and activity recognition using deep convolutional neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sornam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muthusubash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Computing (ICoAC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meta-Dataset</surname></persName>
		</author>
		<title level="m">A dataset of datasets for learning to learn from few examples. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pietro Perona, and Serge Belongie. The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A survey of zero-shot learning: Settings, methods, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<idno>13:1-13:37</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Few-shot learning: A survey. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<title level="m">Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Remp: Rectified metric propagation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="2581" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
