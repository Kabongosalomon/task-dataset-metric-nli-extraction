<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Point Cloud Classification and Segmentation using 3D Modified Fisher Vector Representation for Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Shabat</forename><surname>Yizhak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng</forename><surname>Mechanical</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dept</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Dept. Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Mechanical Eng. Dept. Techion IIT Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Point Cloud Classification and Segmentation using 3D Modified Fisher Vector Representation for Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The point cloud is gaining prominence as a method for representing 3D shapes, but its irregular format poses a challenge for deep learning methods. The common solution of transforming the data into a 3D voxel grid introduces its own challenges, mainly large memory size. In this paper we propose a novel 3D point cloud representation called 3D Modified Fisher Vectors (3DmFV). Our representation is hybrid as it combines the discrete structure of a grid with continuous generalization of Fisher vectors, in a compact and computationally efficient way. Using the grid enables us to design a new CNN architecture for point cloud classification and part segmentation. In a series of experiments we demonstrate competitive performance or even better than state-of-the-art on challenging benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Point clouds are commonly used for representing the sensory data associated with 3D objects and scenes. Recent sensing technologies make them more reliable and accurate and they are already in use in many applications such as modeling tools and autonomous systems.</p><p>We propose a new approach for analyzing a point cloud obtained by direct 3D sensors, using deep neural networks (DNNs), and especially convolutional neural networks (ConvNets). ConvNets have shown remarkable performance in image analysis. Adapting them to point clouds is not, however, straightforward. ConvNets are built for input data arranged in fixed size arrays, on which linear space invariant filters (convolutions) may be applied. Point clouds, unfortunately, are unstructured, unordered, and contain a varying number of points. Therefore, they do not fit naturally into a spatial array (grid).</p><p>Several methods for extending ConvNets to 3D point cloud analysis have been proposed <ref type="bibr">[18,</ref><ref type="bibr">33,</ref><ref type="bibr">23]</ref>. A common approach is to rasterize the 3D point data into a 3D voxel grid (array), on which ConvNets can be easily applied. This approach, however, suffers from a tradeoff between its computational cost and its approximation accuracy. We discuss this approach, as well as other common point cloud representations, in Section 2.</p><p>We take a different approach and use a new point cloud hybrid representation, the 3D Modified Fisher Vector (3DmFV). The representation describes points by their deviation from a Gaussian Mixture Model (GMM). It has some similarity to the Fisher Vector representation (FV) <ref type="bibr">[19,</ref><ref type="bibr">26]</ref> but modifies and generalizes it in two important ways: the proposed GMM is specified using a set of uniform Gaussians with centers on a 3D grid, and the components characterizing the set of points, that, for Fisher vectors, are averages over this set, are generalized to other functions of this set.</p><p>The representation is denoted hybrid because it combines the discrete structure of a grid with the continuous nature of the components. It has several advantages. First, because it keeps the continuous properties of the point cloud, it retains some of the point set's fine detail and, under certain conditions, is lossless and invertible (in principle), and is therefore equivalent to featureless input. Second, the gridlike structure makes it possible to use ConvNets, which yields excellent classification accuracy even with low resolutions (e.g. 8?8?8). Finally, each component of the proposed representation represents a clear and meaningful property.</p><p>The main contributions of this work are:</p><p>? We introduce a new hybrid representation for 3D point clouds (3DmFV) which is structured and order independent.</p><p>? We design a new deep ConvNet architecture (3DmFV-Net) based on this representation and use it for point cloud classification, obtaining state of the art results.</p><p>? We conduct a thorough empirical analysis on the stability of our method.</p><p>? We extend the 3DmFV-Net to part segmentation of point clouds, and achieve state of the art results as well.</p><p>We first review related work on 3D classification, part segmentation, and the FV representation in Section 2. Then, in Section 3 we introduce and discuss the 3DmFV representation and 3DmFV-Net architecture. The classification and part segmentation results are presented in Section 4. Finally, we summarize in Section 5.  <ref type="bibr" target="#b12">13,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b8">9]</ref> and global descriptors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">32,</ref><ref type="bibr" target="#b15">16]</ref>. The comprehensive performance evaluation in <ref type="bibr" target="#b7">[8]</ref> suggests guidelines for feature selection. However, optimal feature selection remains non-trivial and highly data specific.</p><p>Deep learning on 3D representations -3D data is commonly represented using one of the following representations: (a) Multi-view, (b) Volumetric grid, (c) Mesh, (d) Point clouds. Each representation requires a different approach for modifying the data to the form required by deep learning methods.</p><p>Rendering 2D images of a 3D object from multiple views, as in [23], transforms the learning domain from 3D to the well-researched 2D domain. Some information is lost in the projection process, but using multiple projections partially compensates.</p><p>The volumetric, voxelized, representation discretizes 3D space similarly to an image discretizing a camera projection plane. This enables a straightforward extension of learning using 3D CNNs <ref type="bibr">[18,</ref><ref type="bibr">33,</ref><ref type="bibr">23]</ref>. A volumetric representation is associated with a quantization tradeoff: choosing a coarse grid leads to quantization artifacts and to substantial loss of information, whereas choosing a fine grid significantly increases the number of voxels, which are mostly empty but still induce a high computational cost. Usually a grid size of 32?32?32 is chosen. A recent improvement applies an ensemble of very deep networks that extend the principles of Inception <ref type="bibr">[29]</ref> and Resnet <ref type="bibr" target="#b9">[10]</ref> to voxelized data <ref type="bibr">[4]</ref>, thus achieving the highest accuracy to date at the price of high computational cost and training time of weeks.</p><p>For the mesh representation, Spectral ConvNets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">17,</ref><ref type="bibr">2]</ref> and anisotropic ConvNets <ref type="bibr">[3]</ref> can be applied. These approaches utilize mesh topological structure, which is not always available.</p><p>The point cloud representation is challenging because it is both unstructured and point-wise unordered. To overcome these challenges, the PointNet approach [22, 24] applies a symmetric function that is insensitive to the order, on a high-dimensional representation of the individual points. The Kd-Network <ref type="bibr" target="#b13">[14]</ref> imposes a kd-tree structure on the points and uses it to learn shared weights for nodes in the tree. Here we propose the 3DmFV representation, which is directly linked to the point cloud representation but can be used as input to a CNN.</p><p>Part segmentation of 3D point clouds -The objective here is to assign a label, for each point, corresponding to a semantically meaningful part of the shape, e.g., chair back or chair seat. The ShapeNet dataset, introduced in [34], was used in [22] to extend the PointNet approach from point cloud classification to point-wise classification (part segmentation) by concatenating high-dimensional learned global and local features. The Kd-Network architecture was extended to part segmentation by mimicing an encoderdecoder architecture with skip-connections <ref type="bibr" target="#b13">[14]</ref>. Here, we extend the 3DmFV classification network to perform part segmentation using locally and globally learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fisher vectors</head><p>Before the age of deep learning, the bag of visual words (BoV) <ref type="bibr" target="#b5">[6]</ref> was a popular choice for image classification tasks. It extracted a set of local descriptors and assigned each of them to the closest entry in a codebook (visual vocabulary), leading to a histogram of occurrences. Perronin and Dance <ref type="bibr">[19]</ref>, and Perronnin and Thomas [21] proposed an alternative descriptor aggregation method, called the Fisher Vector (FV), based on the Fisher Kernel (FK) principle of <ref type="bibr" target="#b11">[12]</ref>. The FV characterizes data samples of varying sizes by their deviation from a generative model, in this case a Gaussian Mixture Model (GMM). It does so by computing the gradients of the sample's log-likelihood w.r.t. the model parameters (i.e., weight, mean and covariance). FV can be viewed as a generalization of the BoV as the histogram is closely related to the derivative w.r.t. the weight. Furthermore, it was shown in <ref type="bibr" target="#b11">[12]</ref> that, when the label is included as a latent variable of the generative model, the FK is asymptotically as good as the maximum a posteriori (MAP) decision rule for this model. The FV representation optimality and independence of sample size make it a natural choice for representing point cloud data.</p><p>In the context of image classification, the combination of FVs and DNNs was already considered <ref type="bibr">[27,</ref><ref type="bibr">20]</ref>. A network composed of Fisher layers was suggested in <ref type="bibr">[27]</ref>. Each layer performs semi-local FV encoding (on dense handcrafted features) followed by a dimensionality reduction, spatial stacking, and normalization. A network composed of unsupervised and supervised layers was proposed in <ref type="bibr">[20]</ref>. The unsupervised layers calculate features, FVs, and reduce their dimension. They are followed by supervised fully connected layers, trained with back propagation.</p><p>The proposed 3DmFV shares some properties with the representation types above. Like the volumetric approach, it is based on a grid, but not a grid of voxels. It thus maintains the grid structure, which makes it a convenient input to a ConvNet, but it suffers less from quantization. Like the PointNet approach, its features are symmetric functions, making them order and structure independent. The architecture we propose, like that of [20], combines unsupervised and supervised layers. However, it relies on the spatial properties of point clouds, which enables the use of ConvNets, substantially improving its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The 3DmFV-Net</head><p>The proposed 3DmFV-Net classification architecture consists of two main modules. The first converts an input point cloud to the 3D modified Fisher vector (3DmFV) representation and the second processes it in a CNN architecture. These main modules are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> and described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Describing point clouds with Fisher vectors</head><p>The proposed representation builds on the well-known Fisher vector representation. We start by formally describing the Fisher vectors (following the formulations and notation of [26]) in the context of 3D points, and then discuss some of their properties that lead to the proposed generalization and make them attractive as input to deep networks. The Fisher vector is based on the likelihood of a set of vectors associated with a Gaussian Mixture model (GMM). Let X = {p t ? R 3 , t = 1, ...T } be the set of 3D points of a given point cloud, where T denotes the number of points in the set. Let ? be the set of parameters of a K component GMM ? = {(w k , ? k , ? k ), k = 1, ...K}, where w k , ? k , ? k are the mixture weight, expected value, and covariance matrix of k-th Gaussian. The likelihood of a single 3D point (or vector) p associated with the k-th Gaussian density is</p><formula xml:id="formula_0">u k (p) = 1 (2?) D/2 |? k | 1/2 exp ? 1 2 (p ? ? k ) ? ?1 k (p ? ? k ) .<label>(1)</label></formula><p>The likelihood of a single point associated with the GMM density is therefore:</p><formula xml:id="formula_1">u ? (p) = K k=1 w k u k (p).<label>(2)</label></formula><p>Given a specific GMM, and under the common independence assumption, the Fisher vector, G X ? , may be written as the sum of normalized gradient statistics, computed here for each point p t :</p><formula xml:id="formula_2">G X ? = T t=1 L ? ? ? log u ? (p t ),<label>(3)</label></formula><p>where L ? is the square root of the inverse Fisher Information Matrix. The following change of variables, from {w k } to {? k }, ensures that u ? (x) is a valid distribution and simplifies the gradient calculation <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_3">w k = exp(? k ) K j=1 exp(? j ) .<label>(4)</label></formula><p>The soft assignment of point p t to Gaussian k is given by:</p><formula xml:id="formula_4">? t (k) = w k u k (p t ) K j=1 w j u j (p t ) .<label>(5)</label></formula><p>The normalized gradients are:</p><formula xml:id="formula_5">G X ? k = 1 ? w k T t=1 (? t (k) ? w k ),<label>(6)</label></formula><formula xml:id="formula_6">G X ? k = 1 ? w k T t=1 ? t (k) p t ? ? k ? k ,<label>(7)</label></formula><formula xml:id="formula_7">G X ? k = 1 ? 2w k T t=1 ? t (k) (p t ? ? k ) 2 ? 2 k ? 1 .<label>(8)</label></formula><p>These expressions include the normalization by L ? under the assumption that ? t (k) (the point assignment distribution) is approximately sharply peaked <ref type="bibr">[26]</ref>. For grid uniformity reasons, discussed in Section 3.4, we adopt the common practice and work with diagonal covariance matrices. The Fisher vector is formed by concatenating all of these components:</p><formula xml:id="formula_8">G X F V ? = G X ?1 , ..., G X ? k , G X ?1 , ..., G X ? k , G X ?1 , ..., G X ? k .<label>(9)</label></formula><p>To avoid the dependence on the number of points, the resulting FV is normalized by the sample size T [26]:  </p><formula xml:id="formula_9">G X F V ? ? 1 T G X F V ? .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Advantages of Fisher vectors as inputs to DNNs</head><p>A Fisher vector, representing a point set, may be used as an input to a DNN. It is a fixed size representation of a possibly variable number of points in the cloud. Its components are normalized sums of functions of the individual points. Therefore, FV representation of a point set is invariant to order, structure, and sample size. Using a vector of non-learned features instead of the raw data goes against the common wisdom of deep neural network users. The reason is that features usually select some of the raw data properties and may lose some important characteristics. This is true especially when the feature extraction involves discretization, as is the case with voxel based representation. We argue, however, that the Fisher vector representation, being continuous on the point set, suffers less from this disadvantage. We shall give three arguments (not proofs) in favor of this claim. a. Equation counting argument -Consider a K component GMM. A set of T points, characterized by 3T scalar coordinates, is represented using 7K components of the Fisher vector, every one of which is a continuous function of the 3T variables. Can we find another point set associated with the same Fisher components? We argue that for T &lt; 7K/3, the set of equations specifying unknown points from known Fisher components is over-determined and that it is likely that the only solution, up to point permutation, is the original point set. While this equation counting argument is not a rigorous proof, such claims are common for sets of polynomial equations and for points in general position. If the solution is indeed unique, then the Fisher representation is lossless and using it is equivalent to using the raw point set itself. b. Reconstructing the represented point structure in simplified, isolated cases b.1 A single Gaussian representing a single point -Here, T = 1. By the sharply peaked ? t (k) assumption, there is only one Gaussian for which ? t (k) = 1. Inserting its FV components in Eq. 7 provides the point location:</p><formula xml:id="formula_10">p 1 = ? k G X ? k + ? k .<label>(11)</label></formula><p>b.2 A single Gaussian representing multiple points on one plane -We now show that, given a set of points sampled on a plane, it is possible to reconstruct a plane from the FV representing the points. The plane equation isn T p = ?, wheren = (a, b, c) T is the unit normal to the plane and ? is its distance from the origin. Using the assumption that ? t (k) is approximately sharply peaked [26], we consider the k-th Gaussian and the T points for which ? t (k) ? 1. For this Gaussian, eq. 7 is simplified to:</p><formula xml:id="formula_11">G X ? = 1 ? ? w T t=1 (p t ? ?).<label>(12)</label></formula><p>Changing the coordinate system to x y z , for which the origin is at the Gaussian center and an axis x coincides withn, leads to the following expression for the plane parameters (see Appendix 6.1 for a proof and illustrations) :</p><formula xml:id="formula_12">a = G ?x G ? , b = G ?y G ? , c = G ?z G ? , ? = ? G ? ? w<label>(13)</label></formula><p>Objects are often approximately polyhedral, with each facet having at least one close Gaussian for which only this facet is close. This implies that such models may be reconstructed from the FV even for very large T . c. Point cloud reconstruction from FV representation using a deep decoder -A direct expression for reconstructing a point cloud from its FV representation is not available for K &gt; 1. For illustration, we show now such a reconstruction obtained with a deep decoder, taking FV as an input and providing a point cloud. We consider a special FV, associated with a GMM with Gaussian centered on a grid; see Section 3.4 below. The decoder architecture is identical to the convolutional part of the classification network presented in Section 3.5 followed by two fully connected layers: FC(T ),FC(3T ). The loss function between the original and the reconstructed point sets, S 1 and S 2 , should be invariant to point order. We use a loss function that is the sum of Chamfer distance and the Earth mover's distance, which were used (separately) in <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure" target="#fig_1">Figure 2</ref> shows a qualitative comparison between the original point cloud and the reconstructed point cloud. It shows that the decoder captured the overall shape while not positioning the points exactly in their original position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalizing Fisher vectors to 3D modified Fisher vectors</head><p>We propose to generalize the Fisher vector along two directions:</p><p>Choice of the mixture model -Originally, the mixture model was defined as a maximum likelihood model. This makes the model optimally adapted to the training data and gives the Fisher representation of each Gaussian the nice property of being sensitive only to the deviation from the average training data. It is not the only valid option, however, and not a good choice if we prefer to maintain a grid structure. Therefore, in the proposed generalization we shall use other mixture models that rely on Gaussian grids.</p><p>Choice of the symmetric function -As apparent from eq. (3), the components of the Fisher vector are sums over all input points, regardless of their order and any structure they create. They are symmetric in the sense proposed in <ref type="bibr">[22]</ref> and are therefore adequate for representing the orderless and structureless set of points. Note also that any other symmetric function, applied over the summands in eq. (3), would also induce a vector that can represent orderless sets. We will propose such functions and use them instead of or in addition to the Fisher vector sums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The proposed 3DmFV generalization</head><p>Changing the mixture model -For the underlying density model, we use a mixture of Gaussians with Gaussian centers (? k ) on a uniform 3D m ? m ? m grid. Such Gaussians induce a Fisher vector that preserves the point set structure: the presence of points in a specific 3D location would significantly influence only some, pre-known, Fisher components. The other GMM parameters, weight and covariance, are common to all Gaussians. The weights are selected as w k = 1 K and the covariance matrix as ? k = ? k I with ? k = 1 m . Recall that all points are contained in the unit sphere. The uniformity is essential for shared weight (convolutional) filtering. The size of the mixture model is moderate and ranges from m = 3 to 9.</p><p>The proposed uniform model is not as effective as the maximum likelihood model for representing the distribution of point clouds. Recall, however, that the GMM does not represent a specific model or a specific class but rather the average model, which is much closer to uniform. The inaccuracy is more than compensated for by the power of the convolutional network, as we shall see in the comparison between the different models.</p><p>Changing/Adding other symmetric functions -For Fisher vectors, the sum is used as a symmetric function. While the sum is asymptotically optimal, it does not give full information about the input points for finite point sets. For small point sets the Fisher vectors may be invertible, as suggested above, implying that the FVs implicitly carry the full information about the set. For the practical case of large finite point sets, we propose to add information. To maintain the order independence, other summarizing features should be symmetric as well.</p><p>We experimented with several options for additional symmetric functions, and eventually chose the maximum and minimum functions. Note that the maximum was also used in [22] as a single summarizing feature for each of the learned features. Thus, the components of the proposed generalized vector, denoted 3DmFV, are given in eq. 14 and obtained as follows: each component is either a sum, max, or min function, evaluated on the set of one gradient component. In our experiments we found that partial, more compact, representations (especially those focusing on the minimum and maximum function) may lead to improved accuracy, and we describe them as well. However, we also found that the minimal weight derivative is always a constant and omitted this specific function. The minimal value associated with a specific Gaussian corresponds to the farthest point and its ? t (k) value, which is 0 in practice.</p><formula xml:id="formula_13">G X 3DmF V ? = ? ? ? T t=1 L ? ? ? log u ? (p t ) ?=?,?,? max t (L ? ? ? log u ? (p t )| ?=?,?,? min t (L ? ? ? log u ? (p t ))| ?=?,? ? ? ?<label>(14)</label></formula><p>For K = m 3 Gaussians, there are therefore 20?K components in the 3DmFV representation (20 = 3(3 + 3) + 2). The 3DmFV is best visualized as a 20 ? K matrix.  Note that the representation lends itself to intuitive interpretation. For example, many columns are white, except for the first two top entries. These correspond to Gaussians that do not have model points near them; see eq. 6. Normalization Following [21] (Sec. 2.3), we applied two consecutive normalizations on the 3DmFV representation: First, we applied an element-wise signed square root normalization, and then an L2 normalization over all 20 vectors corresponding to all Gaussians and a single feature ? i . This last normalization equalizes the derivatives with respect to different parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">3DmFV-Net classification architecture</head><p>The proposed network receives a point cloud and converts it to a 3DmFV representation on a grid. The main parts of the network consist of an Inception module [30], visualized in <ref type="figure">Figure 4</ref>, maxpooling layers, and finally four fully connected layers. The network output consists of classification scores; see <ref type="figure" target="#fig_0">Figure 1</ref>. The network is trained using back propagation and standard softmax cross-entropy loss with batch normalization after every layer and dropout after each fully connected layer. The network has approximately 4.6M trained parameters, the majority of which are between the last maxpooling layer and the first fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">3DmFV-Net part segmentation architecture</head><p>The 3DmFV-Net architecture is extended to perform part segmentation. We solve this problem using a per-point classification approach. The proposed architecture combines local geometrical information of each point with the global context of the entire point cloud. The local information is encoded in the per-point 3DmFV representation (i.e., the  <ref type="figure">Figure 4</ref>: Inception module used in 3DmFV-Net derivative value of each GMM Gaussian with respect to ? at that specific point), and the global information is added by concatenating the output of the convolutional part of the classification architecture (before the FC classifiers) to each point's 3DmFV representation. Additionally, to fairly compare to other methods that assume a given label, we concatenate a one-hot representation of the ground truth label; however, this is not strictly necessary since ommitting the label only slightly reduces performance (0.2%). These features are then aggregated into a combined representation using a multilayer perceptron with shared weights across points, similarly to <ref type="bibr">[22]</ref>. The details of the 3DmFV-Net segmentation architecture are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first evaluate the classification performance of our proposed 3DmFV-Net and compare it to previous approaches. We then evaluate several variants of the proposed representation. Next, we analyze our method's robustness to noise. Finally, we evaluate the part segmentation results obtained by our method and compare them to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Datasets -We evaluate all classification algorithms on the ModelNet40 dataset <ref type="bibr">[33]</ref>. It consists of 12311 CAD models from 40 object categories, represented as triangle mesh. The data is split into a training set (9843 models) and a test set (2468). To generate point clouds, the mesh is sampled as described in <ref type="bibr">[22]</ref>. We also experimented with the Mod-elNet10 dataset, which contains 4899 CAD models from 10 object classes split into 3991 for training and 908 for testing.</p><p>Training details: Unless otherwise specified, the 3DmFV-Net <ref type="figure" target="#fig_0">(Figure 1</ref>) was trained using an Adam optimizer with a learning rate of 0.001 with a decay of 0.7 every 20 epochs. The point cloud (of 2K points) is centered around the origin and scaled to fit a cube of edge length 2. Data is augmented using random anisotropic scaling (range:  <ref type="table">Table 1</ref> compares the 3DmFV-Net with previous approaches on the ModelNet40 and Modelnet10 datasets. Clearly, the proposed method wins over most methods. It is comparable to the Kd-network, which requires a much larger input(? 32K points) and is not very robust to rotations and noise <ref type="bibr" target="#b13">[14]</ref>. It is less accurate only than the more complex VRN ensemble method <ref type="bibr">[4]</ref>, which operates on voxelized input and averages 6 models, each trained for 6 days. However, it is slightly better than a single VRN model. Note also that direct comparison is somewhat unfair because, unlike the voxelized description, point based methods (like ours) do not have direct access to the mesh. We also tested a combination of the 3DmFV representation (m = 8) with the simpler convolutional network that mimics the one used in VoxNet <ref type="bibr">[18]</ref>. Although this combination (denoted 3DmFV+VoxNet) uses a lower resolution grid than VoxNet (32 3 voxels), it is more accurate. Thus, the performance boost of the 3DmFV-Net may be attributed to both the representation and the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Testing variations of the 3DmFV representation</head><p>We now test partial, more compact variants of the proposed representations. The first variant is the Fisher vectors. The 3DmFV generalization of FV uses different symmetric functions (and not only the sum, as in FV), and different GMMs. We considered the combinations of several symmetric functions, with GMMs obtained either from the   maximum likelihood (ML) optimization obtained by the expectation maximization (EM) algorithm or from Gaussians on a 3D 5?5?5 grid. The tested symmetric functions include maximum (3DmFV-max), minimum (3DmFV-min), and sum of squares (3DmFV-ss). We tested these combinations with a nonlinear classifier (4 fully connected layers of sizes (1024, 256, 128, 40) with ReLU activation). For reference to the original FVs, we tested the ML GMM with a linear classifier as well. All models were represented by 1024 points. <ref type="table" target="#tab_4">Table 2</ref> reveals that the 3DmFV representation always wins over the FV representation, and that using grid GMM is comparable to the optimal ML GMM. Using maximum or minimum as a symmetric function yields comparable results with fewer parameters. Note that with the convolutional network, possible only with grid GMM, the accuracy is much higher ( <ref type="table">Table 1)</ref>. Resolution and standard deviation We found that accuracy increases with both grid size (m) and the number of points representing the model; see <ref type="figure" target="#fig_5">Figure 5</ref>. Note that performance saturates in both parameters. PointNet [22] seems to be more sensitive to the number of points. This is probably because their descriptors are learned as well. The architecture is slightly different for each grid size; see appendix 6.3.</p><p>We also found that the model is insensitive to the selection of standard deviation as long as it is not too small. In    the case of very small ?, most points do not contribute to any Gaussian, rendering the FV representation empty; See appendix 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Robustness evaluation</head><p>To simulate real-world point cloud classification challenges, we tested 3DmFV-Net's robustness under several types of noise: Uniform point deletionrandomly deleting points is equivalent to classifying clouds that are smaller than those used for training. Focused region point deletionselecting a random point and deleting its closest points (the number of points is defined by a given ratio), simulating occlusions.</p><p>Outlier pointsadding uniformly distributed points. Perturbation noiseadding small translations, with a bounded Gaussian magnitude, independently to all points, simulating measurement inaccuracy.</p><p>Random rotationrandomly rotating the point cloud w.r.t. the global reference frame, simulating the unknown orientation of a scanned object. The results ( <ref type="figure" target="#fig_7">Figure 6</ref>) demonstrate that the proposed ap- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Failure cases</head><p>Analyzing misclassifications, we found that most of them occur for similar class pairs: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Part segmentation</head><p>We evaluate the performance of our part segmentation architecture on the ShapeNet part dataset from [34]. It contains 16881 point clouds with 50 annotated parts from 16 categories. Learning this dataset is challenging because it is highly imbalanced. The evaluation metric is mean intersection over union (IoU). It is calculated first for each part of each point cloud and is then averaged over each point cloud, and over all point clouds in that category, yielding the category IoU. The total mean IoU is computed as a weighted average of all category IoUs, where the weights are the respective number of point clouds in that category. We report in <ref type="table" target="#tab_6">Table 3</ref> the total mean IoU and the category IoUs and compare to other state of the art methods. We achieve best performance in 9/16 categories while no other method wins in more than 4/16. Additionally, we achieve best mean IoU.</p><p>Qualitative part-segmentation results are presented in <ref type="figure" target="#fig_8">Figure 7</ref>, and additional qualitative results are presented in the appendix 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a new unsupervised 3D point cloud representation, the 3D modified Fisher vector. It preserves the raw point cloud data while using a grid for structure. This allows the use of the proposed CNN architecture (3DmFV-Net).</p><p>Representing data by non-learned features goes against the deep network principle that the best performance is obtained only by learning all components of the classifier using an end-to-end optimization. The proposed representation achieves state of the art results relative to all methods that use point cloud input, and therefore provides evidence that end-to-end learning is not always essential. 6 Appendix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Point reconstruction from 3DmFV representation</head><p>In Section 3.2 we provide an expression for a plane's parameters, given the FV representation of points sampled on it. This result holds in the asymptotic sense and under the assumption of uniform sampling. It holds for a single Gaussian. Here we prove this result, generalize it to grid GMM and verify it experimentally.</p><p>The plane equation is given byn T p = ?, wheren = (a, b, c) T is a unit normal to the plane and ? is its distance from the origin. Using the assumption that ? t (k) is approximately sharply peaked [26], we consider the k-th Gaussian and the T k points for which ? t (k) ? 1. From eq. 6 we get an expression for T k :</p><formula xml:id="formula_14">T k ? T t=1 ? t (k) = T ? w(G X ? k + ? w)<label>(15)</label></formula><p>For this Gaussian, eq. 7 is simplified to:</p><formula xml:id="formula_15">G X ? = T k ? ? w T t=1 (p t ? ?).<label>(16)</label></formula><p>The G X ? expression becomes clearer when we change the coordinate system to x y z (see <ref type="figure" target="#fig_9">Figure 8</ref> top-right), for which the origin is at the Gaussian center, the axis x coincides withn and the other axes are chosen to constitute an orthogonal system. We select:</p><p>x =n,? =n ?? |n ??| ,? =? ?n |? ?n| .</p><p>This yields the following transformation:</p><formula xml:id="formula_17">p = ? ? x y z ? ? = ? + ? ? ? a ? c ? a 2 +c 2 ? ab ? a 2 +c 2 b 0 ? a 2 + c 2 c a ? a 2 +c 2 ? bc ? a 2 +c 2 ? ? ? ? ? x y z ? ? .<label>(18)</label></formula><p>For all points on the plane, x = ? 0 . By symmetry, for random uniform sampling on the plane, the expected value of both y and z is zero. Therefore, for a large number of samples, inserting eq. 18 in eq. 16 yields</p><formula xml:id="formula_18">G X ? ? T k ? ? w ? ? a b c ? ? x .<label>(19)</label></formula><p>In addition, recall that to remove the dependency on the total number of points T , we divide by T (eq. 10). A simple   Confusion Matrix <ref type="figure">Figure 9</ref>: 3DmFV-Net classification confusion matrix reconstructed planes are similar to the original one, which demonstrate that the FV components hold the information about the plane parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Failure cases</head><p>In order to better understand the 3DmFV representation and the 3DmFV-Net, we explored failure cases. The confusion matrix in <ref type="figure">Figure 9</ref> shows that the majority of misclassified point clouds belong to a few class pairs, specifically tabledesk, plant-flowerpot and dresser-night stand. Further inspection using visualization of the failure cases presented in <ref type="figure" target="#fig_0">Figure 10</ref> provides some insight. First, some classes are inherently hard to distinguish even for humans <ref type="table" target="#tab_7">(table-desk)</ref> and second, the training data imposes a challenge since there is some overlap between categories (plant-flower pot).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">3DmFV architecture for different grid sizes</head><p>We tested several grid resolutions for the 3DmFV representation (see Section 4, <ref type="figure" target="#fig_5">Figure 5</ref>) . Each grid size imposes a slightly different network architecture. The architectures are detailed below. See <ref type="figure">Figure 4</ref> for a description of the parametric inception module.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plant</head><p>Flower pot Failure cases (plant classified as flower pot)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Additional robustness tests</head><p>One of the parameters of the 3DmFV representation is the Gaussian standard deviation ?. The value of ? qualitatively specifies spherical sub-volume whose points contribute to the 3DmFV component associated with a specific Gaussian. Very small ? values create Gaussians with very few or no contributing points and very large ?s create Gaussians that may be affected by many or all points in the cloud. We tested the robustness of 3DmFV-Net to the ? parameter selection. <ref type="figure" target="#fig_0">Figure 11</ref> shows that the network is robust to the ? selection except for very small ?s, for which the network performs poorly (since the representation essentially fails to capture the point cloud).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Part segmentation</head><p>In Section 3.6 we presented qualitative results for part segmentation. Additional part segmentation results are presented here with a comparative visualization. In <ref type="figure" target="#fig_0">Figure 12</ref>, the left column shows the ground truth point labels, the middle column shows the labels predicted by the 3DmFV-Net segmentation network, and the right column shows a color coded comparison between the two, where correctly labeled points are shown in blue and mislabeled in red. It can be seen that mislabeled points sometimes appear in transitional locations between labels (e.g., red points are visible where the chair back meets the chair seat). gt prediction difference <ref type="figure" target="#fig_0">Figure 12</ref>: Part segmentation qualitative comparative results. Ground truth labels (left), predicted labels (middle), and color coded difference (right).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>3DmFV-Net architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Point cloud reconstruction from FV representation using a deep decoder. The original (left), and the reconstructed point cloud (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>3DmFV representation (left) and the corresponding point cloud (right) depicts a point cloud (right) and its 3DmFV representation (m = 5) as a color coded image (left). Each column of the image represents a single Gaussian in a 5?5?5 Gaussian grid. Zero values are white whereas positive and negative values correspond respectively to the red and blue gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>m x m x m x L] Output: [m x m x m x 3N]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Effects of grid resolution and number of input points on the evaluation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>3DmFV-Net robustness to data corruptions. Classification accuracy results for missing data (top-left), outlier insertion (top-right), perturbation noise (botom-left), and rotations (bottom-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>3DmFV-Net part segmentation qualitative results proach is inherently robust to perturbation noise and uniform point deletions. For the other types of data corruptions, training the classifiers with any of these types of noise made it robust to them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Points on a plane with their reconstructed plane. A view from then direction (top-left), a view from the y direction, centered in one of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 .</head><label>1</label><figDesc>Grid 3?3?3: 3DmFV-inception(2,3,64) -inception(2,3,128) -inception(2,3,256) -inception(2,3,256) -inception(2,3,512) -FC(1024) -FC(256) -FC(128) -FC(#classes)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>3DmFV-Net classification failure cases. Table point clouds classified as desks (top), and plants classified as flower pots (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Effects of Gaussian standard deviation (?) selection on the evaluation accuracy FC(256) -FC(128) -FC(#classes)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Deep learning on 3D data Point cloud features -Handcrafted features for point clouds have shown adequate performance for many tasks. They can be divided into two main groups: local descriptors [25,</figDesc><table><row><cell>2 Related Work</cell></row><row><cell>2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="6">Classification accuracy of the proposed 3DmFV</cell></row><row><cell cols="6">and FV representation computed on EM-learned Gaussians</cell></row><row><cell cols="6">and Gaussians positioned on a grid using a linear and non-</cell></row><row><cell cols="6">linear classifier. Note that convolution layers are not used</cell></row><row><cell cols="2">in this experiment.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>93</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>92</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>91</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy %</cell><cell>87 88 89</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>86</cell><cell></cell><cell></cell><cell cols="2">16x16x16 grid 8x8x8 grid</cell></row><row><cell></cell><cell>85</cell><cell></cell><cell></cell><cell>5x5x5 grid</cell></row><row><cell></cell><cell>84</cell><cell></cell><cell></cell><cell>3x3x3 grid</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>PointNet</cell></row><row><cell></cell><cell>83</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Number of Points</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>81.0 78.4 77.7 75.7 87.6 61.9 92.0 85.4 82.5 95.7 70.6 91.9 85.9 53.1 69.8 75.3 3DCNN [22] 79.4 75.1 72.8 73.3 70.0 87.2 63.5 88.4 79.6 74.4 93.9 58.7 91.8 76.4 51.2 65.3 77.1 PointNet [22] 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93 81.2 57.9 72.8 80.6 Kd-Net [14] 77.2 79.9 71.2 80.9 68.8 88.0 72.4 88.9 86.4 79.8 94.9 55.8 86.5 79.3 50.4 71.1 80.2 Ours 84.3 82.0 84.3 86.0 76.9 89.9 73.9 90.8 85.7 82.6 95.2 66.0 94.0 82.6 51.5 73.5 81.8</figDesc><table><row><cell>method</cell><cell>mean aero bag cap car chair</cell><cell>ear phone</cell><cell>guitar knife lamp laptop</cell><cell>motor bike</cell><cell>mug pistol rocket</cell><cell>skate board</cell><cell>table</cell></row><row><cell>Yi [34]</cell><cell>81.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>3DmFV-Net part segmentation performance compared to state of the art. The metric is mean IoU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>table -</head><label>-</label><figDesc></figDesc><table><row><cell>desk, dresser-night stand,</cell></row><row><cell>and plant-flower pot, containing objects which are difficult</cell></row><row><cell>to discriminate even for humans. See appendix 6.2 for some</cell></row><row><cell>typical failures and for the confusion matrix (computed on</cell></row><row><cell>the test set).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>[ 17 ]</head><label>17</label><figDesc>Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 37-45, 2015. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep fisher networks for large-scale image classification. In Advances in Neural Information Processing Systems, pages 163-171, 2013.</figDesc><table><row><cell></cell><cell>[27] [28] Hang Su, Subhransu Maji, Evangelos Kalogerakis,</cell></row><row><cell>[18] Daniel Maturana and Sebastian Scherer. Voxnet: A</cell><cell>and Erik Learned-Miller. Multi-view convolutional</cell></row><row><cell>3d convolutional neural network for real-time object</cell><cell>neural networks for 3d shape recognition. In Proceed-</cell></row><row><cell>recognition. In The IEEE/RSJ International Confer-</cell><cell>ings of the IEEE International Conference on Com-</cell></row><row><cell>ence on Intelligent Robots and Systems (IROS), pages</cell><cell>puter Vision (CVPR), pages 945-953, 2015.</cell></row><row><cell>922-928. IEEE, 2015.</cell><cell>[29] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,</cell></row><row><cell>[19] Florent Perronnin and Christopher Dance. Fisher ker-</cell><cell>and Alexander A Alemi. Inception-v4, inception-</cell></row><row><cell>nels on visual vocabularies for image categorization.</cell><cell>resnet and the impact of residual connections on learn-</cell></row><row><cell>In The IEEE Conference on Computer Vision and Pat-</cell><cell>ing. In AAAI, pages 4278-4284, 2017.</cell></row><row><cell>tern Recognition (CVPR), pages 1-8. IEEE, 2007.</cell><cell>[30] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-</cell></row><row><cell>[20] Florent Perronnin and Diane Larlus. Fisher vectors</cell><cell>manet, Scott Reed, Dragomir Anguelov, Dumitru Er-</cell></row><row><cell>meet neural networks: A hybrid classification archi-</cell><cell>han, Vincent Vanhoucke, and Andrew Rabinovich.</cell></row><row><cell>tecture. In The IEEE Conference on Computer Vision</cell><cell>Going deeper with convolutions. In Proceedings of</cell></row><row><cell>and Pattern Recognition (CVPR), pages 3743-3752,</cell><cell>the IEEE Conference on Computer Vision and Pattern</cell></row><row><cell>2015.</cell><cell>Recognition (CVPR), pages 1-9, 2015.</cell></row><row><cell>[21] Florent Perronnin, Jorge S?nchez, and Thomas</cell><cell>[31] Federico Tombari, Samuele Salti, and Luigi Di Ste-</cell></row><row><cell>Mensink. Improving the fisher kernel for large-scale image classification. Computer Vision-ECCV 2010, pages 143-156, 2010.</cell><cell>fano. Unique signatures of histograms for local sur-face description. In European Conference on Com-puter Vision, pages 356-369. Springer, 2010.</cell></row><row><cell>[22] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In The IEEE Con-ference on Computer Vision and Pattern Recognition</cell><cell>[32] Walter Wohlkinger and Markus Vincze. Ensemble of shape functions for 3d object classification. In The IEEE International Conference on Robotics and Biomimetics (ROBIO), pages 2987-2992. IEEE, 2011.</cell></row><row><cell>(CVPR), July 2017.</cell><cell>[33] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu,</cell></row><row><cell>[23] Charles R Qi, Hao Su, Matthias Nie?ner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Proceedings of the IEEE Conference on Com-puter Vision and Pattern Recognition, pages 5648-</cell><cell>Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumet-ric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912-1920, 2015.</cell></row><row><cell>5656, 2016.</cell><cell>[34] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen,</cell></row><row><cell>[24] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learn-ing on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017.</cell><cell>Mengyan Yan, Hao Su, ARCewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas, et al. A scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (TOG), 35(6):210, 2016.</cell></row><row><cell>[25] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz.</cell><cell></cell></row><row><cell>Fast point feature histograms (fpfh) for 3d regis-</cell><cell></cell></row><row><cell>tration. In The IEEE International Conference on</cell><cell></cell></row><row><cell>Robotics and Automation(ICRA), pages 3212-3217.</cell><cell></cell></row><row><cell>IEEE, 2009.</cell><cell></cell></row><row><cell>[26] Jorge S?nchez, Florent Perronnin, Thomas Mensink,</cell><cell></cell></row><row><cell>and Jakob Verbeek. Image classification with the</cell><cell></cell></row><row><cell>fisher vector: Theory and practice. International Jour-</cell><cell></cell></row><row><cell>nal of Computer Vision, 105(3):222-245, 2013.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table Desk</head><label>Desk</label><figDesc></figDesc><table><row><cell>Failure cases</cell></row><row><cell>(table classified as desk)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>T t=1 ? t (k) value with points on the plane (bottom). inversion of eq. 19 yields:</p><p>Using the constraint thatn is a unit vector,</p><p>we derive the expressions for the plane parameters as a function of (some of) the FV components:</p><p>Note that ? 0 is the distance of the plane alongn in the local coordinate system. Therefore, to compute ? in the global coordinate system we use:</p><p>To validate these expressions in a more realistic case, where ? t (k) is not binary, we sampled points uniformly on a plane specified by 1 ? 2 (0, 1, 1)p = 0.05. We then computed the point cloud's FV representation using a 5?5?5 Gaussian grid (? = 1 10 , w = 1 125 ) using eq. 6 to 8. Next, using eq. 22 and 23 we estimated the plane parameters from the FV representation of each Gaussian that has a nonnegligible T t=1 ? t (k) value, see 8 (bottom). <ref type="figure">Figure 8</ref> (topleft) shows a part of the original sampled points in red and the reconstructed surface in purple. We found that all the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Our-cvfh-oriented, unique and repeatable clustered viewpoint feature histogram for object recognition and 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Aldoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Radu Bogdan Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint DAGM (German Association for Pattern Recognition) and OAGM Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jutta</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cdric</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Learning in Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive performance evaluation of 3d local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai Ming</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="89" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rotational projection statistics for 3d local surface description and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdous</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fusionnet: 3d object classification using multiple data representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishakh</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling spatial layout with fisher vectors for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1487" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combined 2d-3d categorization and classification for multimodal perception systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Pangercic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1378" to="1402" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">64) -inception(3,5,128) -inception(3,5,256) -maxpool</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>3,3,3],2) -inception(2,3,256) -inception(2,3,512) -FC(1024) -FC(256) -FC(128) -FC(#classes</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">5,64) -inception(3,5,128) -inception(3,5,256) -maxpool</title>
	</analytic>
	<monogr>
		<title level="m">Grid 8?8?8 (3DmFV-Net): 3DmFV -inception</title>
		<imprint/>
	</monogr>
	<note>2,2,2],2) -inception(3,5,256) -inception(3,5,512) -maxpool([2,2,2],2) -FC(1024) -FC(256) -FC(128) -FC(#classes</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">64) -inception(4,8,128) -inception(4,8,256) -maxpool</title>
		<imprint>
			<date type="published" when="1024" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>2,2,2],2) -inception(3,5,256) -inception(3,5,512) -maxpool([2,2,2],2) -inception(2,3,512) -inception(2,3,512) -maxpool. 2,2,2],2) -FC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
