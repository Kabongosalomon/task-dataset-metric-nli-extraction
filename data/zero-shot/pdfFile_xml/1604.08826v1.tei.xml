<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Dense Trajectory with Cross Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-04-29">29 Apr 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsunori</forename><surname>Ohnishi</surname></persName>
							<email>ohnishi@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Hidaka</surname></persName>
							<email>hidaka@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Dense Trajectory with Cross Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-04-29">29 Apr 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts ?Computing methodologies ? Activity recognition and understanding; Keywords Action Recognition</term>
					<term>Egocentric Vision</term>
					<term>Video representa- tion</term>
					<term>Local descriptors</term>
					<term>Video classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Improved dense trajectories (iDT) have shown great performance in action recognition, and their combination with the two-stream approach has achieved state-of-the-art performance. It is, however, difficult for iDT to completely remove background trajectories from video with camera shaking. Trajectories in less discriminative regions should be given modest weights in order to create more discriminative local descriptors for action recognition. In addition, the two-stream approach, which learns appearance and motion information separately, cannot focus on motion in important regions when extracting features from spatial convolutional layers of the appearance network, and vice versa. In order to address the above mentioned problems, we propose a new local descriptor that pools a new convolutional layer obtained from crossing two networks along iDT. This new descriptor is calculated by applying discriminative weights learned from one network to a convolutional layer of the other network. Our method has achieved state-of-the-art performance on ordinal action recognition datasets, 92.3% on UCF101, and 66.2% on HMDB51.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION AND RELATED WORK</head><p>Video representation is becoming increasingly important in today's online environment in which a massive amount of videos are uploaded on a daily basis. Various approaches have been proposed to efficiently and accurately represent the videos.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  <ref type="figure">Figure 1</ref>: Illustration of visualized iDT and feature map from convolutional layer in temporal net. We can see that there are some noisy trajectories in background due to camera shaking.</p><p>Dense trajectories <ref type="bibr" target="#b16">[17]</ref> and improved dense trajectories (iDT) <ref type="bibr" target="#b17">[18]</ref> have dominated action recognition. Extracting hand-crafted features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2]</ref> along these trajectories can provide effective local descriptors, and encoding these local descriptors with a Fisher vector (FV) <ref type="bibr" target="#b10">[11]</ref> or a vector of locally aggregated descriptors (VLAD) <ref type="bibr" target="#b4">[5]</ref> can provide an effective video representation <ref type="bibr" target="#b5">[6]</ref>. Fueled by the recent success of convolutional neural networks (CNN) in image classification, video representations based on CNN have also been developed in action classification. The two-stream approach <ref type="bibr" target="#b11">[12]</ref> is one of the most successful methods that learns appearance information and motion information separately using one network whose input is RGB and the other network whose input is optical flow. The idea of this separate learning has been widely used in later works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Aiming at fully end-to-end learning, three-dimensional CNN learning methods that can capture spatial and temporal information simultaneously and automatically <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> have been developed recently. However, three-dimensional CNN learning is still a very difficult task, and these methods have not yet achieved comparable performance to the state-ofthe-art approach.</p><p>Trajectory-pooled deep-convolutional descriptors (TDD) <ref type="bibr" target="#b18">[19]</ref> have shown state-of-the-art performance in action recognition by pooling convolutional two-stream layers along iDT. Because the convolutional layer retains position information, it is possible to combine it with iDT. However, TDD, which Figure 2: Illustration of the proposed local descriptors, named cross-stream pooled descriptors (CPD). After extraction, we encode these descriptors layer by layer and classify each of them. We then obtain final scores by simply summing all of their scores.</p><p>is based on iDT and the two-stream approach, has two main shortcomings: (1) as shown in <ref type="figure">Figure 1</ref>, iDT cannot completely remove the background image for videos captured by a shaking camera. This can be solved by giving modest weights to background trajectories. (2) Although each network in the two-stream approach captures important information for action recognition, separate CNN learning sometimes lacks other important information that can be obtained only when spatial and temporal information are combined together.</p><p>For example, thinking about the action of pitching a ball, it is difficult for spatial CNN to focus on the region around the pitcher's hand from only a single RGB image. This shortcoming makes it difficult to discriminate between similar actions such as the difference between a soccer penalty kick and a field-hockey penalty shot, or between pitching a ball, a cricket shoot, a tennis serve, and a volleyball spike, especially when no background or context information is in the movie. Although iDT helps to solve this problem when recognizing action, iDT trajectories are hand-crafted so that they still contain both discriminative and non-discriminative trajectories. However, focusing on motion-important regions helps to extract more discriminative appearance features. When seeing a field hockey penalty, for example, the motionimportant region is around the shooter's stick and the kipper. Extracting appearance features around these regions enables us to better recognize whether the player uses their own leg or the hockey stick or whether the kipper wears a protector or not. This can be also said in the case where spatial CNN and temporal CNN are reversed.</p><p>In order to address the above-mentioned problems, we utilize both networks in a two-stream approach by crossing two networks. Convolutional layers in spatial CNNs provide discriminative appearance features with position information while those in temporal CNNs provide discriminative motion features with position information. Thus, we propose a new descriptor that uses one network for the weights and gives these to the other network and pools along iDT, named cross-stream pooled descriptors (CPD). This is equivalent to pooling a convolutional layer of one network along iDT weighted by the convolutional layer of the other network, which leads to giving modest weights to iDTs in less discriminative regions. Our method has improved the performance of TDD on the ordinal action recognition datasets, UCF101 <ref type="bibr" target="#b13">[14]</ref> and HMDB51 <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ACTION RECOGNITION REVISITED</head><p>In this section, we describe previous works on which our method is based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Improved Dense Trajectories</head><p>Improved dense trajectories (iDT) <ref type="bibr" target="#b17">[18]</ref> are the improved version of dense trajectories <ref type="bibr" target="#b16">[17]</ref>, which can remove dense trajectories in background images considering camera motion. A video whose size is (Vx, Vy, T ) contains trajectories P k (k = 1 . . . K):</p><formula xml:id="formula_0">P k = {(x k 1 , y k 1 , t k 1 ), (x k 2 , y k 2 , t k 2 ), ? ? ? , (x k L , y k L , t k L )}, (1) where K is the number of trajectories in a video, (x k l , y k l , t k l )</formula><p>is the position of the lth point in trajectory P k , and L is the length of trajectory. Following other works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, we set L = 15 in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Two-Stream Approach</head><p>The two-stream approach <ref type="bibr" target="#b11">[12]</ref> is a method that learns spatial information from RGB images and temporal information from optical flow images with each CNN separately. Since it is extremely difficult for a temporal net to learn motion using only single flow images, a sequence of ten frames are used as input. In this paper, we call the network learned from RGB images a 'spatial network' and a network learned from optical flow images a 'temporal network.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Trajectory-Pooled Deep-Convolutional Descriptors</head><p>Trajectory-pooled deep-convolutional descriptors (TDD) <ref type="bibr" target="#b18">[19]</ref> combine iDT and the two-stream approach and achieves state-of-the-art performance on the UCF101 dataset. Given a ReLU applied convolutional layer C ? R X?Y ?N?T from the two-stream approach, two normalization methods are applied to C, where X and Y are the width and height of the convolutional layer, N is the number of channels, T is the length of the video, and C ? 0. Spatial normalization provides that Cst and channel normalization provides that C ch :</p><p>Cst(x, y, n, t) = C(x, y, n, t)/maxx,y,tC(x, y, n, t),</p><p>C ch (x, y, n, t) = C(x, y, n, t)/maxnC(x, y, n, t),</p><p>where (x, y) is the position of the convolutional layer, n is the channel number of the convolutional layer, and t is the time in the video.</p><p>These Cst and C ch are pooled along iDT instead of the originally pooled features (HOG <ref type="bibr" target="#b0">[1]</ref>, HOF <ref type="bibr" target="#b8">[9]</ref>, and MBH <ref type="bibr" target="#b1">[2]</ref>). Given a normalized convolutional layer C a b , which is the convolutional layer after applying spatiotemporal normalization or channel normalization (b ? {st, ch}) from spatial or temporal nets (a ? {sp, tmp}), proposed descriptors T DD(P k , C a b ) ? R N are obtained as follows:</p><formula xml:id="formula_3">T DD(P k , C a b ) = L l=1 C a ((rx ? x k l ), (ry ? y k l ), t k l ),<label>(4)</label></formula><p>where (?) is the rounding operation and (rx, ry) = (X/Vw, Y /V h ). These descriptors are encoded by FV. The final video representation is obtained by concatenating encoded vectors from both normalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IDT WITH THE CROSS STREAMS</head><p>As described to this point, separate CNN learning cannot always focus on truly important regions to capture an action's characteristics. Additionally, improved dense trajectories (iDT) cannot completely eliminate background trajectories from videos whose capturing camera experiences large motions. We address these problems to improve recognition performance using two equivalent methods. In this section, we describe both approaches in order to evaluate whether each problem can be improved by each calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cross-Stream Pooling Along iDT</head><p>In order to enhance motion-important regions in a spatial convolutional layer and appearance-important regions in a temporal convolutional layer, we propose a new convolutional layer for iDT pooling: the cross-stream layer. As shown in <ref type="figure">Figure 2</ref>, we produce spatial and temporal convolutional layers element-wise and pool the resulting fourdimensional matrix along iDT. We call this method crossstream pooled descriptors (CPD). However, since each of the nth filters in C tmp and C sp do not have the same meaning, the simple element-wise product C tmp (x, y, n, t)?C sp (x, y, n, t) might not work well. A convolutional layer shows large activation for discriminative regions. Thus, we can obtain a discriminative weight map W ? R X?Y ?T by simply taking the sum in the n-direction:</p><formula xml:id="formula_4">W tmp (x, y, t) = N n=1 C tmp (x, y, n, t),<label>(5)</label></formula><p>where C tmp is a normalized layer calculated from C tmp as in equations <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>. With this motion-based weight map, we can enhance the normalized spatial convolutional layer C sp , which contains appearance information:</p><formula xml:id="formula_5">D sp (x, y, n, t) = C sp (x, y, n, t) ? W tmp (x, y, t).<label>(6)</label></formula><p>D sp represents new appearance features enhanced by motionimportant regions.</p><p>Similarly to motion-based weights, we can obtain appearancebased weights W sp from C sp , and D tmp is calculated in the same way. The term???cross stream??? originated from this cross utilization of two networks.</p><p>We then pool this D along iDT as in equation <ref type="formula" target="#formula_3">(4)</ref> to obtain CP D(P k , D a b ) ? R N as follows:  </p><formula xml:id="formula_6">CP D(P k , D a b ) = L l=1 D a b ((rx ? x k l ), (ry ? y k l ), t k l ).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-Stream Pooling Along Weighted iDT</head><p>We next consider our method from a different point of view. Cross-stream pooled descriptors (CPD) can also be calculated as follows. In order to give modest weights to trajectories in the background region, we take advantage of the rest of the network. A convolutional layer C tmp obtained from a temporal CNN in the two-stream approach, for example, has discriminative motion features without losing position information. Using this C tmp as the weight and giving this weight to iDT, we can obtain new trajectories that are emphasized if they are in the region that contains motion-discriminative trajectories and are less emphasized if they are in regions that contain less motion-discriminative trajectories. As in equation <ref type="formula" target="#formula_4">(5)</ref>, we obtain a discriminative weight map W tmp by taking the sum in the n-direction. Each trajectory is weighted by this map W tmp ; then, we can obtain the weighted iDT. As for motion-based weights, an iDT weighted by an appearance-based map is calculated in the same way. We then pool the normalized convolutional layer C a along the emphasized iDT whose weights are calculated from W a and obtain the CPD as follows:</p><formula xml:id="formula_7">CP D(P k , C a b , W a b ) = L l=1 W a b (x k l , y k l , t k l ) ? C a b ((rx ? x k l ), (ry ? y k l ), t k l ).<label>(8)</label></formula><p>This is equivalent to CP D(P k , D a b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS 4.1 Datasets and Settings</head><p>We conducted experiments on widely used action recognition datasets, UCF101 <ref type="bibr" target="#b13">[14]</ref> and HMDB51 <ref type="bibr" target="#b6">[7]</ref>. We chose VGG16 <ref type="bibr" target="#b12">[13]</ref> as our CNN and utilized publicly available models <ref type="bibr" target="#b19">[20]</ref> that had been already trained on UCF101. Because UCF101 has more variety of actions and videos, we used a model learned on UCF101 split 1 as the initial model for HMDB51 training. The learning rate and other training settings were the same as the training settings for UCF101 <ref type="bibr" target="#b19">[20]</ref>. <ref type="table">Table 3</ref>: Mean accuracy of CPD and other baseline methods on HMDB51 and UCF101. The score * 1 of two-stream (VGG16) on HMDB51 in our calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>HMDB51 UCF101 iDT &amp; FV <ref type="bibr" target="#b17">[18]</ref> 57.2% 85.9% Two stream <ref type="bibr" target="#b11">[12]</ref> 59.4% 88.0% TDD &amp; FV <ref type="bibr" target="#b18">[19]</ref> 63 We chose the models that showed the best validation scores during training.</p><p>As the convolutional layer for pooling, we chose conv3 3, conv4 3, and conv5 3 from VGG16. We call these conv3, conv4, and conv5 in this paper, respectively. A final video representation of each layer was obtained by concatenating st-normed and ch-normed Fisher vectors following TDD <ref type="bibr" target="#b18">[19]</ref>. We fused SVM scores from each layer by taking the sum. Note that, in consideration of the calculation cost, we did not use multi-scale CNN, unlike TDD, and did not apply flipping or cropping to input images, unlike the original twostream approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Parameters and Coding Methods: We found the best coding method and parameters for TDD and CPD with UCF101 split1. Some previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> showed that VLAD encoding is also effective for action recognition. Thus, we tried both FV and VLAD for encoding. Through numerous experiments, we found that the best parameters for FV coding were (D, K) = (64, 128), and those for VLAD coding were (D, K) = (128, 64), where D is the dimension after compression by PCA and K is the number of clusters. Details are given in the supplemental material owing to limited space here.</p><p>Convolutional Type Combination: <ref type="table" target="#tab_0">Table 1</ref> shows that weighting the convolutional layer heightens accuracy for every layer and method, and combining our method with TDD improves the recognition accuracy of TDD. It is also shown that VLAD is more effective for all convolutional layer types than FV.</p><p>Layer Combination on Each Network: <ref type="table" target="#tab_1">Table 2</ref> presents the combination patterns of convolutional layers in each network. In all network types, we can see that using all layers showed the best performance. Thus, we simply employed all of them. <ref type="table">Table 3</ref> represents the action accuracy of CPD and related methods on UCF101 <ref type="bibr" target="#b13">[14]</ref> and HMDB51 <ref type="bibr" target="#b6">[7]</ref>, which are widely used action recognition datasets. Note that we did not flip and crop input images when predicting, unlike the original TDD. Although the two-stream approach of VGG16 without flipping and cropping shows worse performance than that of the original two-stream approach, as denoted in <ref type="table">Table 3</ref>, the performance of TDD with FV is improved by replacing the CNN with VGG16. Encoding VLAD instead of FV also improves recognition accuracy. We then combine the scores of this TDD using VLAD with those of CPD, which increases We only show spatial layer weighted by temporal, because it will appear as the same image as temporal layer weighted by spatial with this visualization method. <ref type="table">Table 4</ref>: Comparison with the state-of-the-art methods. The scores written inside of () is the accuracy when combined with iDT &amp; FV <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of CPD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HMDB51</head><p>UCF101 iDT &amp; FV <ref type="bibr" target="#b17">[18]</ref> 57.2% iDT &amp; FV <ref type="bibr" target="#b17">[18]</ref> 85.9% iDT &amp; Stacked FV <ref type="bibr" target="#b9">[10]</ref> 56.2% C3D <ref type="bibr" target="#b15">[16]</ref> 85.2% <ref type="bibr" target="#b14">[15]</ref> 59.1% F ST CN <ref type="bibr" target="#b14">[15]</ref> 88.1% LATE <ref type="bibr" target="#b2">[3]</ref> 62.2% MIFS <ref type="bibr" target="#b7">[8]</ref> 89.1% TDD &amp; FV <ref type="bibr" target="#b18">[19]</ref> 63.2% TDD &amp; FV <ref type="bibr" target="#b18">[19]</ref> 90.3% + iDT &amp; FV (65.9%) + iDT &amp; FV (91.5%) Video darwin <ref type="bibr" target="#b3">[4]</ref> 63.7% Hybrid LSTM <ref type="bibr" target="#b20">[21]</ref> 91.3% MIFS <ref type="bibr" target="#b7">[8]</ref> 65.1% Two stream (VGG16) <ref type="bibr" target="#b19">[20]</ref> 91.4% CPD (ours) 65.2% CPD (ours) 91.8% TDD + CPD (ours) 66.2% TDD + CPD (ours) 92.3%</p><formula xml:id="formula_8">+ iDT &amp; FV (66.8%) + iDT &amp; FV (90.4%) F ST CN</formula><p>the performance of TDD both on UCF101 and HMDB51. <ref type="figure" target="#fig_1">Fig. 3</ref> shows an example of the visualized iDTs and convolutional layer activation. We can see that the spatial convolutional layer shows activation on many other objects whilst the convolutional layer weighted by the temporal convolutional layer shows activation mainly of the players. It can also be seen that some background iDTs still remain in the image due to camera shaking. However, the spatial layer weighted by the temporal layer activates mainly over the shooter and the kipper, ignoring their backgrounds. Thus, we can confirm that our method extracts appearance information mainly from motion-important regions and that these features capture different characteristics from those of TDD, which augments recognition performance. <ref type="table">Table 4</ref> shows the comparison of our method with other methods of action recognition on the UCF101 and HMDB51 datasets. On UCF101, the proposed method achieved stateof-the-art performance: 0.8% improvement over the combination of TDD <ref type="bibr" target="#b18">[19]</ref> and iDT <ref type="bibr" target="#b17">[18]</ref>. On HDMB51, our method achieved comparable performance to state-of-the-art methods. Considering the scores without adding iDT &amp; FV <ref type="bibr" target="#b17">[18]</ref>, our method shows the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state-of-the-art</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This study proposed a new type of local descriptors for action recognition, termed cross-stream pooled descriptors (CPD), that pools crossed convolutional layers along iDT. Our method achieved state-of-the-art performance on the widely used action recognition datasets UCF101 and HMDB51.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PARAMETER ON UCF101 SPLIT1</head><p>Following previous work that encodes CNN-based local descriptors <ref type="bibr" target="#b21">[22]</ref>, we first evaluate dimension reduction. Then, we explore the number of clusters for encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 FV</head><p>We first evaluate descriptor dimensions after compression by PCA with a fixed number of gaussian mixtures K = 256. <ref type="table" target="#tab_3">Table 5</ref> shows that 64-D achieves the best performance on all methods. Thus, we employ 64-D for FV.</p><p>Next we evaluate number of gaussian mixtures K. <ref type="table" target="#tab_4">Table  6</ref> shows that K = 128 achieves the best result on TDD and TDD + CPD, while K = 256 performs the best on CPD. However, K = 128 on CPD shows comparable performance to K = 256. Thus, we fixed K = 128 both on TDD and CPD in this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 VLAD</head><p>We also evaluate dimensions and number of clusters in VLAD. <ref type="table" target="#tab_5">Table 7</ref> shows that 128-D achieves the best performance on CPD and TDD + CPD. Although 128-D is not the best on TDD, it achieves comparable performance. Thus, we employ 128-D for VLAD. Next we evaluate number of k-means clusters K. <ref type="table" target="#tab_6">Table 8</ref> shows its result. We can see the best K is 64 on CPD and TDD + CPD. K = 64 also achieves almost the same result as the best one, K = 32 on TDD. Thus, we fixed K = 64 both on TDD and CPD in this paper.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>WOODSTOCK ' 97</head><label>97</label><figDesc>El Paso, Texas USA c 2016 ACM. ISBN 123-4567-24-567/08/06. . . $15.00 DOI: 10.475/123 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Sum of filter activations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of each layer type on the UCF101 split1 dataset using parameters (D, K) = (64, 128) for FV and (D, K) = (128, 64) for VLAD.</figDesc><table><row><cell>Convolutional layer type</cell><cell>FV</cell><cell>VLAD</cell></row><row><cell>(a) Spatial</cell><cell>81.2%</cell><cell>81.8%</cell></row><row><cell>(b) Temporal</cell><cell>84.7%</cell><cell>85.5%</cell></row><row><cell>TDD: (a) + (b)</cell><cell>90.7%</cell><cell>91.5%</cell></row><row><cell>(c) Spatial weighted by temporal</cell><cell>81.3%</cell><cell>82.9%</cell></row><row><cell cols="2">(d) Temporal weighted by spatial 85.3%</cell><cell>85.9%</cell></row><row><cell>CPD (ours): (c) + (d)</cell><cell>90.4%</cell><cell>91.6%</cell></row><row><cell>TDD + CPD (ours)</cell><cell>90.8%</cell><cell>92.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">The combination of convolutional layers</cell></row><row><cell cols="5">resulting in each network on the UCF101 split1</cell></row><row><cell cols="5">dataset when VLAD is applied using parameters</cell></row><row><cell>(D, K) = (128, 64).</cell><cell cols="4">(a), (b), (c), and (d) represent</cell></row><row><cell cols="5">spatial, temporal, spatial weighted by temporal, and</cell></row><row><cell cols="3">temporal weighted by spatial cases.</cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row><row><cell>Conv3</cell><cell cols="4">71.9% 77.6% 74.1% 77.7%</cell></row><row><cell>Conv4</cell><cell cols="4">78.2% 82.2% 78.5% 82.0%</cell></row><row><cell>Conv5</cell><cell cols="4">76.3% 82.8% 75.7% 81.2%</cell></row><row><cell>Conv3 + Conv4</cell><cell cols="4">79.0% 82.5% 80.3% 83.2%</cell></row><row><cell>Conv4 + Conv5</cell><cell cols="4">81.3% 85.5% 81.5% 85.8%</cell></row><row><cell cols="5">Conv3 + Conv4 + Conv5 82.2% 85.8% 83.3% 86.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Impact of TDD and CPD dimensions after compression with fixed K = 256 in FV. 3% 90.4% 90.4% 90.2% CPD 90.0% 90.6% 90.4% 90.2% TDD+CPD 90.2% 90.8% 90.5% 90.5%</figDesc><table><row><cell>Dimensions</cell><cell>32-D</cell><cell>64-D</cell><cell>128-D 256-D</cell></row><row><cell>TDD</cell><cell>90.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Impact of the number of gaussian mixture K with fixed PCA dimensions of 64-D in FV.</figDesc><table><row><cell>Clusters</cell><cell cols="4">K = 32 K = 64 K = 128 K = 256</cell></row><row><cell>TDD</cell><cell>89.5%</cell><cell>90.4%</cell><cell>90.7%</cell><cell>90.4%</cell></row><row><cell>CPD</cell><cell>89.9%</cell><cell>90.2%</cell><cell>90.4%</cell><cell>90.6%</cell></row><row><cell>TDD+CPD</cell><cell>90.2%</cell><cell>90.6%</cell><cell>90.8%</cell><cell>90.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Impact of the TDD and CPD dimensions after compression with fixed K = 256 in VLAD. 7% 91.2% 91.5% 91.5% TDD+CPD 91.5% 91.2% 91.5% 91.4%</figDesc><table><row><cell>Dimensions</cell><cell>32-D</cell><cell>64-D 128-D 256-D</cell></row><row><cell>TDD</cell><cell cols="2">91.2% 91.1% 90.9% 91.1%</cell></row><row><cell>CPD</cell><cell>90.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Impact of the number of k-means clusters K with fixed PCA dimensions of 128-D in VLAD.</figDesc><table><row><cell>Clusters</cell><cell cols="4">K = 32 K = 64 K = 128 K = 256</cell></row><row><cell>TDD</cell><cell>91.6%</cell><cell>91.5%</cell><cell>91.3%</cell><cell>90.9%</cell></row><row><cell>CPD</cell><cell>90.3%</cell><cell>91.6%</cell><cell>91.3%</cell><cell>91.5%</cell></row><row><cell>TDD+CPD</cell><cell>91.6%</cell><cell>92.0%</cell><cell>91.5%</cell><cell>91.5%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamically encoded actions based on spacetime saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient feature extraction, encoding and classification for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="674" to="679" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<title level="m">Towards good practices for very deep two-stream convnets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
