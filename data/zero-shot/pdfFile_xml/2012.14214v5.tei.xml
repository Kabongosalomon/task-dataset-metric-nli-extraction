<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransPose: Keypoint Localization via Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
							<email>yangsenius@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Nie</surname></persName>
							<email>niemu@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
							<email>wkyang@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransPose: Keypoint Localization via Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While CNN-based models have made remarkable progress on human pose estimation, what spatial dependencies they capture to localize keypoints remains unclear. In this work, we propose a model called Trans-Pose, which introduces Transformer for human pose estimation. The attention layers built in Transformer enable our model to capture long-range relationships efficiently and also can reveal what dependencies the predicted keypoints rely on. To predict keypoint heatmaps, the last attention layer acts as an aggregator, which collects contributions from image clues and forms maximum positions of keypoints. Such a heatmap-based localization approach via Transformer conforms to the principle of Activation Maximization <ref type="bibr" target="#b18">[19]</ref>. And the revealed dependencies are imagespecific and fine-grained, which also can provide evidence of how the model handles special cases, e.g., occlusion. The experiments show that TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets, while being more lightweight and faster than mainstream CNN architectures. The TransPose model also transfers very well on MPII benchmark, achieving superior performance on the test set when fine-tuned with small training costs. Code and pre-trained models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks have achieved impressive performances in the field of human pose estimation. DeepPose <ref type="bibr" target="#b57">[56]</ref> is the early classic method, directly regressing the numerical coordinate locations of keypoints. Afterwards, fully convolutional networks like <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b52">51]</ref> have become the mainstream by predicting keypoints heatmaps, which implicitly learn spatial dependencies between body parts. Yet, most prior works take deep CNN as a powerful black box predictor and focus on improving the network structure, what exactly happens inside the models or how they capture the spatial relationships between body parts remains unclear. However, from the scientific and practical standpoints, the interpretability of the model can aid practitioners the ability to understand how the model associates structural variables to reach the final predictions and how a pose estimator handles various input images. It also can help model developers for debugging, decision-making, and further improving the design.</p><p>For existing pose estimators, some issues make it challenging to figure out their decision processes. (1) Deepness. The CNN-based models, such as <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b52">51]</ref>, are usually very deep non-linear models that hinder the interpretation of the function of each layer. (2) Implicit relationships. The global spatial relationships between body parts are implicitly encoded within the neuron activations and the weights of CNNs. It is not easy to decouple such relationships from large amounts of weights and activations in neural networks. And solely visualizing the intermediate features with a large number of channels (e.g. 256, 512 in SimpleBaseline architecture <ref type="bibr" target="#b62">[61]</ref>) provides little meaning- The desired explanations for the model predictions should be image-specific and fine-grained. When inferring images, however, the static convolution kernels are limited in the ability to represent variables due to the limited working memory <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">27]</ref>. So it is difficult for CNNs to capture image-specific dependencies due to their content-independent parameters yet variable input image contents. (4) Lack of tools. Although there are already many visualization techniques based on gradient or attribution <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b70">69,</ref><ref type="bibr" target="#b1">2]</ref>, most of them focus on image classification rather than localization. They aim to reveal class-specific input patterns or saliency maps rather than to explain the relationships between structure variables (e.g., the locations of keypoints). By far, how to develop explainable pose estimators remains challenging.</p><p>In this work, we aim to build a human pose estimator that can explicitly capture and reveal the image-specific spatial dependencies between keypoints, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Due to the poor scaling property of convolution <ref type="bibr" target="#b46">[45]</ref>, we argue that convolution has advantages in extracting low-level features, but deeply stacking convolutions at high-level to enlarge the receptive field is not efficient to capture global dependencies. And such deepness increases the difficulty in interpreting CNN predictions. Transformer architecture <ref type="bibr" target="#b59">[58]</ref> has a natural advantage over CNNs in terms of drawing pairwise or higher-order interactions. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, attention layers enable the model to capture interactions between any pairwise locations, and its attention map acts as an immediate memory to store these dependencies.</p><p>Based on these considerations, we propose a novel model called TransPose, using convolutions to extract features at low-level and Transformer to capture global dependencies at high-level. In detail, we flatten the feature maps as input to Transformer and recover its output into the 2D-structure heatmaps. In such a design, the last atten-tion layer in Transformer specially acts as an aggregator, which collects different contributions from all image locations by attention scores and finally forms the maximum positions in the heatmaps. This type of keypoint localization approach via Transformer establishes a connection with the interpretability of Activation Maximization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">49]</ref> and extends it to the localization task. The resulting attention scores can indicate what concrete image clues significantly contribute to the predicted locations. With such evidence, we can further analyze the behaviors of the model by examining the influence of different experimental variables. In summary, our contributions are as follow:</p><p>? We introduce Transformer for human pose estimation to predict heatmap-based keypoints positions, which can efficiently capture the spatial relationships between human body parts. ? We demonstrate that our keypoint localization approach based on Transformer conforms to the interpretability of Activation Maximization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">49]</ref>. Qualitative analysis reveals the dependencies beyond intuition, which are image-specific and fine-grained. ? TransPose models achieve competitive performances against state-of-the-arts CNN-based models via fewer parameters and faster speeds. TransPose achieves 75.8 AP and 75.0 AP on COCO validation set and test-dev set, with 73% fewer parameters and 1.4? faster than HRNet-W48. In addition, our model transfers very well on MPII benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Pose Estimation</head><p>Deep CNNs have achieved great success in human pose estimation. The inductive biases of vanilla convolution kernel <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b30">30]</ref> are locality and translation equivariance. It proves to be efficient to extract low-level image feature. For human pose estimation, capturing global dependencies is crucial <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b61">60,</ref><ref type="bibr" target="#b41">40]</ref>, but the locality nature of convolution makes it impossible to capture long-range interactions. A typical but brute solution is to enlarge the receptive field, e.g. by downsampling the resolution, increasing the depth or expanding the kernel size. Further, sophisticated strategies are proposed such as multi-scale fusion <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref>, stacking <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b38">38]</ref>, or high-resolution representation <ref type="bibr" target="#b52">[51]</ref>; meanwhile, many successful architectures have emerged such as CPM <ref type="bibr" target="#b61">[60]</ref>, Hourglass Network <ref type="bibr" target="#b38">[38]</ref>, FPN <ref type="bibr" target="#b64">[63]</ref>, CPN <ref type="bibr" target="#b11">[12]</ref>, SimpleBaseline <ref type="bibr" target="#b62">[61]</ref>, HRNet <ref type="bibr" target="#b52">[51]</ref>, RSN <ref type="bibr" target="#b7">[8]</ref>, even automated architectures <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b69">68]</ref>. But as the architecture is becoming more complex, it is more challenging but imperative than ever to seek the interpretability of human pose estimation models. In contrast, our model can estimate human pose in an efficient and explicit way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Explainability</head><p>Explainability means a better understanding for human of how the model makes predictions. As surveyed by <ref type="bibr" target="#b48">[47]</ref>, many works define the goal for explanation is to determine what inputs are the most relevant to the prediction, which is also the goal we seek in this paper. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">32]</ref> perform gradient descent in the input space to find out what input patterns can maximize a given unit. <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b19">20]</ref> further consider generating the image-specific class saliency maps. <ref type="bibr" target="#b65">[64]</ref> uses DeConvNet to generate feature activities to show what convolutional layers have learned. Some pose estimation methods <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b68">67]</ref> visualize the feature maps by choosing specific neurons or channels but the results fail to reveal the spatial relationship between parts. <ref type="bibr" target="#b55">[54]</ref> estimates the probability distributions and mutual information between keypoints, yet only revealing the statistic information rather than image-specific explanations. There are also works like Network Dissection <ref type="bibr" target="#b2">[3]</ref>, Feature Visualization <ref type="bibr" target="#b39">[39]</ref>, Excitation Backprop <ref type="bibr" target="#b67">[66]</ref>, LRP attribution method <ref type="bibr" target="#b1">[2]</ref>, CAM <ref type="bibr" target="#b70">[69]</ref>, and Grad-CAM <ref type="bibr" target="#b49">[48]</ref>, which aim to explain the prediction of CNN classifier or visualize the saliency area significantly affecting the class. Different from most prior works, we aim to reveal the fine-grained spatial dependencies between body joints variables in the structural skeleton. And our model can directly exploit the attention patterns to holistically explain its predictions without the help of external tools. We also notice a recent paper <ref type="bibr" target="#b9">[10]</ref> that develops LRPbased <ref type="bibr" target="#b1">[2]</ref> method to compute relevance to explain the predictions of Transformer. It takes ViT model <ref type="bibr" target="#b17">[18]</ref> to visualize class-specific relevance map, showing reasonable results. Unlike their goal, we focus on revealing what clues contribute to visual keypoint localizations, and the attentions in our model provide clear evidence for the predictions.</p><p>It is worth noting that there are some works, such as Co-ordConv <ref type="bibr" target="#b35">[35]</ref> and Zero Padding <ref type="bibr" target="#b29">[29]</ref>, to explain how the neural network predicts the positions and stores the position information by designing proxy tasks. We also conduct experiments to investigate the importance of position embedding for predicting the locations and its generalization on unseen input scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Transformer</head><p>Transformer was proposed by Vaswani et al. <ref type="bibr" target="#b59">[58]</ref> for neural machine translation (NMT) task <ref type="bibr" target="#b54">[53]</ref>. Large Transformer-based models like BERT <ref type="bibr" target="#b16">[17]</ref>, GPT-2 <ref type="bibr" target="#b45">[44]</ref> are often pre-trained on large amounts of data and then finetuned for smaller datasets. Recently, Vision Transformer or attention-augmented layers have merged as new choices for vision tasks such as <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b58">57,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b71">70,</ref><ref type="bibr" target="#b60">59]</ref>. DETR <ref type="bibr" target="#b8">[9]</ref> directly predicts a set of object instances by introducing object queries. ViT <ref type="bibr" target="#b17">[18]</ref> is to pre-train a pure Transformer on large data and then fine-tuned on ImageNet for image classification. DeiT <ref type="bibr" target="#b58">[57]</ref> introduces a distillation token to learn knowledge from a teacher. There are also works <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33]</ref> applying Transformers to 3D pose estimation. <ref type="bibr" target="#b26">[26]</ref> fuses features from multi-view images by attention mechanism. <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b33">33]</ref> output 1D sequences composed of joint/vertex coordinates of pose. Unlike them, we use Transformer to predict the 2D heatmaps represented with spatial distributions of keypoints for 2D human pose estimation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to build a model that can explicitly capture global dependencies between human body parts. We first describe the model architecture. Then we show how it exploits self-attention to capture global interactions and establish a connection between our method and the principle of Activation Maximization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, TransPose model consists of three components: a CNN backbone to extract low-level image feature; a Transformer Encoder to capture long-range spatial interactions between feature vectors across the locations; a head to predict the keypoints heatmaps.</p><p>Backbone. Many common CNNs can be taken as the backbone. For better comparisons, we choose two typical CNN architectures: ResNet <ref type="bibr" target="#b25">[25]</ref> and HRNet <ref type="bibr" target="#b52">[51]</ref>. We only retain the initial several parts of the original ImageNet pretrained CNNs to extract feature from images. We name them ResNet-S and HRNet-S, the parameters numbers of which are only about 5.5% and 25% of the original CNNs.</p><p>Transformer. We follow the standard Transformer architecture <ref type="bibr" target="#b59">[58]</ref> as closely as possible. And only the Encoder is employed, as we believe that the pure heatmaps prediction task is simply an encoding task, which compresses the original image information into a compact position representation of keypoints. Given an input image I ? R 3?H I ?W I , we assume that the CNN backbone outputs a 2D spatial structure image feature X f ? R d?H?W whose feature dimension has been transformed to d by a 1?1 convolution. Then, the image feature map is flattened into a sequence X ? R L?d , i.e., L d-dimensional feature vectors where L = H ? W . It goes through N attention layers and feed-forward networks (FFNs).</p><p>Head. A head is attached to Transformer Encoder output E ? R L?d to predict K types of keypoints heatmaps P ? R K?H * ?W * where H * , W * = H I /4, W I /4 by default. We firstly reshape E back to R d?H?W shape. Then we mainly use a 1?1 convolution to reduce the channel dimension of E from d to K. If H, W are not equal H * , W * , an additional bilinear interpolation or a 4?4 transposed convolution is used to do upsampling before 1?1 convolution. Note, a 1?1 convolution is completely equivalent to a position-wise linear transformation layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Resolution Settings.</head><p>Due to that the computational complexity of per selfattention layer is O (HW ) 2 ? d , we restrict the attention layers to operate at a resolution with r? downsampling rate w.r.t. the original input, i.e., H, W = H I /r, W I /r. In the common human pose estimation architectures <ref type="bibr" target="#b61">[60,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b62">61,</ref><ref type="bibr" target="#b52">51]</ref>, 32? downsampling is usually adopted as a standard setting to obtain a very low-resolution map containing global information. In contrast, we adopt r = 8 and r = 4 setting for ResNet-S and HRNet-S, which are beneficial to the trade-off between the memory footprint for attention layers and the loss in detailed information. As a result, our model directly captures long-range interactions at a higher resolution, while preserving the fine-grained local feature information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attentions are the Dependencies of Localized Keypoints</head><p>Self-Attention mechanism. The core mechanism of Transformer <ref type="bibr" target="#b59">[58]</ref> is multi-head self-attention. It first projects an input sequence</p><formula xml:id="formula_0">X ? R L?d into queries Q ? R L?d , keys K ? R L?d and values V ? R L?d by three matrices W q , W k , W v ? R d?d . Then, the attention scores matrix 2 A ? R N ?N is computed by: A = softmax QK ? ? d .<label>(1)</label></formula><p>Each query q i ? R d of the token x i ? R d (i.e., feature vector at location i) computes similarities with all the keys to achieve a weight vector w i = A i,: ? R 1?L , which determines how much dependency is needed from each token in <ref type="bibr" target="#b1">2</ref> Here we consider single-head self attention. For multi-head selfattention, the attention matrix is the average of attention maps in all heads. the previous sequence. Then an increment is achieved by a linear sum of all elements in Value matrix V with the corresponding weight in w i and added to x i . By doing this, the attention maps can be seen as dynamic weights that determined by specific image content, reweighting the information flow in the forward propagation.</p><p>Self-attention captures and reveals how much contribution the predictions aggregate from each image location. Such contributions from different image locations can be reflected by the gradient <ref type="bibr" target="#b50">[49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">48]</ref>. Therefore, we concretely analyze how x j at image/sequence location j affects the activation h i at location i the predicted keypoint heatmaps, by computing the derivative of h i ? R K (K types of keypoints) w.r.t the x j at location j of the input sequence of the last attention layer. And we further assume G := ?hi ?xj as a function w.r.t. a given attention score A i,j . We obtain:</p><formula xml:id="formula_1">G (A i,j ) ? A i,j ? W f ? W ? v + W f = A i,j ? K + B (2) where K, B ? R K?d</formula><p>are static weights (fixed when inferring) and shared across all image locations. The derivations of Eq. 2 are shown in supplementary. We can see that the function G is approximately linear with A i,j , i.e., the degrees of contribution to the prediction h i directly depend on its attention scores at image locations.</p><p>Especially, the last attention layer acts as an aggregator, which collects contributions from all image locations according to attentions and forms the maximum activations in the predicted keypoint heatmaps. Although the layers in FFN and head cannot be ignored, they are positionwise, which means they approximately linearly transform the contributions from all locations by the same transformation without changing their relative proportions.</p><p>The activation maximum positions are the keypoints' locations. The interpretability of Activation Maximization  (AM) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b50">49]</ref> lies in: the input region which can maximize a given neuron activation can explain what this activated neuron is looking for. In this task, the learning target of TransPose is to expect the neuron activation h i * at location i * of the heatmap to be maximally activated where i * represents the groundtruth location of a keypoint:</p><formula xml:id="formula_2">? * = arg max ? h i * (?, I).<label>(3)</label></formula><p>Assuming the model has been optimized with parameters ? * and it predicts the location of a particular keypoint as i (maximum position in a heatmap), why the model predicts such prediction can be explained by the fact that those locations J, whose element j has higher attention score (? ?) with i, are the dependencies that significantly contribute to the prediction. The dependencies can be found by:</p><formula xml:id="formula_3">J = {j|A i,j (? * , I) ? ?} ,<label>(4)</label></formula><p>where A ? R L?L is the attention map of the last attention layer and also a function w.r.t ? * and I, i.e., A = A (? * , I).</p><p>Given an image I and a query location i, A i,: can reveal what dependencies a predicted location i highly relies on, we define it dependency area. A :,j can reveal what area a location j mostly affects, we define it affected area.</p><p>For the traditional CNN-based methods, they also use heatmap activations as the keypoint locations, but one cannot directly find the explainable patterns for the predictions due to the deepness and highly non-linearity of deep CNNs. The AM-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b65">64,</ref><ref type="bibr" target="#b50">49]</ref> may provide insights while they require extra optimization costs to learn explainable patterns the convolutional kernels prefer to look for. Different from them, we extend AM to heatmap-based localization via Transformer, and we do not need extra optimization costs because the optimization has been implicitly accomplished in our training, i.e., A = A (? * , I). The defined dependency area is the pattern we seek, which can show image-specific and keypoint-specific dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Dataset. We evaluate our models on COCO <ref type="bibr" target="#b34">[34]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref> datasets. COCO contains 200k images in the wild and 250k person instances. Train2017 consists of 57k images and 150k person instances. Val2017 set contains 5k images and test-dev2017 consists of 20k images. In Sec 4.2, we show the experiments on MPII <ref type="bibr" target="#b0">[1]</ref>. And we adopt the standard evaluation metrics of these benchmarks.</p><p>Technical details. We follow the top-down human pose estimation paradigm. The training samples are the cropped images with single person. We resize all input images into 256 ? 192 resolution. We use the same training strategies, data augmentation and person detected results as <ref type="bibr" target="#b52">[51]</ref>. We also adopt the coordinate decoding strategy proposed by <ref type="bibr" target="#b66">[65]</ref>  TP-H-A6 models decay from 0.0001 to 0.00001, we recommend using such a schedule for all models. Considering the compatibility with backbone and the memory consumption, we adjust the hyperparameters of Transformer encoder to make the model capacity not very large. In addition, we use 2D sine position embedding as the default position embedding. We describe it in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on COCO keypoint detection task</head><p>We compare TransPose with SimpleBaseline, HRNet, and DARK <ref type="bibr" target="#b66">[65]</ref>. Specially, we trained the DARK-Res50 on our machines according to the official code with TransPose-R-A4's data augmentation, we achieve 72.0AP; when using the totally same data augmentation and long training schedule of TransPose-R-A4 for it, we obtain 72.1AP (+0.1 AP). The other results showed in Tab. 2 come from the papers. We test all models on a single NVIDIA 2080Ti GPU with the same experimental conditions to compute the average FPS. Under the input resolution -256?192, TransPose-R-A4 and TransPose-H-A6 have obviously overperformed SimpleBaseline-Res152 (+0.6AP) <ref type="bibr" target="#b62">[61]</ref>, HRNet-W48 (+0.7AP) <ref type="bibr" target="#b52">[51]</ref> and DARK-HRNet <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer to MPII benchmark</head><p>Typical pose estimation methods often separately train and evaluate their models on COCO and MPII <ref type="bibr" target="#b0">[1]</ref>. Motivated by the success of pre-training in NLP and recent ViT <ref type="bibr" target="#b17">[18]</ref>, we try to transfer our pre-trained models to MPII. We replace the final layer of the pre-trained Trans-Pose model with a uniform-initialized d ? 16 linear layer   for MPII. When fine-tuning, the learning rates for the pretrained and final layers are 1e-5 and 1e-4 with decay.</p><p>For comparisons, we fine-tune the pre-trained DARK-HRNet on MPII with the same settings, and train these models on MPII by standard full-training settings. As shown in Tab. 5 and <ref type="figure" target="#fig_3">Fig. 4</ref>, the results are interesting: even with longer full-training epochs, models perform worse than the fine-tuned ones; even with large model capacity (28.5M), the improvement (+1.4 AP) brought by pre-training DARK-HRNet is smaller than pre-training TransPose (+2.0 AP). With 256?256 input resolution and fine-tuning on MPII train and val sets, the best result on MPII test set yielded by TransPose-H-A6 is 93.5% accuracy, as shown in <ref type="figure" target="#fig_8">Fig. 6</ref>. These results show that pre-training and fine-tuning could significantly reduce training costs and improve the performances, particularly for the pre-trained TransPose models.</p><p>Discussion.</p><p>The pre-training and fine-tuning for Transformer-based models have shown favorable results in NLP <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b45">44]</ref> and recent vision models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. Our initial results on MPII also suggest that training Transformerbased models on large-scale pose-related data may be a  promising way to learn powerful and robust representation for human pose estimation and its downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>The importance of position embedding. Without position embedding, the 2D spatial structure information loses in Transformer. To explore its importance, we conduct experiments on TransPose-R-A3 models with three position embedding strategies: 2D sine position embedding, learnable position embedding, and w/o position embedding. As expected, the models with position embedding perform better, particularly for 2D sine position embedding, as shown in Tab. 4. But interestingly, TransPose w/o any position embedding only loses 1.3 AP, which suggests that 2D-structure becomes less important. See more details in supplementary.</p><p>Scaling the Size of Transformer Encoder. We study how performance scales with the size of Transformer Encoder, as shown in Tab. 7. For TransPose-R models, with the number of layers increasing to 6, the performance improvements gradually tend to saturate or degenerate. But we have not observed such a phenomenon on TransPose-H models. Scaling the Transformer obviously improves the performance of TransPose-H.</p><p>Position embedding helps to generalize better on unseen input resolutions. The top-down paradigm scales all the cropped images to a fixed size. But for some cases even with a fixed input size or the bottom-up paradigm, the body size in the input varies; the robustness to different scales becomes important. So we design an extreme experiment to test the generalization: we test SimpleBaseline-ResN50-Dark and TransPose-R-A3 models on unseen 128?96, 384?288, 512?388 input resolutions, all of which only have been trained with 256?192 size. Interestingly, the results in <ref type="figure" target="#fig_4">Fig. 5</ref> demonstrate that SimpleBaseline and TransPose-R w/o position embedding have obvious performance collapses on unseen resolutions, particularly on 128?96; but TransPose-R with learnable or 2D Sine position embedding have significantly better generalization, especially for 2D Sine position embedding.</p><p>Discussion. For the input resolution, we mainly trained our models on 256?192 size, thus 768 and 3072 sequence lengths for Transformers in TP-R and TP-H models. Higher input resolutions such as 384?288 for our current models will bring prohibitively expensive computational costs in self-attention layers due to the quadratic complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Analysis</head><p>The hyperparameter configurations for TransPose model might affect the its behavior in an unknown way. In this section, we choose trained models, types of predicted keypoints, depths of attention layers, and input images as controlled variables to observe the model behaviors.</p><p>The dependency preferences are different for models with different CNN extractors. To make comparisons between ResNet-S and HRNet-S based models, we use the trained models TP-R-A4 and TP-H-A4 performances as exemplars. Illustrated in <ref type="figure" target="#fig_8">Fig. 6</ref>, we choose two typical inputs A and B as examples and visualize the dependency areas defined in Sec. 3.3. We find that although the predictions from TP-R-A4 and TP-H-A4 are exactly the same locations of keypoints, TP-H-A4 can exploit multiple longerrange joints clues to predict keypoints. In contrast, TP-R-A4 prefers to attend to local image cues around the target joint. This characteristic can be further confirmed by the visualized affected areas in supplementary, in which keypoints have larger and non-local affected areas in TP-H-A4. Although such results are not as commonly expected, they reflect: 1) a pose estimator uses global information from long-range joints to localize a particular joint; 2) HRNet-S is better than ResNet-S at capturing long-range dependency relationships information (probably due to its multi-scale fusion scheme).</p><p>Dependencies and influences vary for different types of keypoints. For keypoints in the head, localizing them mainly relies on visual clues from head, but TP-H-A4 also associates them with shoulders and the joints of arms. Notably, the dependencies of predicting wrists, elbows, knees or ankles have obvious differences for two models, in which TP-R-A4 depends on the local clues at the same side while TP-H-A4 exploits more clues from the joints on the symmetrical side. As shown in <ref type="figure" target="#fig_8">Fig. 6(b)</ref>, <ref type="figure" target="#fig_8">Fig. 6(d)</ref>, and <ref type="figure" target="#fig_9">Fig. 7</ref>, we can further observe that a pose estimator might gather strong clues from more parts to predict the target keypoint. This can explain why the model still can predict the location of an occluded keypoint accurately, and the occluded keypoint with ambiguity location will have less impact on the other predictions or larger uncertain area to rely on (e.g. the occluded left ankle -last map of <ref type="figure" target="#fig_8">Fig. 6(c)</ref> or <ref type="figure" target="#fig_8">Fig. 6(d)</ref>).     Attentions gradually focus on more fine-grained dependencies with the depth increasing. Observing all of attention layers (the 1,2,3-th rows of <ref type="figure" target="#fig_9">Fig. 7)</ref>, we surprisingly find that even without the intermediate GT locations supervision, TP-H-A4 can still attend to the accurate locations of joints yet with more global cues in the early attention layers. For both models, with the depth increasing, the predictions gradually depend on more fine-grained image clues around local parts or keypoints positions <ref type="figure" target="#fig_9">(Fig. 7)</ref>.</p><p>Image-specific dependencies and statistical commonalities for a single model. Different from the static relationships encoded in the weights of CNN after training, the attention maps are dynamic to inputs. As shown in <ref type="figure" target="#fig_8">Fig. 6(a)</ref> and <ref type="figure" target="#fig_8">Fig. 6(c)</ref>, we can observe that despite the statistical commonalities on the dependency relationships for the predicted keypoints (similar behaviors for most common images), the fine-grained dependencies would slightly change according to the image context. With the existence of occlusion or invisibility in a given image such as input B <ref type="figure" target="#fig_8">(Fig. 6(c)</ref>), the model can still localize the position of the partially obscured keypoint by looking for more significant image clues and reduces reliance on the invisible keypoint to predict the other ones. It is likely that future works can exploit such attention patterns for parts-to-whole association and aggregating relevant features for 3D pose estimation or action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We explored a model -TransPose -by introducing Transformer for human pose estimation. The attention layers enable the model to capture global spatial dependencies efficiently and explicitly. And we show that such a heatmap-based localization achieved by Transformer makes our model share the idea with Activation Maximization. With lightweight architectures, TransPose matches state-ofthe-art CNN-based counterparts on COCO and gains significant improvements on MPII when fine-tuned with small training costs. Furthermore, we validate the importance of position embedding. Our qualitative analysis reveals the model behaviors that are variable for layer depths, keypoints types, trained models and input images, which also gives us insights into how models handle special cases such as occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 2D Sine Position Embedding</head><p>Without the position information embedded in the input sequence, the Transformer Encoder is a permutationequivariant architecture:</p><p>Encoder (? (X)) = ? (Encoder (X)) ,</p><p>where ? is any permutation for the pixel locations or the order of sequence. To make the order of sequence or the spatial structure of the image pixels matter, we follow the sine positional encodings but further hypothesize that the position information is independent at x (horizontal) and y (vertical) direction of an image, like the ways of <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b8">9]</ref>. Concretely, we keep the original 2D-structure respectively with d/2 channels for x, y-direction: </p><formula xml:id="formula_5">P E (2i,</formula><p>where i = 0, 1, ..., d/2 ? 1, p x or p y is the position index along x or y-direction. Then they are stacked and flattened into a shape R L?d . The position embedding is injected into the input sequences before self-attention computation. We use 2D sine position embedding by default for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. What position information has been learned in the TransPose model with learnable position embedding?</head><p>We show what position information has been learned in the TransPose (TransPose-R) with learnable position embedding. It has been discussed in the paper. As shown in <ref type="figure">Fig. 8</ref>, we visualize the similarities by calculating the cosine similarity between vectors at any pair of locations of the learnable position embedding and reshaping it into a 2D grid-like map. We find that the embedding in each location of learnable position embedding has a unique vector value in the d-dim vector space, but it has relatively higher cosine similarity values with the neighbour locations in 2D-grid and lower values with those far away from it. The results indicate the coarse 2D position information has been implicitly learned in the learnable position embedding. We suppose that the learning sources of the position information might be the 2D-structure groundtruth heatmaps and the similar features existing in the 1D-structure sequences. The model learns to build associations between position embedding and input sequences, as a result it can predict the target heatmaps with 2D Gaussian peaking at groundtruth keypoints locations. Cosine Similarity <ref type="figure">Figure 8</ref>. The cosine similarities between the learned position embedding vectors, which have been reshaped into 2D grid and interpolated with 0.25 scale factor for a better illustration (the original shape is <ref type="bibr" target="#b23">(24,</ref><ref type="bibr" target="#b32">32)</ref>). Each map in x-row and y-col of the figure represents the cosine similarities between the embedding vector in position (x, y) and the embedding vectors at other locations.</p><p>In the paper, we find that position embedding helps to generalize better on unseen input resolutions, particularly 2D sine position embedding. We conjecture that 1) the models with a fixed receptive field may be hard to adapt the changes in scales; 2) building associations with position information encoded in Sine position embedding <ref type="bibr" target="#b59">[58]</ref> may help model generalize better on different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Transformer Encoder Layer</head><p>The Transformer Encoder layer <ref type="bibr" target="#b59">[58]</ref> we used can be formulated as: Z = LayerNorm (MultiheadSelfAttention (X) + X) ,</p><formula xml:id="formula_7">X * = LayerNorm (FFN (Z) + Z) ,<label>(7)</label></formula><p>where X is the original input sequence that has not yet been added with position embedding. The position embedding will be added to X for computing querys and keys excluding values. X * is the output sequence of the current Transformer Encoder layer, as the input sequence of next encoder layer. The formulations of Multihead Self-Attention and FFN are defined in <ref type="bibr" target="#b59">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gradient Analysis</head><p>From the view of an activation at some location i of the predicted heatmaps, the network weights associating all input tokens across the whole image/sequence with this activation can be seen as a discriminator that judges the presence or absence of a certain keypoint at this location. As revealed by <ref type="bibr" target="#b48">[47,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b49">48]</ref>, the gradient information can indicate the importance (sensitivity) of the input features to a specific output of a non-linear model. That assumption is based on that tiny change in the input (pixel/feature/token) with the most important feature value causes a large change in what the output of the model would be.</p><p>Suppose we have a trained model and a specific image, h i ? R K is the scores for all K types of keypoints at location i of the predicted heatmaps; z i ? R d is the intermediate feature outputted by the last self-attention layer before being fed into FFN. There is only a ReLU excluding the linear and convolutions 3 (head) layers after the last attention layer. ReLU (rectified linear unit) activation function in FFN can be empirically regarded as a negative contribution filter, which only retains positive contributions and maintains the linearity. Next, we choose numerator layout for computing the derivative of a vector with respect to a vector. We thus assume the mapping from z i to h i can be approximated as a linear function f with learned weights W f ? R K?d and bias b ? R K by computing the first-order Taylor expansion at a given local point z 0</p><formula xml:id="formula_8">i , i.e., h i ? W f z i + b, W f = ?hi ?zi z 0 i</formula><p>. Then we compute the partial derivative of h i at location i of the output heatmaps w.r.t the token x j at location j of the input sequence of the last attention layer:</p><formula xml:id="formula_9">?h i ?x j = ?h i ?z i ?z i ?x j = ?f (z i ) ?z i (1 + ?w i V ?x j ) ? W f (1 + ?w i,0 v 0 + ... + w i,j v j + ... + w i,L?1 v L?1 ?x j ) = W f (1 + ?w i,j v j ?x j ) = W f (1 + ?A i,j W ? v x j ?x j ) (8) where v j ? R d is the value vector transformed by: v j = W ? v x j . A i,j</formula><p>is a scalar value that is computed by the dotproduct between q i and k j . We assume G := ?hi ?xj as a function w.r.t. a given attention score A i,j . Under this assumption A i,j is deemed as an observed variable that has <ref type="table">Table 8</ref>. The detailed configurations for ResNet-S. Conv-k7-s2-c64 means a convolutional layer with 7?7 kernel size, 2 stride, and 64 output channels, followed by a BN and ReLU; the same below. The Bottleneck-c64 includes Conv-k1-s1-c64-BN-ReLU, Conv-k3-s1-c64-BN-ReLU, and Conv-k1-s1-c256-BN. Bottleneck-c128 includes Conv-k1-s1-c128-BN-ReLU, Conv-k3-s1-c128-BN-ReLU, and Conv-k1-s1-c512-BN. See details in <ref type="bibr" target="#b25">[25]</ref>. blocked its parent nodes. Then we define:</p><formula xml:id="formula_10">G (A i,j ) = W f (1 + ?A i,j W ? v x j ?x j ) = W f 1 + A i,j W ? v = A i,j W f W ? v + W f = A i,j Image-Specific: dynamic weights ? W f ? W ? v + W f</formula><p>Learned: static weights = A i,j ? K + B (9) where K, B ? R K?d are static weights shared across all positions. We can see that the function G is approximately linear with A i,j , i.e., the degrees of contribution to the prediction h i directly depend on its attention scores at those locations.</p><p>The last attention layer in Transformer Encoder, whose attention scores are seen as the image-specific weights, aggregate contributions from all locations according to attention scores and finally form the maximum activations in the output heatmaps. Though the layers in FFN and head cannot be ignored 4 , they are position-wise operators, which almost linearly transform the attention scores from all the positions with the same transformation. In addition, Q = (X + P) W q , K = (X + P) W k , V = XW v where P is the position embedding. Because A i,j ? Q i K ? j , the position embedding values also affect the attention scores to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Architecture Details</head><p>We report the architecture details of ResNet-S and HRNet-S-W32(48) in Tab. 8 and Tab. 9. The ResNet-S* only differs from ResNet-S in that ResNet-S* has 10 (a) TP-R-A4: predictions and dependency areas for input 1.</p><p>(b) TP-H-A4: predictions and dependency areas for input 1.</p><p>(c) TP-R-A4: predictions and dependency areas for input 2.</p><p>(d) TP-H-A4: predictions and dependency areas for input 2.</p><p>(e) TP-R-A4: predictions and dependency areas for input 3.</p><p>(f) TP-H-A4: predictions and dependency areas for input 3.</p><p>(g) TP-R-A4: predictions and dependency areas for input 4.</p><p>(h) TP-H-A4: predictions and dependency areas for input 4.</p><p>(i) TP-R-A4: predictions and dependency areas for input 5.</p><p>(j) TP-H-A4: predictions and dependency areas for input 5.</p><p>(k) TP-R-A4: predictions and dependency areas for input 5.   Conv-k1-s1-c64(92) <ref type="table">Table 9</ref>. The detailed configurations for HRNet-S-W32 <ref type="bibr" target="#b49">(48)</ref>. More detailed information about the transition layer and stage blocks are described in the HRNet paper <ref type="bibr" target="#b52">[51]</ref>.</p><p>Bottleneck-c128 blocks. More details about HRNet-W32 and HRNet-W48 are described in <ref type="bibr" target="#b52">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. More Attention Maps Visualizations</head><p>In this section, we show more visualization results of the attention maps from TP-R-A4 (TransPose-R-A4) and TP-H-A4 (TransPose-H-A4) models. The attention maps of the last attention layers of two models are shown in <ref type="figure" target="#fig_13">Fig. 9</ref>. The attention maps in different attention layers of two models are shown in <ref type="figure" target="#fig_0">Fig. 10</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>A schematic diagram of TransPose. Below: The inference pipeline. Above: Dependency areas for each predicted keypoint location. In this example, the person's left-ankle is occluded by a dog. Which exact image clues the model uses to infer the occluded joint? The attention map (red box) gives fine-grained evidence beyond intuition: such a pose estimator highly relies on the image clues around the left ankle, left upper leg, and joints on the right leg to estimate the location of occluded left ankle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>CNN vs. Attention. Left: The receptive filed enlarges in the deeper convolutional layer. Right: One self-attention layer can capture the pairwise relationship between any pair of locations. ful explanations. (3) Limited working memory in inferring various images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The architecture. Firstly, the feature maps are extracted by a CNN backbone and flattened into a sequence. Next, the Transformer encode layers iteratively capture dependencies from the sequences by query-key-value attention. Then, a simple head is used to predict the keypoints heatmaps. The attention map in Transformer can reveal what dependencies (regions or joints) significantly contribute to the activation maximum positions in the predicted keypoint heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Performances on validation set when fine-tuning models (listed in Tab. 5) with different epochs on MPII training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Performances on unseen input resolutions. TransPose models w/ Position Embedding generalize better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) TP-R-A4: predicted keypoints and their dependency areas for input A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) TP-H-A4: predicted keypoints and their dependency areas for input A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(c) TP-R-A4: predicted keypoints and their dependency areas for input B.(d) TP-H-A4: predicted keypoints and their dependency areas for input B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Predicted locations and the dependency areas for different types of keypoints by different models: TP-R-A4 (left column) and TP-H-A4 (right column). In each sub-figure, the first one is the original input image plotted with predicted skeleton. The other maps visualized by the defined dependency area (Ai,:) of the attention matrix in the last layer with a threshold value (0.00075). The predicted location of a keypoint is annotated by a WHITE color pentagram (?) in each sub-map. Redder area indicates higher attention scores. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random 0.0 (a) TP-R-A4: predictions and dependency areas for Input C. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random 0.0 (b) TP-H-A4: predictions and dependency areas for Input C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Dependency areas for the particular positions in the different attention layers by the same visualization method of Fig.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>py,:) = sin 2? * p y /(H * 10000 2i/ d 2 ) , P E (2i+1,py,:) = cos 2? * p y /(H * 10000 2i/ d 2 ) , P E (2i,:,px) = sin 2? * p x /(W * 10000 2i/ d 2 ) , P E (2i+1,:,px) = cos 2? * p x /(W * 10000 2i/ d 2 ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(l) TP-H-A4: predictions and dependency areas for input 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Predicted locations and the dependency areas for different types of keypoints in different models: TP-R-A4 (left column) and TP-H-A4 (right column). In each sub-figure, the first one is the original input image plotted with predicted skeleton. The other maps visualized by the defined dependency area (Ai,:) of the attention matrix in the last layer with a threshold value (0.00075). The predicted location of a keypoint is annotated by a WHITE color pentagram (?) in each sub-map. Redder area indicates higher attention scores. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random TP-R-A4: predictions and dependency areas of each keypoint in different attention layers. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random (b) TP-H-A4: predictions and dependency areas of each keypoint in different attention layers. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random TP-R-A4: predictions and dependency areas of each keypoint in different attention layers. l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random (d) TP-H-A4: predictions and dependency areas of each keypoint in different attention layers. eye(l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random 0.0 (e) TP-R-A4: predictions and affect areas of each keypoint in different attention layers. eye(l) eye(r) ear(l) ear(r) sho.(l) sho.(r) elb.(l) elb.(r) wri.(l) wri.(r) hip(l) hip(r) kne.(l) kne.(r) ank.(l) ank.(r) random random 0.0 (f) TP-H-A4: predictions and affect areas of each keypoint in different attention layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>Dependency areas (the first two rows) and Affected areas (the last row) in different attention layers for different input images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Architecture configurations for different TransPose models. More details about the backbones are described in supplementary.</figDesc><table><row><cell>Model Name</cell><cell>Backbone</cell><cell cols="2">Downsampling for Attention</cell><cell cols="2">Upsampling</cell><cell>#Layers</cell><cell>Heads</cell><cell>d</cell><cell>h</cell><cell>#Params</cell></row><row><cell>TransPose-R-A3*</cell><cell>ResNet-Small*</cell><cell>1/8</cell><cell></cell><cell cols="2">Bilinear Interpolation</cell><cell>3</cell><cell>8</cell><cell>256</cell><cell>512</cell><cell>5.0M</cell></row><row><cell>TransPose-R-A3</cell><cell>ResNet-Small</cell><cell>1/8</cell><cell></cell><cell cols="2">Deconvolution</cell><cell>3</cell><cell>8</cell><cell>256</cell><cell>1024</cell><cell>5.2M</cell></row><row><cell>TransPose-R-A4</cell><cell>ResNet-Small</cell><cell>1/8</cell><cell></cell><cell cols="2">Deconvolution</cell><cell>4</cell><cell>8</cell><cell>256</cell><cell>1024</cell><cell>6.0M</cell></row><row><cell>TransPose-H-S</cell><cell>HRNet-Small-W32</cell><cell>1/4</cell><cell></cell><cell>None</cell><cell></cell><cell>4</cell><cell>1</cell><cell>64</cell><cell>128</cell><cell>8.0M</cell></row><row><cell>TransPose-H-A4</cell><cell>HRNet-Small-W48</cell><cell>1/4</cell><cell></cell><cell>None</cell><cell></cell><cell>4</cell><cell>1</cell><cell>96</cell><cell>192</cell><cell>17.3M</cell></row><row><cell>TransPose-H-A6</cell><cell>HRNet-Small-W48</cell><cell>1/4</cell><cell></cell><cell>None</cell><cell></cell><cell>6</cell><cell>1</cell><cell>96</cell><cell>192</cell><cell>17.5M</cell></row><row><cell>Method</cell><cell></cell><cell>Input Size</cell><cell>AP</cell><cell>AR</cell><cell>#Params</cell><cell></cell><cell cols="2">FLOPs</cell><cell>FPS</cell></row><row><cell cols="2">SimpleBaseline-Res50 [61]</cell><cell>256?192</cell><cell>70.4</cell><cell>76.3</cell><cell>34.0M</cell><cell></cell><cell>8.9G</cell><cell></cell><cell>114</cell></row><row><cell cols="2">SimpleBaseline-Res101 [61]</cell><cell>256?192</cell><cell>71.4</cell><cell>76.3</cell><cell>53.0M</cell><cell></cell><cell>12.4G</cell><cell></cell><cell>92</cell></row><row><cell cols="2">SimpleBaseline-Res152 [61]</cell><cell>256?192</cell><cell>72.0</cell><cell>77.8</cell><cell>68.6M</cell><cell></cell><cell>35.3G</cell><cell></cell><cell>62</cell></row><row><cell cols="2">TransPose-R-A3*</cell><cell>256?192</cell><cell>71.5</cell><cell>76.9</cell><cell cols="2">5.0M (?85%)</cell><cell>5.4G</cell><cell></cell><cell>137 (?20%)</cell></row><row><cell>TransPose-R-A3</cell><cell></cell><cell>256?192</cell><cell>71.7</cell><cell>77.1</cell><cell cols="2">5.2M (?85%)</cell><cell>8.0G</cell><cell></cell><cell>141 (?23%)</cell></row><row><cell>TransPose-R-A4</cell><cell></cell><cell>256?192</cell><cell>72.6</cell><cell>78.0</cell><cell cols="2">6.0M (?82%)</cell><cell>8.9G</cell><cell></cell><cell>138 (?21%)</cell></row><row><cell cols="2">HRNet-W32 [51]</cell><cell>256?192</cell><cell>74.4</cell><cell>79.8</cell><cell>28.5M</cell><cell></cell><cell>7.2G</cell><cell></cell><cell>28</cell></row><row><cell cols="2">HRNet-W48 [51]</cell><cell>256?192</cell><cell>75.1</cell><cell>80.4</cell><cell>63.6M</cell><cell></cell><cell>14.6G</cell><cell></cell><cell>27</cell></row><row><cell>TransPose-H-S</cell><cell></cell><cell>256?192</cell><cell>74.2</cell><cell>78.0</cell><cell cols="2">8.0M (?72%)</cell><cell>10.2G</cell><cell></cell><cell>45 (?61%)</cell></row><row><cell>TransPose-H-A4</cell><cell></cell><cell>256?192</cell><cell>75.3</cell><cell>80.3</cell><cell cols="2">17.3M (?73%)</cell><cell>17.5G</cell><cell></cell><cell>41 (?52%)</cell></row><row><cell>TransPose-H-A6</cell><cell></cell><cell>256?192</cell><cell>75.8</cell><cell>80.8</cell><cell cols="2">17.5M (?73%)</cell><cell>21.8G</cell><cell></cell><cell>38 (?41%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on COCO validation set, all provided with the same detected human boxes. TransPose-R-* and TransPose-H-* achieve competitive results to SimpleBaseline and HRNet, with fewer parameters and faster speeds. The reported FLOPs of SimpleBaseline and HRNet only include the convolution and linear layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>to reduce the quantisation error when decoding from downscaled heatmaps. The feed forward layers are trained with 0.1 dropout and ReLU activate function. Next, we name the models based on ResNet-S and HRNet-S TransPose-R and TransPose-H, abbreviated as TP-R and TP-H. The architecture details are reported in Tab. 1. We use Adam optimizer for all models. Training epochs are 230 for TP-R and 240 for TP-H. The cosine annealing learning rate decay is used. The learning rates for TP-R-A4 and Method Input size #Params FLOPs FPS AP AP0.5 AP0.75 APM APL G-RMI [41] 353?257 42.6M 57G -64.9 85.5 71.3 62.3 70.0 Integral [52] 256?256 45.0M 11.0G -67.8 88.2 74.8 63.9 74.0 CPN [12] 384?288 58.8M 29.2G -72.1 91.4 80.0 68.7 77.2 RMPE [20] 320?256 28.1M 26.7G -72.3 89.2 79.1 68.0 78.6 SimpleBaseline [61] 384?288 68.6M 35.6G -73.7 91.9 81.1 70.3 80.0 HRNet-W32 [51] 384?288 28.5M 16.0G 26 74.9 92.5 82.8 71.3 80.9 HRNet-W48 [51] 256?192 63.6M 14.6G 27 74.2 92.4 82.4 70.9 79.7 HRNet-W48 [51] 384?288 63.6M 32.9G 25 75.5 92.5 83.3 71.9 81.5 DarkPose [65] 384?288 63.6M 32.9G 25 76.2 92.5 83.6 72.5 82.4 TransPose-H-S 256?192 8.0M 10.2G 45 73.4 91.6 81.1 70.1 79.3 TransPose-H-A4 256?192 17.3M 17.5G 41 74.7 91.9 82.2 71.4 80.7 TransPose-H-A6 256?192 17.5M 21.8G 38 75.0 92.2 82.3 71.3 81.1 Comparisons with state-of-the-art CNN-based models on COCO test-dev set. Tested on smaller input resolution 256?192 , our models achieve comparable performances with the others.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>65] (+0.2AP), with significantly fewer model parameters and faster speeds. Tab. 3 shows the results on COCO test set.</figDesc><table><row><cell cols="3">Position Embedding #Params FLOPs</cell><cell>AP</cell></row><row><cell>? Learnable</cell><cell>4.999M 5.195M</cell><cell cols="2">7.975G 70.4 7.976G 70.9</cell></row><row><cell>2D Sine (Fixed)</cell><cell>5.195M</cell><cell cols="2">7.976G 71.7</cell></row></table><note>Results for different position embedding schemes for TransPose models. The input size is 256 ? 192.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 6</head><label>56</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">Input size Training Data Mean@0.5</cell></row><row><cell cols="2">Belagiannis &amp; Zisserman, FG'17 [4] 248?248 COCO+MPII ?</cell><cell>88.1</cell></row><row><cell>Su et al., arXiv'19 [50]</cell><cell>384?384 HSSK+MPII ?</cell><cell>93.9</cell></row><row><cell>Bulat et al., FG'20 [7]</cell><cell>256?256 HSSK+MPII ?</cell><cell>94.1</cell></row><row><cell>Bin et al., ECCV'20 [6]</cell><cell>384?384 HSSK+MPII ?</cell><cell>94.1</cell></row><row><cell>Ours (TransPose-H-A6)</cell><cell>256?256 COCO+MPII ?</cell><cell>93.5</cell></row></table><note>Fine-tuning and full-training performances on MPII val- idation set. ? means full-training on MPII without COCO pre- training. ? means transferring the pretrained model and fine- tuning on MPII; adding ? means fine-tuning MPII on input res- olution 384?384 otherwise 256?256.. Results on MPII benchmark test set. ? means pre-training on COCO dataset and fine-tuning on MPII dataset. ? means train- ing both on MPII and HSSK datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Ablation study on the size of Transformer Encoder. #Layers, d and h are the number of encoder layers, the dimensions d, and the number of hidden units of FFN.</figDesc><table><row><cell>Model</cell><cell cols="2">#Layers d</cell><cell cols="4">h #Params FLOPs FPS AP AR</cell></row><row><cell></cell><cell>2</cell><cell cols="2">256 1024</cell><cell>4.4M</cell><cell cols="2">7.0G 174 69.6 75.0</cell></row><row><cell>TransPose-R</cell><cell>3 4</cell><cell cols="2">256 1024 256 1024</cell><cell>5.2M 6.0M</cell><cell cols="2">8.0G 141 71.7 77.1 8.9G 138 72.6 78.0</cell></row><row><cell></cell><cell>5</cell><cell cols="2">256 1024</cell><cell>6.8M</cell><cell cols="2">9.9G 126 72.2 77.6</cell></row><row><cell></cell><cell>6</cell><cell cols="2">256 1024</cell><cell>7.6M</cell><cell cols="2">10.8G 109 72.2 77.5</cell></row><row><cell></cell><cell>4</cell><cell cols="2">64 128</cell><cell>17.0M</cell><cell>14.6G</cell><cell>-</cell><cell>75.1 80.1</cell></row><row><cell>TransPose-H</cell><cell>4 4</cell><cell cols="2">192 384 96 192</cell><cell>18.5M 17.3M</cell><cell cols="2">27.0G 17.5G 41 75.3 80.3 -75.4 80.5</cell></row><row><cell></cell><cell>5</cell><cell cols="2">96 192</cell><cell>17.4M</cell><cell cols="2">19.7G 40 75.6 80.6</cell></row><row><cell></cell><cell>6</cell><cell cols="2">96 192</cell><cell>17.5M</cell><cell cols="2">21.8G 38 75.8 80.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">a 1 ? 1 convolution is also a position-wise linear layer; the 4 ? 4 deconvolution used in TP-R acts as the upsampling operation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">1. Assuming that the used convolutions extract feature in a limited patch, the global interactions mostly occur at the attention layers. 2. The layer normalization does not affect the interactions between locations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by the National Natural Science Foundation of China (61773117 and 62006041).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3319" to="3327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adversarial semantic data augmentation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanrui</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="606" to="622" />
		</imprint>
	</monogr>
	<note>Changxin Gao, and Nong Sang</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Toward fast and accurate human pose estimation via soft-gated skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11098</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shir</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09838,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scalenas: One-shot learning of scale-aware representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Pai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14584</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5669" to="5678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1341</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3449" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Autopose: Searching multi-scale branch aggregation for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07018</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Agnieszka Grabska-Barwi?ska</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="7779" to="7788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hand-transformer: non-autoregressive structured modeling for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hinton. Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous multi-task learning for human pose estimation with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9605" to="9616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Evopose2d: Pushing the boundaries of 2d human pose estimation using neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08446</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Feature visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Distill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image transformer. In ICML</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer Nature</publisher>
			<biblScope unit="volume">11700</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Workshop Poster)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Cascade feature aggregation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianda</forename><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07837</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Does learning specific features for related parts help human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jonathan J Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou. Training data-efficient image transformers &amp; distillation through attention</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shih-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wankou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07068</idno>
		<title level="m">Pose neural fabrics search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1281" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Distribution-aware coordinate representation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Efficientpose: Efficient human pose estimation with neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07086</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<title level="m">Backbone ResNet-S Stem Conv-k7-s2-c64, BN, ReLU Pooling-k3-s2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<title level="m">Blocks 3?Bottleneck-c64 Bottleneck-s2-c128 3?Bottleneck-c128 Conv-k1-s1-c256</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
