<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Environmental Sound Classification on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-08">Available online 8 September 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mohaimenuzzaman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<addrLine>Wellington Rd</addrLine>
									<postCode>3800</postCode>
									<settlement>Clayton</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<addrLine>Wellington Rd</addrLine>
									<postCode>3800</postCode>
									<settlement>Clayton</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>West</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<addrLine>Wellington Rd</addrLine>
									<postCode>3800</postCode>
									<settlement>Clayton</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Meyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science and AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<addrLine>Wellington Rd</addrLine>
									<postCode>3800</postCode>
									<settlement>Clayton</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Environmental Sound Classification on the Edge: A Pipeline for Deep Acoustic Networks on Extremely Resource-Constrained Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-08">Available online 8 September 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.patcog.2022.109025</idno>
					<note type="submission">Article history: Received 2 April 2021 Revised 27 August 2022 Accepted 4 September 2022</note>
					<note>a r t i c l e i n f o</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep learning Audio classification Environmental sound classification Acoustics Intelligent sound recognition Micro-Controller IoT Edge-AI</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a b s t r a c t Significant effort s are being invested to bring state-of-the-art classification and recognition to edge devices with extreme resource constraints (memory, speed, and lack of GPU support). Here, we demonstrate the first deep network for acoustic recognition that is small, flexible and compression-friendly yet achieves state-of-the-art performance for raw audio classification. Rather than handcrafting a once-off solution, we present a generic pipeline that automatically converts a large deep convolutional network via compression and quantization into a network for resource-impoverished edge devices. After introducing ACDNet, which produces above state-of-the-art accuracy on ESC-10 (96.65%), ESC-50 (87.10%), Urban-Sound8K (84.45%) and AudioEvent (92.57%), we describe the compression pipeline and show that it allows us to achieve 97.22% size reduction and 97.28% FLOP reduction while maintaining close to state-of-the-art accuracy 96.25%, 83.65%, 78.27% and 89.69% on these datasets. We describe a successful implementation on a standard off-the-shelf microcontroller and, beyond laboratory benchmarks, report successful tests on real-world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Intelligent sound recognition is receiving strong interest in a growing number of application areas, from technical safety <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> , surveillance and urban monitoring <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> to environmental applications <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> .</p><p>We are specifically interested in acoustic monitoring of animal vocalisations, a useful and well-established methodology in biodiversity management <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> . Traditionally, animal monitoring is conducted using passive acoustic recording and subsequent manual evaluation by human experts. Since this is extremely labourintensive, AI-based solutions have recently been at the center of interest <ref type="bibr" target="#b8">[9]</ref> .</p><p>The fact that animal monitoring often has to take place in remote regions <ref type="bibr" target="#b9">[10]</ref> poses some interesting technical challenges for automated audio recognition. Almost all suitable state-of-the-art (SOTA) methods for environmental sound classification (ESC) are based on Deep Learning (DL) <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> and consequentially have very high computational requirements. Current AI-based animal monitoring systems typically require recordings to be uploaded <ref type="table">Table 1</ref> SOTA models for ESC-10, ESC-50, US8K and AE datasets sorted by year of publication. Abbreviations : ATTN (Attention), CO (Cochleagram), CRP (Cross Recurrence Plot), CT (Chromagram), DGT (The Discrete Gabor Transform), ENS (Ensemble Model), FBE (FilterBank Energies), GT (GammaTone), LP (Log-Power Spectrogram), Mel (Mel Spectrogram), MFCC (Mel-Frequency Cepstral Coeficients), PE (Phase-Encoded), Raw (Raw audio wave), Spec (Spectrogram), TEO (Teagerfis Energy Operator).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy (%) on Datasets</head><p>Networks #Channels Input(s) ESC-10 ESC-50 US8K AE</p><p>Human <ref type="bibr" target="#b36">[38]</ref> --95. <ref type="bibr" target="#b68">70</ref> 81.30 --Piczak-CNN <ref type="bibr" target="#b45">[47]</ref> Multi Mel 90. <ref type="bibr" target="#b19">20</ref> 64.50 73.70 -GSTC TEO-GSTC <ref type="bibr" target="#b46">[48]</ref> Multi TEO, GT -81.95 --GTSC ConvRBM <ref type="bibr" target="#b38">[40]</ref> Multi (PE)FBE -83.00 -FBEs PEFBEs <ref type="bibr" target="#b47">[49]</ref> Multi FBE -84.15 --EnvNet <ref type="bibr" target="#b48">[50]</ref> Single Raw 88.10 74.10 71.10 -EnvNet-v2 <ref type="bibr" target="#b41">[43]</ref> Single Raw 88. <ref type="bibr" target="#b78">80</ref> 81. <ref type="bibr" target="#b58">60</ref> 76.60 -EnvNet-v2 + BC <ref type="bibr" target="#b41">[43]</ref> Single Raw 91.30 84. <ref type="bibr" target="#b68">70</ref> 78.30 -GoogLENet <ref type="bibr" target="#b49">[51]</ref> Multi Mel, MFCC, CRP 86.00 73.00 93.00 -Kumar-CNN <ref type="bibr" target="#b50">[52]</ref> Multi Mel -83.50 --VGG-CNN + Mixup <ref type="bibr" target="#b51">[53]</ref> Multi Mel, GT 91. <ref type="bibr" target="#b68">70</ref> 83.90 83.70 -AclNet (WM = 1.5) <ref type="bibr" target="#b42">[44]</ref> Single Raw -85. <ref type="bibr" target="#b63">65</ref> --TSCNN-DS <ref type="bibr" target="#b14">[15]</ref> ENS, Multi Mel, MFCC, CT --97.20 -Multi-stream <ref type="bibr" target="#b52">[54]</ref> Multi, ATTN Spec 94.20 84.00 --CRNN <ref type="bibr" target="#b53">[55]</ref> Multi + ATTN Spec -85.20 --FBEs ConvRBM <ref type="bibr" target="#b38">[40]</ref> Multi Spec -86.50 --ESResNet <ref type="bibr" target="#b13">[14]</ref> Multi LP 97.00 91.50 85.42 -Multi-CNN <ref type="bibr" target="#b39">[41]</ref> Multi Mel -89.50 --WEANET <ref type="bibr" target="#b12">[13]</ref> Multi Spec -94.10 --CNN <ref type="bibr" target="#b40">[42]</ref> ENS, Multi DGT, Mel, GT, CO -88. <ref type="bibr" target="#b63">65</ref> --BNN-GAP8 <ref type="bibr" target="#b54">[56]</ref> Multi Spec --- <ref type="bibr" target="#b75">77</ref>.90 CNN-CNP <ref type="bibr" target="#b55">[57]</ref> Multi Spec ---85.10 Method B <ref type="bibr" target="#b15">[16]</ref> Multi Spec ---92. <ref type="bibr" target="#b78">80</ref> However, smartphones and similar devices still have computing power orders of magnitude above the embedded devices we are targeting and thus, much more radical minimisation is required. Embedded edge devices use energy efficient microcontroller units (MCUs) and are typically based on system-on-a-chip (SoC) hardware with less than 512kB of RAM and slow clock speeds. GPU support is available on comparatively high-powered specialised edge devices, such as Google's Coral TPU and Nvidia's Jetson, but not on the ultra low-power MCUs we are targeting. The basis of some of the most common energy efficient SoCs, like the Nordic nRF52840 [28] and the STM32F4 <ref type="bibr" target="#b27">[29]</ref> is the 32 bit ARM Cortex M4F CPU, typically run at clock speeds below 200 MHz.</p><p>To the best of our knowledge, practical acoustic classification has not previously been achieved on such extremely small devices beyond very simple tasks, such as wake-word recognition. Deploying DL models on such MCUs requires aggressive minimisation techniques like model compression <ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref><ref type="bibr" target="#b30">[32]</ref><ref type="bibr" target="#b31">[33]</ref> , knowledge distillation <ref type="bibr" target="#b32">[34]</ref> and quantization <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b33">35]</ref> . Some works have used such techniques for efficient models in computer vision <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37]</ref> , but acoustic classification has not benefited from this yet.</p><p>We present an acoustic classification solution for energyefficient edge devices that achieves close to SOTA performance for raw audio classification on ESC-10&amp;50 <ref type="bibr" target="#b36">[38]</ref> , UrbanSound8K (US8K) <ref type="bibr" target="#b37">[39]</ref> and AudioEvent (AE) <ref type="bibr" target="#b15">[16]</ref> , the benchmarks that are used to assess large, non resource-constrained networks. Importantly, we do not specifically design the network for these target devices. Rather, we describe a generic pipeline that automatically compress and quantize a large deep CNN into a network suitable for edge devices.</p><p>We first introduce ACDNet, a new, flexible and compression friendly sound classification architecture that exceeds current SOTA performance on raw audio classification with accuracies of 96.65 ?0 . 06 %, 87.10 ?0 . 02 %, 84.45 ?0 . 05 % and 92.57 ?0 . 05 % on ESC-10, ESC-50, US8K and AE, respectively. These performances are also very close to the overall SOTA for the mentioned benchmarks (see <ref type="table">Table 1</ref> ). We then compress ACDNet using a novel networkindependent compression approach to obtain an extremely small model (Micro-ACDNet). Despite 97.22% size reduction and 97.28% reduction in FLOPs, Micro-ACDNet still achieves classification accuracy of 96.25%, 83.65%, 78.28% and 89.69% on the same datasets, which are significantly higher than human accuracy (i.e., 81.30% for ESC-50) and still very close to the SOTAs.</p><p>To classify 1 . 5 s audio sampled at 20kHz, ACDNet uses 4.74M parameters (18.06MB) and requires approximately 544M FLOPs for a single inference. On the other hand, the micro version, Micro-ACDNet has only 0.131M parameters (0.50MB) and requires only 14.82M FLOPs for an inference, which is well within the capabilities of the MCUs we are targeting. We describe a successful deployment of a model for classifying 50 classes on a standard offthe-shelf MCU and, beyond laboratory benchmarks, report successful tests on real-world data.</p><p>To the best of our knowledge, this is the first time a deep network for sound classification of 50 classes has successfully been deployed on an edge device. While this should be of interest in its own right, we believe it to be of particular importance that this has been achieved with a generic conversion pipeline rather than hand-crafting a network for minimal size.</p><p>Hence, the contributions of this research article are as follows: 1) it introduces the first generic pipeline for sound classification using DL in extremely resource-constrained MCUs; 2) it presents ACDNet, a flexible and compression-friendly SOTA DL model architecture for raw audio classification; 3) instead of hand-crafting a once-off solution, it introduces a novel deep CNN compression approach to obtain tiny models for MCUs; and 4) finally, after 8-bit quantization, we deploy Micro-ACDNet on a standard othe shelf MCU with successful tests on real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>A wide range of DL models for acoustic classification achieve SOTA performance on different environmental sound classification benchmarks. <ref type="table">Table 1</ref> shows the SOTA models developed over the past years for the four datasets discussed (ESC-10, ESC-50, US8K and AE). Unfortunately, most of them have not adequately disclosed their model sizes and computation requirements. In reality, none of the latest SOTA models for any of the four datasets <ref type="table">Table 2</ref> ACDNet architecture. Output shape represents (channel, frequency, time), i _ len is the input length, n _ cls is the number of output classes, sr is the sampling rate in Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head><p>Kernel Stride Filters Output Block Size Shape</p><formula xml:id="formula_0">Input (1, 1, w = i _ len ) conv1 (1, 9) (1, 2) x ( x , 1, w = w ?9 2 + 1 ) SFEB conv2 (1, 5) (1, 2) x = x * 2 3 ( x , 1, w = w ?5 2 + 1 ) Maxpool1 (1, SF EB _ PS) (1, SF EB _ PS) ( x , 1, w = w ps ) swapaxes (1, h = x , w ) conv3 (3, 3) (1,1) x = x * 2 2 ( x , h , w ) TFEB Maxpool2 T F EB _ PS[0] T F EB _ PS[0] ( x , h = h kh , w = w kw ) conv4,5 (3, 3) (1, 1) x = x * 2 3 ( x , h , w ) Maxpool3 T F EB _ PS[1] T F EB _ PS[1] ( x , h = h kh , w = w kw ) conv6,7 (3, 3) (1, 1) x = x * 2 4 ( x , h , w ) Maxpool4 T F EB _ PS[2] T F EB _ PS[2] ( x , h = h kh , w = w kw ) conv8,9 (3, 3) (1, 1) x = x * 2 5 ( x , h , w ) Maxpool5 T F EB _ PS[3] T F EB _ PS[3] ( x , h = h kh , w = w kw ) conv10,11 (3, 3) (1, 1) x = x * 2 6 ( x , h , w ) Maxpool6 T F EB _ PS[4] T F EB _ PS[4] ( x , h = h kh , w = w kw ) Dropout (0.2) conv12 (1, 1) (1, 1) n _ cls ( n _ cls , h , w ) Avgpool1</formula><p>T F EB _ PS <ref type="bibr" target="#b4">[5]</ref> T F EB _ PS <ref type="bibr" target="#b4">[5]</ref> ( n _ cls , 1, 1) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b38">[40]</ref><ref type="bibr" target="#b39">[41]</ref><ref type="bibr" target="#b40">[42]</ref> has provided information on model sizes and computation requirements. EnvNet-v2 <ref type="bibr" target="#b41">[43]</ref> and AclNet <ref type="bibr" target="#b42">[44]</ref> are found to be the only ones of the latest twenty two SOTA models presented in <ref type="table">Table 1</ref> that have fully detailed their computational requirements (see <ref type="table">Table 6</ref> ). The high requirements of EnvNet-v2 are partly due to the use of multiple dense layers <ref type="bibr" target="#b41">[43]</ref> , which, regrettably, do not lend themselves well to compression <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b44">46]</ref> . <ref type="table">Table 1</ref> shows that all the recent SOTA approaches (sorted by year of publication) resort to multi-channel image-like spectrogram-based inputs, multiple parallel blocks or ensembles models and attention mechanisms. While this demonstrates some success in improving performance, it also increases the input and model sizes significantly and thus does not constitute an advantageous starting point for our purposes. The whole SOTA list also shows no spectrogram-based SOTA model to date runs on singlechannel input. The reason behind this lack of success in inputting audio as grey images (single-channel spectrogram) is that unlike vertical and horizontal axes of images, the axes (time &amp; frequency) of spectrograms are not homogeneous and hence do not carry the same spatial information <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b57">59]</ref> . Using multiple channels to input different f eature sets usually compensates for this problem; however, this merely makes the input size bigger than the available primary memory available in a typical MCU. On the other hand, inputting audio as raw waveform does not require expensive handdesigned features, allows us to exploit better modelling capability and learning representations <ref type="bibr" target="#b4">[5]</ref> yet is suitable for single-channel input for MCUs. <ref type="table">Table 1</ref> also shows that all the SOTA models running on single-channel input runs on raw audio waveform. Considering all the above discussed issues, this research works with DL models that run on single-channel raw audio waveform.</p><formula xml:id="formula_1">Flatten ( n _ cls ) Dense1 n _ cls ( n _ cls ) Softmax ( n _ cls ) Output</formula><p>Some recent work in computer vision that has specifically focused on very small CNN models is highly relevant to our work. MCUNet <ref type="bibr" target="#b35">[37]</ref> starts from SOTA image classification models, such as ResNet50 <ref type="bibr" target="#b58">[60]</ref> and MobileNetV2 <ref type="bibr" target="#b22">[23]</ref> , and uses search-based optimisation to find models that fit on MCUs. The authors report &gt; 70% accuracy after deploying the final models on MCUs.</p><p>The recent Sparse Architecture Search (SpArSe <ref type="bibr" target="#b34">[36]</ref> ) appears to be a very promising approach. It uses multi-objective search to automatically construct models small enough for MCU deployment. It optimises accuracy, model size, and working memory size by performing a search over pruning, parameters, and model archi-tecture. The latter fundamentally sets it apart from other work. While other work minimises specific target models, SpArSe includes the model architecture in the search space. In this way, SpArSe achieves impressive results with models that achieve high accuracy on a variety of standard vision benchmarks (MNIST, CI-FAR10, CUReT, Chars4k) and are small enough to be deployed on standard MCUs.</p><p>Model search can require prohibitively vast amounts of computation time. The feasibility of SpArSe is achieved by constraining the search space to model proposals that are specific morphs of a defined starting point. The success of SpArSe thus relies on the availability of suitable models as starting points. While still improving their performance and size, <ref type="bibr" target="#b34">[36]</ref> does in fact start from models which themselves are already constructed to fit on MCUs (Bonsai <ref type="bibr" target="#b59">[61]</ref> and ProtoNN <ref type="bibr" target="#b60">[62]</ref> ). Such starting points are currently not available for audio classification, and, to the best of the authors knowledge, no comparable work exists for this problem domain.</p><p>In the audio domain, the only work that specifically has the reduction of computational requirements as its primary focus is Edge-L 3 <ref type="bibr" target="#b61">[63]</ref> . It achieves a model size of 0.814MB by pruning L 3 -Net <ref type="bibr" target="#b62">[64]</ref> , which has an initial size of 18MB. While this approach achieves good theoretical compression ratios and good prediction accuracy, it relies on unstructured compression. This results in sparse matrix models which, unfortunately, do not ideally lend themselves to a direct implementation on embedded devices. MCUs generally lack the dedicated hardware and software support for sparse matrix computations required to capitalise on these theoretical savings <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b44">46]</ref> . Hence, structured compression techniques that produce dense matrix models promise to be a preferable approach for targeting MCUs.</p><p>In the following sections, we detail the construction of a flexible and compression friendly DL model that betters current SOTA performance for raw audio classification on ESC-10, ESC-50, US8K and AE while also providing a suitable starting point for compression, followed by a discussion of the structured compression methods used to minimise this for MCU deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed base network</head><p>We present a new model architecture (ACDNet) for acoustic classification that is smaller, flexible, compression-friendly and more efficient than the current SOTA networks, yet produces classification accuracy close to them and exceeds classification accuracy of the SOTA models for raw audio classification on the most widely used Environmental Sound Classification datasets ESC-10, ESC-50, US8K and AE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ACDNet Architecture</head><p>Unlike other suggested SOTA models, which use pre-extracted features and multi-channel input (such as hand-crafted features and spectrograms), the ACDNet architecture concentrates solely on feature extraction through convolution layers. All the convolution layers are followed by a batch normalization and a ReLU activation layers. The maxpool layers have strides equal to their pool size to avoid overlapping. The network is fed with raw audio time series (i.e., single-channel input). ACDNet consists of two feature extraction blocks followed by an output block. The feature extraction blocks are Spectral Feature Extraction Block (SFEB) and Temporal Feature Extraction Block (TFEB). <ref type="figure" target="#fig_0">Fig. 1</ref> depicts the layer structure of different blocks of the ACDNet architecture.</p><p>SFEB consists of two 1-D strided convolutions followed by the first pooling layer. This block extracts low level audio features (spectral features) from raw audio through convolutions at a frame rate of 10ms. The output of this block is achieved by downsampling the convolution output using the maxpool with kernel size determined by the following equation:</p><formula xml:id="formula_2">SF EB _ P S = w ((i _ len/sr) * 10 0 0) / 10<label>(1)</label></formula><p>where w is the width of the output of previous layer, i _ len is the input length and sr is the sampling rate. The axes of the data produced by SFEB are swapped from (ch, f, t) to (ch' = f, f' = ch, t' = t), and the result is fed as input to TFEB to convolve over both frequency and time for extracting high level features also known as hierarchical temporal features. Thus, this block of the network works like a convolution on images. TFEB consists of convolutions 3-12 in <ref type="table">Table 2</ref> . The first convolution layer is followed by a pooling layer. After that, the convolution layers (4-11) are stacked like VGG-13 <ref type="bibr" target="#b63">[65]</ref> (two convolutions followed by a pooling layer). The final convolution layer ( conv-12 ) is followed by a single average pooling layer. The kernel sizes of the pooling</p><formula xml:id="formula_3">layers are determined by T F EB _ P S = { ( f (x h , i ) , f (x w , i )) k } k ?{ 1 , 2 , ... ,N}</formula><p>where x h and x w are the height and width of the input to TFEB block, i is the index of the pooling layer, N is the number of pooling layers, and f (x, i ) is defined by:</p><formula xml:id="formula_4">f (x, i ) = 2 if x &gt; 2 and i &lt; N 1 if x = 1 x 2 (N?1) if i = N<label>(2)</label></formula><p>TFEB block ends with an average-pool layer followed by a dense layer. The dense layer have output neurons equal to the number of classes to make sure the size of the output vector of TFEB is always equal to the number of classes. This dense layer is crucial for the compression of the network in a later stage. The output of the TFEB is fed to a softmax output layer for classification. <ref type="table">Table 2</ref> shows the overall ACDNet architecture.</p><p>While the structure of the first two convolution layers has some similarity with EnvNet-v2 <ref type="bibr" target="#b41">[43]</ref> and the other convolution layers with AclNet <ref type="bibr" target="#b42">[44]</ref> , EnvNet-v2 and AclNet architectures cannot flexibly be adapted to different lengths of audio recorded at different sampling rates. Furthermore, they are not compression friendly for deriving tiny network models suitable for MCUs with comparable accuracy.</p><p>EnvNet-v2 has eight 1D convolutions and two 2D convolutions with 1056 filters, followed by two massive dense layers with 4096 neurons each. These dense layers produce approximately 100M parameters out of 101M These dense layers produce approximately 100M parameters out of 101M parameters of the entire network. Hence, these two layers will be heavily pruned during the compression process, which would heavily impact the network's performance.</p><p>On the other hand, AclNet has twelve convolution layers with 3050 filters followed by the softmax output layer. The number of output filters from the last convolution layer always equals to the number of classes. Hence, this last convolution layer cannot be compressed. This might lead to pruning some more important filters from other layers, which may cause further accuracy loss. In addition to that, this may make the network structure pyramid-shaped which is wider towards the end when the number of classes is higher (e.g., 50 or 100). Such wide layers require extra primary memory during inference time that a typical MCU would struggle to provide.</p><p>ACDNet architecture is strongly motivated from the perspective of compressibility of the network and adaptability with respect to different audio lengths recorded at different sampling rates (see <ref type="table">Table 2</ref> ). It addresses the issue of EnvNet-v2 by adding more convolution layers and a tiny dense layer followed by the softmax output layer to allow the network to adapt to the changes in data shape when model compression is applied. The addition of a tiny dense layer also addresses AclNet's compressibility issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental setup</head><p>ACDNet is implemented in PyTorch version 1.7.1 and Wavio audio library is used to process the audio files. The full code is available at: https://github.com/mohaimenz/acdnet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Datasets</head><p>The experiments are conducted on three of the most robust and widely used audio benchmark datasets -Environmental Sound (ESC-10, ESC-50 <ref type="bibr" target="#b36">[38]</ref> and US8K <ref type="bibr" target="#b37">[39]</ref> ) and another audio dataset named AudioEvent(AE) <ref type="bibr" target="#b15">[16]</ref> .</p><p>ESC-50 contains 20 0 0 samples (5-sec-long audio recordings, sampled at 16kHz and 44.1kHz) which are equally distributed over 50 balanced disjoint classes (40 audio samples for each class). Furthermore, a division of the dataset into five splits is available, helping researchers to achieve unbiased comparable results in 5-fold cross validation (CV). ESC-10 is a subset of ESC-50 with ten classes and 400 samples equally distributed over the classes. US8K dataset contains 8732 labelled audio clips ( &lt; = 4s) of urban sounds from 10 classes recorded at 22.05kHz. The clips are pre-sorted to 10 folds for 10-fold CV to achieve unbiased comparable results. The AE dataset has 5223 samples unevenly distributed over 28 classes recorded at 16kHz.</p><p>Our work is ultimately aimed at real-world applications for biodiversity, and these can be more difficult than standard benchmarks, as evidenced in the LifeCLEF competition <ref type="bibr" target="#b64">[66]</ref> . However, LifeCLEF itself is not suitable as a benchmark for the problem tackled here as this starts from strong labeling, whereas LifeCLEF explores weak labeling. As a real-world test, we evaluate ACDNet on a dataset of frog recordings that is known to be challenging for conventional animal monitoring solutions <ref type="bibr" target="#b65">[67,</ref><ref type="bibr" target="#b66">68]</ref> . It contains 9132 field recordings of 10 different frog species across a variety of locations in Australia sampled at 32kHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Data processing and training setup</head><p>For setting up the output filters of the convolutions of ACDNet,</p><p>we set x = 8 to the con v 1 layer of the SF EB block (see <ref type="table">Table 2</ref> .</p><p>The network is trained and tested on 20kHz data with inputs of length 30,225 (approximately 1.51s audio). We downsampled the data to 20kHz in order to reduce input size, model size, and power consumption. We have not observed any difference in performance with audios re-sampled at a lower sampling rate. We follow the data preprocessing, augmentation, and mixing of classes described in EnvNet-v2 <ref type="bibr" target="#b41">[43]</ref> to produce training samples. According to EnvNet-v2, two training sounds belonging to two different classes are randomly picked, padded with T/2 (T = input length) zeros to each side of both the samples and a T-s section from both the sounds is randomly cropped. Then, the two cropped samples are mixed using a random ratio. We denote the maximum gains of the cropped samples s 1 , s 2 by g 1 , g 2 , and r is the random ratio between (0,1). The ratio of the mixed sounds p according to EnvNet-v2, is</p><formula xml:id="formula_5">p = 1 1 + 10 * g1 ?g2 20 * 1 ?r r<label>(3)</label></formula><p>Finally, the mixed sound sample S mix for training is determined by</p><formula xml:id="formula_6">S mix = ps 1 + (1 ? p) s 2 p 2 + (1 ? p) 2<label>(4)</label></formula><p>In the testing phase, we pad T/2 zeros to each side of the test input sample and then extract ten windows (each of length 30,225) at a regular interval of T/9 as input for the network. The input data for training and testing are normalized by dividing them by 32,768, the full range of 16-bit recordings.</p><p>The network is trained for 20 0 0 epochs with an initial learning rate of 0.1 along with a learning rate scheduler {60 0, 120 0, 1800} decaying at a factor of 10. The first 10 epochs are considered as warm-up epochs and a 0.1 times smaller learning rate is used for these 10 epochs. Since we mix up class labels to generate samples, the mini-batch ratio labels should represent the expected class probability distribution. Hence, we use KLDivLoss (Kullback-Leibler Divergence Loss) as the loss function instead of cross-entropy loss <ref type="bibr" target="#b41">[43]</ref> . We optimize it using back-propagation and Stochastic Gradient Descent ( SGD ) with a Nesterov momentum of 0.9, a weight decay of 5e-4, and a batch size of 64. Furthermore, the weights of all convolution and dense layers are initialized using </p><formula xml:id="formula_7">L = 1 n n i =1 (D KL (y (i ) || f ? (x (i ) ))) = 1 n n i =1 m j=1 y (i ) j log y (i ) j ( f ? (x (i ) )) j<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Training and testing ACDNet</head><p>We conduct 5-fold CV for ESC-10&amp;50, 10-fold CV for US8K and 80-20 train-test split for AE over 10 independent runs.</p><p>To establish the statistical significance of comparisons between ACDNet and other SOTA models, we cannot apply typical standard procedures, such as the Friedman test with post-hoc analysis or the ROC curve. This is because the implementations of the comparison models are not available, and we thus have to rely on published data only. Generally, only the mean accuracy is reported (without standard error), and only a few models have been tested on all four datasets. Instead, we use the bootstrap confidence intervals <ref type="bibr" target="#b68">[70,</ref><ref type="bibr" target="#b69">71]</ref> to calculate the 95 percent confidence interval (95-PCI) <ref type="bibr" target="#b70">[72]</ref> of classification accuracy of ACDNet for all four datasets to confirm the model's generalizability and reliability.</p><p>We run ACDNet 10 0 0 times on the test sets of all four datasets using bootstrap sampling with replacement, and then construct the 95-PCI of classification accuracy using the following formula.</p><formula xml:id="formula_8">CI = ? ? Z ? ? N<label>(6)</label></formula><p>where ? is the mean accuracy of all the tests, Z = 1 . 96 <ref type="bibr" target="#b70">[72]</ref> , ? is the standard deviation and N is the number of tests. <ref type="table" target="#tab_0">Table 3</ref> presents the CV accuracy and the estimated 95-PCI of classification accuracy of ACDNet on the mentioned datasets. <ref type="table" target="#tab_0">Table 3</ref> reveals that the CV accuracy of ACDNet and its estimated 95-PCI accuracy on various datasets are almost equivalent, if not identical, indicating that the network is well generalized over all the datasets.</p><p>We now add ACDNet to the SOTA table for raw audio classification. <ref type="table">Table 4</ref> indicates that ACDNet outperforms the current SOTA with an overall accuracy of 96 . 65 ? 0 . 06 % on ESC-10, 87 . 10 ? 0 . 02 % on ESC-50, 84 . 45 ? 0 . 05 % on US8K and 92 . 57 ? 0 . 05 % on AE datasets. Furthermore, when compared to all SOTA models regardless of the types of the input they deal with, ACDNet achieves comparable prediction accuracy (see <ref type="table">Table 5</ref> ).</p><p>For the real-world frog dataset, ACDNet is trained for 10 0 0 epochs. We obtain 5-fold CV accuracy of 88.98% even without data augmentation.</p><p>Few SOTA models report their size and computation requirements in the literature. According to our knowledge on current literature, EnvNet-v2 and AclNet are the only two of the top-ten models for either of the two datasets to report parameter count, size, and FLOPs. <ref type="table">Table 6</ref> shows that ACDNet requires 21.39 ? less </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Network compression</head><p>As discussed before, unstructured compression is not suitable for MCUs. Therefore, we introduce a new class of structured hybrid compression techniques. We first sparsify the weight matrices of ACDNet using unstructured compression (e.g., L0-Norm) and then apply structured compression. The idea of sparsifying the weight matrices is inspired by the fact that bringing sparsity into the network reduces the chance of model over-fitting and enhances the model performance <ref type="bibr" target="#b71">[73]</ref> . Furthermore, many researchers have shown that sparse pruning of neural networks often produces the same or even better accuracy than the base network <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b73">75]</ref> . This allows us to focus the structured compression on the important weights.</p><p>There are different techniques by which structured compression can be achieved, such as Channel Pruning <ref type="bibr" target="#b30">[32]</ref> , Weight Sharing <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b74">76]</ref> , Huffman Coding <ref type="bibr" target="#b75">[77]</ref> , Knowledge Distillation <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b76">78]</ref> and Quantization <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b77">79]</ref> . Though many works in the literature use these techniques in computer vision, they have hardly ever been used for audio tasks. Due to this lack of guidance from the liter-ature, we use the best established pruning-based model compression techniques proposed for computer vision to compress ACDNet. Furthermore, since quantization does not conflict with other compression techniques, we use quantization of the compressed model to further reduce its size before deploying it on the embedded device.</p><p>Structured pruning, namely channel pruning for CNN is conducted by ranking the channels globally and removing the lowest ranked channels. Let z l be the feature maps of a network with z l ? R h l ?w l ?c l where h l ? w l is the dimension and c l are the channels of layers with l ? [1 , 2 , . . . , L ] . z (i ) l is an individual feature map with i ? [1 , 2 , . . . , c l ] . Thus, the ranking is calculated ( Eq. 8 ) followed by , the layer-wise normalization ( Eq. 7 ) of the feature maps. Finally, the pruning candidate is determined by Eq. 9 :</p><formula xml:id="formula_9">(z l ) = | z (i ) l | | z l | 2<label>(7)</label></formula><p>z rank = sort( (z l )) <ref type="bibr" target="#b7">(8)</ref> z prune = argmin (z rank )</p><p>Many ranking criteria exist in the current literature. Few of them are magnitude-based (L2-Norm) ranking <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b31">33]</ref> , Taylor criteriabased ranking <ref type="bibr" target="#b30">[32]</ref> and binary index-based ranking in AutoPruner <ref type="bibr" target="#b78">[80]</ref> . In this paper, we explore the magnitude-based ranking which is one of the very common techniques and the Taylor criteriabased ranking which is proposed in one of the recent state-of-theart channel pruning techniques for computer vision applications. On the other hand, our proposed hybrid pruning technique uses a new approach that combines unstructured and structured ranking methods to achieve structured compression.</p><p>To make sure the network works flawlessly with the updated output shape of different layers during and after compression, the kernel sizes of the pooling layers are adjusted accordingly. Furthermore, after flattening the Avgpool1 output, when the flattened vector has elements less than the number of classes (i.e., 50), the number of input neurons of the Dense1 layer is dynamically adjusted so that it can handle the incoming input data to produce 50 output elements for the softmax output layer. During this process, if the kernel size of a pooling layer becomes (1,1), we remove that layer from the network.</p><p>During the compression process, we remove 80% and 85% of the channels in two runs, respectively, from the original network. These amounts are selected to obtain a model small enough for our target MCUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Channel pruning using magnitude-based ranking</head><p>In this method, the channels are ranked using their individual sum of absolute weights (L1 Norm) followed by layer-wise L2 normalization. Many approaches remove all the channels having a sum below a predefined threshold <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b31">33]</ref> . However, we remove the channels having the lowest sum iteratively (one at a time) and retrain the network to recover the loss until the network reaches the target size required to be fitted in the MCU. Let Z be the a vector containing the absolute sum of the channels layer-wise, l be the layer index and i be the channel index. Then, an iteration of this pruning process is defined by:</p><formula xml:id="formula_11">Z l = | z (i ) l | Z rank = sort( (Z l )) Z prune = argmin (Z rank ) (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Channel pruning using taylor criteria-based ranking</head><p>This is an iterative channel pruning technique recently proposed by Molchanov et al. <ref type="bibr" target="#b30">[32]</ref> . In this approach, the channels are ranked using Taylor criteria and the lowest ranked channel is pruned in a pruning iteration. More specifically, the ranking of filters is calculated by conducting a forward pass of the trained network for the whole dataset and observing the change to the cost function. The least affected channel after layer-wise L2 normalization is ranked highest to be pruned. The iterative pruning and retraining process continues until the model reaches the target size. An iteration of this pruning process is defined by: <ref type="bibr" target="#b10">(11)</ref> where, z l holds the activations of the layers of the network and l is the gradient.</p><formula xml:id="formula_12">Z l = z l + (z l * l ) Z rank = sort( (Z l )) Z prune = argmin (Z rank )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Proposed hybrid pruning approach</head><p>This pruning technique uses a new approach that combines both the unstructured and structured ranking methods to prune weights and channels from a network. In the first step, it prunes unimportant weights by zeroing them out from the network using unstructured global weight ranking methods, for example L0 Norm. In the second step, the model is further pruned through structured pruning. As we apply structured pruning on the weightpruned network, we are not producing sparse matrices, which are generally not supported on embedded devices. In this step, the focus is to prune the important weights through channel pruning. The channels are first ranked by using structured channel ranking methods, such as magnitude-based ranking and Taylor criteriabased ranking. Then the lowest ranked channel is removed from the network and retrained the network (we term it as 'fine-tuning') for few epochs to recover the loss. This pruning and fine-tuning is iterated until the model reaches the target size. Then the existing weights of the resulting model is re-initialized and trained as a fresh network (we term it as 'scratch-training') instead of retraining the resulting model for higher classification accuracy (we term this as 're-training'). We present this method in the following form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental results</head><p>We use the same experimental setup that has been discussed in Section 3.2 for this experimental study. ESC-50 (fold-4) is used to fine-tune the network during the compression process. The resulting network is cross-validated on all the four datasets and the results are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Compressing ACDNet</head><p>To compare our proposed method, at first we compress ACDNet using magnitude based pruning and taylor pruning. Then we experiment with two different combinations of the methods for testing our proposed iterative hybrid pruning approach. The first one consists of L0 Norm followed by channel pruning using magnitudebased ranking and the second one of L0 Norm followed by channel pruning using Taylor criteria-based ranking. Finally, we compare the results.</p><p>In this process, we first sparsify ACDNet by zeroing out 85%, 90%, 95% and 98% weights using L0 norm. We opted for the 95% sparsed network since it produces the same classification accuracy as the original ACDNet (see <ref type="table" target="#tab_2">Table 7</ref> ).</p><p>The sparsified model is further pruned through channel pruning. We prune 80% and 85% channels using all the discussed methods and present the results in <ref type="table" target="#tab_3">Tables 8 and 9</ref> . Models 1-2, 3-4, 5-6 and 7-8 in both the tables are the resulting models of magnitude based pruning, taylor pruning and the two combinations of our proposed hybrid pruning respectively. All models are iteratively pruned and fine-tuned. The column Fine-tuning Accuracy shows the accuracy obtained at the end of the iterative pruning and finetuning process.</p><p>From the tables, we observe that pruning and fine-tuning does not recover the loss in accuracy as hoped for. To achieve the best accuracy, we therefore conduct further full re-training of the networks with their existing weights, scratch-training by reinitializing their weights. We use the base network's training settings for re-training and scratch training. The accuracy of these different training processes are reported in Re-training Accuracy and Scratch-training Accuracy columns respectively. In addition to retraining and scratch-training, we also train the re-initialized fresh networks using knowledge distillation <ref type="bibr" target="#b32">[34]</ref> and the result is presented in <ref type="table" target="#tab_6">Table 10</ref> . <ref type="table" target="#tab_3">Tables 8 and 9</ref> show that the compressed networks with scratch-training produce better classification accuracy. To further justify this argument, we check the results obtained by training the models using knowledge distillation. The current weights of the compressed network are re-initialized and trained as a new  network with distilled loss. In this process, the best performing 80% compressed model (i.e., Model 8 from <ref type="table" target="#tab_3">Table 8</ref> ) is used as the student network while the trained base ACDNet model acts as the teacher network. We apply three different custom loss functions (see Eq. 12, 13 , and 14 using CrossEntropy and KLDivLoss loss, where the student output and the teacher output are denoted by SO and T O , respectively. We use ? = 0 . 1 , ? = 1 ? ? and Entropy (T) <ref type="table" target="#tab_6">Table 10</ref> shows the results of this training approach.</p><formula xml:id="formula_13">? [1 ? 5] .</formula><formula xml:id="formula_14">L 1 = CELoss (SO/T , T O/T ) * ? * T 2 + CELoss (SO, T arget ) * ? (12) L 2 = KLDi v Loss (SO/T , T O/T ) * ? * T 2 + CELoss (SO, T arget ) * ? (13) L 3 = KLDi v Loss (SO/T , T O/T ) * ? * T 2 + KLDi v Loss (SO, T arget ) * ?<label>(14)</label></formula><p>From <ref type="table" target="#tab_3">Tables 8, 9</ref> and 10 it is evident that our proposed hybrid pruning approach including scratch-training outperforms other approaches in terms of size and FLOPs reduction, and prediction accuracy. Note that for all the experiments, we have performed 5-fold CV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Selecting the compressed network</head><p>We choose the best compressed network depending on three factors: compression, FLOP reduction and accuracy. Considering the result on all three measurements for 80% pruning shown in <ref type="table" target="#tab_3">Table 8</ref> and <ref type="figure" target="#fig_1">Fig. 2</ref> , we select Model 8, obtained by our hybrid pruning technique. For this model, our hybrid pruning technique achieves 97.22% model size reduction, 97.28% FLOP reduction <ref type="table" target="#tab_3">( Table 8</ref> ) and 83.65% CV accuracy on ESC-50 dataset <ref type="table">( Table 14 )</ref>. This is still very close to the state of the art and significantly higher than human accuracy (81.30%). We term this network Micro-ACDNet as it only retrains 20% filters/channels.</p><p>With 85% pruning, our hybrid technique produces an even smaller version of Model 8 with 312 (15% retained) and only 240kB model size, which we term Macro-ACDNet <ref type="figure" target="#fig_1">( Fig. 2 b)</ref>. We select Micro-ACDNet over this macro version for three reasons. Firstly, Micro-ACDNet delivers significantly higher accuracy. Secondly, its model size of approximately 500kB already fits into typical MCU flash sizes and can be further decreased fourfold by 8-bit quantization to a size below the memory limits of even small microcontrollers. Thirdly, both models require approximately the same amount of FLOPs. <ref type="table" target="#tab_7">Table 11</ref> presents the architecture of Micro-ACDNet. Step 3: -Re-initialize the weights -Scratch-training: Train the model from scratch</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method: Hybrid Pruning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Compressing other networks</head><p>Now, we perform the same methodology that is used to derive Micro-ACDNet to find a micro version of AclNet. We do this to verify whether our proposed compression approach generalizes well for other networks. We name the resulting compressed AclNet as Micro-AclNet. The output filter configurations of the 12 convolution layers of Micro-AclNet are <ref type="bibr">7, 26, 10, 20, 26, 41, 22, 48, 55, 64, 58 and 50.</ref> Micro-AclNet also produces comparable classification accuracy on ESC-50 while comparing it with its base network accuracy. <ref type="table" target="#tab_8">Table 12</ref> clearly shows that the method works as expected in compressing other networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Compressed networks on different datasets</head><p>Now we test and cross validate Micro-ACDNet and Micro-AclNet on all the four datasets used for this research. Since we have already established that training from scratch works better and Micro-ACDNet achieves the highest prediction accuracy, we conduct scratch-training for all the networks, 5-fold (ESC-10&amp;50) and 10-fold (US8K) CV over five independent runs. <ref type="table" target="#tab_0">Table 13</ref> presents their classification accuracy on different datasets. The results in the table clearly indicates that the compressed models generalize well on all the four datasets. Their classification accuracy is very close to their base models classification accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5.">Comparing micro-ACDNet with other networks</head><p>Now that we have the resulting compressed model in our hand, we can compare it with Edge-L 3 , the only existing edge audio architecture. <ref type="table">Table 14</ref> shows our results in comparison to Edge-L 3 . <ref type="table" target="#tab_3">Tables 8 and 9</ref> provide the experimental results for the different pruning approaches tested on ACDNet. In <ref type="table" target="#tab_3">Table 8</ref> we see that our hybrid approach (Models 5-8) produces the best prediction accuracy and FLOP reduction for a model size reduction that fits the target specifications (Model 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.6.">Experimental findings</head><p>Pruning channels from initial convolution layers leads to more size and FLOPs reduction and causes little accuracy loss, thus making it a suitable approach for extremely resource-constrained MCUs.</p><p>From the experiments, we observe that pruning and fine-tuning (each iteration consists of pruning one channel and retraining the resulting network for two epochs) is not enough to achieve comparable prediction accuracy, which is contrary to earlier findings in the literature, e.g., in Molchanov et al. <ref type="bibr" target="#b30">[32]</ref> . The experimental results ( Table 8 and 9 ) show that the final compressed network  <ref type="bibr" target="#b80">[82]</ref> , that pruning or  compression should be considered as an efficient DL model architecture search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deployment in MCU (edge-AI device)</head><p>Our network has been fully deployed on an off-the-shelf MCU. The most limiting factor for MCU deployment is memory requirements. It is important to distinguish between two memory types: (1) Flash memory, which is not suitable for fast, frequent write access and is used to hold the (fixed) model parameters and <ref type="formula" target="#formula_4">(2)</ref> SRAM, which is used to store inputs and the results of intermediate calculations, i.e. activation values. As mentioned above, typical parameters for widely used highly power-efficient MCUs are less than 1MB of Flash and less than 512kB of SRAM.</p><p>Most off-the-shelf MCUs have either no audio capability or very low quality audio on-board. One could, in principle, add additional audio hardware to achieve better quality. However, this would increase power consumption. An exception is the Sony Spresense <ref type="bibr" target="#b81">[83]</ref> which provides high-quality audio input and processing on-board. We have selected this unit for these reasons.</p><p>While the Spresense is a highly power-efficient device, it has somewhat more generous specifications than the most common MCUs, providing 1.5MB SRAM. It is thus important to note that we are not actually making use of this additional memory. Our final deployed model requires 303kB SRAM for intermediate calculations, which is well below the target. This can be further reduced to below 200kB using a hand-optimised implementation as detailed below. While we have not yet taken this last step in a physical deployment, the implication is that Micro-ACDNet can fit on even smaller MCUs, such as those based on the extremely popular Nordic nRF52840 SoC, which offers 256kB SRAM and 1MB Flash. Flash memory is not a limiting constraint, since Micro-ACDNet requires only 500kB of model storage <ref type="table">( Table 14 )</ref>.</p><p>We picked Tensorflow Lite Micro over the other platformindependent DNN software frameworks for small-device deployment (e.g., PyTorch Mobile, Tensorflow Lite Micro) because it is the most frequently used and the only one that allows us to implement ACDNet directly. ACDNet's transpose layer is not currently supported by PyTorch Mobile.</p><p>To achieve the required model size, we need to quantize ACD-Net. The present paper is not concerned with quantization itself, so that we simply apply the quantization methods directly available in Tensorflow Lite Micro. Quantization, unfortunately, reduces the model accuracy noticeably and the micro version of Tensorflow Lite does not seem to provide the best results here. We used 8-bit post-training quantization which reduces the accuracy to 71.00%. To confirm that better results are possible, we also tested alternative frameworks. We found that Pytorch achieved the highest accuracy with 81.50% for an 8-bit quantized model of equivalent size (see <ref type="table" target="#tab_10">Table 15</ref> ). While our actual deployment uses Tensorflow Lite Micro for implementation-related reasons, we conclude that an alternative version of ACDNet that achieves 81.5% accuracy (i.e. above human performance) can be deployed on a standard MCU of the same size. The required working memory of 303kB SRAM could be reduced further. The current requirement results from essentially computing tensors layer by layer, keeping one intermediate layer in memory at a time. This can be optimised by re-grouping computations. The bottleneck for working memory is conv2 . However, this is immediately followed by a non-overlapping max pooling layer. Therefore, each instance of Maxpool2 can be immediately computed and the corresponding slice of output from conv2 can immediately be discarded after this. In this way, at any point of time, the most we need to keep in memory is the output of conv1 , a 110 units wide segment of conv2 and the complete output of Maxpool1 . This modification would allow us to reduce the working memory bottleneck to 141,636 bytes. While this may be detrimental to GPU speed-ups, it does not impact on an MCU implementation. However, this cannot be done in Tensorflow and would require a manual implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future work and conclusions</head><p>We have presented the first implementation of a 50 class audio classifier that achieves high accuracy, yet is small enough to fit on MCUs commonly used in energy-efficient internet-of-things devices. We constructed a full size network that sets a new standard for all the four datasets ESC-10, ESC-50, US8K and AE benchmarks for raw audio classification and compressed this for MCU deployment. While limitations of the programming environment have restricted the accuracy of our current test deployment on a physical MCU, we have conclusively shown that 81.50% accuracy is achievable on such a resource-impoverished device, close to the stateof-the-art and above human performance. This brings our goal of continuous, autonomous animal monitoring into immediate reach and should open new horizons for many other audio applications on the internet-of-things.</p><p>Our deployment of a DNN originally not designed for MCUs has shown that structured pruning achieves the best results for this task. It has also highlighted the fact that machine learning on the edge, still very much cutting edge, is not yet sufficiently supported by standard frameworks. We particularly expect that future frameworks will provide improved quantization support. Some next steps immediately arise. Firstly, it is likely that the performance can be improved by using quantization-aware training and pruning. Secondly, we would like to try the SpArSe approach for further optimisations now that we have developed Micro-ACDNet as a suitable starting point for its optimisations.</p><p>We believe it to be of particular importance that we have constructed and used a generic pipeline to derive an MCU implementation from a standard DNN. This opens up the same opportunities for a wide range of applications and we are confident that we will be able to transfer our approach to other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Visual representation of ACDNet model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Comparison of 80% pruned models from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 CV</head><label>3</label><figDesc>Accuracy and estimated 95% CI of classification accuracy of ACDNet on ESC-10, ESC-50, US8K and AE datasets. ?0 . 06 96.73 ?0 . 04 ESC-50 87.10 ?0 . 02 87.15 ?0 . 06 US8K 84.45 ?0 . 05 84.34 ?0 . 03 AE 92.57 ?0 . 05 92.56 ?0 . 04 the he_normal [69] initialization method. At the end of the training and validation, the best model is used as the final model. The loss function optimized is shown in Eq. 5 . Here, x is the input minibatch, f ?(x )is the approximation and y is the true distribution of labels for the input data. Furthermore, n is the mini-batch size, m is the number of classes, ? is the learning rate, and ? ? ? ? ? ?L ?? .</figDesc><table><row><cell>Datasets</cell><cell cols="2">Classification Accuracy (%)</cell></row><row><cell></cell><cell>CV</cell><cell>95-PCI</cell></row><row><cell>ESC-10</cell><cell>96.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 Table 5 Table 6</head><label>456</label><figDesc>ACDNet in the SOTA leaderboard for raw audio classification on ESC-10&amp;50, US8K and AE datasets. Accuracy values with asterisk ( * ) are reproduced by us. ACDNet in overall SOTA leaderboard of ESC-10&amp;50, US8K and AE datasets. Column 'Acc' presents model accuracy. Accuracy values with asterisk ( * ) are reproduced by us. Parameters, size, and computation requirements for current SOTA models on the ESC-50 dataset. Here, x denotes the size and FLOPs of ACDNet, for the Size and FLOPs column, respectively.</figDesc><table><row><cell></cell><cell cols="2">Networks</cell><cell></cell><cell cols="3">Accuracy (%) on Datasets</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ESC-10</cell><cell>ESC-50</cell><cell></cell><cell>US8K</cell><cell>AE</cell></row><row><cell></cell><cell cols="2">EnvNet [50]</cell><cell></cell><cell>88.10</cell><cell></cell><cell>74.10</cell><cell></cell><cell>71.10</cell><cell>-</cell></row><row><cell></cell><cell cols="2">EnvNet-v2 [43]</cell><cell></cell><cell>88.80</cell><cell></cell><cell>81.60</cell><cell></cell><cell>76.60</cell><cell>-</cell></row><row><cell></cell><cell cols="3">EnvNet-v2 + BC [43]</cell><cell>91.30</cell><cell></cell><cell>84.70</cell><cell></cell><cell>78.30</cell><cell>-</cell></row><row><cell></cell><cell cols="8">AclNet [44] 79 . 17  Datasets ? 95 . 75  *  85.65 ESC-10 ESC-50 US8K</cell><cell>AE</cell></row><row><cell></cell><cell cols="2">Networks ?</cell><cell></cell><cell>Acc (%)</cell><cell>Rank</cell><cell>Acc (%)</cell><cell>Rank</cell><cell>Acc (%)</cell><cell>Rank</cell><cell>Acc (%)</cell><cell>Rank</cell></row><row><cell></cell><cell cols="2">Piczak-CNN [47]</cell><cell></cell><cell>90.20</cell><cell>7</cell><cell>64.50</cell><cell>13</cell><cell>73.70</cell><cell>9</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GoogLENet [51]</cell><cell></cell><cell>86.00</cell><cell>10</cell><cell>73.00</cell><cell>12</cell><cell>93.00</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">EnvNet [50]</cell><cell></cell><cell>88.10</cell><cell>9</cell><cell>74.10</cell><cell>11</cell><cell>71.10</cell><cell>10</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">EnvNet-v2 [43]</cell><cell></cell><cell>88.80</cell><cell>8</cell><cell>81.60</cell><cell>10</cell><cell>76.60</cell><cell>8</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">EnvNet-v2 + BC [43]</cell><cell>91.30</cell><cell>6</cell><cell>84.70</cell><cell>7</cell><cell>78.30</cell><cell>7</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">VGG-CNN + Mixup [53]</cell><cell>91.70</cell><cell>5</cell><cell>83.90</cell><cell>9</cell><cell>83.70</cell><cell>5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Multi-stream + Attn [54] .</cell><cell>94.20</cell><cell>4</cell><cell>84.00</cell><cell>8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">AclNet (WM = 1.5) [44]</cell><cell>95 . 75  *</cell><cell>3</cell><cell>85.65</cell><cell>6</cell><cell>79 . 17  *</cell><cell>6</cell><cell>90 . 15  *</cell><cell>3</cell></row><row><cell></cell><cell cols="3">FBEs ConvRBM [40]</cell><cell>-</cell><cell>-</cell><cell>86.50</cell><cell>5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">CNN [42]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>88.65</cell><cell>3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">ESResNet [14]</cell><cell></cell><cell>97.00</cell><cell>1</cell><cell>91.50</cell><cell>2</cell><cell>85.42</cell><cell>3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">TSCNN-DS [15]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>97.20</cell><cell>1</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">WEANET [13]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>94.10</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">BNN-GAP8 [56]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.90</cell><cell>5</cell></row><row><cell></cell><cell cols="2">CNN-CNP [57]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.10</cell><cell>4</cell></row><row><cell></cell><cell cols="2">Method B [16]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>92.80</cell><cell>1</cell></row><row><cell></cell><cell cols="2">ACDNet (ours)</cell><cell></cell><cell>96 . 65</cell><cell>2</cell><cell>87 . 10</cell><cell>4</cell><cell>84 . 45</cell><cell>4</cell><cell>92 . 57</cell><cell>2</cell></row><row><cell>Networks</cell><cell>#Filters</cell><cell>Params(M)</cell><cell>Size(MB)</cell><cell></cell><cell>FLOPs(M)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EnvNet-v2 [43]</cell><cell>1056</cell><cell>101.25</cell><cell cols="2">386.25 = 21 . 39 x</cell><cell>1620 = 2 . 98 x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>AclNet [44]</cell><cell>3050</cell><cell>10.63</cell><cell cols="2">40.57 = 2 . 25 x</cell><cell>1070 = 1 . 97 x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ACDNet (ours)</cell><cell>2074</cell><cell>4.74</cell><cell>18.06 = x</cell><cell></cell><cell>544 = x</cell><cell></cell><cell></cell><cell></cell></row></table><note>* 90 . 15* ACDNet (ours) 96 . 65 ? 0 . 06 87 . 10 ? 0 . 02 84 . 45 ? 0 . 05 92 . 57 ? 0 . 05memory and 2.98 ? less FLOPs than EnvNet-v2 and 2.25 ? less memory and 1.97 ? less FLOPs than AclNet, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 7</head><label>7</label><figDesc>Sparsifying the Weights in ACDNet.</figDesc><table><row><cell>% Sparse</cell><cell>Original Accuracy</cell><cell>Final Accuracy</cell></row><row><cell>85%</cell><cell>91.00%</cell><cell>90.50%</cell></row><row><cell>90%</cell><cell>91.00%</cell><cell>89.25%</cell></row><row><cell>95%</cell><cell>91.00%</cell><cell>91.00%</cell></row><row><cell>98%</cell><cell>91.00%</cell><cell>88.75%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 8 .</head><label>8</label><figDesc>(b) Comparison of 85% pruned models fromTable 9. For visualization, the variables are linearly transformed to range[50 -max(variable)].</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8</head><label>8</label><figDesc>Models found after 80% channel pruning using magnitude-based ranking, Taylor criteria-based ranking and our hybrid pruning approach.</figDesc><table><row><cell>Model</cell><cell>Pruning Method</cell><cell>SFEB</cell><cell>TFEB</cell><cell>Parameters</cell><cell></cell><cell></cell><cell>FLOPs</cell><cell></cell><cell>Fine-tuning</cell><cell>CV Accuracy%</cell></row><row><cell>No.</cell><cell></cell><cell></cell><cell></cell><cell>Count(M)</cell><cell>Size(MB)</cell><cell>Reduced%</cell><cell>Count(M)</cell><cell>Reduced%</cell><cell>Accuracy%</cell><cell>Re-training</cell><cell>Scratch-training</cell></row><row><cell>1</cell><cell>Magnitude</cell><cell></cell><cell></cell><cell>0.119</cell><cell>0.45</cell><cell>97.49</cell><cell>36.94</cell><cell>93.22</cell><cell>15.25</cell><cell>81.80</cell><cell>82.30</cell></row><row><cell>2</cell><cell>Magnitude</cell><cell></cell><cell></cell><cell>0.119</cell><cell>0.45</cell><cell>97.57</cell><cell>36.94</cell><cell>93.22</cell><cell>15.25</cell><cell>82.45</cell><cell>82.30</cell></row><row><cell>3</cell><cell>Taylor</cell><cell></cell><cell></cell><cell>0.135</cell><cell>0.52</cell><cell>97.15</cell><cell>11.03</cell><cell>97.97</cell><cell>18.00</cell><cell>77.45</cell><cell>80.05</cell></row><row><cell>4</cell><cell>Taylor</cell><cell></cell><cell></cell><cell>0.140</cell><cell>0.53</cell><cell>97.04</cell><cell>11.40</cell><cell>97.91</cell><cell>6.50</cell><cell>78.40</cell><cell>81.00</cell></row><row><cell>5</cell><cell>Hybrid-Magnitude</cell><cell></cell><cell></cell><cell>0.120</cell><cell>0.46</cell><cell>97.47</cell><cell>36.96</cell><cell>93.21</cell><cell>12.00</cell><cell>81.85</cell><cell>82.30</cell></row><row><cell>6</cell><cell>Hybrid-Magnitude</cell><cell></cell><cell></cell><cell>0.118</cell><cell>0.45</cell><cell>97.50</cell><cell>36.81</cell><cell>93.24</cell><cell>12.50</cell><cell>82.10</cell><cell>82.40</cell></row><row><cell>7</cell><cell>Hybrid-Taylor</cell><cell></cell><cell></cell><cell>0.142</cell><cell>0.54</cell><cell>97.00</cell><cell>8.22</cell><cell>98.49</cell><cell>23.75</cell><cell>80.30</cell><cell>80.85</cell></row><row><cell>8</cell><cell>Hybrid-Taylor</cell><cell></cell><cell></cell><cell>0.131</cell><cell>0.50</cell><cell>97.22</cell><cell>14.82</cell><cell>97.28</cell><cell>48.50</cell><cell>81.30</cell><cell>83.65</cell></row><row><cell>Table 9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Models found after 85% channel pruning using magnitude-based ranking, Taylor criteria-based ranking and our hybrid pruning approach.</cell></row><row><cell>Model</cell><cell>Pruning Method</cell><cell>SFEB</cell><cell>TFEB</cell><cell>Parameters</cell><cell></cell><cell></cell><cell>FLOPs</cell><cell></cell><cell>Fine-Tuned</cell><cell>CV Accuracy%</cell></row><row><cell>No.</cell><cell></cell><cell></cell><cell></cell><cell>Count(M)</cell><cell>Size(MB)</cell><cell>Reduced%</cell><cell>Count(M)</cell><cell>Reduced%</cell><cell>Accuracy%</cell><cell>Re-Trained</cell><cell>Scratch-Training</cell></row><row><cell>1</cell><cell>Magnitude</cell><cell></cell><cell></cell><cell>0.065</cell><cell>0.25</cell><cell>98.63</cell><cell>22.61</cell><cell>95.85</cell><cell>3.00</cell><cell>79.65</cell><cell>79.65</cell></row><row><cell>2</cell><cell>Magnitude</cell><cell></cell><cell></cell><cell>0.065</cell><cell>0.25</cell><cell>98.63</cell><cell>22.61</cell><cell>95.85</cell><cell>3.00</cell><cell>79.20</cell><cell>80.15</cell></row><row><cell>3</cell><cell>Taylor</cell><cell></cell><cell></cell><cell>0.072</cell><cell>0.27</cell><cell>98.48</cell><cell>6.09</cell><cell>98.88</cell><cell>13.25</cell><cell>72.95</cell><cell>76.60</cell></row><row><cell>4</cell><cell>Taylor</cell><cell></cell><cell></cell><cell>0.072</cell><cell>0.28</cell><cell>98.48</cell><cell>6.31</cell><cell>98.84</cell><cell>9.50</cell><cell>73.80</cell><cell>77.50</cell></row><row><cell>5</cell><cell>Hybrid-Magnitude</cell><cell></cell><cell></cell><cell>0.066</cell><cell>0.25</cell><cell>98.60</cell><cell>24.82</cell><cell>95.44</cell><cell>10.75</cell><cell>78.85</cell><cell>80.85</cell></row><row><cell>6</cell><cell>Hybrid-Magnitude</cell><cell></cell><cell></cell><cell>0.065</cell><cell>0.25</cell><cell>98.63</cell><cell>25.39</cell><cell>95.34</cell><cell>6.75</cell><cell>79.90</cell><cell>80.60</cell></row><row><cell>7</cell><cell>Hybrid-Taylor</cell><cell></cell><cell></cell><cell>0.079</cell><cell>0.30</cell><cell>98.34</cell><cell>5.01</cell><cell>99.08</cell><cell>15.50</cell><cell>74.10</cell><cell>77.20</cell></row><row><cell>8</cell><cell>Hybrid-Taylor</cell><cell></cell><cell></cell><cell>0.066</cell><cell>0.25</cell><cell>98.61</cell><cell>10.54</cell><cell>98.68</cell><cell>37.00</cell><cell>76.90</cell><cell>81.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10</head><label>10</label><figDesc>Performance of Micro-ACDNet when trained using Knowledge Distillation.</figDesc><table><row><cell>Loss</cell><cell cols="2">5-Fold CV Accuracy%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>T = 1</cell><cell>T = 2</cell><cell>T = 3</cell><cell>T = 4</cell><cell>T = 5</cell></row><row><cell>L1</cell><cell>81.45</cell><cell>80.15</cell><cell>78.75</cell><cell>78.45</cell><cell>78.35</cell></row><row><cell>L2</cell><cell>81.70</cell><cell>81.90</cell><cell>81.40</cell><cell>81.65</cell><cell>81.95</cell></row><row><cell>L3</cell><cell>81.55</cell><cell>81.95</cell><cell>81.75</cell><cell>81.25</cell><cell>81.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11</head><label>11</label><figDesc>Micro-ACDNet architecture for input length 30,225 (approximately 1.51s audio @ 20kHz). Crowley et al. [81] and Liu et al.</figDesc><table><row><cell>Layers</cell><cell>Kernel Size</cell><cell>Stride</cell><cell>Filters</cell><cell>Output Shape</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell>(1, 1, 30225)</cell></row><row><cell>conv1</cell><cell>(1, 9)</cell><cell>(1, 2)</cell><cell>7</cell><cell>(7, 1, 15109)</cell></row><row><cell>conv2</cell><cell>(1, 5)</cell><cell>(1, 2)</cell><cell>20</cell><cell>(20, 1, 7553)</cell></row><row><cell>Maxpool1</cell><cell>(1, 50)</cell><cell>(1, 50)</cell><cell></cell><cell>(20, 1, 151)</cell></row><row><cell>swapaxes</cell><cell></cell><cell></cell><cell></cell><cell>(1, 20, 151)</cell></row><row><cell>conv3</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>10</cell><cell>(10, 32, 151)</cell></row><row><cell>Maxpool2</cell><cell>(2, 2)</cell><cell>(2, 2)</cell><cell></cell><cell>(10, 16, 75)</cell></row><row><cell>conv4</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>14</cell><cell>(14, 16, 75)</cell></row><row><cell>conv5</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>22</cell><cell>(22, 16, 75)</cell></row><row><cell>Maxpool3</cell><cell>(2, 2)</cell><cell>(2, 2)</cell><cell></cell><cell>(22, 8, 37)</cell></row><row><cell>conv6</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>31</cell><cell>(31, 8, 37)</cell></row><row><cell>conv7</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>35</cell><cell>(35, 8, 37)</cell></row><row><cell>Maxpool4</cell><cell>(2, 2)</cell><cell>(2, 2)</cell><cell></cell><cell>(35, 4, 18)</cell></row><row><cell>conv8</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>41</cell><cell>(41, 4, 18)</cell></row><row><cell>conv9</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>51</cell><cell>(51, 4, 18)</cell></row><row><cell>Maxpool5</cell><cell>(2, 2)</cell><cell>(2, 2)</cell><cell></cell><cell>(51, 2, 9)</cell></row><row><cell>conv10</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>67</cell><cell>(67, 2, 9)</cell></row><row><cell>conv11</cell><cell>(3, 3)</cell><cell>(1, 1)</cell><cell>69</cell><cell>(69, 2, 9)</cell></row><row><cell>Maxpool6</cell><cell>(2, 2)</cell><cell>(2, 2)</cell><cell></cell><cell>(69, 1, 4)</cell></row><row><cell>Dropout (0.2)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conv12</cell><cell>(1, 1)</cell><cell>(1, 1)</cell><cell>48</cell><cell>(48, 1, 4)</cell></row><row><cell>Avgpool1</cell><cell>(1, 4)</cell><cell>(1, 4)</cell><cell></cell><cell>(48, 1, 1)</cell></row><row><cell>Flatten</cell><cell></cell><cell></cell><cell></cell><cell>(48)</cell></row><row><cell>Dense1</cell><cell></cell><cell></cell><cell></cell><cell>(50)</cell></row><row><cell>Softmax</cell><cell></cell><cell></cell><cell></cell><cell>(50)</cell></row><row><cell cols="5">requires full re-training or scratch-training to produce comparable</cell></row><row><cell>prediction accuracy.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Our experiments furthermore suggest, supported by earlier</cell></row><row><cell>work by</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12</head><label>12</label><figDesc>ACDNet vs AclNet performance after 80% compression.</figDesc><table><row><cell>Network</cell><cell>Pruning Method</cell><cell>SFEB</cell><cell>TFEB</cell><cell>Size</cell><cell></cell><cell></cell><cell>FLOPs</cell><cell></cell><cell>Scratch-Training</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base (MB)</cell><cell>Compressed (MB)</cell><cell>Reduced%</cell><cell>Base (M)</cell><cell>Compressed (M)</cell><cell>Reduced%</cell><cell>Accuracy (%)</cell></row><row><cell>ACDNet</cell><cell>Hybrid-Taylor</cell><cell></cell><cell></cell><cell>18.06</cell><cell>0.50</cell><cell>97.22</cell><cell>544.42</cell><cell>14.82</cell><cell>97.28</cell><cell>83.65</cell></row><row><cell>AclNet</cell><cell>Hybrid-Taylor</cell><cell></cell><cell></cell><cell>40.54</cell><cell>0.50</cell><cell>98.79</cell><cell>1806.57</cell><cell>21.50</cell><cell>98.81</cell><cell>80.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 13</head><label>13</label><figDesc>ACDNet, AclNet, Micro-ACDNet and Micro-AclNet performance on different datasets.</figDesc><table><row><cell>Accuracy (%) ?</cell><cell cols="2">Networks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Datasets ?</cell><cell cols="2">ACDNet</cell><cell>AclNet</cell><cell cols="2">Micro-ACDNet</cell><cell>Micro-AclNet</cell></row><row><cell>ESC-10</cell><cell>96.75</cell><cell></cell><cell>95.75</cell><cell>96.25</cell><cell></cell><cell>94.00</cell></row><row><cell>ESC-50</cell><cell>87.10</cell><cell></cell><cell>85.65</cell><cell>83.65</cell><cell></cell><cell>80.05</cell></row><row><cell>US8K</cell><cell>84.45</cell><cell></cell><cell>79.17</cell><cell>78.28</cell><cell></cell><cell>75.80</cell></row><row><cell>AE</cell><cell>92.57</cell><cell></cell><cell>90.15</cell><cell>89.69</cell><cell></cell><cell>87.51</cell></row><row><cell>Table 14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Parameters, size and computation requirements for ACDNet and Micro-ACDNet for</cell></row><row><cell cols="3">approximately 1.51s audio @ 20kHz.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Networks</cell><cell cols="6">#Filters Params (M) Size (MB) FLOPs (B) Accuracy (%)</cell></row><row><cell>ACDNet</cell><cell>2074</cell><cell>4.74</cell><cell cols="2">18.06</cell><cell>0.54</cell><cell>87 . 10 ? 0 . 02</cell></row><row><cell>Edge-L 3 [63]</cell><cell>1920</cell><cell>0.213</cell><cell cols="2">0.814</cell><cell>-</cell><cell>73.75</cell></row><row><cell cols="2">Micro-ACDNet 415</cell><cell>0.131</cell><cell>0.50</cell><cell></cell><cell>0.0148</cell><cell>83.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 15</head><label>15</label><figDesc>Prediction accuracy on ESC-50 after quantization.</figDesc><table><row><cell>Network</cell><cell>Library</cell><cell>Quantized</cell><cell>Quantized</cell></row><row><cell></cell><cell></cell><cell>Size</cell><cell>Accuracy (%)</cell></row><row><cell>Micro-ACDNet</cell><cell>Pytorch</cell><cell>157kB</cell><cell>81.50%</cell></row><row><cell>Micro-ACDNet</cell><cell>TF Lite Micro</cell><cell>153kB</cell><cell>71.00%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the Australian Research Council under grant DE19010 0 045.</p><p>We thank Lin Schwartzkopf and Slade Allen-Ankins of James Cook University in Townsville for the provision of the real-world frog dataset and Akhter Hossain of Sanofi US for advice on statistical significance testing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying sound-based analysis at porsche production: towards predictive maintenance of production machines using deep learning and internet-of-things technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Behrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Derakhshanmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muderack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digitalization cases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="79" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural network constructed by deep learning technique and its application to intelligent fault diagnosis of machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="page" from="619" to="628" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Development of internal sound sensor using stethoscope and its applications for machine monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Manuf</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1072" to="1078" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An overview of applications and advancements in automatic sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Moir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for audio signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Purwins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denet: a deep architecture for audio surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saggese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic acoustic identification of individuals in multiple species: improving identification across recording conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petruskov?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>??lek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Linhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Soc. Interface</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">153</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Acoustic recordings provide detailed information regarding the behavior of cryptic wildlife to support conservation translocations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AI Empowers conservation biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">657</biblScope>
			<biblScope unit="issue">7746</biblScope>
			<biblScope unit="page" from="133" to="134" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic wildlife occupancy models using automated acoustic monitoring data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Balantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Donovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bioacoustics data analysisa taxonomy, survey and open challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kvsn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charleston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="57684" to="57708" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From eDNA to citizen science: emerging tools for the early detection of invasive species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Coon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Gambrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Jonasen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Laracuente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I P</forename><surname>Stowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ruzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Thairu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Suarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Ecol. Environ</title>
		<imprint>
			<biblScope unit="page">2162</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sequential self teaching approach for improving generalization in sound event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ithapu</surname></persName>
		</author>
		<idno>PMLR (2020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="5447" to="5457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Esresnet: environmental sound classification based on visual domain models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th international conference on pattern recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="933" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An ensemble stacked convolutional neural network model for environmental event sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1152</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for acoustic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association</title>
		<meeting>the Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2982" to="2986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acousticloud: a cloud-based system for managing large-scale bioacoustics processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montgomery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environm. Modell. Softw</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">104778</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LCU-Net: A novel low-cost U-Net for environmental microorganism image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grzegorzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirahama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">107885</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ARA: Accurate, reliable and active histopathological image classification framework with bayesian deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>?czkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mo ?ejko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zambonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Szczurek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CNN-RNN: A large-scale hierarchical image classification framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="10251" to="10271" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EF-Net: A novel enhancement and fusion network for RGB-D saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">107740</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end video text detection with online tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page">107791</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>Mobilenetv2: inverted residuals and linear bottlenecks</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet: an extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mnasnet: platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FBNEt: hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="10734" to="10742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>8697-8710 . [28] nrf52840</idno>
		<ptr target="https://www.nordicsemi.com/products/low-power-short-range-wireless/nRF52840" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="https://www.st.com/en/microcontrollers-microprocessors.html" />
		<title level="m">Microcontrollers &amp; microprocessors</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep compression: compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1510.00149" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Resnet can be pruned 60 ?: introducing network purification and unused path removal (P-RM) after weight pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM International Symposium on Nanoscale Architectures</title>
		<meeting>the IEEE/ACM International Symposium on Nanoscale Architectures</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJGCiw5gl" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured compression of deep neural networks with debiased elastic group LASSO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oyedotun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aouada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ottersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2277" to="2286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1050</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model compression via distillation and quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1XolQbRW" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse: sparse architecture search for cnns on resource-constrained microcontrollers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="977" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10319</idno>
		<title level="m">MCUNet: Tiny deep learning on IoT devices</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
		<idno type="DOI">10.1145/2733373.2806390</idno>
		<ptr target="http://dl.acm.org/citation.cfm?doid=2733373.2806390" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual Conference of the International Speech Communication Association</title>
		<meeting>the 18th Annual Conference of the International Speech Communication Association<address><addrLine>Interspeech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3107" to="3111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Urban sound tagging using multi-channel audio feature with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://dcase.community/documents/challenge2020/technical_reports/DCASE2020_JHKim_21_t5.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. Detect. Classif. Acoustic Scenes Event</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An ensemble of convolutional neural networks for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maguolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">5796</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1Gi6LeRZ" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">AclNet: efficient end-to-end audio classification CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J A</forename><surname>Leanos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06669</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structured pruning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM J. Emerg. Technol. Comput. Syst. (JETC)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Thinet: pruning CNN filters for a thinner net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2525" to="2538" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Environmental sound classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IEEE International Workshop on Machine Learning for Signal Processing</title>
		<meeting>the 25th IEEE International Workshop on Machine Learning for Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Novel TEO-based gammatone features for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th European Signal Processing Conference</title>
		<meeting>the 25th European Signal Processing Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1809" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Novel phase encoded mel filterbank energies for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition and Machine Intelligence</title>
		<meeting>the International Conference on Pattern Recognition and Machine Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning environmental sounds with end-to-end convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="2721" to="2725" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Classifying environmental sounds using image recognition networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boddapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rasmusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lundberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="2048" to="2056" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Knowledge transfer from weakly labeled audio using convolutional neural network for sound events and scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khadkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>F?gen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="326" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network with mixup for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Chinese Conference on Pattern Recognition and Computer Vision</title>
		<meeting>the Chinese Conference on Pattern Recognition and Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="356" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-stream network with temporal attention for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chebiyyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3604" to="3608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning attentive representations for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="130327" to="130339" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sound event detection with binary neural networks on tightly power-constrained IoT devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Andri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Farella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE International Symposium on Low Power Electronics</title>
		<meeting>the ACM/IEEE International Symposium on Low Power Electronics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Efficient convolutional neural network for audio event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.09888</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">What&apos;s wrong with spectrograms and CNNs for audio processing?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothmann</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd" />
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Audio spectrogram representations for processing with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wyse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Deep Learning and</title>
		<meeting>the First International Conference on Deep Learning and<address><addrLine>Music</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Resource-efficient machine learning in 2 KB RAM for the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1935" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">ProtoNN: compressed and accurate kNN for resource-scarce devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Suggala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Simhadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Edgel ? 3: compressing l ? 3-net for mote scale urban noise monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Parallel and Distributed Processing Symposium Workshops</title>
		<meeting>the IEEE International Parallel and Distributed Processing Symposium Workshops</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="877" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision, ICCV 2017</title>
		<meeting>the IEEE International Conference on Computer Vision, ICCV 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations, ICLR 2015, OpenReview.net</title>
		<meeting>the 3rd International Conference on Learning Representations, ICLR 2015, OpenReview.net</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Overview of lifeCLEF 2020: a system-oriented evaluation of automated species identification and species distribution prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Go?au</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deneu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Picek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>De Castaneda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<meeting>the International Conference of the Cross-Language Evaluation Forum for European Languages</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="342" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Automated species identification of frog choruses in environmental recordings using acoustic indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brodie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Allen-Ankins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Towsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schwarzkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Indic</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">106852</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Acoustic monitoring reveals year-round calling by invasive toads in tropical australia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brodie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yasumiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Towsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<idno type="DOI">10.1080/09524622.2019.1705183</idno>
		<ptr target="https://doi.org/10.1080/09524622.2019.1705183" />
	</analytic>
	<monogr>
		<title level="j">Bioacoustics</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision, ICCV 2015</title>
		<meeting>the IEEE international conference on computer vision, ICCV 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bootstrap confidence intervals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Diciccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="189" to="228" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Empirical methods for artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>MIT press</publisher>
			<biblScope unit="page">139</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bootstrap confidence intervals: when, which, what? a practical guide for medical statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bithell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1141" to="1164" />
		</imprint>
	</monogr>
	<note>20 0 0</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Training pruned neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Neural network compression framework for fast model inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kozlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lazarevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shamporov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lyalyushkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gorbachev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08679</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Designing energy-efficient convolutional neural networks using energy-aware pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="5687" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">On the construction of huffman trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Leeuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Colloquium on Automata, Languages and Programming</title>
		<meeting>the 3rd International Colloquium on Automata, Languages and Programming</meeting>
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">1976</biblScope>
			<biblScope unit="page" from="382" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Lightweightnet: toward fast and lightweight convolutional neural networks via architecture distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="272" to="284" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Haq: hardware-aware automated quantization with mixed precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="8612" to="8620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Autopruner: an end-to-end trainable filter pruning method for efficient deep model inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107461</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;boyle</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=r1lbgwFj5m" />
	</analytic>
	<monogr>
		<title level="m">Pruning neural networks: is it time to nip it in the bud? in: Proceedings of the NIPS workshop on Compact Deep Neural Networks with industrial application</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJlnB3C5Ym" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Product, specifications -spresense -sony developer world</title>
		<ptr target="https://developer.sony.com/develop/spresense/specifications" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
