<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Long-Term Dependencies in Irregularly-Sampled Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
							<email>mlechner@ist.ac.at</email>
							<affiliation key="aff0">
								<orgName type="institution">IST</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
							<email>rhasani@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">TU</orgName>
								<address>
									<settlement>Wien &amp; MIT</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Long-Term Dependencies in Irregularly-Sampled Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent neural networks (RNNs) with continuous-time hidden states are a natural fit for modeling irregularly-sampled time series. These models, however, face difficulties when the input data possess long-term dependencies. We prove that similar to standard RNNs, the underlying reason for this issue is the vanishing or exploding of the gradient during training. This phenomenon is expressed by the ordinary differential equation (ODE) representation of the hidden state, regardless of the ODE solver's choice. We provide a solution by designing a new algorithm based on the long short-term memory (LSTM) that separates its memory from its time-continuous state. This way, we encode a continuous-time dynamical flow within the RNN, allowing it to respond to inputs arriving at arbitrary time-lags while ensuring a constant error propagation through the memory path. We call these RNN models ODE-LSTMs. We experimentally show that ODE-LSTMs outperform advanced RNN-based counterparts on non-uniformly sampled data with long-term dependencies. All code and data is available at https://github. com/mlech26l/ode-lstms.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Irregularly-sampled time series, routine data streams in medical and business settings, can be modeled effectively by a time-continuous version of recurrent neural networks (RNNs). These class of RNNs whose hidden states are identified by ordinary differential equations, termed an ODE-RNN <ref type="bibr" target="#b47">[48]</ref>, provably suffer from the vanishing and exploding gradient problem (see <ref type="figure" target="#fig_1">Figure 1</ref>, the first two models), when trained by reverse-mode automatic differentiation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>An elegant solution to the vanishing gradient phenomenon <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref>, which results in difficulties in learning long-term dependencies in RNNs, is the long short term memory networks (LSTM) <ref type="bibr" target="#b26">[27]</ref>. LSTMs enforce a constant error propagation through the hidden states, learn to forget, and disentangle the hidden states (memory) from their output states. Despite becoming the standard choice in modeling regularly-sampled temporal dynamics, LSTMs similar to other discretized RNN models, face difficulties when the time-gap between the observations are irregular.</p><p>In this paper, we propose a compromise to design a novel recurrent neural network algorithm that simultaneously enjoys the approximation capability of ODE-RNNs in modeling irregularly-sampled time series and capability of learning long-term dependencies of the LSTMs' computational graph.</p><p>To perform this, we let an LSTM cell compute its implicit memory mechanism by their typical (input, forget, and output) gates while receiving their feedback inputs from a time-continuous output state representation. This way, we incorporate a continuous-time dynamical flow within the LSTM network, enabling cells to respond to data arriving at arbitrary time-lags, while avoiding the vanishing gradient problem, a model we call ODE-LSTMs (See <ref type="figure" target="#fig_1">Figure 1</ref>, the last model).  ODE-LSTMs are a solution to keep a constant gradient flow to avoid these phenomena in modeling irregularly sampled data.</p><formula xml:id="formula_0">! ?( " ) ! " #$% # ( ) ( + ) ? ! ? " ? #$% ? # 1 0 ! ?( " ) ( + ) ? ! ? " ? #$% ? # 1 0 ODE RNN RNN Decay ! ( " ) ( + ) ? ! ? " ? #$% ? # 1 0 ODE LSTM " #$% # ! ( + )</formula><p>We compare ODE-LSTMs to standard and advanced continuous-time RNN variants, on a set of synthetic and real-world sparse time-series tasks, and discover consistently better performance.</p><p>To put this in context, we first theoretically prove that the class of ODE-RNNs suffers from the exploding and vanishing gradient problem, making them unable to learn long-term dependencies efficiently. We show that learning ODE-RNNs by the adjoint method <ref type="bibr" target="#b9">[10]</ref> does not help with this problem. As a solution, we propose ODE-LSTMs, a continuous-time RNN model capable of learning long-term dependencies of irregularly-sampled time-series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>ODE-RNNs Instead of explicitly defining a state update function, ODE-RNNs identify an ordinary differential equations in the following form <ref type="bibr" target="#b17">[18]</ref>:</p><formula xml:id="formula_1">?h ?t = f ? (x t+T , h t , T ) ? ? h,<label>(1)</label></formula><p>where x t is the input sequence, h t is an RNN's hidden state, and ? is a dampening factor. The time-lag T specifies at what times the inputs x t have been sampled.</p><p>ODE-RNNs were recently rediscovered <ref type="bibr" target="#b47">[48]</ref> and have shown promise in approximating irregularly-sampled data, thanks to the implicit definition of time in their resulting dynamical systems. ODE-RNNs can be trained by backpropagation through time (BPTT) <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> through ODE solvers, or by treating the solver as a black-box and apply the adjoint method <ref type="bibr" target="#b44">[45]</ref> to gain memory efficiency <ref type="bibr" target="#b9">[10]</ref>. In Section 3, we show this family of recurrent networks faces difficulty to learn long-term dependencies.</p><p>Long Short-term Memory LSTMs <ref type="bibr" target="#b26">[27]</ref> express their discretized hidden states as a pair (c t , h t ) and its update function, f ? (x t+1 , (c t , h t ), 1) ? (c t+1 , h t+1 ) is defined as follows:</p><formula xml:id="formula_2">z t+1 = tanh(W z x t+1 + R z h t + b z ) input update (2) i t+1 = ?(W i x t+1 + R i h t + b i ) input gate (3) f t+1 = ?(W f x t+1 + R f h t + b f + 1) forget gate (4) o t+1 = ?(W o x t+1 + R o h t + b o ) output gate (5) c t+1 = z t+1 i t+1 + c t f t+1 cell update (6) h t+1 = tanh(c t+1 ) o t+1 output state,<label>(7)</label></formula><p>where ? is the sigmoid function x ? 1/(1 + exp(?x)), the matrices W x , R x , and vectors b x for x ? {z, i, f, o} are the weights of the RNN. The formulation shown in Equations (2-7) extends the original LSTM graph by a biased forget gate <ref type="bibr" target="#b18">[19]</ref> (as implemented in PyTorch <ref type="bibr" target="#b42">[43]</ref> and TensorFlow <ref type="bibr" target="#b0">[1]</ref>). LSTMs demonstrate great performance on learning equidistant streams of data <ref type="bibr" target="#b21">[22]</ref>, however similar to other discrete-state RNNs, they are puzzled with the events arriving in-between observations. In Section 4, we introduce a continuous-time long short-term memory algorithm to tackle this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ODE-RNNs suffer from vanishing or exploding gradient</head><p>In this section, we show that ODE-RNNs trained via backpropagation through time (BPTT) are susceptible to vanishing and exploding gradients. We also illustrate that the adjoint method is not immune to these gradient issues. We first formally define the gradient problems of the RNNs, and progressively construct Theorem 1.</p><p>Gradient propagation in recurrent networks Hochreiter <ref type="bibr" target="#b25">[26]</ref> discovered that the error-flow in the BPTT algorithm realizes a power series that determines the effectiveness of the learning process <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref>. In particular, the state-previous state Jacobian of an RNN:</p><formula xml:id="formula_3">?h t+T (x t+T , h t , T ) ?h t ,<label>(8)</label></formula><p>governs whether the propagated error exponentially grows (explodes), exponentially vanishes, or stays constant. Formally:</p><p>Definition 1 (Vanishing or exploding gradient) Let h t+T = f (x t+T , h t , T ) be a recurrent neural network, then we say unit i of the network f suffers from a vanishing gradient if for some small ? &gt; 0 it hold that</p><formula xml:id="formula_4">N j=1 ?h i t+T ?h j t &lt; 1 ? ?,<label>(9)</label></formula><p>where N is the dimension of the hidden state h t and super-script v i denotes the i-th entry of the vector v. We say unit i of the network f suffers from an exploding gradient if it holds that</p><formula xml:id="formula_5">N j=1 ?h i t+T ?h j t &gt; 1.<label>(10)</label></formula><p>We say the whole network f suffers from a vanishing or respectively exploding gradient problem if the above condition hold for some of its units.</p><p>The factor ? in Eq. 9 is essential as Gers et al. <ref type="bibr" target="#b18">[19]</ref> observed that a learnable vanishing factor in the form of a forget-gate significantly benefits the learning capabilities of RNNs, i.e., the network can learn to forget. Note that a RNN can simultaneously suffer from a vanishing and an exploding gradient by the definition above.</p><p>Now, consider an ODE-RNN given by Eq. 1 is implemented either by an Explicit Euler discretization or by a Runge-Kutta method <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b12">13]</ref>. We can formulate their state-previous state Jacobian in the following two lemmas:</p><formula xml:id="formula_6">Lemma 1 Let? = f ? (x, h, T ) ? h? be an ODE-RNN.</formula><p>Then state-previous state Jacobian of the explicit Euler is given by the following equation:</p><formula xml:id="formula_7">?h t+T ?h t = I + T ?f ?h h=ht ? ? T I.<label>(11)</label></formula><p>Lemma 2 Let? = f ? (x, h, T ) ? h? be an ODE-RNN. Then state-previous state Jacobian of the Runge-Kutta method is given by</p><formula xml:id="formula_8">?h t+T ?h t = I + T M j=1 b i ?f ?h h=Ki ? ? T I.,<label>(12)</label></formula><p>, where M j=1 b i = 1 and some K i .</p><p>The proofs for Lemma 1 and Lemma 2 is provided in the supplements. Consequently, we have: </p><formula xml:id="formula_9">Theorem</formula><formula xml:id="formula_10">h 0 = 0 ODE state c 0 = 0 Memory cell for i = 1 . . . N do (c i , h i ) = LSTM(? l , (c i?1 , h i?1 ), x i ) h i = ODESolve(f ? , h i?1 , h i , t t ? t i?1 ) Post-process LSTM output by ODE-RNN o i = h i W output + b output end for return {o i } i=1...N</formula><p>The proof is given in full in the supplementary materials. A brief outline of the proof is as follow: First, we look a the special cases of ?f ?h ? ? = 0. While such f would enforce a constant error propagation by making the Jacobians equal to the identity, it also removes all dynamics from the ODE state. In other words, it would operate the ODE as a memory element. Intuitively, any interesting function f ? pushes the Jacobians away from the identity matrix, creating a vanishing or exploding gradient depending on f ? . Theorem 2 (ODE-RNNs suffer from a vanish or exploding gradient regardless of the choice of ODE-solver) Let? = f ? (x, h, T ) ? h? , with f ? being uniformly Lipschitz continuous. Moreover, let h(t) be the solution of the initial value problem with initial state h 0 . Then, the gradients ?h(T ) ?h0 , i.e, the Jacobian of the ODE state at time T with respect to the initial state h 0 , can vanish and explode, except for parameter configurations which give rise to the non-trainable constant dynamics f ? (h, x) = 0, and cases where f ? (h, x) constant, for a particular input sequence x and parameters ?.</p><p>The proof is given in full in the supplementary materials. A brief outline of the proof is as follow:</p><p>We start by approximating the initial-value problem by an explicit Euler method with a uniform step-size. We then let the step-size approach zero which due to the Picard-Lindel?f theorem, makes the series converge to the true solution of the ODE. Based on bounds on ?f ?h , we can obtain bounds of the gradients in the limit, which can vanish or explode depending on f ? .</p><p>Does the adjoint method solve the vanishing gradient problem? Adjoint sensitivity method <ref type="bibr" target="#b44">[45]</ref> allows for performing memory-efficient reverse-mode automatic differentiation for training neural networks with their hidden states defined by ODEs <ref type="bibr" target="#b9">[10]</ref>. The method, however, possesses lossy reverse-mode integration steps, as it forgets the computed steps during the forward-pass. Consequently, at each reverse-mode step, the backward gradient pass diverges from the true forward pass <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b19">20]</ref>. This is because the auxiliary differential equation in the adjoint sensitivity method, a(t), still contains state-dependent components at each reverse-step, which depends on the historical values of the hidden states' gradient. Therefore, both vanilla BPTT and the adjoint method face difficulties for learning long-term dependencies. In the next section, we propose a solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ODE-LSTM architecture</head><p>The RNN state of a standard LSTM network, is represented by a pair (c t , h t ), where c t is the memory cell and h t the output state, i.e., see Equations <ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref>. The memory c t ensures a constant error propagation and the output state h t enables the LSTM to learn non-linear dynamics. We modify the way the output state h t is computed while preserving its gating mechanisms and memory cell.</p><p>To perform this, we declare the output dynamics of a cell by a continuous-time representation, which realizes an ODE-RNN. This way, the output state depend on the elapsed time when processing irregularly sampled time-series. Nonetheless, as the LSTM gates receive feedback connections from the cells' outputs, the gating dynamics become dependent on the time-lag as well. The resulting architecture termed an ODE-LSTM is shown in algorithm 1.</p><p>The fundamental distinction of ODE-LSTM to other variants is that they leave the RNN's memory mechanism untouched and assert the continuous dynamics into output function that processes the state. This way, ODE-LSTMs can learn long-term dependencies when trained by gradient descent. <ref type="table">Table 1</ref>: Change to the hidden states of an RNN between two observations t and t + T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>State between observation</p><formula xml:id="formula_11">Standard RNN h t GRU-D h t e ?T ? ODE-RNN ODE-Solve(f ? , h t , T ) ODE-LSTM c t , ODE-Solve(f ? , h t , T )</formula><p>On the contrary, recurrent network variants such as CT-RNN <ref type="bibr" target="#b17">[18]</ref>, continuous-time gated recurrent units (CT-GRU) <ref type="bibr" target="#b39">[40]</ref>, and GRU-D <ref type="bibr" target="#b7">[8]</ref> incorporate the elapsed-time by a decay apparatus on the state, while preserving the rest of the RNN architecture. This decaying memory originates the vanishing factor during backward error-propagation, which results in difficulties in learning long-term dependencies.</p><p>Our ODE-LSTMs are immune to this shortcoming. More precisely, <ref type="table">Table 1</ref> lists how the transition of the hidden states between two observations of the ODE-LSTM differs from other architectures. Similar to the LSTM, we can ensure a near-constant error propagation at the beginning of the training process with a proper weight initialization.</p><formula xml:id="formula_12">Theorem 3 Let f with (c t+T , h t+T ) = f (x t+T , (c t , h t ),</formula><p>T ) be an ODE-LSTM described by Algorithm 1. Moreover, we assume the weights R z , R i , R f , W f and b f are initialized close to 0. Then, the units c t of the state pair (c t , h t ) do not suffer from a vanishing or exploding gradient at the beginning of the training process.</p><p>The proof is given in full in the supplements. A brief outline: We assume that R z , R i , R f , W f and b f are initialized close to 0 and we are at the beginning of the training, thus these values have not</p><p>changed much yet. Consequently, we can neglect them and get N j=1</p><formula xml:id="formula_13">?c i t+T (x t+T ,(ct,ht),T ) ?c j t = ?(1) ? 0.7310586</formula><p>, which is less than 1 (no exploding) but much greater than 0 (no vanishing). Note that exact value of the Jacobian at the beginning of the training can be controlled by the forget gate bias. If the underlying data express very long-term dependencies, we can increase the forget gate bias in Eq. (4) and bring the error flow factor closer to 1.</p><p>The ODE-LSTM can be viewed as a memory cell with gates controlled by a time-continuous process realized by ordinary differential equations. Next, we evaluate the performance of ODE-LSTMs in multiple time-series prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental evaluation</head><p>We constructed quantitative settings with synthetic and real-world benchmarks. We assessed the generalization performance of time-continuous RNN architectures on datasets that are deliberately created to express long-term dependencies and are of irregularly-sampled nature. All code and data is available at https://github.com/mlech26l/ode-lstms.</p><p>Baselines. We compare ODE-LSTM to a large variety of continuous-time RNNs introduced to model irregularly-sampled data. This set includes RNNs with continuous-state dynamics such as ODE-RNN <ref type="bibr" target="#b47">[48]</ref> and CT-RNNs <ref type="bibr" target="#b17">[18]</ref>, state-decay mechanisms such as CT-GRU <ref type="bibr" target="#b39">[40]</ref>, RNN Decay <ref type="bibr" target="#b47">[48]</ref>, CT-LSTM <ref type="bibr" target="#b38">[39]</ref>, and GRU-D <ref type="bibr" target="#b7">[8]</ref>, in addition to oscillatory models such as Phased-LSTM <ref type="bibr" target="#b40">[41]</ref>.</p><p>Furthermore, we tested ODE-LSTMs against intuitive time-gap modeling approaches we built here, termed an augmented LSTM topology as well as bi-directional RNNs <ref type="bibr" target="#b51">[52]</ref>. Experimental settings are given in the supplements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic benchmark -Bit-stream sequence classification</head><p>We formulated a modified time-series variant of the XOR problem <ref type="bibr" target="#b37">[38]</ref>. In particular, the model observes a block of binary data in the form of a bit-after-bit time-series. The objective is then to learn an XOR function of the incoming bit-stream. This setup is equivalent to the binary-classification of the input sequence, where the labels are obtained by applying an XOR function to the inputs.</p><p>While any non-linear recurrent neural network architecture can learn the correct function, training the network to do so is non-trivial. For the model to make an accurate prediction, all bits in an upcoming chunk are required to be taken into account. However, the error signal is only provided after the last bit is observed. Consequently, during learning, the prediction error needs to be propagated to the first input time-step to precisely capture the dependencies, (see <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>We designed two modes, a dense encoding mode in which the input sequence is represented as a regular, periodically sampled time-series, and an event-based mode which compresses the data into irregularly sampled bit-streams, e.g., 1, 1, 1, 1 is encoded as (1, t = 4). (See <ref type="table" target="#tab_1">Table 2</ref>). We observed that a considerable number of RNN variants faced difficulties in modeling these tasks, even in the dense-encoding model.</p><p>In particular, ODE-RNNs, CT-RNNs, RNN-Decay, Phased-LSTM, and GRU-ODE could not solve the XOR problem in the first mode. Phased-LSTM and RNN-Decay improved their performance in the second modality, whereas ODE-RNNs, CT-RNNs, and GRU-ODE still could not solve the task. The core reason for their mediocre performance is the exploitation of the vanishing gradient problem during training. The rest of the RNN variants (except CT-GRU) were successful in solving the task in both modes, with ODE-LSTM outperforming others in an event-based encoding scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Person activity recognition with irregularly sampled time-series</head><p>We consider the person activity recognition dataset from the UCI repository <ref type="bibr" target="#b13">[14]</ref>. This task's objective is to classify the current activity of a person, from four inertial measurement sensors worn on the person's arms and feet. Even though the four sensors are measured at a fixed period of 211ms,   the random phase-shifts between them creates an irregularly sampled time-series. Rubanova et al. <ref type="bibr" target="#b47">[48]</ref> showed that ODE-based RNN architectures perform remarkably well on this dataset. Here, we benchmarked the performance of the ODE-LSTM model against other variants.</p><p>This setting realizes a per-time-step classification problem. That is a new error signal is presented to the network at every time-step which makes the vanishing gradient less of an issue here. The results in <ref type="table" target="#tab_2">Table 3</ref> shows that the ODE-LSTM outperforms other RNN models on this dataset. While the significance of an evaluation on a single dataset is limited, it demonstrates that the supreme generalization ability of ODE-LSTM architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Event-based sequential MNIST</head><p>We determined a challenging sequence classification task by designing an event-based version for the sequential-MNIST dataset. For doing this we followed the procedure described below:</p><p>1. Sequentialization + encoding long-term dependencies transform the 28-by-28 image into a time-series of length 784 2. Compression + non-uniform sampling encode binary time-series in a event-based format, to get rid of consecutive occurrences of the same binary value, e.g., 1, 1, 1, 1 is transformed to <ref type="bibr">(1, t = 4)</ref>. (Read more about this experiment in supplements)</p><p>Using this sequentialization mechansim, we compress the sequences from 784 to padded sequences of 256 irregularly-sampled datapoints. To perform well on this task, RNNs must learn to store some information up to 256 time-steps, while taking the time-lags between them into account. Since an error signal is issued at the end of the sequence, only an RNN model immune to vanishing gradients can achieve high-degrees of accuracy. <ref type="table" target="#tab_3">Table 4</ref> demonstrates that ODE-based RNN architectures, such as the ODE-RNN, CT-RNN, and the GRU-ODE <ref type="bibr" target="#b11">[12]</ref> struggle to learn a high-fidelity model of this dataset. On the other hand, RNNs built based on a memory mechanism, such as the Bi-directional RNN and GRU-D <ref type="bibr" target="#b7">[8]</ref> perform reasonably well, while the performance of ODE-LSTM surpasses that of other models. In this experiment, we evaluated how well ODE-LSTM can model a physical dynamical system. To this end, we collected simulation data of the Walker2d-v2 OpenAI gym <ref type="bibr" target="#b5">[6]</ref> environment using a pre-trained policy (see <ref type="figure" target="#fig_3">Figure  3</ref>). The objective of the model was to learn the kinematic simulation of the MuJoCo physics engine <ref type="bibr" target="#b54">[55]</ref> in an autoregressive fashion and a supervised learning modality. We increased the complexity of this task by using the pretrained policy at different training stages (between 500 to 1200 Proximal Policy Optimization (PPO) iterations <ref type="bibr" target="#b50">[51]</ref>) and overwrote 1% of all actions by random actions. Moreover, we simulated frameskips by removing 10% of the time-steps. Consequently, the dataset is irregularly-sampled. The results, shown in <ref type="table" target="#tab_4">Table 5</ref>, indicate that ODE-LSTM can capture the kinematic dynamics of the physics engine better than other algorithms with a high margin. What if we feed in samples' time-lag as an additional input feature to network? The Augmented LSTM architecture we benchmarked against realizes this concept, which is a simplistic approach to making LSTMs compatible with irregularly sampled data. The RNN could then learn to make sense of the time input, for instance, by making its change proportional to the elapsed-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Walker2d kinematic simulation</head><p>Nonetheless, the time characteristic of an augmented RNN depends purely on its learning process. Consequently, we can only hope that the augmented RNN generalize to unseen time-lags. Our experiments showed that an augmented LSTM performs reasonably well while being outperformed by models that explicitly declare their state by a continuous-time modality, such as ODE-LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference between bidirectional RNNs and ODE-LSTM?</head><p>A bi-directional architecture consists of two different types of RNNs reciprocally linked together in an auto-regressive fashion <ref type="bibr" target="#b51">[52]</ref>. In our context, the first RNN could be designed to handle irregularly-sample time series while the second one is capable of learning long-term dependencies <ref type="bibr" target="#b6">[7]</ref>. For example, an LSTM bidirectionally coupled with an ODE-RNN could, in principle, overcome both challenges. However, the use of heterogeneous RNN architectures might limit the learning process. In particular, due to different learning speeds, the LSTM could already be overfitting long before the ODE-RNN has learned useful dynamics.</p><p>Contrarily, our ODE-LSTM interlinks LSTMs and ODE-RNNs not in an autoregressive fashion, but at an architectural level, avoiding the problem of learning at different speeds. Our experiments showed that ODE-LSTMs consistently outperform a bi-directional LSTM-ODE-RNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Works</head><p>Time-continuous RNNs The notion of CT-RNNs <ref type="bibr" target="#b17">[18]</ref> was introduced around three decades ago. It is identical to the ODE-RNN architecture <ref type="bibr" target="#b47">[48]</ref> with an additional dampening factor ? . In our experiments, however, we observed a competitive performance to our ODE-LSTMs achieved by the GRU-D architecture <ref type="bibr" target="#b7">[8]</ref>. GRU-D encodes the dependence on the time-lags by a trainable decaying mechanism, similar to RNN-decay <ref type="bibr" target="#b47">[48]</ref>. While this mechanism enables modeling irregularly sampled time-series, it also introduces a vanishing gradient factor to the backpropagation path.</p><p>Similarly, CT-GRU <ref type="bibr" target="#b39">[40]</ref> adds multiple decay factors in the form of extra dimensions to the RNN state. An attention mechanism inside the CT-GRU then selects which entry along the decay dimension to use for computing the next state update. The CT-GRU aims to avoid vanishing gradients by including a decay rate of 0, i.e., no decay at all. This mechanism nevertheless, fails as illustrated in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Phased-LSTM <ref type="bibr" target="#b40">[41]</ref> adds a learnable oscillator to LSTM. The oscillator modulates LSTM to create dependencies on the elapsed-time, but also introduces a vanishing factor in its gradients. <ref type="bibr" target="#b11">[12]</ref> modifies the GRU <ref type="bibr" target="#b10">[11]</ref> topology by incorporating a continuous dynamical system. First, GRU is expressed as a discrete difference equation and then transformed into a continuous ODE. This process makes the error-propagation time-dependent, i.e., the near-constant error propagation property of GRU is abolished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU-ODE</head><p>CT-LSTM <ref type="bibr" target="#b38">[39]</ref> combines the LSTM architecture with continuous-time neural Hawkes processes. At each time-step, the RNN computes two alternative next state options of its hidden state. The actual hidden state is then computed by interpolating between these two hidden states depending on the elapsed time.</p><p>Learning Irregularly-Sampled Data Statistical <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47]</ref> and functional analysis <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref> tools have long been studying non-uniformly-spaced data. An alternative and a natural fit for this problem is the use of time-continuous recurrent networks <ref type="bibr" target="#b47">[48]</ref>. We showed that although ODE-RNNs are performant models in these domains, their performance tremendously drops when the incoming samples have long-range dependencies. We solved this shortcoming by introducing ODE-LSTMs.</p><p>Learning Long-term Dependencies The notorious question of vanishing/exploding gradient <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5]</ref> was identified as the core reason for RNNs' lack of generalizability when trained by gradient descent <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b52">53]</ref>. Recent studies used state-regularization <ref type="bibr" target="#b55">[56]</ref> and long memory stochastic processes <ref type="bibr" target="#b20">[21]</ref> to analyze long-range dependencies. Apart from the original LSTM model <ref type="bibr" target="#b26">[27]</ref> and its variants <ref type="bibr" target="#b21">[22]</ref> that solve the problem in the context of RNNs, very few alternative researches exist <ref type="bibr" target="#b8">[9]</ref>.</p><p>As the class of CT RNNs become steadily popularized <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>, it is important to characterize them better <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> and understand their applicability and limitations <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b24">25]</ref>. In this paper, we proposed a method to enable ODE-based RNNs to learn long-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed a solution to learn long-term dependencies in irregularly-sampled input data streams. To perform this, we designed a novel long short term memory network, that possesses a continuous-time output state, and consequently modifies its internal dynamical flow to a continuous-time model. ODE-LSTMs resolve the vanishing and exploding of the gradient problem of the class of ODE-RNNs while demonstrating an attractive performance in learning long-term dependencies on data arriving at non-uniform intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Who will benefit from this research? Time series data with missing values and non-uniform intervals are the routine settings in many safety-critical application domains, such as medical, business, social, and the automation of industries.</p><p>The results of this paper enable users to construct learning systems that not only help handle irregularly sampled data efficiently but also to learn long-term dependencies that might be vital to their application.</p><p>For instance, consider the decision-critical domain of surgical processes or the treatment of patients in intensive care units (ICU) in which the medical team has to have access to the process actively, and the steps are taken throughout a surgical procedure, to make/take a current decision/action. An intelligent agent in use as an assistant during surgery must be able to do the same and carefully assign credits to the actions taken in the past (long-term dependencies) to output an accurate decision. This example simultaneously consists of irregularly-sampled inputs and long-term dependencies. Our proposed method enables these modalities.</p><p>Preventing failure of the system Like any other intelligent system, our proposed algorithm has to go through robustness analysis (perturbations, noise, and adversarial attack), before being deployed in high-stakes decision-making applications. This process would dramatically reduce the chance of failure of intelligent systems such as ours.</p><p>Whether the method leverages biases in the data The mechanisms of "learning to forget" and "learning long-term dependencies" are encoded in our proposed method. Both processes can be used as the controller of biases in data, and help us design fair machine learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials S1 Proofs</head><p>Derivation of the Euler's method Jacobian Let? = f ? (x, h, T ) ? h? be an ODE-RNN. Then the explicit Euler's method with step-size T is defined as the discretization</p><formula xml:id="formula_14">h t+T = h t + T (f ? (x, h, T ) ? h? ) h=ht .</formula><p>(S1) Therefore, state-previous state Jacobian is given by</p><formula xml:id="formula_15">?h t+T ?h t = I + T ?f ?h h=ht ? ? T I. (S2)</formula><p>Derivation of the Runge-Kutta Jacobian Let? = f ? (x, h, T ) ? h? be an ODE-RNN. Then the Runge-Kutta method with step-size T is defined as the discretization</p><formula xml:id="formula_16">h t+T = h t + T M j=1 b i (f ? (x, h, T ) ? h? ) h=Ki ,<label>(S3)</label></formula><p>where the coefficients b i and the values K i are taken according to the Butcher tableau with</p><formula xml:id="formula_17">M j=1 b i = 1 and K 1 = h t .</formula><p>Then state-previous state Jacobian of the Runge-Kutta method is given by the following equation :</p><formula xml:id="formula_18">?h t+T ?h t = I + T M j=1 b i ?f ?h h=Ki ? ? T I.,<label>(S4)</label></formula><p>Note that the explicit Euler method is an instance of the Runge-Kutta method with M = 1 and b 1 = 1.</p><p>Proof of ODE-RNN suffering from vanishing or exploding gradients Let? = f ? (x, h, T ) ? h? be an ODE-RNN with latent dimension N . Without loss of generality let h 0 be the initial state at t = 0 and h T denote the ODE state which should be computed by a numerical ODE-solver. Then ODE-solvers, including fixed-step methods <ref type="bibr" target="#b49">[50]</ref> and variable-step methods such as the Dormand-Prince method <ref type="bibr" target="#b12">[13]</ref>, discretize the interval [0, T ] by a series t 0 , t 1 , . . . t n , where t 0 = 0 and t n = T and each h ti is computed by a single-step explicit Euler or Runge-Kutta method from h ti?1 .</p><p>Our proof closely aligns with the analysis in Hochreiter and Schmidhuber <ref type="bibr" target="#b26">[27]</ref>. We refer the reader to <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42]</ref> for a rigorous discussion on the vanishing and exploding gradients.</p><p>We first prove the theorem for a scalar RNN, i.e., n = 1, and then extend the discussion to the general case. The error-flow per RNN step between t = 0 and t = T is given by</p><formula xml:id="formula_19">?h T ?h 0 = n m=1 1 + (t m ? t m?1 ) M j=1 b i ?f ?h h=Km i ? ? (t m ? t m?1 ) ,<label>(S5)</label></formula><p>which realizes a power series depending on the value</p><formula xml:id="formula_20">|1 + (t m ? t m?1 ) M j=1 b i ?f ?h h=Km i ? ? (t m ? t m?1 )|.<label>(S6)</label></formula><p>Obviously, the condition that this term is equal to 1 is not enforced during training and violated for any non-trivial f ? , such as f ? (h, x) = ?(W h h+W x x+b) with ? being a sigmoidal or rectified-linear activation function. The exact magnitude depends on the weights W h , as</p><formula xml:id="formula_21">?f ? (h, x) ?h = W h ? (W h h + W x x +b).<label>(S7)</label></formula><p>A non-zero time-constant ? pushes the gradient toward a vanishing region.</p><p>Note that the Equation (S6) only becomes equal to 1, if M j=1 b i ?f ?h h=Km i = ? . This would imply that ?ht m ht m ?1 = 0, i.e., when the change in ODE-state between two time-points is zero. A variable that does not change over time is a memory element. Thus the only solution of enforcing a constant-error propagation is to include an explicit memory element in the architecture <ref type="bibr" target="#b26">[27]</ref> which does not change its value between two arbitrary time-points t m and t m?1 .</p><p>For the general case n ? 1, the error-flow per RNN step between t = 0 and t = T is given by</p><formula xml:id="formula_22">?h T ?h 0 = n m=1 I + (t m ? t m?1 ) M j=1 b i ?f ?h h=Km i ? ? (t m ? t m?1 )I .<label>(S8)</label></formula><p>As h is a vector, we need to consider all possible error-propagation paths. The error-flow from unit u to unit v is then given by summing all N n?1 possible paths between u to v,</p><formula xml:id="formula_23">?h v T ?h u 0 = N l1 ? ? ? N ln?1 n m=1 I + (t m ? t m?1 ) M j=1 b i ?f ?h h=Km i ? ? (t m ? t m?1 )I lm,lm?1 ,<label>(S9)</label></formula><p>where l 0 = u and l n = v.</p><p>The arguments of the scalar case hold for every individual path in Equation (S9). The only difference between the the scalar case and the individual paths in the vectored version is the non-diagonal connections in the general case do not include the constant 1 and ? . The error-propagation magnitude between u and v with u = v is given by</p><formula xml:id="formula_24">|(t m ? t m?1 ) M j=1 b i ?f ?h h=Km i u,v |.<label>(S10)</label></formula><p>Again, for f ? (h, x) = ?(W h h + W x x +b) we obtain an error-flow that depends on the weights W h and can be either vanishing or exploding, depending on its magnitude.</p><p>Proof that even gradients of the ODE solution can vanish or explode Let? = f ? (x, h, T ) ? h? be an ODE-RNN with latent dimension N , with f ? being uniformly Lipschitz continuous. Without loss of generality let h 0 be the initial state at t = 0 and h T denote the ODE state which should be computed by a numerical ODE-solver. We approximate the interval [0, T ] by a uniform discretization grid, i.e. t i ? t i?1 = t j ? t j?1 = T /n for all i, j t 0 , t 1 , . . . t n , where t 0 = 0 and t n = T and each h ti is computed by a single-step explicit Euler from h ti?1 .</p><p>Even when making the discretization grid t 0 , t 1 , . . . t n finer and finer, the gradient propagation issue is not resolved. Let h i denote the intermediate values computed by the Picard-iteration, i.e., the explicit Euler. By the Picard-Lindel?f theorem, we know that h T converges to the true solution h(T ).</p><p>First, we assume there exists a ? &gt; 0 such that ? ? ?f ?h h=hm ? ? for all m. Note that this situation can naturally occur if we have a f ? (h, x) = ?(W h h + W x x +b). In the limit n ? ? we get</p><formula xml:id="formula_25">lim n?? ?h T ?h 0 = lim n?? n m=1 1 + (t m ? t m?1 ) ?f ?h h=hm ? ? (t m ? t m?1 ) = lim n?? n m=1 1 + T n ?f ?h h=hm ? ? T n ) ? lim n?? n m=1 1 + T n ? , with some 0 &lt; ? ? ?f ?h h=hm ? ? for all m = lim n?? 1 + T n ? n = e T ? &gt; 1,</formula><p>i.e., we have an exploding gradient.</p><p>Conversely, lets assume there exists a ? &lt; 0 such that ? ? ?f ?h h=hm ? ? for all m. Note that this situation can also naturally occur, for instance if ? &gt; 0 and regions where f is small. In the limit n ? ? we get</p><formula xml:id="formula_26">lim n?? ?h T ?h 0 = lim n?? n m=1 1 + (t m ? t m?1 ) ?f ?h h=hm ? ? (t m ? t m?1 ) = lim n?? n m=1 1 + T n ?f ?h h=hm ? ? T n ) ? lim n?? n m=1 1 + T n ? , with some 0 &gt; ? ? ?f ?h h=hm ? ? for all m = lim n?? 1 + T n ? n = e T ? &lt; 1,</formula><p>i.e., we have a vanishing gradient.</p><p>Similar to the argument in the proof above, we can extend the scalar case to the general case. However, summing over all possible path might not be trivial, as the number of possible path also growths to infinity.</p><formula xml:id="formula_27">lim n?? ?h v T ?h u 0 = lim n?? N l1 ? ? ? N ln?1 n m=1 I +(t m ?t m?1 ) ?f ?h h=hm ?? (t m ?t m?1 )I lm,lm?1 . (S11)</formula><p>Instead, we assume u = v = l 1 = . . . l n ? 1, i.e., we only look at the error-propagation through the diagonal element u. , the term f h depends on the value W u,u h . By assuming W w,z for any (w, z) = (u, u) is neglectable small, we can infer that the effects of the gradient by any other path in Equation (S11) is neglectable small. Thus the global error flow depends on W u,u h , which can make the error-flow either explode or vanish depending on its value.</p><p>Note that this argument is similar to arguing that as the multi-dimensional case properly contains the scalar case, the multi-dimensional case can express an exploding or vanishing gradient too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof that the ODE-LSTM does not suffer from a vanishing or exploding gradient</head><p>Recall that we assume that R z , R i , R f , W f and b f are initialized close to 0 and that we are at the beginning of the training process, i.e., we assume the weights do not differ significantly from their initialized values.</p><p>We have ?c t+1</p><formula xml:id="formula_28">?c t = ?z t+1 i t+1 + c t f t+1 ?c t = ?z t+1 ?c t diag(i t+1 ) + ?i t+1 ?c t diag(z t+1 ) + diag(f t+1 ) + ?f t+1 ?c t diag(c t ).</formula><p>For the derivative of the input update activation we can simply apply the chain-rule and get</p><formula xml:id="formula_29">?z v t+1 ?c u t = tanh (W z x t+1 + R z h t + b z ) v R u,v z ?h u t ?c u t = tanh (W z x t+1 + R z h t + b z ) v R u,v z o u t ,</formula><p>where tanh denotes the functional derivative of the hyperbolic tangent. As 0 ? tanh ? 1, 0 ? o u t ? 1 and most importantly R z is initialized close to 0, we can safely assume that ?z v t+1 ?c u t ? 0.</p><p>Similar argument holds for the input and forget gate derivatives, where we assumed that R i and R f are initialized close to 0. Therefore</p><formula xml:id="formula_30">?i v t+1 ?c u t = ? (W i x t+1 + R i h t + b i ) v R u,v i o u t ? 0 and ?f v t+1 ?c u t = ? (W f x t+1 + R f h t + b f + 1) v R u,v f o u t ? 0,</formula><p>where ? denotes the functional derivatives of the sigmoid function.</p><p>Consequently, with a proper weight initialization, the Jacobian simplifies to</p><formula xml:id="formula_31">?c t+1 ?c t ? diag(f t+1 ).</formula><p>We assumed that W f and b f are initialized close to 0. Hence,</p><formula xml:id="formula_32">f v t+t = ?(W f x t+1 + R f h t + b f + 1) v ? ?(1) ? 0.7310586. Hence, we have N j=1 ?c i t+1 ?c j t ? 0.7310586,</formula><p>, which is less than 1 (no exploding) but much greater than 0 (no vanishing) and ensures a nearconstant error propagation at the beginning of the training process.</p><p>As already mentioned in the paper, the exact value of the error flow can be controlled by changing the forget gate bias from its default value of 1. If the underlying data distribution contains dependencies with a very long time-lag, we can bring the error flow factor closer to 1 by increasing forget gate bias. Thus enabling the ODE-LSTM to learn even very long-term dependencies in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Experimental evaluation</head><p>For models containing differential equations, we used the ODE-solvers as listed in <ref type="table" target="#tab_5">Table S1</ref>. Hyperparameter settings used for our evaluation is shown in <ref type="table" target="#tab_1">Table S2</ref>.</p><p>Batching Sequences of our event-based bit-stream classification task and event-based seqMNIST can have different lengths. To allow an arbitrary batching of several sequences, we pad all sequences to equal length and apply a binary mask during training and evaluation.  <ref type="figure" target="#fig_1">Figure S1</ref>: Dense and event-based coding of the same time-series. An event-based coding is more efficient than a dense coding at encoding sequences where the transmitted symbol changes only sparsely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1 Dataset description</head><p>The individual datasets are created as follows:</p><p>Bit-stream XOR dataset Every data point is a block of 32 random bits. The binary labels are created by applying an XOR function on the bit block, i.e., class A if the number of 1s in the bit-stream are even, class B if the number of 1s in the bit-stream is odd. For training, a cross-entropy loss on these two classes is used. The training set consists of 100,000 samples, which are less than 0.0024% of all possible bit-streams that can occur. The test set consists of 10,000 samples.</p><p>For the event-based encoding, we introduce a time-dimension. The time is normalized such that the complete sequence equals 1 unit of time, i.e., 32 bits corresponds to exactly 1 second. An illustration of the two different encodings is shown in <ref type="figure" target="#fig_1">Figure S1</ref>.</p><p>Person Activity We consider a variation of the "Human activity" dataset described in <ref type="bibr" target="#b47">[48]</ref> form the UCI machine learning repository <ref type="bibr" target="#b13">[14]</ref>. The dataset is comprised of 25 recordings of human participants performing different physical activities. The eleven possible activities are "lying down", "lying", "sitting down", "sitting", "standing up from lying", "standing up from", "sitting", "standing up from sitting on the ground", "walking", "falling", "on all fours", and "sitting on the ground". The objective of this task is to recognize the activity from inertial sensors worn by the participant, i.e., a per-time-step classification problem. We group the eleven activities listed above into seven different classes, as proposed by <ref type="bibr" target="#b47">[48]</ref>.</p><p>The input data consists of sensor readings from four inertial measurement units placed on the participant's arms and feet. The sensors are read at a fixed period of 211 ms but have different phase-shifts in the 25 recordings. Therefore, we treat the data as irregularly sampled time-series.</p><p>The 25 recordings are split into partially overlapping sequences of length 32, to allow an efficient training of the machine learning models.</p><p>Our results are not directly comparable to the experiments in <ref type="bibr" target="#b47">[48]</ref>, as we use a different representation of the input features. While <ref type="bibr" target="#b47">[48]</ref> represents each input feature as a value-mask pair, i.e., 24 input features, we represent the data in the form of a 7-dimensional feature vector. The first four entries of the input indicate the senor ID, i.e., which arm or foot, whereas the remaining three entries contain the sensor reading.</p><p>Event-based seqMNIST The MNIST dataset consists of 70,000 data points split into 60,000 training and 10,000 test samples <ref type="bibr" target="#b34">[35]</ref>. Each sample is a 28-by-28 grayscale image, quantized with 8-bits and represents one out of 10 possible digits, i.e., a number from 0 to 10.</p><p>We pre-process each sample as follows: We first apply a threshold to transform the 8-bits pixel values into binary values. The threshold is 128, on a scale where 0 represents the lowest possible and 255 the larges possible pixel value. We further transform the 28-by-28 image into a time-series of length 784. Next, we encode binary time-series in a event-based format. Essentially, the encoding step gets rid of consecutive occurrences of the same binary value, i.e., 1, 1, 1, 1 is transformed into (1, t = 4). By introducing a time dimension, we can compress the sequences from 784 to an average of 53 time-steps.</p><p>To allow an efficient batching and training, we pad each sequence to a length of 256. Note that no information was lost during this process. We normalize the added time dimension such that 256  Walker2d kinematic modeling Here we create a dataset based on the Walker2d-v2 OpenAI gym <ref type="bibr" target="#b5">[6]</ref> environment and the MuJoCo physics engine <ref type="bibr" target="#b54">[55]</ref>. Our objective is to benchmark how well the RNN architecture can model kinematic dynamical systems in an irregularly sampled fashion. The learning setup is based on an auto-regressive supervised learning, i.e., the model predicts the next state of the Walker2d environment based on the current state.</p><p>In order to obtain interesting simulation rollouts, we trained a non-recurrent policy by Proximal Policy Optimization (PPO) <ref type="bibr" target="#b50">[51]</ref> using the Rllib <ref type="bibr" target="#b36">[37]</ref> reinforcement learning framework. We then collect the training data for our benchmark by performing rollouts on the Walker2d-v2 environment using our pre-trained policy. Note that because the policy is deterministic, there is no need to include the actions produced by the policy in the training data.</p><p>We introduce three sources of uncertainty to make this task more challenging. First of all, for each rollout we uniformly sample a checkpoint of policy at 562, 822, 923, or 1104 PPO iterations. Secondly, we overwrite 1% of all actions by random actions. Thirdly, we exclude 10% of the time-steps, i.e., we simulate frame-skips/frame-drops. Note that the last step transforms the rollouts into irregularly sampled time-series and introduces a time dimension.</p><p>In total, we collected 400 rollouts, i.e., 300 used for training, 40 for validation, and 60 for testing. For an efficient training, we align the rollouts into sequences of length 64. We use the mean-square-error as training loss and evaluation metric. We train each RNN for 200 epochs and log the validation error after each training epochs. At the end, we restore the weights that achieved the best (lowest) validation error and evaluate them on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility statement</head><p>We publish all code and data used in our experimental setup at this link https://github.com/mlech26l/ode-lstms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Magnitude of the states' error propagation in time-continuous recurrent neural networks gives rise to the vanishing or exploding of the gradient (first two models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: Illustration of how vanishing gradients make the training process of RNNs difficult when the data express long-term dependencies. The prediction error can be thought of as a teaching signal indicating how the dynamics should be changed to minimize the loss. The vanishing gradient of the ODE-RNN makes the teaching signal weaker when propagating it back in time. Conversely, the teaching signal stays near-constant in the ODE-LSTM. Right: The resulting loss surfaces of the ODE-RNN is much flatter than ODE-LSTM, making the training difficult.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Walker-2d kinematic dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>I</head><label></label><figDesc>+ (t m ? t m?1 ) ?f ?h h=hm ? ? (t m ? t m?1 )I t m ? t m?1 ) ?f u ?h u h u =h um ? ? u (t m ? t m?1 ) to the scalar case. For an interesting f such as f ? (h, x) = ?(W h h + W x x +b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>t = 4 b:? t = 3 a</head><label>43</label><figDesc>:? t = . . . (b) Event-based coding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Datapoints and their timestamps {(x t , t i )} i=1...N Parameters: LSTM weights ? l , ODE-RNN weights ?, output weight and bias W output , b output</figDesc><table><row><cell>Algorithm 1 The ODE-LSTM</cell></row><row><cell>Input:</cell></row><row><cell>problem, except for</cell></row><row><cell>parameter configurations which give the non-trainable constant dynamics f ? (h, x) = 0, and cases</cell></row><row><cell>where f ? (h, x) is constant, for a particular input sequence x and ?.</cell></row></table><note>1 (ODE-RNNs suffer from a vanish or exploding gradient) Let? = f ? (x, h, T ) ? h? , and h t the RNN obtained by simulating the ODE by a solver based on the explicit Euler or Runge- Kutta method. Then the RNN suffers from a vanishing and exploding gradient</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Bit-stream sequence classification 00% ? 0.00 89.71% ? 3.48 CT-GRU 100.00% ? 0.00 61.36% ? 4.87 RNN Decay 60.28% ? 19.87 75.53% ? 5.28 Bi-directional RNN 100.00% ? 0.00 90.17% ? 0.69 GRU-D 100.00% ? 0.00 97.90% ? 1.71</figDesc><table><row><cell>Model</cell><cell>Dense encoding</cell><cell>Event-based encoding</cell></row><row><cell>ODE-RNN</cell><cell>50.47% ? 0.06</cell><cell>51.21% ? 0.37</cell></row><row><cell>CT-RNN</cell><cell>50.42% ? 0.12</cell><cell>50.79% ? 0.34</cell></row><row><cell cols="2">Augmented LSTM 100.PhasedLSTM 50.99% ? 0.76</cell><cell>80.29% ? 0.99</cell></row><row><cell>GRU-ODE</cell><cell>50.41% ? 0.40</cell><cell>52.52% ? 0.35</cell></row><row><cell>CT-LSTM</cell><cell>97.73% ? 0.08</cell><cell>95.09% ? 0.30</cell></row></table><note>ODE-LSTM (ours) 100.00% ? 0.00 98.89% ? 0.26Note: Test accuracy (mean ? std, N = 5). While all of the above RNN can represent the correct function, training is difficult due to long-term dependencies.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Per time-step classification. Person activity recognition. Test accuracy (mean ? std, N = 5)</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>ODE-RNN</cell><cell>80.43% ? 1.55</cell></row><row><cell>CT-RNN</cell><cell>83.65% ? 1.55</cell></row><row><cell>Augmented LSTM</cell><cell>84.11% ? 0.68</cell></row><row><cell>CT-GRU</cell><cell>79.48% ? 2.12</cell></row><row><cell>RNN Decay</cell><cell>62.89% ? 3.87</cell></row><row><cell cols="2">Bi-directional RNN 83.85% ? 0.45</cell></row><row><cell>GRU-D</cell><cell>83.57% ? 0.40</cell></row><row><cell>PhasedLSTM</cell><cell>83.33% ? 0.69</cell></row><row><cell>GRU-ODE</cell><cell>82.56% ? 2.63</cell></row><row><cell>CT-LSTM</cell><cell>84.13% ? 0.11</cell></row><row><cell cols="2">ODE-LSTM (ours) 84.15% ? 0.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Event sequence classification. Irregular sequential MNIST. Test accuracy (mean ? std, N = 5)</figDesc><table><row><cell>Model</cell><cell>Accuracy</cell></row><row><cell>ODE-RNN</cell><cell>72.41% ? 1.69</cell></row><row><cell>CT-RNN</cell><cell>72.05% ? 0.71</cell></row><row><cell>Augmented LSTM</cell><cell>82.10% ? 4.36</cell></row><row><cell>CT-GRU</cell><cell>87.51% ? 1.57</cell></row><row><cell>RNN Decay</cell><cell>88.93% ? 4.06</cell></row><row><cell cols="2">Bi-directional RNN 94.43% ? 0.23</cell></row><row><cell>GRU-D</cell><cell>95.44% ? 0.34</cell></row><row><cell>PhasedLSTM</cell><cell>86.79% ? 1.57</cell></row><row><cell>GRU-ODE</cell><cell>80.95% ? 1.52</cell></row><row><cell>CT-LSTM</cell><cell>94.84% ? 0.17</cell></row><row><cell cols="2">ODE-LSTM (ours) 95.73% ? 0.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Per time-step regression. Walker2d kinematic dataset. (mean ? std, N = 5)</figDesc><table><row><cell>Model</cell><cell>Square-error</cell></row><row><cell>ODE-RNN</cell><cell>1.904 ? 0.061</cell></row><row><cell>CT-RNN</cell><cell>1.198 ? 0.004</cell></row><row><cell>Augmented LSTM</cell><cell>1.065 ? 0.006</cell></row><row><cell>CT-GRU</cell><cell>1.172 ? 0.011</cell></row><row><cell>RNN-Decay</cell><cell>1.406 ? 0.005</cell></row><row><cell cols="2">Bi-directional RNN 1.071 ? 0.009</cell></row><row><cell>GRU-D</cell><cell>1.090 ? 0.034</cell></row><row><cell>PhasedLSTM</cell><cell>1.063 ? 0.010</cell></row><row><cell>GRU-ODE</cell><cell>1.051 ? 0.018</cell></row><row><cell>CT-LSTM</cell><cell>1.014 ? 0.014</cell></row><row><cell cols="2">ODE-LSTM (ours) 0.883 ? 0.014</cell></row><row><cell>6 Discussions, Scope and Limitations</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S1 :</head><label>S1</label><figDesc>ODE-solvers used for the different RNN architectures involving ordinary differential</figDesc><table><row><cell>equations</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>ODE-solver</cell><cell>Time-step ratio</cell></row><row><cell>CT-RNN</cell><cell>4-th order Runge-Kutta</cell><cell>1/3</cell></row><row><cell cols="2">ODE-RNN 4-th order Runge-Kutta</cell><cell>1/3</cell></row><row><cell>GRU-ODE</cell><cell>Explicit Euler</cell><cell>1/4</cell></row><row><cell>ODE-LSTM</cell><cell>Explicit Euler</cell><cell>1/4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S2</head><label>S2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Hyperparameters</cell></row><row><cell>Parameter</cell><cell>Value</cell><cell>Description</cell></row><row><cell>RNN latent dimension</cell><cell>64</cell><cell>number of neurons in the RNN</cell></row><row><cell>Minibatch size</cell><cell>256</cell><cell></cell></row><row><cell>Optimizer</cell><cell>RMSprop [54]</cell><cell></cell></row><row><cell>Learning rate</cell><cell>5e-3</cell><cell></cell></row><row><cell>Training epochs</cell><cell>500/200</cell><cell>Synthetic/real datasets</cell></row><row><cell cols="3">symbols correspond to 1 second or unit of time. The resulting task is a per-sequence classification</cell></row><row><cell cols="2">problem of irregularly sampled time-series.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Dan Man?, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can sgd learn recurrent neural networks with provable generalization?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10331" to="10341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transcripts: An algebraic approach to coupled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Amig?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Monetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Aschenbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13105</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scalable linear causal inference for irregularly sampled time series with long range dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">R</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Bayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03336</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Openai gym</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6775" to="6785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13431" to="13442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gru-ode-bayes: Continuous modeling of sporadically-observed time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaak</forename><surname>Edward De Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7377" to="7388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A family of embedded runge-kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Augmented neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural spline flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Bekasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7509" to="7520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wavelets for period analysis of unevenly sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astronomical Journal</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page">1709</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approximation of dynamical systems by continuous time recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Ken-Ichi Funahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to forget: Continual prediction with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Anode: Unconditionally accurate memoryefficient gradients for neural odes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Biros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10298</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A statistical investigation of long memory in language and music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Tunnell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2394" to="2403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lstm: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On robustness of neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan Hanshu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Jiawei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Grosu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04439</idno>
		<title level="m">Liquid time-constant networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The natural lottery ticket winner: Reinforcement learning with ordinary neural circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Machine Learning. JMLR. org</title>
		<meeting>the 2020 International Conference on Machine Learning. JMLR. org</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>TU M?nich</orgName>
		</respStmt>
	</monogr>
	<note>in german] diploma thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Holl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07457</idno>
		<title level="m">Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differentiable physics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural jump stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin R</forename><surname>Benson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9843" to="9854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Functional autoregression for sparsely sampled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Daniel R Kowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Matteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business &amp; Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural circuit policies enabling auditable autonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="642" to="652" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gershgorin loss stabilizes the recurrent neural network compartment of an end-to-end robot learning scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing worm-inspired neural networks for interpretable robotic control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A scalable end-to-end gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin M Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rllib: Abstractions for distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3053" to="3062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsky</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptrons</title>
		<imprint>
			<date type="published" when="1969" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6754" to="6764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert V</forename><surname>Kazakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindsey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04110</idno>
		<title level="m">Discrete event, continuous time rnns</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch? Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imbalanced clustering for microarray time-series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Goney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Shwaber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Mathematical theory of optimal processes. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Semenovich Pontryagin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Snode: Spectral discretization of neural odes for system identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Quaglino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gallieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutn?k</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust landsat-based crop time series modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="page">110810</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">?ber die numerische aufl?sung von differentialgleichungen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="1895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sherstinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page">132306</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">State-regularized recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6596" to="6606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generalization of backpropagation with application to a recurrent gas market model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="339" to="356" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adaptive checkpoint adjoint method for gradient estimation in neural ode</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntang</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicha</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sekhar</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xenophon</forename><surname>Papademetris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Duncan</surname></persName>
		</author>
		<idno>PMLR 119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
