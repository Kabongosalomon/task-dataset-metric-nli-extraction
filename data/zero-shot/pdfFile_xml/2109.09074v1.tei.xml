<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Urban-scale Point Clouds Segmentation with BEV Projection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhong</forename><surname>Zou</surname></persName>
							<email>zouzhehong@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Vehicle and Mobility</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Information Science and Engineering</orgName>
								<orgName type="institution">Xinjiang University Urumqi</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Urban-scale Point Clouds Segmentation with BEV Projection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-point clouds</term>
					<term>semantic segmentation</term>
					<term>multi- modal learning</term>
					<term>urban-scale</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point clouds analysis has grasped researchers' eyes in recent years, while 3D semantic segmentation remains a problem. Most deep point clouds models directly conduct learning on 3D point clouds, which will suffer from the severe sparsity and extreme data processing load in urban-scale data. To tackle the challenge, we propose to transfer the 3D point clouds to dense bird's-eye-view projection. In this case, the segmentation task is simplified because of class unbalance reduction and the feasibility of leveraging various 2D segmentation methods. We further design an attention-based fusion network that can conduct multi-modal learning on the projected images. Finally, the 2D out are remapped to generate 3D semantic segmentation results. To demonstrate the benefits of our method, we conduct various experiments on the SensatUrban dataset, in which our model presents competitive evaluation results (61.17% mIoU and 91.37% OverallAccuracy). We hope our work can inspire further exploration in point cloud analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3D semantic segmentation is the critical technology of point cloud learning with the purpose of assigning a semantic label to each individual point data, which has been extensively applied in automatic driving <ref type="bibr" target="#b0">[1]</ref>, virtual reality <ref type="bibr" target="#b1">[2]</ref>, 3D reconstruction <ref type="bibr" target="#b2">[3]</ref>, etc. Although deep learning has prominent performance in 2D semantic segmentation tasks, it unable to directly process the point data which is irregular, unordered and unstructured <ref type="bibr" target="#b3">[4]</ref>. Therefore, several methods <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b11">[11]</ref> currently convert unstructured points into certain efficient intermediate representations, such as voxels <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b12">[12]</ref>and multiviews <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, to process point clouds utilizing classic CNN models. With the increasing demand for 3D scene understanding, more and more 3D point cloud datasets are proposed. From the indoor datasets (e.g., S3DIS <ref type="bibr" target="#b15">[15]</ref> and ScanNet <ref type="bibr" target="#b16">[16]</ref>) to roadway-level datasets (e,g., SemanticKITTI <ref type="bibr" target="#b17">[17]</ref>), the spatial size of datasets is also larger. Recent work <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[18]</ref> have proposed urban-level datasets which bringing several fire-new challenges to semantic segmentation of largescale datasets.</p><p>Different from the LiDAR-based datasets, these urban-scale point cloud are mostly obtained from UAV photogrammetry, which may lead to the the following characteristics in the dataset. Firstly, the scanning of UAV Photogrammetry is uneven, the scanning area is not concentrated, and the captured <ref type="figure" target="#fig_1">Fig. 1</ref>. Comparison results with other published methods. We implemented our models using ResNet-34 and HRNet, both of which achieved competitive mean IoU and overall accuracy on the SensatUrban dataset. Notice that our results are computed on the validation set due to the unaccessibility of test set labels, while other models' results are provided by the dataset publishers and evaluated with testing data. images include scattered areas on the edges. Secondly, the reconstructed point cloud is partially missing. We observed this phenomenon in the SensatUrban <ref type="bibr" target="#b2">[3]</ref> dataset, a typical example is that, after visualizing the point cloud, there is no corresponding wall points under the roof, which makes the roof seem to be suspended in the air. Interestingly, We found that the category overlap rate of the vertical points in UAV-based point clouds is lower, e.g., SensatUrban is 2.3%, meaning that bird's-eye-view is a suitable projection method which is simpler, more efficient, and is able to maximize retain the point details. Moreover, for the projected images, the 2D pixel-level dataset with richer markers can be used for pre-training. Therefore, in this paper, we propose a BEV projection segmentation methods to deal with urban-scale 3D segmentation problem. Our main contributions are: 1) conduct point-level analysis for urban-scale point clouds; 2) propose a multi-modal fusion segmentation model with specific BEV projection algorithm; 3) we evaluate our method in the Sen-satUrban dataset, our competitive result proves the efficiency of our design.  Overview of the 3D-2D task transfer process. The urban-scale point clouds map are previously partitioned into grids with side length under 400 meters. We further generated squares of 25 ? 25m 2 with 20x magnification. The 2D segmentation output will be remapped to the large 3D map according to their x/y coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Semantic Segmentation</head><p>In general, according to the form of point cloud data entered into the network, most of the existing 3D semantic segmentation approaches can be divided into three categories: point-based, 3D representation-based and projection-based.</p><p>Point-based approaches directly deal with the raw point cloud, and the representative methods of it is PointNet which has high computation overhead. Although <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref> have made some beneficial improvements to PointNet, it is still hard to accelerate because these methods deal with the sprase data directly. Recent RandLA-Net <ref type="bibr" target="#b1">[2]</ref> introduces random sampling and a light network architecture which speed the model greatly. However, as mentioned in <ref type="bibr" target="#b21">[21]</ref>, an issue couldn't be ignored in point-based method is that the large time waste (80%) of processing sprase data caused by inefficient random memory access, meaning only small amount of time is actually used to extract features. In addition, large memory overhead is also a severe problem.</p><p>3D representation-based approaches transform the raw point cloud data into certain 3D representation (e.g., voxels and lattices) and then leverage 3D convolution <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[8]</ref>. However, it is difficult to balance the relationship between resolution and memory <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>. The lower resolution, the more serious information loss of point cloud because the points in the same grid are merged together. The higher resolution, the greater computational overhead and memory usage. Moreover, the pre-processing and post-processing steps require a lot of time <ref type="bibr" target="#b23">[23]</ref>.</p><p>Instead processing points directly, projection-based approach utilizes mature 2D convolution models to process images projected from 3D point cloud. Projection-based approaches include several specific categories such as multiview, spherical-based method. Multi-view method <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref> projects point clouds into multiply virtual camera view. For example, <ref type="bibr" target="#b10">[10]</ref> utilizes a multi-stream CNN to process images generated from each view and then fuse the prediction scores from different images of each point, <ref type="bibr" target="#b13">[13]</ref> defines a rotated camera and proposes Katz projection to choose points in each camera angle, <ref type="bibr" target="#b14">[14]</ref> generates depth images and RGB images in different camera positions. <ref type="bibr" target="#b24">[24]</ref> utilize spherical projection method to transform the 3D point cloud into the images, uses the SqueezeSeg network for segmentation and applies CRF (Conditional Random Field) to optimize the segmentation results. <ref type="bibr" target="#b11">[11]</ref> proposes Context Aggregation Module (CAM) to expand receptive field based on SqueezeSeg and <ref type="bibr" target="#b3">[4]</ref> introduces Spatially-Adaptive Convolution (SAC) to further improve the segmentation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Semantic Segmentation for Large-scale Scene</head><p>Several urban-scale 3D point cloud datasets <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b25">[25]</ref> photographed by UAV has been proposed in recent work and the largest among them is the SensatUrban <ref type="bibr" target="#b2">[3]</ref> dataset, which covers the area of 7.64?10 6 m 2 , with 3 billion annotated points. However, these large and dense datasets bring new challenges to semantic segmentation.</p><p>Firstly, facing the massive data, the choice of pre-process methods, e.g., data partition, down-sampling, etc. is of great significance. Secondly, urban-scale point cloud exists the problem of the imbalance class distribution. Thirdly, a significant difference between the UAV-based datasets and Lidar-based datasets is that the former contains the RGB features. For large-scale datasets, whether to incorporate RGB features into the network and how to utilize RGB features efficiently is worth considering. Recent work, e.g., RandLA-Net <ref type="bibr" target="#b1">[2]</ref> and BAAF-Net <ref type="bibr" target="#b23">[23]</ref> have utilized the RGB color and achieved positive segmentation results. For the images generated by BEV projection, we design a multi-model fusion network based on attention, which fuses RGB and gometric details efficiently. Compared with the single-modal network, the segmentation effect achieves a certain improvement, which further verifies the significance of RGB color for segmentation.</p><p>Rencently, several semantic segmentation algorithms for large dataset have been proposed <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref>. For example, RandLA-Net <ref type="bibr" target="#b1">[2]</ref> introduces the random sampling to improve efficiency of computation and memory, TagentConv <ref type="bibr" target="#b9">[9]</ref> utilizes a U-type network based on tangent convolution for the semantic segmentation of large and dense datasets and SPGraph <ref type="bibr" target="#b27">[27]</ref> proposes a superpoint graph (SPG), a novel point cloud representation which is capable to capture the contextual structure of 3D points. More large-scale point cloud segmentation algorithms need to be proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>The purpose of 3D point cloud semantic segmentation is to assign a semantic label to each individual point, while 2D segmentation is to assign a specific label to each pixel. To some extent, these two types of tasks have similar purposes and solutions. According to our statement above, the 3D point clouds semantic segmentation task can be transferred to a 2D Bird's-eye-view Segmentation problem. The main processes include Bird's-eye-view Mapping and 2D multimodal segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Why Bird's-eye-view projection is reasonable</head><p>It requires the consistency in input data and expected output when we transfer one task to another. To evaluate our idea, we conduct point level analysis before building models. We first project the 3D points onto the BEV map (which will be detailed in the following) and count the overlap ratio. When we scale the coordinates at 0.04m in projection, about 25.44% points will be lost. For those places with dense points, the ratio will raise up to 50% or more. However, we found that most of the overlapped points belongs to the same categories as the top points. The class overlapped ratio is lower than 2.3% and g scale : magnified scale; g size : grid size; g step : step size; Output: B: BEV projected multi-modal images RGB, Alt 1: for X i ? X do 2:</p><p>x min , y min ? X i ; <ref type="bibr">3:</ref> for (x i , y i , z i ) ? X i do 4:</p><p>x i ? int((x i ? x min )/g scale );</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>y i ? int((y i ? y min )/g scale );</p><p>6: end for 7:</p><p>x max , y max ? X i ; <ref type="bibr">8:</ref> for x s ? 0; x s ? x max ; x s + +; g step do 9:</p><p>for y s ? 0; y s ? y max ; y s + +; g step do 10:</p><p>initialize grids; <ref type="bibr">11:</ref> if x i ? x s + g size then <ref type="bibr">12:</ref> grids? (x i , y i , z i ); <ref type="bibr">13:</ref> end if <ref type="bibr">14:</ref> for (x i , y i , z i ) ? grids do <ref type="bibr">15:</ref> select top z i for x i , y i ; <ref type="bibr">16:</ref> remove other points; <ref type="bibr" target="#b17">17</ref>:</p><formula xml:id="formula_0">RGB[x i , y i ] ? X i [x i , y i , z i ];</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>18:</head><p>Alt[x i , y i ] ? z i ; <ref type="bibr">19:</ref> end for <ref type="bibr">20:</ref> initialize RGB, Alt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>RGB ? f (RGB, (3,3)); <ref type="bibr" target="#b22">22</ref>:</p><p>Alt ? f (Alt, (3,3)); <ref type="bibr">23:</ref> return RGB, Alt, x s , y s ; <ref type="bibr">24:</ref> end for <ref type="bibr">25:</ref> end for 26: end for mIoU can be up to 93.7%. In this case, it is possible to transfer the 3D segmentation task to 2D BEV segmentation. And our goal will be precise recognition on the BEV images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bird's-eye-view Mapping</head><p>In order to optimize the data processing load of such large point clouds, we separate the whole work into three parts: 3D-to-BEV projection, sparse BEV images completion, and BEV-to-3D remapping. The processes of the first two parts are presented as pseudo code in the Algorithm 1 below. We set a sliding window to process points and generate BEV images. Before projection, we need to initialize the parameters g scale , g scale , g scale , that controls the scale, size, and moving steps of the sliding windows. For each sliding step, we sort the points by x/y coordinates, and query points from current BEV projection window start/end coordinates, after which processed points will be removed to reduce the following data processing volume. To obtain the optimal parameters, we test the spatial overlap ratio with different projected scales from 0.01 to 0.04, as shown in the <ref type="figure" target="#fig_2">Fig.3</ref>. When we set the scale in [0.01,0.03], it will result in the very close overlap distribution for different parts of the point clouds, that means the minimum interval of points in urban-scale point clouds is within [0.03,0.05](m). Besides, the proper length of windows is within <ref type="bibr" target="#b20">[20,</ref><ref type="bibr">50]</ref>(m) according to our projected images number estimation. Therefore, we set the parameters as g scale = 0.05, g size = g step = 25. However, we also suggest multi-scale, multi-size, and multi-step sampling for better training in the future work or other similar tasks.</p><p>As for points in a single sliding window, we map the points to the pixels by integralizing the x/y coordinates. Inevitably it will bring the loss of value quantization, however, it will not affect the label retrieve process if we conduct the same process in 3D remapping. The BEV map is updated with points at the top, generating the RGB and Altitude(Alt) images with their color and z-coordinate values. Considering the significant sparsity of projected point clouds on the BEV images, which will introduce severe noise into the label and model learning, it is necessary to conduct pixel-level completion for the projection, especially for the internal area and margin around different class points. In our experiments, we iteratively conduct three times 2D maxpooling for each channels in each images. The progressive changes in labels are shown in <ref type="figure">Fig.4</ref>. <ref type="figure">Fig. 4</ref>. Completion test. For left to right, we present the raw BEV label and label with one/two/three times maxpooling completion.</p><p>As for 3D remapping, we store the absolute x/y coordinate for every projected windows, and use theme to query the extract places in the raw large-scale point clouds for the 2D segmentation outputs. Points corresponding to the same pixel will be valued the same classes as the pixels. After that, we are able to evaluate the 3D semantic segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-modal Segmentation</head><p>With the altitude and RGB images from the BEV projection, we can leverage a multi-modal network to learn from different aspects of the data. In order to quickly develop a suitable model, we consider an Encoder-Decoder network UNet as our baseline not only for its popular model architecture, but also for its efficiency in modification, training, and inference. It comprises 4 blocks in the encoder and 5 in the decoder, in which two are ResNet-34 blocks, the last four layers use transposed convolution and the rest are convolutional blocks. All convolutional blocks have a batch-normalization layer and a ReLU layer following the convolution layer, and all kernels size are 3x3. Each block in the encoder is linked to the corresponding blocks in the decoder with a dash line, that concatenate the output of them to retrieve low-level features.</p><p>Generally, multi-modal fusion depends on the feature communication in various layers. Driven by it, we proposed a flexible multi-stage fusion network, which supports different times and places to fusion multi-pipeline data. The fusion layers comprise several constant shape fusion blocks. Each block accepts two equal-shape tensors from two pipelines, and adopts an attention layer to select the key channel from the concatenated feature map. In this way, the fusion blocks tend to drop the unrelated features and fuse those are easy to be activated in the following layers. For the attention block, we refer to our previous work??, which proposed a cross-channel multi-modal fusion attention block for semantic segmentation. After that, we add a 1x1 convolution to reduce the dimensionality, and repeat such a fusion block for image feature and fusion features, altitude features and fusion features. It is important that out block keeps the constant shape of feature maps, that mean we can stacked unlimited blocks with various network shape as we want.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Setup 1) Dataset: SensatUrban <ref type="bibr" target="#b2">[3]</ref> captured in 3 large cities in UK includes 2847M points and covers an area of 7.64?10 6 m 2 in real world, which is the largest 3D point cloud dataset at present. After obtaining areial image sequences captured by UAV, the SensatUrban point cloud dataset is reconstructed from these images. It contains 13 semantic classes, including major categories such as ground, building, traffic road and also several minor categories such as bike, rail and bridge. In the experiment, 37 point clouds are used for training and 6 point clouds are used for testing. Each point contains the features of 3D coordinates, RGB color and semantic class. Notice that due to the lack of test set labels, we randomly split the train set by 4:1, using 80% data for training and 20% for testing. All test data are not used in training.</p><p>2) Metrics: We compared our model with several bechmarks <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b27">[27]</ref>- <ref type="bibr" target="#b29">[29]</ref> which utilize different approaches (e.g., point-based method, projection-based method, etc.) and are published recently. Mean IoU (mIoU) and Overall Accuracy (OA) are chose as evaluation indicators.</p><p>3) Implementation: We use CrossEntropy as the loss function in our training. Considering the imbalance in different classes, we use log inverse weights to adjust the loss in learning. We set the batch size as 8 and the input size as the projection size at 500x500. Our models are trained on two GPUs, RTX 3090 with 24G RAM and E5-2678v3 CPU. Besides, we use the following software setup: Ubuntu 16.04 64 bit Operating System, Python 3.6, gcc5.4.0, PyTorch 1.7 with CUDA 11.0 hardware acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>We implementa our model using three backbones, UNet and ResNet34, Deeplabv3 and ResNet101, OCRNet and HRNet. The last two models are trained to explore the potential performance under our BEV segmentation framework. We have presented our segmentation results(remapped to 3D point clouds and evaluate in 3D) in the <ref type="table">Table.</ref>I. Comparing with existing models, our model can achieve rather competitive results in most classed and the overall performancee in OA, mAcc, and mIoU. The drawback is that our BEV segmentation still fail to recognize some of the small objects like bike, because they also occupy very limited pixels in the projection images. The problem may be solved by fuse 3D and our BEV models in the future work. The visualization are shown in the <ref type="figure">Fig.5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we designed a preprocessing method for large-scale UAV point clouds, that is, projecting 3D point clouds to dense bird's-eye views to solve the problems of data sparseness and serious data processing burden in largescale datasets. In addition, we also propose an multi-modal fusion network based on attention to segment the generated 2D images, making full use of RGB color and geometric information. We have obtained 61.17% mIoU and 91.37% OverallAccuracy test results on the SensatUrban dataset. We hope that our work can inspire large-scale point cloud semantic segmentation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the 3D-2D task transfer process. The urban-scale point clouds map are previously partitioned into grids with side length under 400 meters. We further generated squares of 25 ? 25m 2 with 20x magnification. The 2D segmentation output will be remapped to the large 3D map according to their x/y coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Bird's-eye-view Projection &amp; Completion Input: X: point clouds set; f (x): point completion function;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Spatial overlap ratio statistical result. The x-axis indicates the rank of sliding windows (1m ? 1m) by their point numbers, top 0% means windows with the most points. The y-axis indicates the overlap ratio of points projected onto BEV images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DETAILED</head><label>I</label><figDesc>COMPARISON RESULTS ON THE SENSATURBAN DATASET. * MARKS THE RESULTS ON VALIDATION SET. OTHERS ARE ON THE TEST SET AND PROVIED BY THE DATA PUBLISHERS.</figDesc><table><row><cell></cell><cell>O A ( % )</cell><cell>m A c c ( % )</cell><cell>m I o U ( % )</cell><cell>g r o u n d</cell><cell>v e g .</cell><cell>b u i l d i n g</cell><cell>w a l l</cell><cell>b r i d g e</cell><cell>p a r k i n g</cell><cell>r a i l</cell><cell>t r a f fi c .</cell><cell>s t r e e t .</cell><cell>c a r</cell><cell>f o o t p a t h</cell><cell>b i k e</cell><cell>w a t e r</cell></row><row><cell>PointNet [19]</cell><cell>80.78</cell><cell>30.32</cell><cell>23.71</cell><cell>67.96</cell><cell>89.52</cell><cell>80.05</cell><cell>0.00</cell><cell>0.00</cell><cell>3.95</cell><cell>0.00</cell><cell>31.55</cell><cell>0.00</cell><cell>35.14</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>PointNet++ [28]</cell><cell>84.30</cell><cell>39.97</cell><cell>32.92</cell><cell>72.46</cell><cell>94.24</cell><cell>84.77</cell><cell>2.72</cell><cell>2.09</cell><cell>25.79</cell><cell>0.00</cell><cell cols="3">31.54 11.42 38.84</cell><cell>7.12</cell><cell cols="2">0.00 56.93</cell></row><row><cell>TagentConv [9]</cell><cell>76.97</cell><cell>43.71</cell><cell>33.30</cell><cell>71.54</cell><cell>91.38</cell><cell>75.90</cell><cell>35.22</cell><cell>0.00</cell><cell>45.34</cell><cell>0.00</cell><cell cols="3">26.69 19.24 67.58</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SPGraph [27]</cell><cell>85.27</cell><cell>44.39</cell><cell>37.29</cell><cell>69.93</cell><cell>94.55</cell><cell>88.87</cell><cell cols="2">32.83 12.58</cell><cell>15.77</cell><cell cols="4">15.48 30.63 22.96 56.42</cell><cell>0.54</cell><cell cols="2">0.00 44.24</cell></row><row><cell>SparseConv [7]</cell><cell>88.66</cell><cell>63.28</cell><cell>42.66</cell><cell>74.10</cell><cell>97.90</cell><cell>94.20</cell><cell>63.30</cell><cell>7.50</cell><cell>24.20</cell><cell>0.00</cell><cell cols="3">30.10 34.00 74.40</cell><cell>0.00</cell><cell cols="2">0.00 54.80</cell></row><row><cell>KPConv [29]</cell><cell>93.20</cell><cell>63.76</cell><cell>57.58</cell><cell>87.10</cell><cell>98.91</cell><cell>95.33</cell><cell cols="2">74.40 28.69</cell><cell>41.38</cell><cell>0.00</cell><cell cols="3">55.99 54.43 85.67</cell><cell>40.39</cell><cell cols="2">0.00 86.30</cell></row><row><cell>RandLA-Net [2]</cell><cell>89.78</cell><cell>69.64</cell><cell>52.69</cell><cell>80.11</cell><cell>98.07</cell><cell>91.58</cell><cell cols="2">48.88 40.75</cell><cell>51.62</cell><cell>0.00</cell><cell cols="3">56.67 33.23 80.14</cell><cell>32.63</cell><cell cols="2">0.00 71.31</cell></row><row><cell>Ours-UNet-R34*</cell><cell>91.45</cell><cell>67.76</cell><cell>59.90</cell><cell>78.45</cell><cell>95.39</cell><cell>97.89</cell><cell cols="2">60.72 59.80</cell><cell>40.45</cell><cell cols="4">40.35 67.32 51.42 82.11</cell><cell>41.94</cell><cell cols="2">0.00 62.87</cell></row><row><cell>Ours-Deeplabv3-R101*</cell><cell>90.53</cell><cell>72.32</cell><cell>60.28</cell><cell>81.27</cell><cell>90.09</cell><cell>93.98</cell><cell cols="2">52.28 59.82</cell><cell>49.32</cell><cell cols="4">15.88 72.81 48.72 76.86</cell><cell>46.23</cell><cell cols="2">0.00 61.51</cell></row><row><cell>Ours-OCRNet-HRNet*</cell><cell>91.37</cell><cell>71.87</cell><cell>61.17</cell><cell>83.21</cell><cell>92.16</cell><cell>94.40</cell><cell cols="2">54.84 28.61</cell><cell>52.25</cell><cell cols="4">36.55 74.46 50.91 80.10</cell><cell>48.19</cell><cell cols="2">0.00 65.37</cell></row></table><note>Fig. 5. Visualization of BEV segmentation. From top to down are: RGB images, altitude images, labels(dark means invalid area without points), and 2D segmentation outputs.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Salsanext: Fast semantic segmentation of lidar point clouds for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tiago Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aksoy</surname></persName>
		</author>
		<idno>abs/2003.03653</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of large-scale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11105" to="11114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards semantic segmentation of urban-scale 3d point clouds: A dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheikh</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenna</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<idno>abs/2009.03137</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?ter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Subhransu Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latticenet: Fast point cloud segmentation using permutohedral lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Schutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<idno>abs/1912.05905</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep projective 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Felix J?remo Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAIP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Point cloud labeling using 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suya</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2670" to="2675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semantic segmentation of point clouds using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Tosteberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<idno>3DOR@Eurographics</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Iro Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno>abs/1702.01105</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2432" to="2443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantickitti: A dataset for semantic scene understanding of lidar sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Quenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9296" to="9306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Campus3d: A photogrammetry point cloud benchmark for hierarchical understanding of outdoor scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongshou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Point-voxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic segmentation for real point cloud scenes via bilateral augmentation and adaptive fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Shi Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic segmentation on swiss3dcities: A benchmark study on aerial photogrammetric 3d pointcloud dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mantegazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Abbate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chappuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<idno>abs/2012.12996</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fg-net: Fast large-scale lidar point cloudsunderstanding network leveraging correlatedfeature mining and geometric-aware modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2012.09439</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6410" to="6419" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
