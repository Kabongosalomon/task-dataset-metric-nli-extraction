<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Road Lane Marking Detection with Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Rong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yuan</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsueh-Ming</forename><surname>Hang</surname></persName>
							<email>hmhang@nctu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Wei</forename><surname>Chan</surname></persName>
							<email>shengweichan@itri.org.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Jhih</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Chiao Tung University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Industrial Technology Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Road Lane Marking Detection with Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>semantic segmentation</term>
					<term>lane detection</term>
					<term>dilated convolution</term>
					<term>deep convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane mark detection is an important element in the road scene analysis for Advanced Driver Assistant System (ADAS). Limited by the onboard computing power, it is still a challenge to reduce system complexity and maintain high accuracy at the same time. In this paper, we propose a Lane Marking Detector (LMD) using a deep convolutional neural network to extract robust lane marking features. To improve its performance with a target of lower complexity, the dilated convolution is adopted. A shallower and thinner structure is designed to decrease the computational cost. Moreover, we also design post-processing algorithms to construct 3 rd -order polynomial models to fit into the curved lanes. Our system shows promising results on the captured road scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>In general, lane detection algorithms include the following steps: (1) lane marking generation, (2) lane marking grouping, (3) lane model fitting, and (4) temporal tracking. Extracting the correct lane is critical for successful lane-mark generation. Many conventional approaches detect the lane using the information of edge <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, color, intensity and shape. In addition, lane detection can be viewed as an image segmentation problem <ref type="bibr" target="#b5">[6]</ref>. However, most methods are sensitive to illumination changes, weather condition and noises; and thus many traditional lane detection systems fail when the external environment has significant variation.</p><p>In recent years, the Deep Convolutional Neural Network (DCNN) based methods have been proposed and they outperform the traditional approaches on many applications. It also demonstrates a huge success on image semantic segmentation; we thus use this technique to extract stable lane features. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the flowchart of a lane marking detector (LMD) system. A CNN-based method is used to produce lanemarks in the first step, but the other two steps still adopt the traditional approaches. For a typical CNN, down-sampling is employed to enable a deeper architecture and also to enlarge the receptive field to capture large-scale objects in images. However, this operation usually reduces the detailed spatial information, which is very important for a semantic segmentation task. Therefore, several network architectures have been proposed to solve the problem.  In general, two methods are used to recover or retain detailed information. The first one is using an encoder-decoder architecture. The encoder is similar to many classification networks, such as VGG <ref type="bibr" target="#b6">[7]</ref> and ResNet <ref type="bibr" target="#b7">[8]</ref>. The decoder consists of consecutive up-sampling operations in order to reconstruct the same resolution as the input image. Deconvolution, a learnable up-sampling layer, is the most common approach to up-sample the feature maps, such as DeconvNet <ref type="bibr" target="#b8">[9]</ref>, FCN <ref type="bibr" target="#b9">[10]</ref> and U-Net <ref type="bibr" target="#b10">[11]</ref>. After up-sampling, FCN <ref type="bibr" target="#b9">[10]</ref> and U-Net <ref type="bibr" target="#b10">[11]</ref> use the feature maps directly coming from the encoder to recover more details. SegNet <ref type="bibr" target="#b0">[1]</ref> applied another up-sampling method by transferring the max-pooling indices from encoder to decoder. It tends to be more efficient in term of memory usage because of storing fewer indices.</p><p>The other approach is using the dilated convolution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. This method removes some down-sampling layers so that the feature maps can maintain spatial resolution and thus retain details. Nevertheless, removing down-sampling is not favorable to have large receptive fields. Thus, the dilated convolution is employed to enlarge the receptive fields at different rates. Since lane detection is a real-time application, reducing computation is of high priority. In this work, we combine the advantages of the above two approaches and modify the system to achieve low complexity and maintain similar accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. NETWORK ARCHITECTURE</head><p>The architecture of LMD network is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. It is an encoder-decoder architecture that follows the structure similar to U-Net <ref type="bibr" target="#b0">[1]</ref> and SegNet <ref type="bibr" target="#b1">[2]</ref>. The encoder is a variant of VGG16 <ref type="bibr" target="#b6">[7]</ref>. It consists of 14 3x3 convolutional layers; the first 13 convolutional layers correspond to the convolutional layers of VGG16, and the last one is inserted for matching the number of feature channels of the decoder input. Each convolutional layers are followed by a batch normalization layer <ref type="bibr" target="#b11">[12]</ref> and a rectified linear unit (ReLU). We discard the fully-connected layers of VGG16 network to decrease the number of parameters for speed consideration and to maintain the feature map resolution for accurate localization purpose <ref type="bibr" target="#b2">[3]</ref>. Since the VGG16 network is designed for image classification, it consists of many max-pooling layers for downsampling. However, the downsampling operation loses a significant amount of spatial information, which is critical for semantic segmentation. In order to solve the spatial localization problem, we only retain the first three 2x2 max-pooling layers with stride 2 in VGG16 that located right after the 2nd, 4th and 7th convolutional layer respectively. So, the resolution of the feature maps at the end of the encoder network is increased by a factor 4. This modification enables our network to capture small classes and boundary details.</p><p>Because the feature maps are enlarged after the 10th convolutional layers, the convolutional kernels of the following layers have to be enlarged by the corresponding factor to keep the same receptive field of the network. We implement this efficiently by employing the dilated convolution <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Dilated convolution is dilating the convolutional filters by expanding its size and filling the empty positions with zeros, which can expand its receptive field without any additional parameters and computational cost. Thus, we set the 11th to 13th convolutional layers with dilation 2.</p><p>The decoder in SegNet, which is responsible for recovering the image resolution, is an exact mirror of the encoder. In contrast, benefited by using larger feature maps, the decoder of LMD can be considerably simplified, which consists of only 7 convolutional layers and 2 max-unpooling layers. We use thinner convolutional layers in the decoder for speeding up without sacrificing the accuracy. The indices of the max locations computed by the max-pooling units in the encoder are stored and passed to the corresponding max-unpooling layers in the decoder for upsampling. Finally, the softmax classifier is inserted after the decoder for pixel-wise classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CLASS WEIGHTING ANALYSIS ON LANES</head><p>We use CamVid <ref type="bibr" target="#b13">[14]</ref> dataset (the details are in Sec. V) and take SegNet <ref type="bibr" target="#b0">[1]</ref> to investigate how class weighting affects the output results on the lane class. First, we apply the median frequency balancing scheme to compute the class weights for all the classes in this dataset. Then, we adjust the class weight of lane by multiplying the factor of 0.6, 2 and 5, respectively. <ref type="table" target="#tab_0">Table I</ref> shows the results. The class accuracy is positively correlated with the class weight, while the IoU is negatively correlated with the class weight. That is, the larger the class weight is, the wider area the lane class will be. These results indicate that the class accuracy metric does not include the false alarm case. Thus, a thicker lane leads to better class accuracy. In contrast, since IoU is punished for false alarms severely, the thinner lanes receive better points. The above analyses show that by adjusting the class weight, we can change the thickness of the segmented lane class according to the requirement of the lane mark post-processing operations (e.g., grouping, curve fitting). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. POST-PROCESSING SCHEME</head><p>A conventional lane detection algorithm can be divided into three steps: Lane marking generation, Lane Grouping and Lane Model Fitting. As introduced in the previous sections, we generate lane-marks by CNN semantic segmentation. We train LMD with a three-class dataset, including lane, road and the others. After lane detection by LMD, lanes are extracted as binary images. In this section, we explain the next two steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Lane Grouping</head><p>The concept of lane grouping consists of two steps. The first step is to cluster neighboring pixels belonging to the same lanesegment to form a supermarking <ref type="bibr" target="#b12">[13]</ref>. Different from the approach used in <ref type="bibr" target="#b12">[13]</ref>, the connected component labeling (CCL) technique is used to detect the connected regions and assign one label value to each region. One region is described only by one label value instead of by lots of pixels, thereby reducing the complexity problem. The second step is to connect supermarkings, which are on the same lane marking. It is very important to design measurement functions properly to calculate the cost of connecting supermarkings. Inspired by <ref type="bibr" target="#b12">[13]</ref>, our measurement functions are defined by <ref type="figure" target="#fig_3">Fig. 3</ref>. However, the distributions of these two costs may be different, and thus it results in a hard way to combine them. To solve the problem, we normalize the distribution of directionality into the distribution of geometric distance, as <ref type="formula">(1)</ref>  </p><p>where meandir and meangeo are the mean of the four distri and distgi in <ref type="figure" target="#fig_3">Fig 3.</ref> And vardir and vargeo are the variance of the four distri and distgi, i=1~4. After (1), the distributions of these two costs are identical so that we can combine these two costs by a simple average operation, as written in <ref type="bibr" target="#b1">(2)</ref>. If the cost of connecting two supermarkings is sufficiently small, they belong to the same lane-marking and thus should be connected to each other. Hence, they are now assigned to the identical label value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Lane Model Fitting</head><p>Lane model fitting is used to represent both straight lanes and curve lanes. So, we adopt the 3 rd order polynomial for the lane model, which can describe a high curvature lane, = ax 3 + bx <ref type="bibr" target="#b1">2</ref> </p><formula xml:id="formula_1">+ cx + d<label>(3)</label></formula><p>where x and y are coordinates, a, b, c and d are the coefficients determined by a series of data points. After grouping, it becomes easier to fit curves to the properly assigned label values. However, due to an amount of lane features detected by segmentation model, curve fitting is a tedious work. To solve this problem, we divide the image into several blocks along its vertical axis, then we compute the mean position of lane features in every block. By doing so, a lane is fitted by very few candidates instead of a number of lane feature points. After the lane model fitting step, the coefficients of a polynomial of degree three are determined by fitting. A 3 rd -order model is adopted to match the curved lanes with rather large curvature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We conduct a series of experiments for validating the performance of LMD. The details are given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>We include the CamVid <ref type="bibr" target="#b13">[14]</ref> road scene dataset to the performance benchmark on LMD. This dataset contains 367 training and 233 testing RGB images at 360x480 resolution. There are 11 classes, such as road, cars, traffic signs, sky, etc. The undefined class is ignored during training. We compare our results with the state-of-the-art methods based on these classes. Moreover, we also perform experiments for 12 classes, in which the lane class is added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup:</head><p>All the experiments are conducted on the Caffe <ref type="bibr" target="#b14">[15]</ref> platform with cuDNN v5.1 back-end and running on a single GTX 1080 GPU. The popular stochastic gradient decent (SGD) algorithm is adopted to train LMD. We use the ImageNet <ref type="bibr" target="#b15">[16]</ref> pre-trained model of VGG16 <ref type="bibr" target="#b6">[7]</ref> as the initial values of our encoder. Training is conducted with a weight decay of 5e-4, momentum of 0.9, batch size of 3, and the initial learning rate is set to 0.01 and is decreased by a factor of 10 after each 5,000 training iterations, and totally there are 20,000 iterations.</p><p>Since the number of pixels varies significantly among the classes in the training set, we employ a class balancing technique to differently weight the loss according to class frequencies. We use the median frequency balancing scheme <ref type="bibr" target="#b16">[17]</ref> that specifies the class weight of each class as wc = median(p) / pc, where pc is the number of pixels of class c divided by the total number of pixels in images where c is present, and median(p) is the median of these frequencies. A. Performance Analysis <ref type="table" target="#tab_0">Table II</ref> compares the forward inference time (fwt) and the number of frames per second (fps) associated with a single input image of 360x480 resolution. These are computed by the Caffe time command. The model size is referred to the Caffe model size on the hard disk. LMD is faster than SegNet by 22.4% with a significantly smaller model size. These results show that LMD is more suitable for real-time applications and also more portable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accuracy Evaluation</head><p>We evaluate the performance of LMD on the CamVid dataset by class accuracy and intersection-over-union (IoU) metrics. <ref type="table" target="#tab_0">Table III</ref> displays our results and the comparisons with two state-of-the-art segmentation models: SegNet <ref type="bibr" target="#b0">[1]</ref> and ENet <ref type="bibr" target="#b17">[18]</ref>, which are also designed for faster inference speed, fewer parameters and requiring less memory.</p><p>LMD significantly outperforms the other models on both class average accuracy and mean IoU (mIoU) metrics. Particularly, it achieves considerable improvement on all the challenging classes (e.g., traffic signs, pedestrian, bicyclist), which contain less training samples or complicated shapes. The segmentation results are provided in <ref type="figure" target="#fig_5">Fig. 4</ref>. Furthermore, we add the lane as the 12th class for training and testing because it is indispensable to autonomous driving applications. With this additional class, LMD still surpasses the other models. Finally, we combine CamVid <ref type="bibr" target="#b13">[14]</ref>, KITTI <ref type="bibr" target="#b18">[19]</ref> and SYNTHIA <ref type="bibr" target="#b19">[20]</ref> to form an ensemble of 4,004 images for pre-training; it achieves a better performance at a 79.6% class average accuracy and 65.2% mIoU. Apparently, the method we applied that enlarges the feature maps by using dilated convolution is effective. <ref type="figure" target="#fig_6">Fig. 5</ref> demonstrates the experimental results of all the steps in our lane detection algorithm. <ref type="figure" target="#fig_6">Fig. 5(a)</ref> is an input image, and <ref type="figure" target="#fig_6">Fig.  5(b)</ref> shows the segmentation result, which is trained on 3 classes: road, lane and others. <ref type="figure" target="#fig_6">Fig. 5</ref>(c)-(d) are the results in the postprocessing stage, including grouping and curve fitting. Each color represents a road-marking in <ref type="figure" target="#fig_6">Fig. 5 (c)</ref>, and the green points in <ref type="figure" target="#fig_6">Fig. 5(d)</ref> are candidates introduced in the section of Lane Model Fitting. <ref type="figure" target="#fig_6">Fig. 5</ref>(e)-(h) show the processed results of 4 road scenes, and our algorithm is able to achieve high quality in all cases. It is sufficiently stable to get over the complicated environment even when there're lots of distraction signs on road.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results after Post-Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we employ encoder-decoder architecture, dilated convolution and fine-tuned modifications to develop a modified CNN for road lane detection, called Lane Mark Detector (LMD). We improve the accuracy to achieve 65.2% mIoU on the CamVid dataset, and we also improve the testing speed to 34.4 fps. For ADAS applications, we combine the idea in <ref type="bibr" target="#b12">[13]</ref> to develop a simple post-processing algorithm, and to construct an accurate 3 rd -order lane model. The experimental results indicate that our model is stable and is able of tolerating many variations on road scenes.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>* P.-R. Chen and S.-Y. Lo contributed equally to this work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Flowchart of the proposed LMD system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Architecture of the proposed LMD network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Specifications of supermarking connecting cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ACKNOWLEDGMENT</head><label></label><figDesc>This work was supported in part by the Mechanical and Mechatronics Systems Research Lab., ITRI, under Grant 3000518795.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Results of segmentation produced by LMD. From top to bottom: (a) Input image, (b) Ground truth, (c) LMD output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Samples of lane detection results: (a) captured image, (b) detected lane class, (c)-(d) grouping and curve fitting, (e)-(h) final results of 4 test images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>THE IMPACT OF WEIGHT ADJUSTMENT</figDesc><table><row><cell>The class weight of lane</cell><cell>Class accuracy</cell><cell>IoU</cell></row><row><cell>Balanced * 0.6</cell><cell>80.7</cell><cell>53.6</cell></row><row><cell>Balanced</cell><cell>83.9</cell><cell>52.6</cell></row><row><cell>Balanced * 2</cell><cell>82.9</cell><cell>51.3</cell></row><row><cell>Balanced * 5</cell><cell>88.7</cell><cell>47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>shows.</figDesc><table><row><cell>dist ri ? =</cell><cell>dist ri -mean dir ?vardir</cell><cell cols="3">? ? var geo + mean geo i=1,2,3,4</cell><cell>(1)</cell></row><row><cell></cell><cell cols="2">cost=</cell><cell>1 4</cell><cell>? ( dist gi +dist ri ? 2 4 i=1</cell><cell>)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc>COMPARASIONS WITH FORWARD TIME AND MODEL SIZE</figDesc><table><row><cell>Network</cell><cell>Inference time (ms)</cell><cell>Frames per sec. (fps)</cell><cell>Model size (MB)</cell></row><row><cell>SegNet [1]</cell><cell>35.5</cell><cell>28.1</cell><cell>117</cell></row><row><cell>LMD (ours)</cell><cell>29.1</cell><cell>34.4</cell><cell>66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III .</head><label>III</label><figDesc>COMPARISONS WITH CLASS ACCURACY AND MIOU ACCURACY</figDesc><table><row><cell>Network</cell><cell>Buil.</cell><cell>Tree</cell><cell>Sky</cell><cell>Car</cell><cell>Sign</cell><cell cols="3">Road Pede. Fenc.</cell><cell>Pole</cell><cell>Side.</cell><cell>Bike</cell><cell>Lane</cell><cell>Class avg.</cell><cell>mIoU</cell></row><row><cell>SegNet [1]</cell><cell>88.8</cell><cell>87.3</cell><cell>92.4</cell><cell>82.1</cell><cell>20.5</cell><cell>97.2</cell><cell>57.1</cell><cell>49.3</cell><cell>27.5</cell><cell>84.4</cell><cell>30.7</cell><cell>-</cell><cell>65.2</cell><cell>55.6</cell></row><row><cell>ENet [18]</cell><cell>74.7</cell><cell>77.8</cell><cell>95.1</cell><cell>82.4</cell><cell>51.0</cell><cell>95.1</cell><cell>67.2</cell><cell>51.7</cell><cell>35.4</cell><cell>86.7</cell><cell>34.1</cell><cell>-</cell><cell>68.3</cell><cell>51.3</cell></row><row><cell>LMD-11</cell><cell>89.2</cell><cell>86.4</cell><cell>93.7</cell><cell>83.8</cell><cell>58.1</cell><cell>95.4</cell><cell>79.3</cell><cell>52.7</cell><cell>48.6</cell><cell>90.5</cell><cell>61.6</cell><cell>-</cell><cell>76.3</cell><cell>63.5</cell></row><row><cell>LMD-12</cell><cell>88.1</cell><cell>86.8</cell><cell>94.0</cell><cell>84.3</cell><cell>55.4</cell><cell>90.1</cell><cell>80.1</cell><cell>51.9</cell><cell>48.4</cell><cell>92.3</cell><cell>64.7</cell><cell>83.9</cell><cell>76.7</cell><cell>62.2</cell></row><row><cell>LMD-12 (pre-tr.)</cell><cell>89.3</cell><cell>87.9</cell><cell>94.1</cell><cell>87.0</cell><cell>63.7</cell><cell>91.2</cell><cell>86.0</cell><cell>55.2</cell><cell>54.8</cell><cell>93.9</cell><cell>67.0</cell><cell>85.4</cell><cell>79.6</cell><cell>65.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient-enhancing conversion for illumination-robust lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1083" to="1094" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple robust road lane detection algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamzuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Mazlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent and Advanced Systems (ICIAS)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lane detection using color-based segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-lane detection in urban driving environments using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
