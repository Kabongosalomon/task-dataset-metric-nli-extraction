<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoDiM: Learning with Noisy Labels via Contrastive Semi-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<email>xin.zhang@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Xiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Han</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CoDiM: Learning with Noisy Labels via Contrastive Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Labels are costly and sometimes unreliable. Noisy label learning, semi-supervised learning, and contrastive learning are three different strategies for designing learning processes requiring less annotation cost. Semi-supervised learning and contrastive learning have been recently demonstrated to improve learning strategies that address datasets with noisy labels. Still, the inner connections between these fields as well as the potential to combine their strengths together have only started to emerge. In this paper, we explore further ways and advantages to fuse them. Specifically, we propose CSSL, a unified Contrastive Semi-Supervised Learning algorithm, and CoDiM (Contrastive DivideMix), a novel algorithm for learning with noisy labels. CSSL leverages the power of classical semi-supervised learning and contrastive learning technologies and is further adapted to CoDiM, which learns robustly from multiple types and levels of label noise. We show that CoDiM brings consistent improvements and achieves state-of-the-art results on multiple benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep learning methods with annotated label supervision have achieved great success in recent years <ref type="bibr" target="#b17">(He et al. 2016;</ref><ref type="bibr" target="#b30">Tan and Le 2019)</ref>, whereas obtaining high-quality label annotations is usually difficult due to constraints on time and labor cost, or the lack of domain knowledge <ref type="bibr" target="#b9">(Cheplygina, de Bruijne, and Pluim 2019)</ref>. Many alternative efforts have been made to detour such expensive processes by developing automated labeling techniques or mining large-scale data with labels through web searching, introducing label noise inevitably, and thus leading models to learn from bias. Furthermore, recent studies have claimed the severity of the over-fitting problem of deep neural networks caused by noisy label bias , which downgrades the model performance significantly. All of these suggest the necessity and importance to develop methods that could Learn with Noisy Labels (LNL). Enormous researches have been studied to deal with noisy labels. Inspired by recent improvements achieved by Semi-Supervised Learning (SSL) techniques <ref type="bibr">(Berthelot et al. 2019b,a;</ref><ref type="bibr" target="#b29">Sohn et al. 2020)</ref>, some methods (Arazo et al. <ref type="bibr">*</ref> These authors contributed equally. Work done when Xin and Zixuan worked as interns at Tencent AI Lab. ? Corresponding Author 2019; <ref type="bibr" target="#b21">Li, Socher, and Hoi 2020)</ref> address the potential of designing LNL algorithms in an iterative noise detection &amp; semi-supervised learning manner. However, performances of these methods will downgrade under scenarios with high ratio label noise. Recently, Contrastive Learning (CL) approaches <ref type="bibr" target="#b4">(Chen et al. 2020a;</ref><ref type="bibr" target="#b16">He et al. 2020;</ref><ref type="bibr">Chen et al. 2020b,c)</ref> have shown great potential on learning good representations by learning a feature extractor and a projector where in projection space, similar samples will be closer while dissimilar samples will be far apart. Seeing its potential on feature learning, some methods try to utilize contrastive learning to help to learn with high ratio noisy labels, by using it to learn a good network initialization <ref type="bibr" target="#b40">(Zheltonozhskii et al. 2021)</ref> or an unsupervised pre-trained label corrector <ref type="bibr" target="#b38">(Zhang and Yao 2020)</ref>. Nevertheless, such methods fail to further utilize contrastive learning techniques. This is mainly due to the lack of exploration on designing and evaluating methods that could better combine CL and SSL together. Furthermore, better ways to strengthen SSL-style LNL methods with contrastive learning techniques need to be explored.</p><p>In this work, we present CSSL, a simple yet general Contrastive Semi-Supervised Learning algorithm, and CoDiM, a novel learning with noisy labels framework combining the advantages of contrastive learning and SSL-style LNL methods in a more harmonious way. The overall framework of these two algorithms are illustrated in <ref type="figure">Fig. 1</ref>. The key contributions of our work are:</p><p>? We design a new algorithm named CSSL, which has a self-supervised pre-training phase and a sequential jointly contrastive and semi-supervised learning phase via multi-task learning and address its effectiveness on providing extra consistency regularization.</p><p>? We adapt CSSL to CoDiM with several simple yet critical modifications inspired by the state-of-the-art LNL algorithm. We further address the advantage of maintaining a self-supervised/supervised contrastive learning regularization when learning with noisy labels.</p><p>? Experimentally, we show that CSSL can bring improvements through learning better representations on semisupervised learning tasks. We further present extensive experimental results on multiple synthetic and real-world noisy label learning benchmarks and show that CoDiM arXiv:2111.11652v1 <ref type="bibr">[cs.</ref>LG] 23 Nov 2021</p><p>achieves state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Contrastive Learning</head><p>Contrastive learning approaches directly regularize the representation space by pushing representations of different views of the same image closer and spreading representations of views from different images apart. Contrastive learning requires randomly augmented views of source data to construct new data pairs. In an unsupervised manner, some methods treat different views from the same source as positive pairs, and views from different sources as negative pairs <ref type="bibr" target="#b4">(Chen et al. 2020a)</ref>. In a supervised way, with label supervision, views from the same class will be seen as positive pairs, and views from different classes will be regarded as negative pairs <ref type="bibr" target="#b19">(Khosla et al. 2020)</ref>. It is non-trivial to apply contrastive learning. First, stochastic augmentation for different views of samples is necessary and crucial to the performance. Second, trivial solutions of the optimization problem should be avoided through using large batches of negative samples, momentum encoder <ref type="bibr" target="#b13">Grill et al. 2020)</ref> or stop-gradient scheme (Chen and He 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-Supervised Learning</head><p>Semi-supervised learning tries to utilize unlabeled data via self-training to achieve better performance. Typical semi-supervised learning methods perform self-training by pseudo-labeling unlabeled data and design extra regularization objectives. Two classes of regularization are mainly pursued and proved to be useful: consistency regularization <ref type="bibr" target="#b32">(Tarvainen and Valpola 2017)</ref> and entropy minimization <ref type="bibr" target="#b12">(Grandvalet and Bengio 2004)</ref>. The former encourages the model to generate consistent predictions on source data and randomly augmented views. The latter guides the model to output low-entropy predictions with confidence. Recently, MixMatch <ref type="bibr" target="#b2">(Berthelot et al. 2019b)</ref> incorporates MixUp augmentations ) and proposes a unified framework containing both of these regularizations. Following its success, UDA , ReMixMatch <ref type="bibr" target="#b1">(Berthelot et al. 2019a) and</ref><ref type="bibr">FixMatch (Sohn et al. 2020)</ref> proposes to use weakly augmented images to produce labels and enforce consistent predictions against strongly augmented samples through different designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning with Noisy Labels</head><p>Many studies focus on reducing the effect of noise and generalizing from the correct label. On one hand, some methods explore ways to apply loss correction by estimating noise transition matrix <ref type="bibr" target="#b28">(Patrini et al. 2017;</ref><ref type="bibr">Goldberger and Ben-Reuven 2016)</ref>, re-weighting samples by designing criterions such as small-loss <ref type="bibr" target="#b14">Han et al. 2018)</ref> and prediction disagreement , or directly applying regularization through early-stop strategy ). On the other hand, some methods focus on correcting wrong labels by learning class prototypes <ref type="bibr" target="#b15">(Han, Luo, and Wang 2019)</ref>, predicting pseudo labels, or treating labels as learnable latent variables <ref type="bibr" target="#b31">(Tanaka et al. 2018;</ref><ref type="bibr" target="#b34">Yi and Wu 2019)</ref>. Recently, DivideMix  proposes to learn with noisy labels in a semi-supervised learning manner and achieves impressive performance. It detects the noisy samples by fitting a Gaussian Mixture Model (GMM) with the training loss, regards them as unlabeled samples, and applies modified MixMatch. DM-AugDesc ) further explores augmentation strategies to boost DivideMix. Also, some approaches attempt to leverage self-supervised pre-trained representation encoder through contrastive learning. REED <ref type="bibr" target="#b38">(Zhang and Yao 2020)</ref> tries to use it as the initial label corrector. C2D <ref type="bibr" target="#b40">(Zheltonozhskii et al. 2021</ref>) evaluates its effectiveness to initialize the model for different LNL methods such as DivideMix and ELR+ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Empowering Semi-Supervised Learning with Contrastive Learning</p><p>We first introduce general contrastive learning and semisupervised learning algorithms and then propose CSSL, which combines the advantages of CL and SSL together.</p><p>Contrastive Learning (CL) We introduce two algorithms here. Both self-supervised contrastive learning (SelfCon) and supervised contrastive learning (SupCon) algorithms have a stochastic augmentation function A cntr (.) and train a model M CL = {P roj(F 1 (.))|? f eat,CL , ? proj } with a feature extractor F 1 (.) parameterized by ? f eat,CL and a projector P roj(.) parameterized by ? proj . WLOG, for a sample x,x = A cntr (x) is a randomly augmented view of x, the feature extractor will map x to a representation vector r = F 1 (x) ? R Dr , where D r is the dimension of representation space, and the projector will map r to a vector z = P roj(r) ? R Dz . Given a bunch of K pairs of data with label {d k , l k } K k=1 , both algorithms need 2K augmented pairs {d b ,l b } 2K b=1 for training, whered 2k andd 2k?1 are two different augmented views of d k through A cntr (.) andl 2k = l 2k?1 = l k (k = 1...K). In other words, two views are generated for each data source. The family of contrastive loss basically follows Info-NCE loss <ref type="bibr" target="#b28">(Oord, Li, and Vinyals 2018)</ref>, which tries to maximize/minimize the mutual information of positive/negative pairs. The main difference between Self-Con and SupCon happens during loss calculation, as Self-Con will not use label supervision while SupCon will take categories into consideration. Let i ? I {1, 2, .., 2K}, C(i) I\{i}, and j(i) be the index of the other augmented view from the same source. Given the projected vector {z k } 2K k=1 , SelfCon calculates the following loss:</p><formula xml:id="formula_0">L self = ? i?I log exp(z i ? z j(i) /? ) c?C(i) exp(z i ? z c /? )<label>(1)</label></formula><p>Here ? is the temperature hyperparameter. The numerator counts for positive pairs, and the denominator contains both positive and negative pairs. Let S(i) {s ? C(i) :l s =l i }, SupCon calculates the following loss:  <ref type="figure">Figure 1</ref>: The overall framework of CSSL and CoDiM. The former deals with a Semi-Supervised Learning (SSL) task where labels of the labeled set are assumed clean, while the latter is designed to handle a Learning with Noisy Labels (LNL) problem in which a part of training samples are mislabeled. In the first phase, both CSSL and CoDiM regard the whole training set as unlabeled and learn representations by self-supervised contrastive learning (SelfCon). In the second phase, CSSL employs SelfCon loss to the unlabeled set and SupCon loss to the labeled set along with a standard SSL algorithm; CoDiM splits a possibly clean set from the noisy set based on small-loss criterion, discards labels of the noisy set and then applies SSL in a similar manner like CSSL. Note only SupCon or SelfCon loss for the clean set is considered in CoDiM. For brevity, we omit augmentation strategies and regularization rules used in the second phase.</p><formula xml:id="formula_1">L sup = i?I ?1 |S(i)| s?S(i) log exp(z i ? z s /? ) c?C(i) exp(z i ? z c /? ) (2)</formula><p>For each anchor vector z i , only the other view generated from the same source z j(i) is seen as positive in SelfCon, yet in SupCon all the other views generated from data with the same label are seen as positive.</p><p>Semi-Supervised Learning (SSL) Consider a partially-</p><formula xml:id="formula_2">labeled dataset D = {X , U}, X = {(x i , y i )} N i=1 and U = {u j } M j=1 ,</formula><p>where {x i , u j } are samples and y i ? {0, 1} C is the one-hot label vector over C classes. Semi-supervised learning algorithms solve a C-class classification task by training a model M SSL = {G(F 2 (.))|? f eat,SSL , ? cls } with a feature extractor F 2 (.) parameterized by ? f eat,SSL and a cascaded classifier G(.), parameterized by ? cls . Many successful SSL algorithms try to exploit unlabeled data with consistency regularization, entropy minimization, and randomized augmentation. Specifically, let Semi Alg(X , U, H, F) be the chosen semi-supervised learning algorithm, where H and F are the set of hyperparameters and functions. For each training epoch, it tries to generate an augmented labeled set X = {x i , p i } i=1,2,... and an unlabeled set U = {u j , q j } j=1,2,... , here p i , q j refers to processed labels. Let A semi (.) be a stochastic augmentation function. Then, it will minimize the following objectives:</p><formula xml:id="formula_3">L Semi = Lx 1 |X | x,p?X H(p, p M SSL (A semi (x))) + ? u ? Lu 1 |U | u,q?U H(q, p M SSL (A semi (u))) +? r L reg .<label>(3)</label></formula><p>Note, to measure entropy between processed labels and model's predictions, Cross-Entropy (CE) loss and L2-Loss are commonly uesd.</p><p>Contrastive Semi-Supervised Learning (CSSL) Now we study how to combine CL and SSL into one unified algorithm. We propose a general multi-task learning algorithm (Alg.1) that employs SupCon to utilize label supervision of the labeled set, and uses SelfCon in two ways: 1) to provide self-supervised representation learning (a.k.a. SelfCon pretraining) on the whole dataset before multi-task learning; 2) to keep learning self-supervised features from the unlabeled set during the multi-objective optimization. Because the optimization objectives of CL and SSL are different, we use a model M CSSL = {G(F (.)), P roj(F (.))|? f eat , ? cls , ? proj } with two different heads G(.) and P roj(.), and one feature extractor F (.) = F 1 (.) = F 2 (.), by sharing the weights of ? f eat,SSL and ? f eat,CL . For the sake of generality, we slightly abuse the notation of Semi Alg and wrap up hyperparameters and functions used for SSL algorithm with H, F (e.g. ? u , ? r ? H and A semi (.) ? F). Thus, one advantage of CSSL is that many popular SSL algorithms (e.g. Mixmatch, ReMixMatch, and Fixmatch) can be directly plugged in and contributed as the SSL module without inner modification at all. We apply SupCon/SelfCon to data batches from labeled/unlabeled set to match the style of SSL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CL Introduces Extra Consistency Regularization</head><p>A key factor to the success of semi-supervised learning methods is to pursue consistency regularization, which encourages the model outputs same predictions for input with small perturbation. Concretely, recall the formula that calculates L Semi , we can see consistency regularization has been implicitly enforced as for a processed sample x or u, the algo-Algorithm 1: A Multi-task Contrastive Semi-Supervised Learning Algorithm with Pre-training.</p><p>Input: Dataset D = {X , U}, pre-training and multi-task training steps N 1 , N 2 , Supcon and SelfCon loss weight ? sup , ? self , temperature ? 1 , ? 2 , ? 3 , SSL used hyper-Paramer set H, SSL used function set F 1: while t &lt; N 1 do // SelfCon pre-training 2:</p><p>Draw raw data batchD B from D // Ignore labels 3:</p><formula xml:id="formula_4">D B,1 = A cntr (D B ) // Different views 4:D B,2 = A cntr (D B ) // As A cntr (.) is stochastic 5: L self = Self Con(D B,1 ,D B,2 , ? 1 ) 6: ? f eat , ? proj = SGD(L self , ? f eat , ? proj ) 7: end while 8: while t &lt; N 2 do // Multi-task learning 9: Draw data batch {X B ,? B },? B from X , U 10: for j = 1, 2 do 11:X B,j = A cntr (X B ) 12:? B,j = A cntr (? B ) 13: end for 14: L self = Self Con(? B,1 ,? B,2 , ? 2 ) 15: L sup = SupCon(X B,1 ,X B,2 ,? B , ? 3 ) 16: L Semi ,X B ,? B = Semi Alg(X B ,? B ,? B , H, F) //X B ,? B are intermediate augmented data 17: L = L semi + ? sup L sup + ? self L self 18: ? f eat , ? proj , ? cls = SGD(L, ? f eat , ? proj , ? cls ) 19: end while</formula><p>rithm will try to minimize the entropy between the processed labels and predictions of its augmented views, i.e. A semi (x) or A semi (u). Recently, a study <ref type="bibr" target="#b33">(Wei et al. 2020</ref>) proposes a unified theoretical analysis on this kind of self-training with constructed consistency regularization, by assuming expansion effect. Specifically, let P i be the data distribution conditioned on class label i. For a small subset S of samples labeled i, expansion effect assumes that,</p><formula xml:id="formula_5">P i (neighbourhood of S) ? cP i (S)<label>(4)</label></formula><p>Here, c &gt; 1 is the expansion factor, and the neighbourhood of S is defined to introduce data augmentation. Generally speaking, the neighbourhood of S can be sampled by applying a stochastic augmentation function to samples in S. This expansion assumption indicates that data distribution within each class has good continuity. With this assumption, consistency regularization can be defined as:</p><formula xml:id="formula_6">R(G(F )) = E x max neighbor x 1(G(F (x )) = G(F (x))) (5)</formula><p>Contrastive learning methods also use randomized data augmentation techniques to produce 'weak supervision'. Researches have empirically provided positive evidences that representation encoder can also benefit from such supervision even if using a non-linear MLP P roj ?proj (.). We suggest that this kind of 'weak supervision' also implicitly implies consistency regularization to the representation space by regularizing the weights of the feature extractor(as illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>). On one hand, SelfCon builds 'self-supervision' for different views from the same sample. Recalling the SelfCon loss, for a sample d, suppos? d 1 ,d 2 are two different augmented views of d and let</p><formula xml:id="formula_7">z 1 = P roj ?proj (F ? f eat (d 1 )),? 2 = P roj ?proj (F ? f eat (d 2 ))</formula><p>, an easy way to minimize the distance between? 1 and? 2 is to encourage the feature extractor learn to map a sample and its neighbour in data space to similar representation, i.e,</p><formula xml:id="formula_8">r 1 = F ? f eat (d 1 ) andr 2 = F ? f eat (d 2 )</formula><p>should be similar. On the other hand, SupCon further tries to cluster data from the same class in the projected space, which further encourages the feature extractor to learn a more continuous representation conditioned on class label. This empirically lead the model to better fit expansion property and have more consistent predictions on augmented samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CSSL with Noisy Labels</head><p>We adapt CSSL to solve LNL tasks by first introducing some key designs to leverage SSL algorithms, and then propose CoDiM for LNL tasks with several simple yet critical modifications inspired by DivideMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning with Noisy Labels via SSL Consider a set of training data with label noiseD</head><formula xml:id="formula_9">= {(x i ,? i )} N i=1 wher? y i ? {0, 1}C</formula><p>, our target is to learn from correct label supervision as well as avoid over-fitting incorrect labels without any prior knowledge of noise distribution. Here we only consider the case that no extra clean labeled data are accessible, and the only guarantee is that for each class i, the population of samples with correct labels are larger than any other population of samples that should have correct labels j ? {1, ...,C}\{i} but are incorrectly labeled with i. Let C, I be the set of samples inD that are correctly/incorrectly labeled, if knowing the partition of C and I, one can solve it as a semi-supervised learning task. However, it's challenging to specify the partition. A direct extension is to design a two-stage algorithm which tries to decide a partition that di- Draw raw data batchD B from D // Ignore labels 3:</p><formula xml:id="formula_10">D B,1 = A simc (D B ) // Different views 4:D B,2 = A simc (D B ) 5: L self = Self Con(D B,1 ,D B,2 , ? 1 ) 6: ? f eat , ? proj = SGD(L self , ? f eat , ? proj ) 7: end while 8: ? (1) = {? (1) f eat , ? (1) proj , ? (1) cls } = {? f eat , ? proj ,r(? cls )} 9: ? (2) = {? (2) f eat , ? (2) proj , ?<label>(2)</label></formula><p>cls } = {? f eat , ? proj ,r(? cls )} // Randomly initialized ? cls 10: ? (1) , ? (2) =WarmUp(D, ? (1) , ? (2) , N 2 ) // Initialize &amp; WarmUp classifier 11: while e &lt; E do // Learning with label noise 12:</p><formula xml:id="formula_11">C (1) , I (1) =GMM(D, ? (2) ) 13: C (2) , I (2) =GMM(D, ? (1) ) 14:</formula><p>while iter &lt; num iter do 15:</p><p>for j = 1, 2 do 16:</p><formula xml:id="formula_12">Draw data batch {X B ,? B } from C (j) 17: Draw data batch {? B } from I (j) 18:X B,1 = A sa (X B ),X B,2 = A sa (X B ) 19:</formula><p>if mode is SelfCon do 20:</p><p>L cl = Self Con(X B,1 ,X B,2 , ? 2 ) end while 30: end while videsD intoC and? via a noise detection module, and then apply an SSL algorithm on {C,?}. Since the noise detection task can be regarded as a clean-or-noisy classification task, the noise detection module typically has a module which measures the dataset and decide a partition thersholdR(T ). A commonly-used measure is to choose samples with lower training loss based on the SSL classifier. To better leverage this measure, warming-up the classifier by training with traditional CE-loss for a few epochs is also a good choice.</p><p>Adapting CSSL to CoDiM Generally speaking, CoDiM also serves as a two-phase algorithm, which is specified in Alg.2 (for brevity, details of SSL are summarized as H and F). In the first phase, SelfCon pre-training will be applied using all data ignoring labels, and then a very short 'warming up' using CE-loss will be used as the initialization of the classifier head. In the second phase, CoDiM first decides the partition imitating DivideMix via fitting a GMM model to choose samples with lower classification loss as clean samples. Then it will apply contrastive semi-supervised learning based on the partition, taking the modified MixMatch algorithm used in DivideMix as the SSL module. However, one critical change is that here CoDiM only apply Sup-Con or SelfCon to the possibly clean set as we find that keep applying SelfCon to the possibly noisy set will downgrade the performance. Also, when dealing with high ratio label noise or noise among similar classes, we suggest to replace SupCon with SelfCon to learn from possibly clean set to further avoid learning from biases. We follow Di-videMix to use 'co-divide', which uses two networks, and for each iteration, one network use the partition threshold decided by the other. Also, we find that other customized techniques proposed by DivideMix (e.g. label co-guessing and co-refinement) can be maintained here. Following the 'AugDesc-WS' augmentation strategy , we use so-called 'weak augmentation' (random crop and flip) to generate views for querying prediction, use so-called 'strong augmentation' (AutoAugment) to generate views for gradient descent. We further note in pre-training phase, we use augmentation functions proposed by SimCLR <ref type="bibr" target="#b4">(Chen et al. 2020a)</ref> in SelfCon to get better pre-training results, and use same 'strong augmentation' used in 'AugDesc-WS' in SupCon/SelfCon during the second phase to reduce the computation cost and in some sense reduce the difficulty of the optimization problem, which are both critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset and Experimental Setup</head><p>We conduct multiple experiments on CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b20">(Krizhevsky, Hinton et al. 2009</ref>) for SSL tasks and LNL problems. The two datasets both contain 50K training and 10K test images of size 32 ? 32 from 10 and 100 classes, respectively. Following previous work , we use PreAct Resnet18 as the feature extractor. We first examine the performance of our CSSL algorithm on CIFAR-10 under two ratios of labeled samples (20% and 80%, respectively). We then evaluate CoDiM on learning with different types and levels of synthetic label noise. Two types of label noise: symmetric and asymmetric are tested. Symmetric noise is produced by selecting a percentage of the training data and assigning them uniformly random labels. Asymmetric noise is generated to simulate real world noise, where only the labels of similar classes will be assigned. We then apply CoDiM on ANIMAL-10N (Song, Kim, and Lee 2019) and WebVision , two datasets with real world label noise. ANIMAL-10N contains 5 pairs of confusing animals with 55K noisy human-labeled online images in total. The noisy label ratio is about 8%. We use VGG19 backbone to stay consistent with previous work. WebVision contains 2.4M images collected by searching the 1,000 concepts in ImageNet ILSVRC12 on the Internet. For fair comparison, we use the inception-resnet v2 to evaluate the first 50 classes of the Google image subset. More implementation details are described in Appendix A. <ref type="bibr">Dataset</ref> CIFAR-10 Methods/labeled ratio 20% 80% w/o SelfCon pre-training SSL <ref type="figure">(Fig 3.a)</ref> Best 89.3 96.2 Last 89.0 96.0 SSL-L(Self)-U(Self) Best 89.1 96.2 Last 88.9 96.0 SSL-L(Sup)-U(Self) Best 91.7 96.6 <ref type="figure">(Fig 3.b)</ref> Last 91.5 96.4 w/ SelfCon pre-training SSL <ref type="figure">(Fig 3.c)</ref> Best 94.0 96.8 Last 93.9 96.7 SSL-L(Self)-U(Self) Best 94.6 96.6 Last 94.4 96.5 SSL-L(Sup)-U(Self)</p><p>Best 94.7 96.9 <ref type="figure">(Fig 3.d</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>) (CSSL)</head><p>Last 94.4 96.8 SelfCon Pre-training and Contrastive Metrics Improves Performance of SSL Note that though being an algorithm that is compatible with many SSL algorithms, here we only evaluate a certain realization of CSSL, which uses a modified version of Mix-Match used in DivideMix as the SSL module, as such algorithm will also be used by CoDiM. We report the results in <ref type="table" target="#tab_2">Table 1</ref> and address two key observations. First, SelfCon pre-training improves the performance of SSL, especially when the labeled ratio is low, as accuracy of all methods with SelfCon pre-training boost 0.3%-0.6% given 80% label and 3%-5% given 20% label. This also supports the discovery that SelfCon pre-training provides more robust results when dealing with high ratio label noise. Secondly, contrastive learning helps the performance of the classifier, as CSSL always outperforms basic SSL algorithms in both cases. This empirically supports our claim that contrastive learning will further provide consistency regularization. We also show that methods that leverage contrastive learning tend to have more clustered representations via showing t-SNE visualization of data representations of test set, certain experimental cases. Besides comparing the performance of CSSL and modified MixMatch on CIFAR-10, we also apply ablation studies to show the effect of each extension contained in CSSL, which can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoDiM on Noisy CIFAR-10 and CIFAR-100</head><p>We compare two versions of CoDiM, depending on which contrastive learning algorithm are used in the second phase, namely CoDiM-Sup(use SupCon) and CoDiM-Self(use SelfCon) with other baseline methods on CIFAR-10 and CIFAR-100 with different levels and types of label noise. Results gained by proposed methods, important baselines, and two ablation studies are shown in <ref type="table" target="#tab_1">Table 2</ref>. We see that in all cases CoDiM achieves state-of-the-art performances.</p><p>In the symmetric case, while the noise ratio is not extremely high, CoDiM-Sup outperforms other methods. However, CoDiM-Self shows competitive performances under high ratios of symmetric and asymmetric noise. This suggests that CoDiM combines online contrastive learning with semisupervised learning in a simple yet better way. More ablation studies and visualizations results are in Appendix C and F. 95.0 80.9 77.9 67.0 54.9 <ref type="table" target="#tab_1">Table 2</ref>: Comparison with existing methods on CIFAR-10/100 with different noise settings. We re-implement C2D and REED here. Note CoDiM-bare can be regarded as a combination of SelfCon pre-training and then apply DM-AugDesc. CoDiM-CSSL apply SupCon on C and SelfCon on I following CSSL. Full table of results can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoDiM on Real World Noisy Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Acc SELFIE (Song, Kim, and Lee 2019) 81.8 PLC  83.4 Nested Co-teaching <ref type="bibr" target="#b8">(Chen et al. 2021)</ref> 84.1 DivideMix (w/o ImageNet pre-training) 85.8 DM-AugDesc  86.0 DivideMix  88.8 C2D <ref type="bibr" target="#b40">(Zheltonozhskii et al. 2021)</ref> 88.9 DM-AugDesc  89.1 CoDiM-bare 89.1 CoDiM-Sup 89.2 CoDiM-Self 89.4 <ref type="table" target="#tab_3">Table 3</ref>: Comparison with existing methods on ANIMAL-10N. We re-implement methods starting from DivideMix.</p><p>set, while CoDiM-Self has the best generalization performance on the ILSVRC12 validation set. As C2D has shown the advantages of applying SelfCon pre-training, our methods show the performance engagement on large-scale realworld noisy datasets by further providing contrastive regularization, designing suitable augmentation strategies, and combine all of these techniques in a harmonious way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we unify recent efforts on combining cuttingedge semi-supervised learning, contrastive learning, and noisy label learning together. We propose CSSL and  CoDiM, which leverage contrastive learning not only to provide self-supervised pre-training but also to further provide consistency regularization besides classical semi-supervised learning processes. We evaluate our methods through extensive experiments on multiple benchmarks across many datasets and show that CoDiM steadily outperforms stateof-the-art methods. Through this work, we address the new possibilities to combine popular methods in different weakly supervised learning fields together and will then explore new ways to accelerate and strengthen the fusion of these methods as our future targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10/100</head><p>In the first phase, we use an 18-layer PreAct Resnet as the network backbone with a 2-layer projection head. The dimensions of hidden and output layers of the projection head are both 256. The input size is 32 ? 32. SimCLR is used to conducted self pre-training. The model is optimized using SGD with a batch size of 512. The weight decay and momentum are set as 0.0005 and 0.9, respectively. We train the model for 800 epochs. In the first 10 epochs, the learning rate gradually increases from 0 to 0.06 and then decreases to 0 at the last epoch in a Cosine Annealing manner. The temperature ? 1 when computing the contrastive loss is set to 0.5. Following DivideMix  and AugDesc-WS , we keep most parameters in the second phase unchanged. The backbone and the projection layers are initialized with pre-trained parameters in the first layer, and a 2-layer classification head is randomly initialized. The whole model is firstly warmed up for 10 epochs for CIFAR-10 (except that we set it to 1 for 'CIFAR-10 with 90% symmetric noise') and 30 epochs for CIFAR-100, with a batch size of 128 and then optimized with Self-Con/SupCon loss and SSL loss with a batch size of 512. The temperature ? 2 of SelfCon is 0.5 and the temperature ? 3 of SupCon is 0.07. The weak augmentation involves random crop and horizontal flip. The strong augmentation used is AutoAugment following AugDesc-WS. The initial learning rate is 0.02 for all settings except that for settings of 'CIFAR-10 with 90% symmetric noise' and 'CIFAR-100 with 90% symmetric noise', the initial learning rate is 0.002. The total training epochs are 300 for 'CIFAR-10 with 20% and 50% symmetric noise', 350 epochs for 'CIFAR-10 with 80% and 90% symmetric noise, and 40% asymmetric noise', and 400 epochs for all 'CIFAR-100' experiments. The learning rate drops to 10% of the original value when running for roughly half of the total epochs. The SelfCon/SupCon loss weight is set to 1 for all experiments except 0.1 for 'CIFAR-10 with 80% symmetric noise' and 0.01 for 'CIFAR-10 with 40% asymmetric noise'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WebVision</head><p>In the first phase, we use an inception-resnet v2 as the network backbone with a 2-layer projection head. The dimensions of hidden and output layers of the projection head are both 256. The input size is 299 ? 299. SimCLR is used to conducted self pre-training. The model is optimized using SGD with a batch size of 256. The weight decay and momentum are set as 0.0005 and 0.9, respectively. The model is trained for 300 epochs. The learning rate increases from 0 to 0.01 in the first 10 epochs and decreases to 0 eventually via Cosine Annealing. The temperature ? 1 when computing the contrastive loss is set to 0.5. In the second phase, the model is initialized similarly to that of CIFAR-10/100 experiments. The model is warmed up for 1 epoch with a batch size of 64, then optimized in a multitask way with a batch size of 32. The temperature ? 2 of SelfCon is 0.5 and the temperature ? 3 of SupCon is 0.07. The total number of epochs is 100. The initial learning rate is 0.01 and decreased to 0.001 at the 50th epoch. We apply SupCon loss on the clean subset with the weight of 0.1. The weak augmentation includes resize, random crop and horizontal flip. The strong augmentation used is AutoAugment following AugDesc-WS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Animal-10N</head><p>We apply VGG19 with batch normalization as the network backbone with a 2-layer projection head. The dimensions of hidden and output layers of the projection head are both 256. The input size is 64 ? 64. SimCLR is used to conducted self pre-training. The model is optimized using SGD with a batch size of 1024. The weight decay and momentum are set as 0.0005 and 0.9. The model is trained for 300 epochs. The learning rate increases from 0 to 0.12 in the first 10 epochs and decreases to 0 in the end by Cosine Annealing. The temperature ? 1 when computing the contrastive loss is set to 0.5. In the second phase, the model shares the pre-trained parameters as initialization. The model is warmed up for 5 epochs with a batch size of 256, then optimized in a multitask manner with a batch size of 128. The temperature ? 2 of SelfCon is 0.5 and the temperature ? 3 of SupCon is 0.07. The model is trained for 100 epochs with an initial learning rate of 0.01. The learning rate is divided by 5 at the 50-th and 75-th epochs, respectively. SupCon loss is computed on the clean subset and the weight is set to 1. The weak augmentation contains random crop and horizontal flip. The strong augmentation used is AutoAugment following AugDesc-WS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B: Full ablation studies on CSSL</head><p>In <ref type="table" target="#tab_6">Table 5</ref>, we show the full results of experiments conducted in main paper ( <ref type="table" target="#tab_2">Table 1</ref> in main paper). Besides the results shown in main paper, we test three more alternatives (only apply SupCon/SelfCon on labeled set, or only apply SelfCon on unlabeled set) to combine contrastive learning with semi-supervised learning. Our first key observation is that all trials benefit from leveraging SelfCon pre-training. Note that the improvements are much more obvious when given less labeled samples. Our second observation is that, CSSL achieves the best given 20% labeled samples and best runner-up given 80% labeled samples, which provides competitive performances on different levels of labeled ratio. Our third observation is that when the labeled ratio is high, it's actually useful to only apply SupCon on the labeled set, as it achieves the best given 80% of the labels. This also empirically supports our findings that CoDiM-Sup can acquire improvements under a low ratio of label noise. Also, we can see that, when given fewer labels, it's beneficial to apply SelfCon on the unlabeled set. However, when dealing with label noise is that, as the result of GMM can not  <ref type="table">)</ref>. S(alg) denotes applying alg on samples drew from set S in the second phase. S ?{Labeled,Unlabeled}, alg ? {SelfCon, SupCon}. We find that CSSL achieves competitive performance on different levels of labeled ratio. We also find that when given many labels, only applying SupCon to labeled set is useful, while given less labels, it's good to apply SelfCon to unlabeled set.</p><p>Here bold number means the best one, and underlined number means the best runner-up one in each setting.</p><p>fully specify label noise, it's actually harmful to further apply SelfCon on a possibly noisy set (As shown in <ref type="table" target="#tab_1">Table 2</ref>, main paper).</p><p>C: Ablation studies on CIFAR-10/100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choices of pre-training and network architecture</head><p>In this section, we evaluate the effects of using different pre-trainings and network architectures. We conduct experiments under two settings: CIFAR-10 with 50% symmetric noise and 40% asymmetric noise. We evaluate two network backbones: PreAct Resnet-18 and Basic Resnet-18, which are both common choices when mining from CIFAR-10. We also evaluate 3 pre-training options: using SelfCon pretraining, ImageNet pre-training, and no pre-training. Since public ImageNet pre-training for PreAct Resnet-18 is not accessible, we only evaluate this option on the basic Resnet-18 backbone. As C2D is simply to leverage SelfCon pretraining before applying DivideMix, we use it to notify this setting. <ref type="table">Table 6</ref> shows the results of these experiments. Note here we are not interested in the average gap between using PreAct Resnet-18 and Basic Resnet-18, and only want to see the differences between experiment pairs when only one option is adjusted. Firstly, we find that our methods achieve relatively best and consistent performance when using the same backbone, as CoDiM-Sup wins in 50% symmetric noise case and CoDiM-Self wins in 40% asymmetric noise case. Secondly, we find that using SelfCon pre-training is always a better choice, regardless of which backbone is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label correction of 90% noise ratio on CIFAR-100</head><p>In this section, we notify a customized label correction step used when dealing with 90% symmetric noise on CIFAR-100. Note this technique is applied to C2D as well, in order to make fair comparisons. Also, as REED already contains a more complex label correction stage, we follow the original setting when re-implementing REED. The idea of this label correction step is simple. During the 'warmingup' stage (just after pre-training using SelfCon), we copy and fix the pre-trained weights to the feature extractor, and train the classifier head with traditional CE-loss using all data with noisy labels for 100 epoch, using SGD optimizer with a learning rate of 0.005. The weight decay and momentum are set as 0.0005 and 0.9, respectively. Then, we directly utilize the predictions of the classifier and change all the labels to the class which the classifier outputs with the largest probabilities. Then, we randomly re-set the weight of the classifier head and start the second phase of CoDiM or other algorithms like DivideMix. Note here we do not re-set the weights used in feature extractor, but make it changeable again. We evaluate the effect of this small process, and show the result in <ref type="table" target="#tab_8">Table 8</ref>. We note that this label correction step is crucial to our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation studies on augmentation strategies and other technologies</head><p>In this section, we provide more results of ablation studies on CIFAR-10 and CIFAR-100, as shown in <ref type="table" target="#tab_7">Table 7</ref>. We see that though altering augmentation strategies of CoDiM-Sup can provide even better results, CoDiM-Sup provides more robust and competitive results across all cases. Note here we see that under relatively low ratio of symmetric noise, using different augmentation strategies might be a good choice, this indicates more efforts are needed on specifying better augmentation strategies.</p><p>D: Full Comparison with existing methods on CIFAR-10/100.</p><p>In this section, we provide the full version table <ref type="table" target="#tab_2">(Table 10)</ref> showing the results on CIFAR-10 and CIFAR-100 (as noticed in <ref type="table" target="#tab_1">Table 2</ref>, main paper). Note that we already provide the most recent and important baselines in the main paper. We also provide more results on 40% asymmetric noise on CIFAR-10 setting in <ref type="table">Table 9</ref>.   <ref type="table">Table 9</ref>: Comparison with existing methods on CIFAR-10 with 40% asymmetric noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F: more t-SNE results</head><p>Here we provide t-SNE visualizations of learning with noisy labels for different experimental settings on the test set of CIFAR-10. We first show visualizations of self-supervised pre-training. Then CoDiM-Sup-bare (CoDiM-Sup without self pre-training), CoDiM-Self and CoDiM-Sup are shown on settings of '20% symmetric noise, 50% symmetric noise, 80% symmetric noise, 90% symmetric noise, and 40% asymmetric noise', respectively. We can basically observe that experiments on high-ratio noise benefit more from self pre-training. Even with the noise ratio as high as 90%, the models learned by our method still cluster test samples well. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>An example on how contrastive learning introduces extra consistency regularization. Models maintain consistent predictions for representations in orange manifolds. Both SelfCon and SupCon encourage representation space to maintain consistency in the batch. Given data batches drew from dataset, SelfCon only learn a small consistent manifold around each sample based on 'selfsupervision'. SupCon can learn a more continuous manifold as samples with same label can be clustered together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>LL</head><label></label><figDesc>cl = SupCon(X B,1 ,X B,2 ,? B , ? 2 ) Semi ,X B ,? B =D MixMat(X B ,? B , H, F) 25: // D M ixM at refers to the used SSL method 26: L = L semi + ? cl L cl 27: ? (j) = SGD(L, ? (j) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) SSL w/o pre-training (b) CSSL w/o pre-training (c) SSL w/ pre-training (d) CSSL w/ pre-training Figure 3: t-SNE visualizations of different methods on the test set of CIFAR-10. 'Pre-training' refers to pre-train a model using SelfCon. Comparing horizontally, it is obvious that leveraging SelfCon pre-training largely promotes the clustering of each class. Comparing vertically, we can see that using SupCon regularization further sets clusters apart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigureFigure 6 Figure 9</head><label>69</label><figDesc>: t-SNE visualizations of experiments on 50% Symmetric Noise. : t-SNE visualizations of experiments on 40% Asymmetric Noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 2 :</head><label>2</label><figDesc>CoDiM: A Learning with Noisy Labels Algorithm via Contrastive Semi-Supervised Learning. Input: Dataset D = {X , Y}, SelfCon, SupCon, Warmup, training steps N 1 , N 2 , epoch E, Contrastive loss weight ? cl , temperature ? 1 , ? 2 , SimCLR and strong augmentation function A simc (.), A sa (.), D MixMat hy-perParamer set H, D MixMat function set F = {A wa (.), A sa (.), . . . }, algorithm mode mode 1: while t &lt; N 1 do // SelfCon pre-training</figDesc><table /><note>2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparing CSSL with basic SSL algorithm in terms of test accuracy. S(alg) denotes applying alg on samples drew from set S in the second phase. S ?{Labeled,Unlabeled}, alg ? {SelfCon, SupCon}.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>shows the results on the ANIMAL-10N dataset. All</cell></row><row><cell>CoDiM-style methods achieve 89%+ accuracy. These re-</cell></row><row><cell>sults clearly show the improvements gained by adding dif-</cell></row><row><cell>ferent components of CoDiM like SelfCon pre-training, aug-</cell></row><row><cell>mentation strategy, and extra contrastive learning scheme,</cell></row><row><cell>as C2D can be regarded as using SelfCon pre-training &amp;</cell></row><row><cell>DivideMix and CoDiM-bare can be regarded as SelfCon</cell></row><row><cell>pre-training &amp; DM-AugDesc. CoDiM-Self and CoDiM-Sup</cell></row><row><cell>beat basic DivideMix and DM-AugDesc by large and even</cell></row><row><cell>beat the updated versions with prior information learned</cell></row><row><cell>from extra data domain(via using model pre-trained on Im-</cell></row><row><cell>ageNet). Here CoDiM-Self achieves the state-of-the-art per-</cell></row><row><cell>formance, and we conjecture this is due to the type of noise</cell></row><row><cell>in ANIMAL-10N is closer to asymmetric noise.</cell></row><row><cell>Table 4 shows the results on WebVision. The full version ta-</cell></row><row><cell>ble can be found in Appendix E. We see that CoDiM-Sup</cell></row><row><cell>achieves the best performance on the WebVision validation</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with existing methods on WebVision.</figDesc><table><row><cell>We re-implement DM-AugDesc and C2D. We provide ref-</cell></row><row><cell>erences of some baseline methods here: Co-teaching (Han</cell></row><row><cell>et al. 2018), Iterative-CV (Chen et al. 2019), LongReMix</cell></row><row><cell>(Cordeiro et al. 2021), GJS (Englesson and Azizpour 2021).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparing CSSL with basic SSL algorithm in terms of test accuracy (full table</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation study results in terms of test accuracy (%) on CIFAR-10 and CIFAR-100.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-100</cell></row><row><cell>Noise Type &amp; Ratio</cell><cell cols="2">Sym. / 90%</cell></row><row><cell>Methods/Label correction</cell><cell>No</cell><cell>Yes</cell></row><row><cell>C2D</cell><cell cols="2">Best 39.4 40.1 Last 39.1 40.1</cell></row><row><cell>CoDiM-bare</cell><cell cols="2">Best 39.4 40.1 Last 39.1 40.1</cell></row><row><cell>CoDiM-Sup</cell><cell cols="2">Best 45.2 55.2 Last 45.0 54.9</cell></row><row><cell>CoDiM-Self</cell><cell cols="2">Best 48.6 56.4 Last 48.4 56.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The effect of label correction step used in 90% symmetric noise settings on CIFAR-100.</figDesc><table><row><cell>Method</cell><cell cols="2">Best Last</cell></row><row><cell>Cross-Entropy</cell><cell cols="2">85.0 72.3</cell></row><row><cell>F-correction (Patrini et al. 2017)</cell><cell cols="2">87.2 83.1</cell></row><row><cell>M-correction (Arazo et al. 2019)</cell><cell cols="2">87.4 86.3</cell></row><row><cell>Iterative-CV (Chen et al. 2019)</cell><cell cols="2">88.6 88.0</cell></row><row><cell>P-correction (Yi and Wu 2019)</cell><cell cols="2">88.5 88.1</cell></row><row><cell>Joint-Optim (Tanaka et al. 2018)</cell><cell cols="2">88.9 88.4</cell></row><row><cell>Meta-Learning (Li et al. 2019)</cell><cell cols="2">89.2 88.6</cell></row><row><cell>PENCIL</cell><cell>91.2</cell><cell>-</cell></row><row><cell>Distilling</cell><cell>90.2</cell><cell>-</cell></row><row><cell>REED (Zhang and Yao 2020)</cell><cell cols="2">92.4 92.3</cell></row><row><cell cols="3">DivideMix (Li, Socher, and Hoi 2020) 93.4 92.1</cell></row><row><cell>C2D (Zhang and Yao 2020)</cell><cell cols="2">93.6 93.3</cell></row><row><cell>DM-AugDesc (Nishi et al. 2021)</cell><cell cols="2">94.6 94.3</cell></row><row><cell>CoDiM-CSSL</cell><cell cols="2">94.2 94.0</cell></row><row><cell>CoDiM-bare</cell><cell cols="2">94.4 94.1</cell></row><row><cell>CoDiM-Sup (Ours)</cell><cell cols="2">95.2 95.0</cell></row><row><cell>CoDiM-Self (Ours)</cell><cell cols="2">95.5 95.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A: Implementation details Both CSSL and CoDiM contain two phases: a SelfCon pre-training and a contrastive semi-supervised learning style process, except that CoDiM maintains two networks and an iterative Gaussian Mixture Model (GMM)-based clean/noisy data separation within the DivideMix framework. Note that without loss of generality, we keep the same for the shared parameters of CSSL and CoDiM. Also for all experiments, training samples are sampled randomly without replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E: Full Comparison with existing methods on</head><p>Webvision.</p><p>Here we provide full version table of results on WebVision <ref type="table">(Table 11</ref>). Note that we already provide important baselines in the table in main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Back-CIFAR-10 Bone Noise type Sym. Asym. Methods/Noise ratio 50% 40%</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Big Self-Supervised Models are Strong Semi-Supervised Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22243" to="22255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<title level="m">Exploring Simple Siamese Representation Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Boosting Co-teaching with Compression Regularization for Label Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Suykens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2688" to="2692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Notso-supervised: a survey of semi-supervised, multi-instance, and transfer learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="280" to="296" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04173</idno>
		<title level="m">LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Englesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04522</idno>
	</analytic>
	<monogr>
		<title level="m">Goldberger, J.; and Ben-Reuven, E. 2016. Training deep neural-networks using a noise adaptation layer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing Systems</title>
		<meeting>the 17th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07394</idno>
		<title level="m">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">WebVision Database: Visual Learning and Understanding from Web Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Early-Learning Regularization Prevents Memorization of Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionalitydriven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Augmentation strategies for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hollerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8022" to="8031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Representation learning with contrastive predictive coding</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
	</analytic>
	<monogr>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5552" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03622</idno>
		<title level="m">Theoretical analysis of self-training with deep networks on unlabeled data</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<title level="m">Understanding deep learning requires rethinking generalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Decoupling Representation and Classifier for Noisy Label Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08145</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with Feature-Dependent Label Noise: A Progressive Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13646</idno>
		<title level="m">Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy Labels</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CIFAR-10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssl-L</surname></persName>
		</author>
		<title level="m">Sup)-U(Self)(CSSL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Codim-Sup</surname></persName>
		</author>
		<imprint>
			<publisher>Ours</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CIFAR-10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Bootstrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Co</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Mixup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P-Correction</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Yi and Wu</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta-Learning (</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-Correction (</forename><surname>Arazo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Dividemix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoi</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dm-Augdesc ; Nishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">We re-implement C2D and REED here. Note CoDiM-bare can be regarded as a combination of SelfCon pre-training and then apply</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<publisher>DM-AugDesc</publisher>
		</imprint>
	</monogr>
	<note>Comparison with existing methods on CIFAR-10/100 with different noise settings (full table)</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Note, we leverage the label correction step discussed in the earlier section for all experiments on 90% symmetric noise on CIFAR-100 we re-implemented except for REED</title>
		<idno>2017) 61.12 82.68 57.36 82.36</idno>
	</analytic>
	<monogr>
		<title level="m">CoDiM-CSSL apply SupCon on C and SelfCon on I following CSSL</title>
		<imprint/>
	</monogr>
	<note>Method WebVision ILSVRC12 top1 top5 top1 top5 F-correction</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Decoupling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>D2l (ma</surname></persName>
		</author>
		<idno>62.68 84.00 57.80 81.36</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Mentornet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<idno>63.00 81.40 57.80 79.92</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Co-Teaching (han</surname></persName>
		</author>
		<idno>63.58 85.20 61.48 84.70</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Cv (</forename><surname>Iterative</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>65.24 85.34 61.60 84.98</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Dividemix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Socher</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoi</forename></persName>
		</author>
		<idno>77.32 91.64 75.20 90.84</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elr+ (liu</surname></persName>
		</author>
		<idno>77.78 91.68 70.29 89.76</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Longremix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cordeiro</surname></persName>
		</author>
		<idno>78.92 92.32 - - DM-AugDesc(Nishi et al. 2021) 78.64 93.20 75.52 92.12</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Comparison with existing methods on WebVision. We re-implement DM-AugDesc and C2D</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
