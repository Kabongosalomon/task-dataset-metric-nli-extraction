<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRA-Net: Point Relation-Aware Network for 3D Point Cloud Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silin</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
						</author>
						<title level="a" type="main">PRA-Net: Point Relation-Aware Network for 3D Point Cloud Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-point cloud</term>
					<term>shape analysis</term>
					<term>intra-region con- texts</term>
					<term>inter-region relations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning intra-region contexts and inter-region relations are two effective strategies to strengthen feature representations for point cloud analysis. However, unifying the two strategies for point cloud representation is not fully emphasized in existing methods. To this end, we propose a novel framework named Point Relation-Aware Network (PRA-Net), which is composed of an Intra-region Structure Learning (ISL) module and an Inter-region Relation Learning (IRL) module. The ISL module can dynamically integrate the local structural information into the point features, while the IRL module captures inter-region relations adaptively and efficiently via a differentiable region partition scheme and a representative point-based strategy. Extensive experiments on several 3D benchmarks covering shape classification, keypoint estimation, and part segmentation have verified the effectiveness and the generalization ability of PRA-Net. Code will be available at https://github.com/XiwuChen/PRA-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the proliferation of 3D sensing devices, 3D point cloud analysis has attracted increasing attention due to its generalized applications in numerous prospective fields such as autonomous driving <ref type="bibr" target="#b0">[1]</ref>, robotics <ref type="bibr" target="#b1">[2]</ref>. However, this task remains a great challenge since point clouds are normally irregular and non-uniform. In light of the great success of deep learning on 2D images, there have been many efforts to extend the techniques for 3D point clouds. A straightforward way is to transform point clouds into regular voxel grids <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> or multi-view images <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b7">[8]</ref>. Nonetheless, these methods generally suffer from high computational complexity, large memory overhead, or loss of geometric information. As a pioneering work, PointNet <ref type="bibr" target="#b8">[9]</ref> applies shared pointwise multilayer perceptrons (MLPs) followed by max-pooling to directly process point clouds. However, it has the following drawbacks that limit its ability to learn more discriminative representations for point clouds: (1) lack of full exploration on the intraregion contextual information. <ref type="bibr" target="#b1">(2)</ref> The inter-region relations are also not sufficiently captured.</p><p>Recently, several works focus on deriving the intra-region contexts of the point cloud. For instance, PointNet++ <ref type="bibr" target="#b9">[10]</ref> introduces a hierarchical framework to capture local contexts by sampling and grouping point clouds. However, since it treats each point individually at local regions and simply uses max-pooling layers to aggregate local features, the geometric relationships among points are not yet fully exploited. To better capture the local structures, DGCNN <ref type="bibr" target="#b10">[11]</ref> proposes a novel EdgeConv to extract local features by linearly aggregating each point features with its k-nearest neighbors. PointWeb <ref type="bibr" target="#b11">[12]</ref> considers all the point pairs inside each region and aggregates local features by weighting all edges among points based on feature differences. Nonetheless, the densely connected graph formed in this way may introduce redundant information and lead to high computational overhead. Besides, these methods focus on exploring intra-region contexts while not sufficiently exploit the inter-region relations.</p><p>However, the inter-region relations also provide beneficial information for 3D understanding, such as the similarity, proximity, and symmetry among different parts of an object, which has been found to be important in human visual perception <ref type="bibr" target="#b12">[13]</ref>. Several works thus have been proposed to exploit the inter-region relations to further enhance the capacity of the learned 3D shape representations. For example, SPG <ref type="bibr" target="#b13">[14]</ref> breaks down point clouds into geometrically simple parts in an unsupervised manner, where each part is assumed to be semantically homogeneous and assigned with a hard label. However, it may misclassify those points with different labels but having been partitioned into the same parts. LRC-Net <ref type="bibr" target="#b14">[15]</ref> encodes the inter-region context information based on spatial similarity measures, which is inversely proportional to the Euclidean distance among points. It indicates that nearer regions contribute more than the farther regions and suppress the impact of distant regions, which limits the capacity and flexibility of the model to capture long-range inter-region relations.</p><p>In this paper, we argue that both the intra-region contexts and inter-region relations should be combined to complement each other and produce better feature representations for 3D point cloud analysis. To this end, we propose a novel end-toend framework named Point Relation-Aware Network (PRA-Net) to simultaneously exploit the intra-region contexts and inter-region relations. It consists of two core modules: the Intra-region Structure Learning (ISL) module and the Interregion Relation Learning (IRL) module. The ISL module is employed to capture intra-region contextual information, while the IRL module is designed for exploring inter-region relations. Specifically, for the ISL module, it adopts a novel arXiv:2112.04903v1 [cs.CV] 9 Dec 2021 adaptive fusion mechanism to integrate the local structural information into the point features, thus producing more discriminative intra-region contextual features. For the IRL module, it first dynamically divides the point cloud into a set of regions and then samples one representative point per region. Based on the sampled points, we can easily use them as proxies to model the inter-region relations in an efficient way. This procedure may repeat multiple times in order to give a more comprehensive evaluation of the interregion relations. The two modules are interleaved multiple layers to form our final framework, in order to closely and densely capture both the intra-region contexts and inter-region relations. The validity of our method has been verified by conducting extensive experiments on four public benchmark datasets, i.e., ModelNet40, ScanObjectNN, KeypointNet, and ShapeNet Part.</p><p>In summary, our key contributions are threefold:</p><p>? We propose an Intra-region Structure Learning (ISL) module with a novel fusion mechanism, to adaptively incorporate the local structure information into each point feature, thereby producing more discriminative intraregion contextual information for point clouds. <ref type="bibr">?</ref> We present an Inter-region Relation Learning (IRL) module to effectively capture inter-region relations of point clouds. This module dynamically partitions a point cloud into several local regions and samples points from each local region to serve as representative proxies, which can learn the inter-region relations both efficiently and effectively. ? Based on the proposed ISL and IRL modules, we establish an end-to-end framework PRA-Net for point cloud analysis, which has demonstrated its effectiveness and generality on various 3D benchmark datasets involving 3D object classification, keypoint estimation, and part segmentation. The rest of the paper is organized as follows: we review some related works in Section II and present the details of the proposed method in Section III. Section IV provides the details of the experimental results and analysis. Finally, we draw conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>View-based method. Given the superiority of the imagebased deep learning techniques, many methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref> project 3D objects into a set of 2D views and learn feature representations from these view images. For instance, MVCNN <ref type="bibr" target="#b5">[6]</ref> applies a shared 2D CNN to extract view-wise features and simply maxpools them into a global descriptor. However, the max-pooling operation may lead to information loss by only retaining the maximum values for each view. To address this issue, GVCNN <ref type="bibr" target="#b18">[19]</ref> introduces a hierarchical view-group-shape framework to model the correlation in multiple views, and improves the recognition accuracy accordingly. Yang et al. <ref type="bibr" target="#b20">[21]</ref> proposes a Relation Network to effectively capture the region-to-region and view-to-view relationships, and achieves better performance. View-GCN <ref type="bibr" target="#b19">[20]</ref> constructs a view-graph to model the relationship among  <ref type="figure">Fig. 1</ref>. Intra-region Structure Learning (ISL) module, which includes (a) a NFL module, (b) a SFL module, and (c) a DFA module. The NFL module is designed to encode local structural information, the SFL module is responsible for capturing global shape information and the DFA module is present for adaptively combining local structural information and global shape information. Fp i denotes the point feature.Fp i,j represents the edge feature between point p i and its j-th nearest neighbor p i,j . T p i and T p i encode the global shape information and local structural information of point p i , respectively. Tp i is the output of the ISL module. C is the number of feature channels. ? indicates the sigmoid function. ? is implemented by two fully-connected layers with batch-normalization. wp i is the attention vector to weight T p i and T p i . ? and are the element-wise summation and Hadamard product, respectively. multiple views and adopts a graph convolutional network to learn the global shape descriptor. However, those approaches are restricted mainly by occlusion surfaces and rendering techniques.</p><p>Voxel-based method. Another way is to voxelize 3D objects into voxels. VoxNet <ref type="bibr" target="#b21">[22]</ref> converts input points into voxels and utilizes a 3D CNN to extract its features. However, due to the limitation of sparsity and high computational overhead, some space partition methods are proposed to relieve this issue. OctNet <ref type="bibr" target="#b22">[23]</ref> uses a set of unbalanced octrees to hierarchically partition the space, which reduces the number of empty voxels and thus is both memory and computation efficient. O-CNN presents an octree-based convolutional neural network for 3D shape analysis, whose inputs are the average normal vectors of a 3D model sampled in the finest leaf octants, resulting in compact memory footprint and efficient computation overhead. Submanifold sparse convolution <ref type="bibr" target="#b23">[24]</ref> introduces a sparse convolutional operation to reduces the computation overhead, which strictly operates on submanifolds.</p><p>Point-based method. Recently, there exist many works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b27">[28]</ref> attempting to directly process 3D point clouds. PointNet <ref type="bibr" target="#b8">[9]</ref> performs shared MLPs and a symmetric function to handle the unordered point clouds. However, the intraregion structure information is not fully exploited. To solve this problem, many endeavors have been made to effectively aggregate local features. PointNet++ <ref type="bibr" target="#b9">[10]</ref> applies the PointNet recursively on each partitioned region and aggregates local features by max-pooling. However, since it treats each point independently, the geometric relationships among points are not fully explored. Motivated by the success of graph neural networks in dealing with irregular data, several works apply it to capture local geometric topologies. DGCNN <ref type="bibr" target="#b10">[11]</ref> employs an EdgeConv operator to capture local contexts by linearly aggregating center point features with edge features. Spider-CNN <ref type="bibr" target="#b28">[29]</ref> introduces a novel filter defined by a product of a step function and a Taylor polynomial. Grid-GCN <ref type="bibr" target="#b27">[28]</ref> adopts a novel grid-based data structuring strategy and aggregates the local structure information by graph convolution. In addition, some methods utilize kernel points to capture local structure information. For instance, KCNet <ref type="bibr" target="#b29">[30]</ref> leverages a kernel correlation layer and a graph pooling layer to further exploit the geometric feature. InterpCNN <ref type="bibr" target="#b30">[31]</ref> utilizes a set of discrete kernel weights and an interpolation function to model geometric relationships between points and kernelweight coordinates. To overcome the geometric and scale variance problem, KPConv <ref type="bibr" target="#b31">[32]</ref> and 3DGCN <ref type="bibr" target="#b32">[33]</ref> adopt deformable kernels to capture local structure information of the point cloud. RSCNN <ref type="bibr" target="#b33">[34]</ref>, PointConv <ref type="bibr" target="#b34">[35]</ref>, and PCCN <ref type="bibr" target="#b35">[36]</ref> obtain the dynamic kernel weights from low-level geometric information (i.e., xyz coordinates), which are then used to aggregate intra-region contexts.</p><p>Unlike these methods, our approach adaptively incorporates the local structure information into each point feature, thus can produce more discriminative intra-region features. Furthermore, these methods mainly focus on exploiting local information while not fully explore the inter-region relations.</p><p>Besides, several works further enhance the learned representations by modeling the inter-region relations. SRN <ref type="bibr" target="#b36">[37]</ref> employs shared MLPs to learn structural relational features between different local regions. LRC-Net <ref type="bibr" target="#b14">[15]</ref> encodes the inter-region context information based on spatial similarity measures, which is inversely proportional to the Euclidean distance among points. A-SCN <ref type="bibr" target="#b37">[38]</ref>, PointASNL <ref type="bibr" target="#b38">[39]</ref>, and Point2Node <ref type="bibr" target="#b24">[25]</ref> use a global attention mechanism to aggregate features by considering relationships among all the points, which may lead to redundancy in both computation and representation. Instead of using self-attention as an auxiliary module, PointTransformer <ref type="bibr" target="#b39">[40]</ref> and PCT <ref type="bibr" target="#b40">[41]</ref> design a more general framework which is based on Transformer <ref type="bibr" target="#b41">[42]</ref> for 3D point cloud processing. Different from these methods, we dynamically partition the point cloud into different regions and utilize representative points within each region as proxies to model the inter-region relations, which is more efficient and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The proposed PRA-Net consists of two key elements. The first is an Intra-region Structure Learning (ISL) module, which tries to enhance each point feature using its corresponding contextual region. The second is a novel Inter-region Relation Learning (IRL) module, which aims at modeling the relations across different local regions. The two modules are combined together to drive our PRA-Net to learn more discriminative representations from point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intra-region Structure Learning</head><p>Let P = {p i } N i=1 represents a point cloud of N points with the associated point-wise features F = {F pi } N i=1 , where F pi ? R 1?C denotes the C-dimentional feature of point p i . For a given sampled point p i , we use itsk nearest neighbors N (p i ) = {p i,j }k j=1 to model the local region 1 . The goal of the ISL module is to obtain more discriminative representations of local regions. Concretely, the computation of ISL can be decomposed into three stages: the Neighbor-based Feature Learning (NFL) stage, the Self-based Feature Learning (SFL) stage, and the Dynamic Feature Aggregation (DFA) stage. NFL. Exploring the local geometric structure is essential for point cloud understanding. Inspired by EdgeConv <ref type="bibr" target="#b10">[11]</ref>, we obtain the edge featureF pi,j ? R 1?C in a local coordinate system by computing the difference between the point feature F pi and its j-th nearest neighbor F pi,j as followsF pi,j = F pi ? F pi,j , where j ? {1, 2, ...,k}. Then a shared MLP denoted by MLP 1 (?) is applied to encode the local structural information, which can be formulated as</p><formula xml:id="formula_0">F pi,j = MLP 1 (F pi,j ),<label>(1)</label></formula><p>where F pi,j ? R 1?C and C is the number of output channels. Finally, we use a max-pooling layer, denoted by MaxPooling(?), to further aggregate local structural information into a compact one, which is written as</p><formula xml:id="formula_1">T pi = MaxPooling([F pi,1 ; ...; F pi,j ; ...; F p i,k ]),<label>(2)</label></formula><p>where T pi ? R 1?C encodes the local structural information of the region centered at p i . SFL. Since the point feature F pi is represented in the global coordinate system, it contains the global shape information of the object <ref type="bibr" target="#b10">[11]</ref>. As shown in <ref type="figure">Fig. 1 (b)</ref>, we obtain the point feature T pi ? R 1?C by employing another MLP denoted by MLP 2 (?) to extract point features as follows</p><formula xml:id="formula_2">T pi = MLP 2 (F pi ).<label>(3)</label></formula><p>DFA. To exploit both local structure information captured by T pi and global shape information captured by T pi , Edge-Conv <ref type="bibr" target="#b10">[11]</ref> utilizes linear aggregation (i.e., addition) to integrate them. However, linear aggregation is not feature-adaptive and thus limits the capacity of the learned point features. To better leverage the local structural information and global shape information, we propose a dynamic fusion mechanism to achieve this. As shown in <ref type="figure">Fig.1</ref> (c), we first fuse T pi and T pi via an element-wise summation to form a compact feature representation, which is then compressed into an attention vector w pi ? R 1?C as follows</p><formula xml:id="formula_3">w pi = ?(?(T pi + T pi )),<label>(4)</label></formula><p>where ?(?) represents the sigmoid activation function, ?(?) is the transformation network which is implemented by two fully-connected layers with batch normalization. Then the final feature representation T pi ? R 1?C of point p i is computed as follows</p><formula xml:id="formula_4">T pi = w pi T pi + (1 ? w pi ) T pi ,<label>(5)</label></formula><p>where denotes the Hadamard product and 1 ? R 1?C is an all ones-vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inter-region Relation Learning</head><p>The IRL module aims at enhancing point features by modeling inter-region relations. It first adaptively decomposes the point cloud into a set of local regions, where each local region is composed of k-nearest neighbors of a center point. Concretely, it first selects S points {p 1 , p 2 , ..., p S } as centroids via a dynamic region partition method and then applies the k-NN algorithm to construct the corresponding regions for each sampled center point, which are denoted by {N (p 1 ), N (p 2 ), ..., N (p S )}. In this way, we obtain S local regions with each containing k neighbor points, as shown in <ref type="figure">Fig. 2 (a)</ref>. Lastly, the proposed IRL module tries to "connect" these local regions directly to model their relations. Dynamic Region Partition. A simple choice is adopting the Farthest Point Sampling (FPS) algorithm to sample key points and use their k-nearest neighbors to construct local regions. However, FPS is heuristic and task-irrelevant, which limits its ability to generalize to various scenarios. Based on this observation, we propose a novel region partition strategy that is capable of partitioning the regions adaptively in a datadriven way. Specifically, we first employ a shared MLP to project T pi into a score s pi as where s pi is the score which implies the importance of the point p i . Then we select S points from the point cloud according to the calculated importance scores. One straightforward approach is to sample the top-S points as centroids based on the scores to construct local regions, as shown in <ref type="figure">Fig. 3 (a)</ref>. Nonetheless, considering that points close to each other tend to get similar scores, it may select a bunch of points clustered around a small region, while suppresses those points that also contain discriminative information but scatter at other parts of the input point cloud. To solve this problem, we propose a dilated top-S partition method to keep a balance between the importance and the diffusion of points during the region partition process. Concretely, instead of taking the top-rank points as centroids, we sample S points with stride N/S, thus make the selected region evenly distributed on the input point cloud, as shown in <ref type="figure">Fig. 3 (b)</ref>.</p><formula xml:id="formula_5">s pi = ?(MLP 3 (T pi )),<label>(6)</label></formula><formula xml:id="formula_6">? ? (a) (b)</formula><p>Finally, instead of adopting a discrete region partition strategy, we perform a scalar multiplication between the score s pi of each center point p i and the feature T pi,j of p i 's neighbors to make this process differentiable as</p><formula xml:id="formula_7">G pi,j = s pi T pi,j ,<label>(7)</label></formula><p>where G pi,j ? R 1?C is updated point features, p i,j ? N (p i ), and j ? {1, 2, 3, ..., k}.</p><p>Inter-region Relations Learning. In order to learn the interregion relations, one naive approach is to enhance each point's feature using all the points from other regions. As shown in <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>, in order to update the selected point feature (i.e., the yellow point), one has to connect all the points from other regions to it. In this case, the number of edges equals (S?1)k 2 per region. Thus the total number of edges is S(S ? 1)k 2 . However, this approach relies on constructing a very dense graph to achieve this, which is intractable to use in practice.</p><p>To overcome the issues incurred by the above naive approach, we improve upon it by sampling representative points from each region and then use self-attention to model the interregion relations, as shown in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>. Since a sampled point from a local region can give us some cues about a region, thus learning relations about the sampled S points (i.e., one point per region) can give us a hint on the inter-region relations. Such a strategy not only endows our method with efficiency but also improves the representation learning by sparsifying the connections among different regions in a balanced manner. <ref type="figure">Fig. 2</ref> gives an overview of the IRL module. Specifically, it consists of the following three stages: 1) Sampling Representative Points. It is the key step in our methods. Let us define h m as the sampling strategy that takes a local region N (p i ) as input, and outputs m representative points, which is written as</p><formula xml:id="formula_8">? i = h m (N (p i )) = [r i,1 , ..., r i,t , ..., r i,m ],<label>(8)</label></formula><p>where r i,t ? N (p i ) represents the t-th representative point for the local region N (p i ). A natural choice of h m is to use a random sampling method. In light of the randomness introduced by random sampling, we adopt a stable sampling strategy named k-NN based method. The k-NN based method samples m representative points one by one from each region in ascending order based on their distances to the center point. Moreover, we also explore using max-pooling and meanpooling methods to aggregate features to represent each local region. The different choices of h m are analyzed in Sec. VII. Such a sampling process will be applied to all the regions in the point cloud. Then we pack representative points of the same order in each ? i , which gives us ? = [? 0 ; ? 1 ; ...; ? S ] T . It should be noted that each row in ? represents representative points from each region, which can be seen as an aspect of the original point cloud.</p><p>2) Inter-region Relation Learning. After we have obtained ?, we then perform self-attention <ref type="bibr" target="#b41">[42]</ref> over the features corresponding to each row, i.e., G t = {G r1,t , G r2,t , ..., G r S,t }, where G ri,t represents the feature of point r i,t obtained in the dynamic region partition stage. With self-attention, the interregion relations can be easily captured. Mathematically, it is formulated a?</p><formula xml:id="formula_9">G ri,t = W z S j=1 f (G ri,t , G rj,t ) Z(G) (W v ? G rj,t ),<label>(9)</label></formula><p>where W z and W v represent linear transform matrices. We denote</p><formula xml:id="formula_10">w ijt = f (Gr i,t ,Gr j,t ) Z(G)</formula><p>as a normalized similarity score (or attention score) of the query point r i,t and the key point r j,t , where f (G ri,t , G rj,t ) denotes the relationship between point r i,t and r j,t and Z(G) is the normalization factor. As softmax is a commonly used normalization technique, we use it to normalize w ijt , which can be written as</p><formula xml:id="formula_11">w ijt = exp( W q ? G ri,t , W k ? G rj,t ) S l=1 exp( W q ? G ri,t , W k ? G r l,t ) ,<label>(10)</label></formula><p>where W q and W k denote linear transform matrices corresponding to the query point and key point, respectively. One superiority of our method is that we can reduce the computational complexity to some extent compared with the naive method. In particular, for the naive version, we have to build a complete graph as each pair of points from different regions is "connected" by an edge. In summary, this heavilyconnected graph contains S(S ? 1)k 2 edges in total. While our improved representative-point based attention only needs to build a very sparse graph to model the inter-region relations. In this case, the number of edges is (S ? 1)m per region, and the total number of edges is reduced to S(S ? 1)m, which is just m k 2 of the naive implementation. To be more specific, if m equals 4 and k equals 8, the number of connections in the graph would become approximately 1/16 of the naive approach.</p><p>3) Feature Interpolation. For the IRL module, we further add a residual link in order to alleviate the information loss and accelerate the learning process. However, since the output is of different size to the original input, we apply feature interpolation to interpolate it back to the input space of the IRL module. Similar to PointNet++, we obtain the interpolated feature ?G pv of p v by utilizing an inverse Euclidean distance weighted average method based on its 3-nearest neighbors {p u } 3 u=1 in ? as</p><formula xml:id="formula_12">?G pv = 3 u=1 ? u (p u )? pu 3 u=1 ? u (p u ) ,<label>(11)</label></formula><p>where v ? {1, 2, ..., N }, ? u (p u ) = 1 d(pv,pu) 2 is an inverse L2 distance weight. Then the interpolated feature will be added to the original points' embeddings to generate the final output of the IRL module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture Details</head><p>By combining both the ISL module and IRL module, we propose three different frameworks for the classification, keypoint estimation, and segmentation tasks, respectively. Classification Network. As shown in <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>   of the first LFE module and all the IRL modules. A shared fully-connected layer is then used to obtain the global feature. Inspired by <ref type="bibr" target="#b42">[43]</ref>, we concatenate max-pooling and meanpooling outputs to get a better representation. Finally, we feed it into three fully-connected layers to make the final prediction. Keypoint Saliency Estimation Network. The keypoint saliency estimation network is similar to the classification network, which is shown in <ref type="figure" target="#fig_3">Fig. 5 (a)</ref> (the bottom branch). Following <ref type="bibr" target="#b43">[44]</ref>, we obtain the pointwise features by concatenating the output of the first LFE module, all the IRL modules, and the penultimate layer of the network. Lastly, we use them to estimate keypoint saliency for each point. Segmentation Network. The part segmentation network is shown in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>. Similar to the classification network, three ISL modules and two IRL modules are interleaved to capture the intra-region contexts and inter-region relations. Then a shared MLP and a max-pooling layer are further utilized to learn the global feature. In addition, we also use an embedding layer to encode the object label and apply a shortcut connection to fuse it with all previous outputs to get a more discriminative representation. Finally, another IRL module is used, which is followed by three fully-connected layers for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Network Configuration Details</head><p>Architecture protocol. In this section, we detail the parameter configurations of the Intra-region Structural Learning (ISL) module and the Inter-region Relation Learning (IRL) module in classification network, keypoint saliency estimation network, and part segmentation network, respectively. For clarity, we first introduce several notations as follows:</p><p>ISL(k, [l 1 ;...;l d ]): the ISL module, wherek is the number of the nearest neighbors of each point in the local region, [l 1 ;...;l d ] represents shared MLPs of d layers with width l i (i=1,...,d).</p><p>IRL(S, k, m): the IRL module, where S is the number of regions, k and m represent the total number of points and the number of representative points in each local region, respectively.</p><p>Then, we list the configuration details of the ISL and IRL modules from left to right in <ref type="figure" target="#fig_3">Fig.5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To demonstrate the effectiveness of the proposed PRA-Net, we conduct our experiments on various tasks such as shape classification, keypoint saliency estimation, and part segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Point Cloud Classification</head><p>Datasets. We evaluate our method on two types of public datasets: synthetic dataset, i.e., ModelNet40 <ref type="bibr" target="#b44">[45]</ref>, and realworld dataset, i.e., ScanObjectNN <ref type="bibr" target="#b45">[46]</ref>. ModelNet40 consists of 12,311 3D synthetic objects in 40 categories, with 9,843 models for training and 2,468 models for testing. ScanOb-jectNN is a challenging real-world object dataset which is built upon the SceneNN <ref type="bibr" target="#b46">[47]</ref> and ScanNet <ref type="bibr" target="#b47">[48]</ref> datasets. It includes 15,000 objects in 15 classes. Implementation Details. For experiments on the ModelNet40 dataset, we sample 1,024 points for each 3D object following <ref type="bibr" target="#b8">[9]</ref>. We use the k-NN based strategy to sample representative points. Data augmentation techniques including random scaling and translation are utilized. The random scaling factor ranges from 0.66 to 1.33 and the translation ranges from -0.2 to 0.2. For optimization, we use SGD to train our model for 250 epochs with a batch size of 32. The initial learning rate is set to 0.1 and the cosine annealing method is used to dynamically adjust the learning rate. To alleviate the overfitting problem, the label smoothing technique <ref type="bibr" target="#b48">[49]</ref> is used during training. For ScanObjectNN, we conduct experiments on its hardest variant, i.e., PB T50 RS as stated in <ref type="bibr" target="#b45">[46]</ref>, which contains various perturbations including translation that randomly shifts the bounding box up to 50% of its size, rotation ,and scaling on each object. We use the same data augmentation techniques following <ref type="bibr" target="#b45">[46]</ref>. The model and training strategy are the same as those on the ModelNet40. The metrics are overall accuracy (OA) and average class accuracy (mAcc). Synthetic object classification. We compare our method with representative state-of-the-art methods on the ModelNet40 dataset and the results are summarized in <ref type="table" target="#tab_2">Table I</ref>. As shown, our method achieves 90.5% and 93.2% in terms of mAcc and OA, respectively, which outperforms almost all the stateof-the-art methods using only 1,024 points as input. Since our method is point-cloud based, we mainly compare our method with previous point cloud based methods. Compared with InterpCNN <ref type="bibr" target="#b30">[31]</ref> and DGCNN <ref type="bibr" target="#b10">[11]</ref>, which concentrate on aggregating intra-region contexts, we improve upon them by 0.2% and 0.3% in OA, respectively. In addition, for those methods focusing on how to effectively integrate intra-region contexts and inter-region relations of the point cloud, we also obtain competitive results over them. Specifically, when compared with PointASNL <ref type="bibr" target="#b38">[39]</ref>, which adopts 5-time voting during testing, we still outperform it by 0.3%. It should also be mentioned that compared with KPConv <ref type="bibr" target="#b49">[50]</ref> uses 6k points as input, while our method uses only 1k points as input. However, we still achieve a superior result (92.9% vs. 93.2% in OA). What's more, when using 2k points as the input, the proposed PRA-Net can achieve the best performance among all reported results. All these results consistently demonstrate the competitiveness of our method. Real-world object classification. The ScanObjectNN dataset introduces great challenges to existing point cloud classification techniques due to the existence of background noise, missing parts, and diverse deformations. The classification results are presented in <ref type="table" target="#tab_2">Table I</ref>. As shown, when using only 1,024 points as input, our approach outperforms all the other point-based methods by a large margin. Particularly, it significantly surpasses the second-best point-based method, i.e., PointCNN <ref type="bibr" target="#b50">[51]</ref> with an increase of 2.5% in OA and 2.8% in mAcc. Moreover, compared with the recent DGCNN <ref type="bibr" target="#b10">[11]</ref>, we obtain 2.8% improvements in terms of OA. To investigate the impact of the number of input points on PRA-Net, we also train the network with 2048 points and obtain an impressive accuracy of 82.1% in OA. The superior performance on the real-world dataset indicates that our method has great potential in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Keypoint Saliency Estimation</head><p>Dataset. Keypoint saliency estimation is a challenging task which requires the model to predict whether a point is the Implementation Details. In keypoint saliency estimation network, we use the same architecture and hyperparameters as the classification network. We use Adam <ref type="bibr" target="#b68">[69]</ref> to optimize our model and the initial learning rate is set to 0.001. Following <ref type="bibr" target="#b43">[44]</ref>, we report mean Intersection over Unions (mIoU) and mean Average Precisions (mAP) with a distance threshold of 0.01.</p><p>Results. As shown in <ref type="table" target="#tab_2">Table II and Table III</ref>, our method achieves the best performance with mIoU of 28.6% and mAP of 43.2%. Compared with the second-best method DGCNN <ref type="bibr" target="#b10">[11]</ref>, we improve upon it by 7.8% and 14.4% in mIoU and mAP, respectively. Moreover, we also provide the qualitative results shown in <ref type="figure">Fig. 7</ref>. It can be observed that our method can accurately identify the keypoints annotated on each object. These results demonstrate the effectiveness of our method on this challenging dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Part Segmentation</head><p>Dataset. We further evaluate PRA-Net on part segmentation task. ShapeNet Part benchmark <ref type="bibr" target="#b67">[68]</ref> is adopted for the experiment. This dataset consists of 16 shape categories and 50 different part classes. Each object is annotated with one shape category and 2 to 6 part classes. There are 16,880 models in total, with 14,006 for training and 2,874 for testing. Implementation Details. In part segmentation network, we setk to 32 in each ISL module and set S to 256, 256, 128 in each IRL module respectively. The sampling strategy and data augmentation techniques are the same as the classification task. Following <ref type="bibr" target="#b9">[10]</ref>, we randomly select 2, 048 points of each object to train our model. We use Adam <ref type="bibr" target="#b68">[69]</ref> to optimize our model with a mini-batch size of 16. The initial learning rate is set to 0.001 and decays with a rate of 0.5 every 30 epochs. The initial momentum value of batch normalization is set to 0.9 and reduces by half every 30 epochs.</p><p>Results.  <ref type="bibr" target="#b10">[11]</ref> and A-CNN <ref type="bibr" target="#b58">[59]</ref>. When compared with RSCNN <ref type="bibr" target="#b33">[34]</ref>, our model shows better performance in instance mIoU and slightly worse performance in class mIoU. The visualization of segmentation results is shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. It can be seen that our model is capable of segmenting out semantic parts of each object faithfully. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussions</head><p>To further illustrate the effectiveness of the proposed ISL and IRL module, we design an ablation study on both the shape classification and keypoint saliency estimation tasks. Effect of Different Components. The goal of this ablation study is to show the effect of each component in PRA-Net. Results are summarized in <ref type="table" target="#tab_6">Table V</ref>. Here, we set two baselines: model A and model B. Both two modules mainly focus on exploiting the intra-region contextual information by the ISL module. The difference is that model A encodes global shape information by SFL while model B encodes local structural information by NFL. The baseline model A obtains a low classification accuracy of 66.5% and model B gets 76.6%, respectively. When we use linear aggregation, i.e., addition, to combine global shape information and local structural information (model C), an improvement of 2.1% is obtained. Then, when we replace the linear aggregation of global shape information and local structural information with dynamic feature aggregation (model D), the accuracy can be further improved to 79.8%. The results demonstrate that our dynamic feature aggregation can better leverage the local structural information and global shape information and produce more discriminative features than the linear aggregation method. Eventually, in order to validate the effectiveness of the IRL module, we add it to model D which gives us model E. It can be seen that with all the components (model E), PRA-Net achieves the best performance. Model E gains a 1.2% improvement compared to model D, which indicates the importance of learning inter-region relations. Different Region Partition Methods. To better understand the influence of different three region partition methods, we conduct the ablation experiments on the ScanObjectNN and KeypointNet dataset. The quantitative results are summarized in <ref type="table" target="#tab_2">Table VI</ref>. As shown, the proposed dilated top-S method can obtain more discriminative results than the other alternatives. To further conceptualize the difference between top-S and dilated top-S partition methods, we visualize the selected points of the two methods in <ref type="figure">Fig. 8</ref>, respectively. As shown, the points selected by the top-S method are clustered around a small region. However, the points selected by the dilated top-S method are more evenly distributed, thus can better describe the geometric topology of the object and lead to more promising results than the top-S method. Though the FPS can also select points that are evenly scattered over the whole point set, it is task agnostic and not learnable, which limits its representation power. Different Sampling Strategies h m . Two choices for h m are studied: random sampling based and k-NN based. For this experiment, we fix the region numbers of the three IRL modules (from the shallow to the deep layers in our classification network) to 256, 128, and 64 respectively, and the point number k per region for all the IRL modules to 6. Then we vary the sampled representative point number m to be 1, 2, 4, and 6, to study the effects. As shown in <ref type="table" target="#tab_2">Table VII</ref>, for both sampling strategies, we can obtain strong results, especially for the k-NN based method which achieves the best performance with 81.0% in OA. Moreover, we notice that either too small or too large value for the number of representative points m will degrade the performance. The best classification result is obtained when we set m to 4. We assume that too many points may contaminate the representation learning by introducing more redundant information, while fewer points are not sufficient to characterize the geometric information of a local region. Furthermore, we perform experiments by aggregating the point features in the local region. For simplicity, we use max-pooling (max) and mean-pooling (mean). However, both methods are worse than the k-NN based method. We hypothesize that the performance degradation can be attributed to the information loss caused by max-pooling or averagepooling, which only takes the maximum or average values of point features in each local region.  fixed to the default values for fair comparisons. To give a more comprehensive analysis, results with m = 2 and m = 4 are reported. The detailed comparison results are summarized in <ref type="table" target="#tab_2">Table VIII</ref>. As shown, when we increase the rate of the base group numbers from 1 4 to 1 and set m to 4, we can gain consistent improvement in OA. However, the performance drops when increasing the rate from 1 to 4. We find that too small and too large values for the group number are harmful. The best configurations for S are (256, 128, 64) in the three different IRL layers, respectively. Similar phenomena can be observed when m = 2. Specifically, the best performance is achieved when the rates are set to 1 and 2. The reason behind this may be that a small number of regions are insufficient to describe a 3D object, while a large number of regions may introduce redundant information and obstruct representation learning. The visualization of the IRL module. To further investigate  <ref type="figure">Fig. 9. (a)</ref> The visualization of the top-20 points (red points) that have the most influence on the anchor points (green points) according to the attention weights of the last IRL module on the ShapeNet Part dataset and (b) the corresponding segmentation ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose an end-to-end architecture named PRA-Net to exploit the intra-region contexts and inter-region relations for 3D point cloud analysis. Specifically, the ISL module extracts the intra-region contexts by dynamically integrating the local structural information into the point features, while the IRL module models the inter-region relations with the representative points adaptively sampled from each local region. Finally, two key components are combined in a principled way for deriving a more effective framework to learn highly discriminative representations for point clouds. Comprehensive experiments on four public benchmark datasets demonstrate the effectiveness and generality of our method. In the future, we would like to apply our framework to other point cloud analysis tasks such as 3D object detection and instance segmentation. Xiang Bai received his B.S., M.S., and Ph.D. degrees from the Huazhong University of Science and Technology (HUST), <ref type="bibr">Wuhan, China, in 2003</ref><ref type="bibr">, 2005</ref><ref type="bibr">, and 2009</ref>, respectively, all in electronics and information engineering. He is currently a Professor with the School of Artificial Intelligence and Automation, HUST. His research interests include object recognition, shape analysis, and OCR. He has published more than 150 research papers. He is an editorial member of IEEE TPAMI, Pattern Recognition, and Frontier of Computer Science. He is the recipient of 2019 IAPR/ICDAR Young Investigator Award for his outstanding contributions to scene text understanding. He is a senior member of IEEE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>+</head><label></label><figDesc>!!,#, ? , + !!,$, ? , + ! !,% : + ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Inter-region Relation Learning (IRL) module. This module contains four stages, including dynamic region partition, sampling representative points, inter-region relation modeling, and feature interpolation. (a) Dynamic region partition. Regions are constructed based on the dynamic sampled points (yellow points). (b) Sampling representative points (red points). (c) Inter-region relation modeling. (d) Feature interpolation. Illustration of the dynamic region partition stage. (a) The top branch is the top-S partition method, (b) the bottom branch is the dilated top-S partition method. The numbers in the red rectangles represent the selected point indices. For simplicity, we set S = 3 here for visualization. (Best viewed in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Two different ways to model inter-region relations. (a) The naive version approach. The orange point features are enhanced by all the points (blue points) from other regions. (b) The representative point-based version. The point features (orange color) are enhanced by the sampled representative points (red points) from other regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Network architectures. (a) The architecture designed for classification (the top branch) and keypoint saliency estimation (the bottom branch) tasks. (b)The architecture designed for part segmentation task. They all contain two components: the ISL module and the IRL module. N is the number of input points. c 1 and c 2 are the number of classification and part segmentation classes, respectively. "MLP" stands for a multi-layer perceptron. ?: concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The qualitative results on the ShapeNet Part dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>The qualitative results on the KeypointNet dataset. The red points are keypoints. The visualization of the sampled points (orange points) by different dynamic region partition methods. (a) Top-S method. (b) Dilated top-S method. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Silin</head><label></label><figDesc>Cheng received his B.S. degree from the School of Computer Science and Technology, Huazhong University of Science and Technology (HUST), China in 2019. He is currently a master student with the School of Electronic Information and Communications, HUST. His main research interests include 3D shape analysis, 3D scene understanding and continue learning. Xiwu Chen received his B.S. degree from the School of Computer Science and Technology, Huazhong University of Science and Technology (HUST), China in 2019. He is currently a master student with the School of Electronic Information and Communications, HUST. His main research interests include 3D shape analysis and 3D object detection. Xinwei He received his Ph.D. degree in Electronics and Information Engineering from Huazhong University of Science and Technology (HUST), Wuhan, China. His research interests include image caption, 3D shape analysis and 3D object detection. Zhe Liu received his B.S. from Hubei University of Technology (HBUT) and M.S. from Huazhong University of Science and Technology (HUST). He is currently studying for his doctor in HUST. He has published 3 papers in the areas of computer vision such as ECCV, AAAI. He also served as a reviewer for IJCAI-2021 and AAAI-2021. His current research interests include pattern recognition, deep learning, 3D computer vision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(the top branch), our classification framework consists of four LFE and three IRL modules. For the input layer, the first LFE module is used to extract the low-level local features. While for the remaining layers, we insert LFE and IRL modules alternately to learn more informative representations for each point by considering both the intra-region contextual information and inter-region relations. Shortcut connections are further used to aggregate richer representations via concatenating the outputs</figDesc><table><row><cell>?3</cell><cell>ISL</cell><cell>?64</cell><cell>ISL</cell><cell>IRL</cell><cell>?64</cell><cell>ISL</cell><cell>IRL</cell><cell></cell><cell>?128</cell><cell></cell><cell>ISL</cell><cell></cell><cell>IRL</cell><cell></cell><cell>?256</cell><cell>?</cell><cell cols="2">MLP{1024} pooling</cell><cell>1024</cell><cell>MLP {512,256}</cell><cell>256</cell><cell>MLP { !}</cell><cell>!</cell><cell>classification branch</cell></row><row><cell></cell><cell>-NN</cell><cell></cell><cell>? ?3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">? repeating</cell><cell>MLP {2}</cell><cell>?2</cell><cell>keypoint saliency estimation branch</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-NN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>? ?3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?3</cell><cell>ISL</cell><cell>?64</cell><cell>ISL</cell><cell>IRL</cell><cell>?128</cell><cell>ISL</cell><cell>IRL</cell><cell>?256</cell><cell>?</cell><cell>?448</cell><cell cols="2">max-pooling MLP{1024}</cell><cell>1024</cell><cell>repeating</cell><cell cols="3">MLP {1024,512} ?</cell><cell>?512</cell><cell>IRL</cell><cell>?512</cell><cell>MLP {256,128, "}</cell><cell>? "</cell><cell>part segmentation branch</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">repeating</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">categorical label</cell><cell cols="3">embedding {64}</cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>RESULTS (%) ON THE MODELNET40 AND SCANOBJECTNN DATASETS ("-": UNKNOWN, NOR.: NORMAL).</figDesc><table><row><cell>Methods</cell><cell>Inputs</cell><cell cols="2">ModelNet40 mAcc OA</cell><cell cols="2">ScanObjectNN mAcc OA</cell></row><row><cell>Subvolume [3]</cell><cell>voxels</cell><cell>-</cell><cell>89.2</cell><cell>-</cell><cell>-</cell></row><row><cell>OctNet [23]</cell><cell>octree</cell><cell>83.8</cell><cell>86.5</cell><cell>-</cell><cell>-</cell></row><row><cell>ECC [52]</cell><cell>graphs</cell><cell>82.4</cell><cell>87.0</cell><cell>-</cell><cell>-</cell></row><row><cell>KPConv [32]</cell><cell>6k</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>-</cell></row><row><cell>PointConv [35]</cell><cell>1k, nor.</cell><cell>-</cell><cell>92.5</cell><cell>-</cell><cell>-</cell></row><row><cell>SO-Net [50]</cell><cell>2k, nor.</cell><cell>87.3</cell><cell>90.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Point2Cap [53]</cell><cell>1k, nor.</cell><cell>-</cell><cell>93.7</cell><cell>-</cell><cell>-</cell></row><row><cell>?-CNN [54]</cell><cell>1k</cell><cell>88.7</cell><cell>92.0</cell><cell>-</cell><cell>-</cell></row><row><cell>PAT [55]</cell><cell>1k</cell><cell>-</cell><cell>91.7</cell><cell>-</cell><cell>-</cell></row><row><cell>RSCNN 2 [34]</cell><cell>1k</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>-</cell></row><row><cell>DensePoint 2 [56]</cell><cell>1k</cell><cell>-</cell><cell>92.8</cell><cell>-</cell><cell>-</cell></row><row><cell>A-SCN [38]</cell><cell>1k</cell><cell>87.6</cell><cell>90.0</cell><cell>-</cell><cell>-</cell></row><row><cell>InterpCNN [31]</cell><cell>1k</cell><cell>-</cell><cell>93.0</cell><cell>-</cell><cell>-</cell></row><row><cell>FPConv [27]</cell><cell>1k</cell><cell>-</cell><cell>92.5</cell><cell>-</cell><cell>-</cell></row><row><cell>3D-GCN [33]</cell><cell>1k</cell><cell>-</cell><cell>92.1</cell><cell>-</cell><cell>-</cell></row><row><cell>PointASNL [39]</cell><cell>1k</cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Point2Seq [57]</cell><cell>1k</cell><cell>90.4</cell><cell>92.6</cell><cell>-</cell><cell>-</cell></row><row><cell>MAP-VAE [58]</cell><cell>1k</cell><cell>-</cell><cell>90.2</cell><cell>-</cell><cell>-</cell></row><row><cell>A-CNN [59]</cell><cell>1k</cell><cell>90.3</cell><cell>92.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Point2Node [25]</cell><cell>1k</cell><cell>-</cell><cell>93.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Point2Cap [53]</cell><cell>1k</cell><cell>-</cell><cell>93.4</cell><cell>-</cell><cell>-</cell></row><row><cell>PosPool [60]</cell><cell>1k</cell><cell>-</cell><cell>93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>FatNet [61]</cell><cell>1k</cell><cell>90.6</cell><cell>93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>SpiderCNN [29]</cell><cell>1k</cell><cell>-</cell><cell>-</cell><cell>69.8</cell><cell>73.7</cell></row><row><cell>3DmFV [62]</cell><cell>1k</cell><cell>-</cell><cell>91.4</cell><cell>58.1</cell><cell>63.0</cell></row><row><cell>PointNet++ [10]</cell><cell>1k</cell><cell>-</cell><cell>90.7</cell><cell>75.4</cell><cell>77.9</cell></row><row><cell>PointNet [9]</cell><cell>1k</cell><cell>86.2</cell><cell>89.2</cell><cell>63.4</cell><cell>68.2</cell></row><row><cell>DGCNN [11]</cell><cell>1k</cell><cell>90.2</cell><cell>92.9</cell><cell>73.6</cell><cell>78.2</cell></row><row><cell>PointCNN [51]</cell><cell>1k</cell><cell>88.1</cell><cell>92.2</cell><cell>75.1</cell><cell>78.5</cell></row><row><cell>PRA-Net (Ours)</cell><cell>1k</cell><cell>90.6</cell><cell>93.2</cell><cell>77.9</cell><cell>81.0</cell></row><row><cell>PRA-Net (Ours)</cell><cell>2k</cell><cell>91.2</cell><cell>93.7</cell><cell>79.1</cell><cell>82.1</cell></row><row><cell cols="6">keypoint or not. We evaluate the proposed PRA-Net for</cell></row><row><cell cols="6">this task on the KeypointNet dataset. KeypointNet is built</cell></row><row><cell cols="6">on ShapeNetCore [68]. It contains 8,329 mesh models with</cell></row><row><cell cols="6">83,231 keypoints from 16 categories. Each model is downsam-</cell></row><row><cell cols="6">pled to 2,048 points during data preprocessing. The dataset is</cell></row><row><cell cols="6">split into train, validation, and test datasets, with 7:1:2 ratio.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II KEYPOINT</head><label>II</label><figDesc>SALIENCY ESTIMATION RESULTS IN MIOU (%) WITH A DISTANCE THRESHOLD OF 0.01 ON THE KEYPOINTNET BENCHMARK ("-": UNKNOWN). Methods mIoU air. bat. bed bot. cap car cha. gui. hel. kni. lap. mot mug ska. tab. ves. RSCNN [34] 15.9 21.0 11.9 19.3 11.6 18.9 15.8 17.6 17.9 0.0 24.2 25.3 13.4 17.2 5.9 23.7 10.1 25.8 37.1 13.4 31.4 27.2 32.3 14.2 29.4 44.1 26.7 26.3 22.8 37.6 21.7 TABLE III KEYPOINT SALIENCY ESTIMATION RESULTS IN MAP (%) WITH A DISTANCE THRESHOLD OF 0.01 ON THE KEYPOINTNET BENCHMARK ("-": UNKNOWN). Methods mAP air. bat. bed bot. cap car cha. gui. hel. kni. lap. mot mug ska. tab. ves.</figDesc><table><row><cell>SpiderCNN [29]</cell><cell>11.7</cell><cell cols="4">22.2 7.2 17.7 4.1</cell><cell>2.7</cell><cell cols="3">5.5 15.9 7.1</cell><cell cols="3">0.0 30.0 22.4 14.5 4.9</cell><cell>0.0 23.9 8.5</cell></row><row><cell>PointNet++ [10]</cell><cell>12.3</cell><cell cols="8">13.1 8.3 16.3 11.1 19.9 13.0 12.6 9.5</cell><cell cols="3">2.1 18.5 19.3 16.3 9.2</cell><cell>5.6 14.0 8.0</cell></row><row><cell>PointNet [9]</cell><cell>2.8</cell><cell>9.1</cell><cell>0.5</cell><cell>6.4</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>4.5</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0 11.6 1.9</cell><cell>0.0</cell><cell>0.0 11.0 0.0</cell></row><row><cell>PointConv [35]</cell><cell>17.9</cell><cell cols="9">25.3 15.2 32.4 7.3 13.5 20.3 21.7 21.2 2.1</cell><cell cols="3">5.0 27.8 18.9 21.7 13.2 26.8 13.9</cell></row><row><cell>RSNet [63]</cell><cell>15.1</cell><cell cols="12">20.5 12.8 19.2 13.1 15.7 15.1 13.9 16.4 8.4 18.3 22.8 20.2 16.8 4.0 15.4 9.7</cell></row><row><cell>DGCNN [11]</cell><cell>20.8</cell><cell cols="12">32.3 17.7 21.6 15.0 21.5 15.1 23.8 20.7 3.5 29.4 30.1 23.5 18.1 12.8 31.7 15.6</cell></row><row><cell cols="14">PRA-Net (Ours) 38.6 28.1 RSCNN [34] 28.6 23.2 34.4 17.3 28.4 16.8 31.6 16.3 21.7 18.2 3.1 30.6 37.9 23.8 25.4 9.7 41.4 14.7</cell></row><row><cell>SpiderCNN [29]</cell><cell>13.7</cell><cell cols="4">25.8 6.7 19.8 2.7</cell><cell>4.0</cell><cell cols="6">6.5 18.9 10.5 0.4 28.4 34.3 15.0 5.3</cell><cell>1.7 30.2 8.9</cell></row><row><cell>PointNet++ [10]</cell><cell>16.5</cell><cell cols="12">16.9 11.4 21.9 15.2 27.7 16.7 14.5 12.6 3.4 19.7 27.0 22.0 11.4 5.7 30.5 7.8</cell></row><row><cell>PointNet [9]</cell><cell>5.6</cell><cell>8.5</cell><cell>3.6</cell><cell>6.4</cell><cell>1.3</cell><cell>3.2</cell><cell>2.3</cell><cell>9.6</cell><cell>1.0</cell><cell cols="2">0.4 16.3 14.5 2.6</cell><cell>3.4</cell><cell>1.7 12.0 2.2</cell></row><row><cell>PointConv [35]</cell><cell>25.5</cell><cell cols="12">28.1 24.6 45.8 10.1 15.7 24.6 30.8 21.7 2.0 17.3 46.5 29.3 27.3 18.9 42.4 22.6</cell></row><row><cell>RSNet [63]</cell><cell>22.0</cell><cell cols="12">31.1 17.8 29.5 12.8 21.8 21.8 15.4 16.1 6.1 31.5 35.0 26.1 23.2 5.4 45.3 12.8</cell></row><row><cell>DGCNN [11]</cell><cell>28.8</cell><cell cols="12">43.8 26.2 33.4 20.7 27.6 21.4 30.3 22.9 4.8 40.5 46.4 29.2 24.9 17.3 52.1 19.7</cell></row><row><cell>PRA-Net (Ours)</cell><cell>43.2</cell><cell cols="12">47.0 45.2 50.9 61.2 17.0 54.5 38.7 43.1 16.4 34.7 70.4 43.5 38.5 39.2 61.4 30.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV PART</head><label>IV</label><figDesc>SEGMENTATION RESULTS (%) ON THE SHAPENET PART BENCHMARK ("-": UNKNOWN). cap car cha. ear. gui. kni. lam. lap. mot. mug pis. roc. ska. tab.</figDesc><table><row><cell cols="4">Methods air. bag PointNet [9] Cls. mIoU Ins. mIoU 80.4 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6</cell></row><row><cell>PointNet++ [10]</cell><cell>81.9</cell><cell>85.1</cell><cell>82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6</cell></row><row><cell>SCN [38]</cell><cell>81.8</cell><cell>84.6</cell><cell>83.8 80.8 83.5 79.3 90.5 69.8 91.7 86.5 82.9 96.0 69.2 93.8 82.5 62.9 74.4 80.8</cell></row><row><cell>SGPN [64]</cell><cell>82.8</cell><cell>85.8</cell><cell>80.4 78.6 78.8 71.5 88.6 78.0 90.9 83.0 78.8 95.8 77.8 93.8 87.4 60.1 92.3 89.4</cell></row><row><cell>SyncSpecCNN [65]</cell><cell>82.0</cell><cell>84.7</cell><cell>81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1</cell></row><row><cell>RSNet [63]</cell><cell>81.4</cell><cell>84.9</cell><cell>82.7 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2</cell></row><row><cell>PointCNN [51]</cell><cell>84.6</cell><cell>86.1</cell><cell>84.1 86.5 86.0 80.8 90.6 79.7 92.3 88.4 85.3 96.1 77.2 95.3 84.2 64.2 80.0 83.0</cell></row><row><cell>DGCNN [11]</cell><cell>82.3</cell><cell>85.1</cell><cell>84.2 83.7 84.4 77.1 90.9 78.5 91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7 75.5 82.0</cell></row><row><cell>Point2Seq [57]</cell><cell>-</cell><cell>85.2</cell><cell>82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8</cell></row><row><cell>A-CNN [59]</cell><cell>-</cell><cell>85.9</cell><cell>83.9 86.7 83.5 79.5 91.3 77.0 91.5 86.0 85.0 95.5 72.6 94.9 83.8 57.8 76.6 83.0</cell></row><row><cell>RSCNN [34]</cell><cell>84.0</cell><cell>86.2</cell><cell>83.5 84.8 88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.0 73.7 94.1 83.4 60.5 77.7 83.6</cell></row><row><cell>DensePoint [56]</cell><cell>84.2</cell><cell>86.4</cell><cell>84.0 85.4 90.0 79.2 91.1 81.6 91.5 87.5 84.7 95.9 74.3 94.6 82.9 64.6 76.8 83.7</cell></row><row><cell>DPAM [66]</cell><cell>-</cell><cell>86.1</cell><cell>84.3 81.6 89.1 79.5 90.9 77.5 91.8 87.0 84.5 96.2 68.7 94.5 81.4 64.2 76.2 84.3</cell></row><row><cell>ShellNet [67]</cell><cell>82.8</cell><cell>-</cell><cell>84.3 79.6 88.9 79.1 90.0 79.4 91.3 85.9 82.3 95.4 68.6 94.9 82.7 61.5 79.7 81.7</cell></row><row><cell>Point2Cap [53]</cell><cell>-</cell><cell>85.3</cell><cell>83.5 83.4 88.5 77.6 90.8 79.4 90.9 86.9 84.3 95.4 71.7 95.3 82.6 60.6 75.3 82.5</cell></row><row><cell>PRA-Net (Ours)</cell><cell>83.7</cell><cell>86.3</cell><cell>84.4 86.8 89.5 78.4 91.4 76.4 91.5 88.2 85.3 95.7 73.4 94.8 82.1 62.3 75.5 84.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV</head><label>IV</label><figDesc></figDesc><table><row><cell>summarizes quantitative experimental re-</cell></row><row><cell>sults of various typical methods, where our method achieves</cell></row><row><cell>competitive values in instance mIoU of 86.3% and class</cell></row><row><cell>mIoU of 83.7%. Concretely, our model outperforms Point-</cell></row><row><cell>Net++ [10], DGCNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V EFFECTIVENESS</head><label>V</label><figDesc></figDesc><table><row><cell></cell><cell cols="5">OF DIFFERENT COMPONENTS EVALUATED ON THE</cell></row><row><cell></cell><cell></cell><cell cols="3">SCANOBJECTNN DATASET (%)</cell><cell></cell></row><row><cell>Model A B C D E</cell><cell>SFL ? ? ? ?</cell><cell>ISL NFL ? ? ? ?</cell><cell>DFA ? ?</cell><cell>IRL ?</cell><cell>OA 66.5 76.6 78.7 79.8 81.0</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="6">EFFECTIVENESS OF REGION PARTITION METHODS EVALUATED ON THE</cell></row><row><cell cols="5">SCANOBJECTNN AND KEYPOINTNET DATASET (%)</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>ScanObjectNN OA</cell><cell cols="3">KeypointNet mIoU mAP</cell></row><row><cell>FPS</cell><cell></cell><cell>80.4</cell><cell>28.1</cell><cell cols="2">41.6</cell></row><row><cell>top-S</cell><cell></cell><cell>80.6</cell><cell>28.1</cell><cell cols="2">42.0</cell></row><row><cell cols="2">dilated top-S</cell><cell>81.0</cell><cell>28.6</cell><cell cols="2">43.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII THE</head><label>VII</label><figDesc>RESULTS (%) ON THE SCANOBJECTNN OF DIFFERENT SAMPLING STRATEGIES h WITH VARYING NUMBER OF REPRESENTATIVE POINTS m IN EACH REGION, DESCRIBED IN EQ. (8). FOR THE RANDOM SAMPLING BASED METHOD, WE RUN THE TRAINED MODEL 10 TIMES ON TEST SET TO REPORT THE RESULTS IN THE MEAN ? STD FORMAT our IRL module, we visualize how the other points influence the anchor points (red) in Fig. 9. We observe that our IRL module can successfully capture different types of inter-region relations, such as symmetrical parts in space (the first three columns), semantic dependencies of different parts (the last three columns), which demonstrates the validity of our design logic. The Naive Version vs. the Representative Point-based Version of the IRL module. To quantify the efficiency brought by the representative point-based version over the naive version of the IRL module, we measure their inference time (ms). For fair comparisons, all the experiments are conducted on a platform with a single NVIDIA RTX 2080TI GPU. The batch size, the channel of the input feature, and point number k per region are fixed to 32, 256, and 6, respectively. The detailed results are reported in Table IX. As shown, compared with the naive version, the representative point-based method</figDesc><table><row><cell>(a)</cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell></cell><cell></cell></row><row><cell>h</cell><cell>m</cell><cell>OA</cell></row><row><cell></cell><cell>1</cell><cell>80.3</cell></row><row><cell>k-NN</cell><cell>2 4</cell><cell>80.6 81.0</cell></row><row><cell></cell><cell>6</cell><cell>80.5</cell></row><row><cell></cell><cell>1</cell><cell>80.17 ? 0.31</cell></row><row><cell>random</cell><cell>2 4</cell><cell>80.39 ? 0.35 80.52 ? 0.28</cell></row><row><cell></cell><cell>6</cell><cell>80.44 ? 0.29</cell></row><row><cell>mean</cell><cell>-</cell><cell>80.2</cell></row><row><cell>max</cell><cell>-</cell><cell>80.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII THE</head><label>VIII</label><figDesc>RESULTS (%) ON THE SCANOBJECTNN DATASET BY VARYING THE GROUP NUMBER S AND REPRESENTATIVE POINT NUMBER m IN EACH IRL TABLE IX COMPARISON OF INFERENCE TIME (INF.) BETWEEN THE NAIVE AND THE REPRESENTATIVE POINT-BASED (rep.) METHODS. THE LOWER THE VALUE, decreases the inference time by 67.8%, when the point number N , the representative number m, and the group number S are set to 1,024, 1, and 256. More significant improvements can be observed when we increase the input point number. For instance, when the point number reaches 4,096, the representative point-based version is around 7 times faster on inference. These results consistently demonstrate the efficiency of the representative point-based method.</figDesc><table><row><cell></cell><cell cols="2">MODULE</cell><cell></cell><cell></cell></row><row><cell>m</cell><cell>S</cell><cell></cell><cell></cell><cell>OA</cell></row><row><cell></cell><cell cols="2">(64, 32, 16)</cell><cell></cell><cell>80.4</cell></row><row><cell></cell><cell cols="2">(128, 64, 32)</cell><cell></cell><cell>80.7</cell></row><row><cell>4</cell><cell cols="2">(256, 128, 64)</cell><cell></cell><cell>81.0</cell></row><row><cell></cell><cell cols="2">(512, 256, 128)</cell><cell></cell><cell>80.7</cell></row><row><cell></cell><cell cols="2">(1024, 512, 256)</cell><cell></cell><cell>80.4</cell></row><row><cell></cell><cell cols="2">(64, 32, 16)</cell><cell></cell><cell>80.4</cell></row><row><cell></cell><cell cols="2">(128, 64, 32)</cell><cell></cell><cell>80.5</cell></row><row><cell>2</cell><cell cols="2">(256, 128, 64)</cell><cell></cell><cell>80.6</cell></row><row><cell></cell><cell cols="2">(512, 256, 128)</cell><cell></cell><cell>80.6</cell></row><row><cell></cell><cell cols="2">(1024, 512, 256)</cell><cell></cell><cell>80.3</cell></row><row><cell></cell><cell cols="2">THE BETTER</cell><cell></cell><cell></cell></row><row><cell>Points</cell><cell>Methods</cell><cell>S</cell><cell>m</cell><cell>Inf. (ms)</cell></row><row><cell></cell><cell>naive</cell><cell>256</cell><cell>-</cell><cell>18.3</cell></row><row><cell>1,024</cell><cell>rep.</cell><cell>256 256</cell><cell>1 4</cell><cell>5.9 8.9</cell></row><row><cell></cell><cell>naive</cell><cell>512</cell><cell>-</cell><cell>53.3</cell></row><row><cell>2,048</cell><cell>rep.</cell><cell>512 512</cell><cell>1 4</cell><cell>13.8 22.1</cell></row><row><cell></cell><cell>naive</cell><cell>512</cell><cell>-</cell><cell>180.0</cell></row><row><cell>4,096</cell><cell>rep.</cell><cell>512 512</cell><cell>1 4</cell><cell>24.4 33.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that here we assume a point is its 1st nearest neighbor to itself for convenience (i.e., p i,1 = p i ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For fair comparisons, we report the results of RSCNN and DensePoint without voting strategy according to their paper.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Edge and corner detection for unorganized 3d point clouds with application to robotic welding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mamun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IROS</title>
		<meeting>IROS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7350" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution tree networks for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vv-net: Voxel vae net with group convolutions for point cloud segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8500" to="8508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d2seqviews: Aggregating sequential views for 3d global feature learning by cnn with hierarchical attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3986" to="3999" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>1, 2, 6, 7, 8</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS<address><addrLine>1, 2, 7, 8</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointweb: Enhancing local neighborhood features for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5565" to="5573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Laws of organization in perceptual forms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wertheimer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1923" />
		</imprint>
	</monogr>
	<note>A source book of Gestalt Psychology</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lrc-net: Learning discriminative features on point clouds by encoding local region contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Aided Geom Des</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gift: A realtime and scalable 3d shape search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5023" to="5032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seqviews2seqlabels: Learning 3d global features via aggregating sequential views by rnn with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Vong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="658" to="672" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View n-gram network for 3d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7515" to="7524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">View-gcn: View-based graph convolutional network for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="1850" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning relationships for multi-view 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7505" to="7514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IROS</title>
		<meeting>IROS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Point2node: Correlation learning of dynamic-node for point cloud feature modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geometry sharing network for 3d point cloud classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="500" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fpconv: Learning local flattening for point convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpolated convolutional networks for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation-shape convolutional neural network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="8895" to="8904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structural relational reasoning of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="949" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09688</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Keypointnet: A large-scale 3d keypoint dataset aggregated from numerous human annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Uy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scenenn: A scene meshes dataset with annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DV</title>
		<meeting>3DV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G. Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NurIPS</title>
		<meeting>NurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Point2spatialcapsule: Aggregating features and spatial relationships of local regions on point clouds using spatial-aware capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="8855" to="8869" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Octree guided cnn with spherical kernels for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9631" to="9640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3323" to="3332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5239" to="5248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Point2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8778" to="8785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-angle point cloudvae: Unsupervised feature learning for 3d point clouds from multiple angles by joint self-reconstruction and half-to-half prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zwicker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="441" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A-cnn: Annularly convolutional neural networks on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Komarichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7421" to="7430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A closer look at local aggregation operators in point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="326" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fatnet: A feature-attentive network for 3d point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Pears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR, 2020</title>
		<meeting>ICPR, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">3dmfv: Threedimensional point cloud classification in real-time using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3145" to="3152" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2626" to="2635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2282" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic points agglomeration for hierarchical point sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7546" to="7555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
