<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Contour-Saliency Blending Network for Accurate Salient Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">Yi</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">AI Technology Lab</orgName>
								<address>
									<postCode>OPEN8</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Tsubono</surname></persName>
							<email>tsubonot@open8.com</email>
							<affiliation key="aff0">
								<orgName type="institution">AI Technology Lab</orgName>
								<address>
									<postCode>OPEN8</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Contour-Saliency Blending Network for Accurate Salient Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contour information plays a vital role in salient object detection. However, excessive false positives remain in predictions from existing contour-based models due to insufficient contour-saliency fusion. In this work, we designed a network for better edge quality in salient object detection. We proposed a contour-saliency blending module to exchange information between contour and saliency. We adopted recursive CNN to increase contour-saliency fusion while keeping the total trainable parameters the same. Furthermore, we designed a stage-wise feature extraction module to help the model pick up the most helpful features from previous intermediate saliency predictions. Besides, we proposed two new loss functions, namely Dual Confinement Loss and Confidence Loss, for our model to generate better boundary predictions. Evaluation results on five common benchmark datasets reveal that our model achieves competitive state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Salient object detection (SOD) aims to detect and segment the most attention-attractive region or object in a visual scene. Unlike eye fixation prediction (FP) <ref type="bibr" target="#b33">[34]</ref>, SOD requires obtaining the entire region with clear boundaries. Due to its essential role and wide applications in image understanding <ref type="bibr" target="#b52">[53]</ref>, image captioning <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b41">[42]</ref>, and video summarization <ref type="bibr" target="#b24">[25]</ref>, recently, various methods have been proposed in the field.</p><p>Since 2015, Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b14">[15]</ref> have been adopted for SOD tasks. Though algorithms like PiCANet <ref type="bibr" target="#b22">[23]</ref>, BMPM <ref type="bibr" target="#b44">[45]</ref>, and PAGRN <ref type="bibr" target="#b46">[47]</ref> achieved significantly better results, the predicted object usually has poor boundaries. To obtain more precise boundaries, proposed by Qin et al., BASNet <ref type="bibr" target="#b30">[31]</ref> adopted a boundary refinement U-Net <ref type="bibr" target="#b31">[32]</ref> at the end of the saliency detection network and trained their model using various losses. <ref type="bibr">*</ref> Corresponding author (a) (b) (c) (d) <ref type="figure">Figure 1</ref>: Visual comparisons between contour-based models. Saliency (first row) and corresponding contour predictions (last row) are listed. Ground truth contours are obtained via erosion and dilation with kernel size of 5. (a) ground truth, (b) ours, (c) ITSD <ref type="bibr" target="#b51">[52]</ref> and (d) PoolNet <ref type="bibr" target="#b21">[22]</ref>.</p><p>In <ref type="bibr" target="#b4">[5]</ref>, Chen et al. proposed Contour Loss (CTLoss), which was a weighted Binary Cross-Entropy (BCE) loss, to improve the boundary predictions. Alternatively, models like EGNet <ref type="bibr" target="#b48">[49]</ref>, PoolNet <ref type="bibr" target="#b21">[22]</ref>, and ITSD <ref type="bibr" target="#b51">[52]</ref> fused contour predictions with saliency by explicitly supervising a contour branch. With contour cues, models yielded better boundary predictions. However, above mentioned models still hold several problems that can be further improved. First, for better performance, many studies have introduced a huge number of trainable parameters. EGNet contains 108 million parameters, BASNet and its extended work, U2Net <ref type="bibr" target="#b29">[30]</ref>, have more than 87 and 44 million parameters, respectively <ref type="figure">(Fig. 2)</ref>. The huge number of parameters not only leads to increased consumption of computational resource, but also makes the model difficult to train. Second, though object boundary is greatly improved for contour-based networks <ref type="bibr" target="#b21">[22]</ref>[49] <ref type="bibr" target="#b51">[52]</ref>, predictions still have excessive false positives, as shown in <ref type="figure">Fig. 1c and 1d</ref>. Finally, to our best knowledge, for all deep learning-based models in the current field, intermedi- <ref type="figure">Figure 2</ref>: Mean F-score, total parameter and MAE comparison between RCSB with 13 state-of-the-art models, including EGNet <ref type="bibr" target="#b48">[49]</ref>, PiCANet <ref type="bibr" target="#b22">[23]</ref>, C2SNet <ref type="bibr" target="#b17">[18]</ref>, PoolNet <ref type="bibr" target="#b21">[22]</ref>, PAGE <ref type="bibr" target="#b36">[37]</ref>, Amulet <ref type="bibr" target="#b45">[46]</ref>, ITSD <ref type="bibr" target="#b51">[52]</ref>, BASNet <ref type="bibr" target="#b30">[31]</ref>, U2Net <ref type="bibr" target="#b29">[30]</ref>, NLDF <ref type="bibr" target="#b23">[24]</ref>, AFNet <ref type="bibr" target="#b8">[9]</ref>, F3Net <ref type="bibr" target="#b38">[39]</ref> and CPD <ref type="bibr" target="#b39">[40]</ref> on DUTS-TE <ref type="bibr" target="#b34">[35]</ref> dataset. Bubble size represents model size.</p><p>ate saliency or contour predictions are generated and supervised via side branches, which introduce redundant parameters and inefficiency.</p><p>To address the abovementioned issues, we proposed a recursive contour-saliency blending network, namely, RCSB-Net, for high accuracy salient object detection. We adopted a recursive CNN to reduce total trainable parameters while we can make our model deep. Unlike previous studies <ref type="bibr" target="#b21">[22]</ref>[49] <ref type="bibr" target="#b51">[52]</ref>, where the contour and saliency are explicitly trained via two branches, we introduced a Contour-Saliency Blending (CSB) module in our network so that contour and saliency are intertwined and fused every step in the recursion. Meanwhile, to further improve the efficiency, we proposed a Stage-wise Feature Extraction (SFE) module to directly supervise intermediate saliency and contour predictions in the primary network without using any side branch. Lastly, we divided the training task into accuracy and confidence, and proposed Dual Confinement Loss (DCLoss) and Confidence Loss (CLoss) respectively for better model performance. To sum up, our contributions are as follows:</p><p>(1) We proposed an efficient and accurate network, RCSBNet. By using a recursive CNN and the proposed Stage-wise Feature Extraction (SFE) module, contour and saliency are fused more efficiently and effectively.</p><p>(2) We developed two loss functions, DCLoss and CLoss, to further help the boundary prediction.</p><p>(3) Our model has only 27.9 million parameters, which is significantly smaller and efficient than most of the networks in the field <ref type="figure">(Fig. 2</ref>).</p><p>(4) We conducted comprehensive evaluations on 5 widely used benchmark datasets and compared with 13 state-of-the-art methods. Our method achieves competitive state-of-the-art results among all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early approaches based on hand-crafted priors <ref type="bibr" target="#b5">[6]</ref>[12] <ref type="bibr" target="#b43">[44]</ref> have limited effectiveness and generalization ability. The very first deep salient object detection (SOD) methods <ref type="bibr" target="#b16">[17]</ref>[50] used multi-layer perceptron to predict saliency score for each image. These methods suffered from low efficiency and damage of feature structures due to flattening. Later, some studies introduced a fully convolutional network (FCN) and achieved promising results.</p><p>Recurrent Networks. In <ref type="bibr" target="#b19">[20]</ref>, a recurrent convolutional neural network (RCNN) was proposed for object detection. The main idea was to unfold the same convolution layer several times while weights are shared. It had the advantage that model depth can now be deeper by unfolding, while the total number of trainable parameters remains the same. It also revealed that, by increasing the number of recursions, better results would be obtained.</p><p>In 2016, the recurrent CNN was introduced to salient object detection task, and proposed by Wang et al. RFCN <ref type="bibr" target="#b35">[36]</ref> recursively refines the saliency prediction from previous time step. Later, proposed by Kuen et al. <ref type="bibr" target="#b15">[16]</ref>, a recurrent network was designed to refine selected image subregions iteratively. In <ref type="bibr" target="#b46">[47]</ref>, Zhang et al. designed a multipath recurrent model for saliency detection by transferring global information from deep layers to shallower layers. Hu et al. <ref type="bibr" target="#b32">[33]</ref> proposed their salient object detection model by concatenating multi-layer deep features recurrently. It was proved that saliency predictions will be refined by using recurrent mechanism.</p><p>Utilizing Contour Information. In recent years some studies explored and verified the effectiveness of involving contour information to improve the accuracy of saliency prediction. In <ref type="bibr" target="#b4">[5]</ref>, Chen et al. considered boundary pixels as hard samples and proposed a contour loss, which was a weighted BCE loss, to train their network. Qin et al. <ref type="bibr" target="#b30">[31]</ref> combined Structural Similarity Index (SSIM), Intersection over Union (IOU), and BCE as their contour-aware loss function to achieve better boundary quality. In another seminal work, Salient Edge Detector (SED) <ref type="bibr" target="#b36">[37]</ref> was introduced to simultaneously generate saliency and contour predictions by using a residual structure. Furthermore, PoolNet <ref type="bibr" target="#b21">[22]</ref> applied multi-task training and fused the contour information with saliency predictions. Later, ITSD <ref type="bibr" target="#b51">[52]</ref> proposed a two-stream network to convert saliency and contour interactively and yielded good boundary predictions. These studies further corroborated the importance of employing contour information to improve saliency predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channel</head><p>Pooling RCSB SFE Refinement Legend At the end of the network, a refinement module is adopted to further refine the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Architecture</head><p>In previous contour-related networks <ref type="bibr" target="#b21">[22]</ref>[49] <ref type="bibr" target="#b51">[52]</ref>, either the contour was supervised in a separate branch to guide the saliency prediction, or it was fused with saliency stage by stage to achieve better boundary predictions. Both approaches gave promising results, but there are two major disadvantages: 1) late fusion: contours are fused with saliency at the end of each stage. 2) limited fusion: the number of fusion is limited by the number of U-Net stages. For late fusion, we designed a Contour-Saliency Blending Unit (CSBU) so that contour and saliency information can be exchanged at a much earlier stage, while for limited fusion, a recursive mechanism was adopted to circumvent this constraint.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 3</ref>, the proposed RCSBNet is essentially a U-Net, where we employ pre-trained ResNet-50 as our encoder and a customized decoder adopting recursive CNNs. Contour and saliency are blended in the recursion block by the Contour-Saliency Blending Unit (CSBU). Then saliency and contour features are split and fed into the Stage-wise Feature Extraction (SFE) module for supervised learning. At the last stage of the decoder, we concatenate the prediction of contour and saliency with the input image, followed by an additional recursive block, to generate the final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ResNet-50 as the Encoder</head><p>We employ the pre-trained ResNet-50 as our encoder.</p><p>Since it contains a large number of feature maps in highlevel blocks, same as <ref type="bibr" target="#b51">[52]</ref>, we apply channel pooling (CP) to reduce the number of channels to 64, and the operation can be expressed as:</p><formula xml:id="formula_0">CP = collect j?[0,m?1] (max k?[0, n m ?1] X j? n m +k ) (1)</formula><p>where j, k are integers, and we divide total n channels into m groups then apply max-pooling. A convolution layer is attached after the channel pooling layer to prepare the features for further processing by the decoder, as shown in <ref type="figure" target="#fig_0">Fig.  3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contour-Saliency Blending Unit (CSBU)</head><p>Contour information is usually supervised and fused with saliency at the end of each U-Net stage. For us, we want to fuse the contour and saliency at earlier stages and as many times as possible. Inspired by Shuffle-Net <ref type="bibr" target="#b47">[48]</ref>, where convolution was divided into groups and information is exchanged by shuffling the weight channels, we designed our Contour-Saliency Blending Unit (CSBU). As illustrated in <ref type="figure" target="#fig_1">Fig. 4a</ref>, the CSBU has a contour branch and saliency branch. Features generated by each branch are concatenated and blended. Mathematically, let F sal k?k and F ctr k?k denote the convolution for saliency and contour with kernel size k respectively, while (C n?1 , S n?1 ) and (C n , S n ) represent contour and saliency features before and after CSBU. For simplicity, we omit ReLU <ref type="bibr" target="#b1">[2]</ref> and BatchNorm <ref type="bibr" target="#b10">[11]</ref> in our formula, and then the proposed CSBU can be modeled as:</p><formula xml:id="formula_1">CT R = F ctr 3?3 (C n?1 ) (2) SAL = F sal 3?3 (S n?1 ) (3) C n = F ctr 3?3 (F ctr 1?1 ([CT R, SAL]))<label>(4)</label></formula><formula xml:id="formula_2">S n = F sal 3?3 (F sal 1?1 ([CT R, SAL]))<label>(5)</label></formula><p>and thus:</p><p>S n , C n = CSBU (S n?1 , C n?1 )</p><p>By applying CSBU, contour and saliency information can be utilized by the network at a much earlier stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Recursive Block</head><p>We now introduce more details of the recursive block, as illustrated in <ref type="figure" target="#fig_1">Fig. 4b</ref>. Firstly let us consider a single recursive block then we extend to more general cases. We denote R as the total number of recursions in a single block, and f r (r = 1, 2, ..., R) denote the r th recursion of the CSBU. Based on Eq. 6, let X n?1 denote the input tuple of (S n?1 , C n?1 ), and X R represent the output of the R th recursion, then we have:</p><formula xml:id="formula_4">X R = f R (...(f 2 (f 1 (X n?1 )+X n?1 )+X n?1 )+...+X n?1 )<label>(7)</label></formula><p>note that weights for CSBU are shared in recursion, though they have different superscripts in the formula. Then we apply a single convolution layer, denoted as F 3?3 , at the end of recursion with skip connection. Thus the output of our recursive block, X n , is:</p><formula xml:id="formula_5">X n = F 3?3 (X R + X n?1 ) + X n?1 = h b (X n?1 ) (8)</formula><p>where h b represents the recursive block function. Now let G represent the total number of recursive blocks, as shown in <ref type="figure" target="#fig_1">Fig. 4c</ref>, in our network we simply stack all the recursive blocks together. Thus the output of g-th block, X n , is:</p><formula xml:id="formula_6">X n = h g b (h g?1 b (...h 2 b (h b (X n?1 ))))<label>(9)</label></formula><p>where h g b represents the g-th recursive block function. By applying recursive blocks and stacking them together, contour and saliency can now be fused G ? R times at each stage of the U-Net, which will improve the network performance significantly. We will show more results in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Stage-wise Feature Extraction (SFE) Module</head><p>It is prevalent that saliency networks are densely supervised, where intermediate stage features are generated and supervised against ground truths to guide the model for better convergence. Common practices create a side branch with a few convolution layers to generate predictions from current U-Net stage features, following which the stage features are passed on to the next U-Net stage in the primary network ( <ref type="figure" target="#fig_1">Fig. 4e)</ref>. Different from others, we regard stage predictions as the best result the network can obtain so far and apply a new round of feature extraction based on current stage predictions. Therefore, stage predictions are now in the network's main branch, acting as a single channel layer, and supervised against ground truths, as illustrated in <ref type="figure" target="#fig_1">Fig. 4d</ref>. By doing so, the next block is expected to extract valuable features from current best results and discard all useless features, which might otherwise be carried along by recursion and residual connections.</p><p>To generate stage predictions, unlike previous studies <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b21">[22]</ref>[52], we do not use max-pooling because it will bring false positives to the next stage, nor the element-wise multiplication between contour and saliency like SCRN <ref type="bibr" target="#b40">[41]</ref> due to the introduction of false negatives. Instead, we employ a 1 ? 1 convolution and a scaling factor to help sigmoid function classify saliency and background. To formulate the SFE module mathematically, let C i and S i represent incoming i channels of feature maps, and K represent the scaling factor learned by the network, then stage prediction P j with j channels can be expressed as:</p><formula xml:id="formula_7">P 1 sal = F sal 1?1 (S 64 n?1 ) * K sal<label>(10)</label></formula><formula xml:id="formula_8">P 1 ctr = F ctr 1?1 (C 64 n?1 ) * K ctr<label>(11)</label></formula><p>where * represents element-wise multiplication. For extracted feature maps M j n with j channels:</p><formula xml:id="formula_9">M 64 n = F 3?3 (U p ?2 [?(P 1 sal ), ?(P 1 ctr )])<label>(12)</label></formula><p>where [ ] stands for concatenating, U p ?2 is up-sampling, and ? stands for sigmoid function. SFE is a simple module but it is very effective, we will domonstrate more in ablation studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Loss Functions</head><p>Unsure prediction is very common in salient object detection in the form of shady areas <ref type="figure" target="#fig_2">(Fig. 5b)</ref>. Many studies try to improve the performance by focusing on hard pixels near the boundary <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b38">[39]</ref>. Boundary pixels are indeed hard samples, but not all the hard samples are near the boundary. We notice that the network can correctly predict the saliency for some images, but with low pixel values; while for some other images, the network is confident of generating false negatives (FN) or false positives (FP), as illustrated in <ref type="figure" target="#fig_2">Fig.  5e</ref> and 5f. It points out two kinds of difficulties encountered by the network: 1) unconfident but accurate predictions and 2) confident but inaccurate predictions. Hence in addition to accuracy, we factor in the confidence of predictions, and we introduce Confidence Loss (CLoss) to our training.</p><p>Confidence Loss. Recently focal loss [21] <ref type="bibr" target="#b37">[38]</ref> has been explored in saliency tasks due to its high weight on wrong predictions: W f ocal = ?(? i,j ? x i,j ) ? . However, focal loss becomes less sensitive as predictions approach ground truth, which eventually leaves a large area of unsure predictions.</p><p>In order to guide the model focus more on the unconfident predictions, we propose a confidence score, W c , for each predicted pixel x i,j : where ? is empirically set to 2, and x i,j is the prediction after sigmoid. When x i,j = 0.5 the score reaches its maximum. Then with ground truth? i,j , our confidence loss (CLoss), L con , is defined as:</p><formula xml:id="formula_10">W c = ? * x i,j * (1 ? x i,j )<label>(13)</label></formula><formula xml:id="formula_11">L con (?, ?) = ? 1 n n i=1 [W c * BCE(x i,j ,? i,j )+?W c ] (14)</formula><p>where ? is set to 0.3 by parameter search. By applying this loss, it will encourage the model to make more confident predictions into either foreground (close to 1) or background (close to 0). Dual Confinement Loss. Most of the contour-based models use a separate contour branch to guide saliency predictions. In <ref type="bibr" target="#b4">[5]</ref>, and <ref type="bibr" target="#b51">[52]</ref>, a weight map generated from contour ground truth is applied to BCE loss to improve saliency boundary quality while the contour branch is supervised by BCE loss without any weights. It is reasonable because contour information is from a separate branch and is only used to guide saliency. Since in our network, saliency and contour streams are going to exchange information; thus we not only use contour to guide saliency predictions but also want to use saliency to guide contour predictions. Based on this, we designed our Dual Confinement Loss (DCLoss). The loss uses weight map generated by contour information to confine saliency predictions, and uses saliency information to generate weight map to guide contour predictions. This is due to the high correlation of contour and saliency information. The losses for each stream, L sal and L ctr , are defined as:</p><formula xml:id="formula_12">L sal = ? 1 n n i=1 [W sal * BCE(x sal i ,? sal i )] (15) L ctr = ? 1 n n i=1 [W ctr * BCE(x ctr i ,? ctr i )]<label>(16)</label></formula><p>where weight matrix W sal and W ctr are calculated by:</p><formula xml:id="formula_13">W sal = max(x ctr i ,? ctr i ) * ? + 1 (17) W ctr = max(x sal i ,? sal i ) * ? + 1<label>(18)</label></formula><p>where x,? stand for prediction and ground truth, and empirically, we set ? = 4 in our experiments. Then the DCLoss, L DC , is defined as:</p><formula xml:id="formula_14">L DC = L sal + L ctr<label>(19)</label></formula><p>Loss for Training. Compared with low-level predictions, it should be relatively easy to emphasize accuracy on highlevel predictions due to its small feature dimension. Meanwhile, a failure in high-level predictions will impact its following decoders and eventually cause false positives or negatives. Thus, we train stages 1 to 5 of our network against accuracy-related losses, i.e. DCLoss and weighted IOU loss mentioned in <ref type="bibr" target="#b38">[39]</ref>, and train the refinement module against CLoss only. Our loss functions for saliency and contour are defined as:</p><formula xml:id="formula_15">L saliency = L 1?5 DC + L 1?5 wIOU + L ref con (?, ? = 2, 0.3) (20) L contour = L 1?5 DC + L ref con (?, ? = 2, 0.3)<label>(21)</label></formula><p>where superscripts 1 ? 5 and ref represent the five decoder stages and refinement module in <ref type="figure" target="#fig_0">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use DUT-OMRON <ref type="bibr" target="#b43">[44]</ref>(5168 images), ECSSD <ref type="bibr" target="#b42">[43]</ref>(1000 images), PASCAL-S <ref type="bibr" target="#b18">[19]</ref>(850 images), HKU-IS <ref type="bibr" target="#b16">[17]</ref>(4447 images), and DUTS-TE <ref type="bibr" target="#b34">[35]</ref>(5019 Images) as our evaluation datasets. <ref type="bibr" target="#b34">[35]</ref> is used for training with input images resized to 256?256, then random horizontal flipping and 90 ? rotation are applied as the augmentation. We use pre-trained ResNet-50 <ref type="bibr" target="#b9">[10]</ref> as the encoder. The number of recursive blocks G is set to (1,2,2,3,3,5) with recursion R = 3 for the network <ref type="figure" target="#fig_0">(Fig. 3)</ref>. Besides, we apply Leaky ReLU <ref type="bibr" target="#b12">[13]</ref> and adopt the Adam optimizer <ref type="bibr" target="#b13">[14]</ref> with default hyperparameters to train our network. Learning rates for encoder and decoder are set to 10 ?5 and 10 ?4 respectively, and they are halved every 20 epochs with a total of 100 epochs using a batch size of 4. During testing, images are resized to 256?256, and the predictions (256?256) are resized back to their original size by using bilinear interpolation. We use PyTorch <ref type="bibr" target="#b28">[29]</ref> and a single RTX 3090 GPU for our model and experiments. Code will be released soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUTS-TR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Precision-Recall (PR) curve, F ? -measure <ref type="bibr" target="#b0">[1]</ref>, Mean Absolute Error (MAE), F ? ? -measure <ref type="bibr" target="#b25">[26]</ref>, and E ? -measure <ref type="bibr" target="#b6">[7]</ref> are adopted in our experiments.</p><p>PR-Curve. By applying different thresholds from 0 to 255, PR curve is obtained by comparing the ground truth masks against the binarized saliency predictions.</p><p>F-measure. The F ? -measure is calculated by precision and recall value of saliency maps:</p><formula xml:id="formula_16">F ? = (1+? 2 )?P recision?Recall ? 2 ?P recision+Recall</formula><p>where ? 2 is set to 0.3 <ref type="bibr" target="#b0">[1]</ref>. We report the average score over all thresholds from 0 to 255 and denote as F ? [24] <ref type="bibr" target="#b44">[45]</ref>.</p><p>MAE. MAE is the mean value of the sum of pixelwise absolute differences between predictions x and ground truths?:</p><formula xml:id="formula_17">M AE = 1 n n i=1 |x i ?? i |.</formula><p>Weighted F-measure. F ? ? uses weighted precision and weighted recall to measure both exactness and completeness of the prediction against ground truth. It is designed to improve the existing F ? -measure. E-measure. By using local pixel values and the imagewise mean, E ? calculates the similarity between the prediction and the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparisons with State-of-the-art Results</head><p>We compare our results with 13 state-of-the-art salient object detection networks, including EGNet <ref type="bibr" target="#b48">[49]</ref>, PoolNet <ref type="bibr" target="#b21">[22]</ref>, ITSD <ref type="bibr" target="#b51">[52]</ref>, AFNet <ref type="bibr" target="#b8">[9]</ref>, PAGE <ref type="bibr" target="#b36">[37]</ref>, CPD <ref type="bibr" target="#b39">[40]</ref>, BAS-Net <ref type="bibr" target="#b30">[31]</ref>, CAGNet <ref type="bibr" target="#b26">[27]</ref>, GateNet <ref type="bibr" target="#b50">[51]</ref>, U2Net <ref type="bibr" target="#b29">[30]</ref>, GCPA <ref type="bibr" target="#b3">[4]</ref>, MINet <ref type="bibr" target="#b27">[28]</ref>, and F3Net <ref type="bibr" target="#b38">[39]</ref>. Saliency maps used are provided by authors.</p><p>Quantitative Evaluation. To compare our work with the state-of-the-art networks, detailed experimental results in terms of four metrics are listed in <ref type="table" target="#tab_0">Table 1</ref>. Among all the models, RCSBNet achieves outstanding results across all four metrics on most datasets. Besides, PR and F-measure curves are demonstrated in <ref type="figure" target="#fig_4">Fig. 7</ref>. Our F-measure curves are flatter than all other models, which reveals that our results are closer to binary predictions and invariant to threshold changes.</p><p>Qualitative Evaluation. Visual comparisons are listed in <ref type="figure" target="#fig_3">Fig. 6</ref>. Compare with other contour-based network results, our method yields better boundary predictions. As shown in the graph, our model can produce accurate and complete saliency maps with better edges.  <ref type="bibr" target="#b48">[49]</ref> .   </p><formula xml:id="formula_18">Method DUTS-TE HKU-IS PASCAL-S ECSSD DUT-OMRON F ? ? M ? E ? ? F ? ? ? F ? ? M ? E ? ? F ? ? ? F ? ? M ? E ? ? F ? ? ? F ? ? M ? E ? ? F ? ? ? F ? ? M ? E ? ? F ? ? ? Contour-based Methods EGNet19</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>Effectiveness of the early fusion (EF). To prove that early fusion of contour and saliency information will boost model performance, an experiment was conducted by removing the fusion branch illustrated in <ref type="figure" target="#fig_1">Fig. 4a</ref> and make it into two seperate streams. We list both quantitative and qualitative measures between the two approaches on DUTS-TE and ECSSD datasets, as shown in <ref type="table" target="#tab_2">Table 2</ref> and <ref type="figure" target="#fig_5">Fig. 8</ref>.  Effectiveness of refinement module and supervised on confidence. To study the importance of the refinement module and prove the effectiveness of supervision on confidence, we conducted 4 experiments on DUTS-TE and EC-SSD datasets covering all the cases for our comparison, as listed in <ref type="table" target="#tab_3">Table 3</ref>. For conciseness, we denote reference module and supervision on confidence as Ref. and Conf.. It can be observed that each approach will boost the performance, and when they are combined, we obtained the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of SFE module and different loss functions.</head><p>To study the importance of each loss function and SFE module, we conduct a series of controlled experiments on the DUTS-TE dataset. We train the model by using BCE loss only, then include weighted IOU Loss <ref type="bibr" target="#b38">[39]</ref>, DCLoss, and CLoss step by step. Detailed results are listed in <ref type="table" target="#tab_4">Table  4</ref>. To prove the effectiveness of SFE module, qualitative comparisons are illustrated in <ref type="figure" target="#fig_6">Fig. 9</ref>. As can be seen, with SFE module, model can effectively suppress execessive wrong predictions. Though SFE is a simple feature extraction module, it improves model predictions greatly. More ablation studies please refer to the suplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have introduced an efficient and accurate model using a recursive CNN together with a Contour-Saliency Blending (CSB) module. To further improve the model's efficiency, a Stage-wise Feature Extraction (SFE) module is adopted. It is a simple module but can suppress wrong predictions effectively. Furthermore, we divided the training objectives into accuracy and confidence and proposed two loss functions to guide model convergence. The predicted salient objects achieved competitive state-of-theart results on five benchmark datasets. Detailed ablation studies were conducted, which further proved the model's performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Network architecture for RCSBNet. Pre-trained ResNet-50 is used as the backbone, channel number is reduced to 64 via Channel Pooling (CP) layer. Recursive Contour-Saliency Blocks (RCSB) with G blocks and R recursions are then attached and followed by Stage-wise Feature Extraction (SFE) module to generate contour and saliency predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(a) Contour-Saliency Blending Unit (CSBU). It contains two streams where contour and saliency information are blended and intertwined. (b) Single Recursive Contour-Saliency Blending (RCSB) Block. In order to increase the number of contour-saliency fusion, recursive mechanism is applied. Weights are shared among all CSBU blocks used in the RCSB. (c) Network branch contains G blocks of RCSB, each with R times of recursion. (d) Stage-wise Feature Extraction (SFE) module. (e) Conventional methods for generating intermediate stage predictions by using side branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>(a) ground truth saliency. (b) predicted saliency. (c) focal loss (?, ? = 2, 2) weight map calculated using (a) and (b). (d) confidence loss (?, ? = 2, 1) weight map calculated using (a) and (b). (e) unconfident but correct area (red) in prediction. (f) confident false positives (orange) and false negatives (green) in prediction. Compare (d) with (c), focal loss is less sensitive to unsure predictions, while confidence loss generates high weights especially on object boundaries, which eventually help the model generate sharper edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Visual comparisons between our method and 10 state-of-the-art networks. * stands for models utilizing contour information. More comparisons are provided in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>First row: Precision-Recall Curves comparison on five saliency benchmark datasets. Second row: F-measure Curves comparison on five saliency benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>(a) GT (b) w/ EF (c) w/o EF (d) w/ EF (e) w/o EF Qualitative comparisons for early fusion. (a) Ground-truth. (b) &amp; (d) Saliency and contour prediction with early fusion. (c) &amp; (e) Saliency and contour prediction without early fusion. As can be seen with early fusion, results are more complete for both contour and saliency predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>w/ SFE (c) w/o SFE (d) w/ SFE (e) w/o SFE Qualitative comparisons for SFE module. (a) Ground-truth. (b) &amp; (d) Saliency and contour prediction with SFE module. (c) &amp; (e) Saliency and contour prediction without SFE module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparisons between RCSBNet and other 13 methods on five benchmark datasets in terms of the average F-measure F ? , MAE M , E ? and F ? ? . ? / ? means the larger/smaller the value, the better the results. Red, Green, and Blue indicate the best, second best and third best performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparisons for the effectiveness of early fusion. ?M ?E ? ?F ? ? ?F ? ?M ?E ? ?F ? ? ? w/ early fusion .855 .034 .903 .840 .927 .033 .923 .916 w/o early fusion .849 .037 .897 .833 .925 .035 .919 .908</figDesc><table><row><cell>DUTS-TE</cell><cell>ECSSD</cell></row><row><cell>F ?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparisons for different model con-figurations. ? M ? E ? ? F ? ? ? F ? ? M ? E ? ? F ? ? ? .842 .039 .849 .825 .916 .037 .914 .902 .848 .038 .861 .830 .920 .036 .916 .908 .849 .037 .870 .837 .922 .035 .921 .909 .855 .034 .903 .840 .927 .033 .923 .916</figDesc><table><row><cell>Ref. Conf.</cell><cell>F ?</cell><cell>DUTS-TE</cell><cell>ECSSD</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for different loss functions and presence of SFE module.BCE wIOU DCLoss CLossSFE DUTS-TE F ? ? M ? E ? ? F ? ? ? .788 .058 .862 .776 .793 .047 .881 .789 .829 .043 .890 .813 .847 .040 .896 .825 .855 .034 .903 .840</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units (relu). CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno>abs/1803.08375</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic human matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
		<editor>Susanne Boll, Kyoung Mu Lee, Jiebo Luo, Wenwu Zhu, Hyeran Byun, Chang Wen Chen, Rainer Lienhart, and Tao Mei</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global context-aware progressive aggregation network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10599" to="10606" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contour-aware loss: Boundary-aware learning for salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="431" to="443" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="704" />
		</imprint>
	</monogr>
	<note>J?r?me Lang. ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Empirical evaluation of activation functions in deep convolution neural network for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junaid</forename><surname>Baber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mumraiz</forename><surname>Khan Kasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maheen</forename><surname>Bakhtyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 43rd International Conference on Telecommunications and Signal Processing (TSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="204" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<editor>Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L?on Bottou, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent attentional networks for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3668" to="3677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contour knowledge transfer for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11219</biblScope>
			<biblScope unit="page" from="370" to="385" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for realtime salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonlocal deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Kumar Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">A</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6593" to="6601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM International Conference on Multimedia 2002</title>
		<editor>Lawrence A. Rowe, Bernard M?rialdo, Max M?hlh?user, Keith W. Ross, and Nevenka Dimitrova</editor>
		<meeting>the 10th ACM International Conference on Multimedia 2002<address><addrLine>Juan les Pins, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cagnet: Content-aware guidance for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Ghofrani Majelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">107303</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-scale interactive network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9410" to="9419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U 2 -net: Going deeper with nested u-structure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Vincent Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Za?ane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichen</forename><surname>Vincent Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masood</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>J?gersand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells III, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Salient object detection via recurrently aggregating spatial attention weighted cross-level deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinzhong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo, ICME 2019</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1546" to="1551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garry</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="3796" to="3805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Salient object detection with pyramid attention and salient edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1448" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Focal boundary guided salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecai</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2813" to="2824" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">F3net: Fusion, feedback and focus for salient object detection. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stacked cross refinement network for edge-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="7263" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A bi-directional message passing model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqing</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangjiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-02" />
			<biblScope unit="page" from="8778" to="8787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Suppress and balance: A simple gated network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="35" to="51" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interactive two-stream decoder for accurate and fast saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9138" to="9147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised object class discovery via saliency-guided multiple class learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="862" to="875" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
