<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Label Noise Modeling and Loss Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Label Noise Modeling and Loss Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at https://git.io/fjsvE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) have recently become the par excellence base approach to deal with many computer vision tasks <ref type="bibr" target="#b4">(DeTone et al., 2016;</ref><ref type="bibr" target="#b17">Ono et al., 2018;</ref><ref type="bibr" target="#b1">Beluch et al., 2018;</ref><ref type="bibr" target="#b21">Redmon et al., 2016;</ref><ref type="bibr">Zhao et al., 2017;</ref><ref type="bibr" target="#b12">Krishna et al., 2017)</ref>. Their widespread use is attributable to their capability to model complex patterns <ref type="bibr">(Ren et al., 2018)</ref> when vast amounts of labeled data are available. Obtaining such volumes of data, however, is not trivial and usually involves an error prone automatic or a manual labeling process <ref type="bibr" target="#b36">Zlateski et al., 2018)</ref>. These errors lead to noisy samples: samples annotated with incorrect or noisy labels. As a result, dealing with label noise is a common adverse scenario that requires attention * Equal contribution <ref type="bibr" target="#b13">1</ref>    <ref type="figure">Figure 1</ref>. Cross-entropy loss on CIFAR-10 under 80% label noise for clean and noisy samples. Left: training with cross-entropy loss results in fitting the noisy labels. Right: using our proposed objective prevents fitting label noise while also learning from the noisy samples. The heavy lines represent the median losses and the shaded areas are the interquartile ranges.</p><p>to ensure useful visual representations can be learnt <ref type="bibr">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b29">Wang et al., 2018a;</ref><ref type="bibr" target="#b31">Wu et al., 2018;</ref><ref type="bibr" target="#b10">Jiang et al., 2018a;</ref><ref type="bibr" target="#b36">Zlateski et al., 2018)</ref>. Automatically obtained noisy labels have previously been demonstrated useful for learning visual representations <ref type="bibr">(Pathak et al., 2017;</ref><ref type="bibr" target="#b6">Gidaris et al., 2018)</ref>; however, a recent study on the generalization capabilities of deep networks <ref type="bibr">(Zhang et al., 2017)</ref> demonstrates that noisy labels are easily fit by CNNs, harming generalization. This overfitting also arises in biases that networks encounter during training, e.g., when a dataset contains class imbalances <ref type="bibr" target="#b0">(Alvi et al., 2018)</ref>. However, before fitting label noise, CNNs fit the correctly labeled samples (clean samples) even under high-levels of corruption <ref type="figure">(Figure 1, left)</ref>.</p><p>Existing literature on training with noisy labels focuses primarily on loss correction approaches <ref type="bibr">(Reed et al., 2015;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018;</ref><ref type="bibr">Jiang et al., 2018b)</ref>. A well-known approach is the bootstrapping loss <ref type="bibr">(Reed et al., 2015)</ref>, which introduces a perceptual consistency term in the learning objective that assigns a weight to the current network prediction to compensate for the erroneous guiding of noisy samples. Other approaches modify class probabilities <ref type="bibr">(Patrini et al., 2017;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018)</ref> by estimating the noise associated with each class, thus computing a loss that guides the training process towards the correct classes. Still other approaches use curriculum learning to formulate a robust learning procedure <ref type="bibr">(Jiang et al., 2018b;</ref><ref type="bibr">Ren et al., 2018)</ref>. Curriculum learning <ref type="bibr" target="#b2">(Bengio et al., 2009</ref>) is based on the idea that ordering training examples in a meaningful (e.g. easy to hard) sequence might improve convergence and gen-arXiv:1904.11238v2 [cs.CV] 5 Jun 2019 eralization. In the noisy label scenario, easy (hard) concepts are associated with clean (noisy) samples by re-weighting the loss for noisy samples so that they contribute less. Discarding noisy samples, however, potentially removes useful information about the data distribution. <ref type="bibr">(Wang et al., 2018b)</ref> overcome this problem by introducing a similarity learning strategy that pulls representations of noisy samples away from clean ones. Finally, mixup data augmentation <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref> has recently demonstrated outstanding robustness against label noise without explicitly modeling it.</p><p>In light of these recent advances, this paper proposes a robust training procedure that avoids fitting noisy labels even under high levels of corruption <ref type="figure">(Figure 1</ref>, right), while using noisy samples for learning visual representations that achieve a high classification accuracy. Contrary to most successful recent approaches that assume the existence of a known set of clean data <ref type="bibr">(Ren et al., 2018;</ref><ref type="bibr" target="#b9">Hendrycks et al., 2018)</ref>, we propose an unsupervised model of label noise based exclusively on the loss on each sample. We argue that clean and noisy samples can be modeled by fitting a two-component (clean-noisy) beta mixture model (BMM) on the loss values. The posterior probabilities under the model are then used to implement a dynamically weighted bootstrapping loss, robustly dealing with noisy samples without discarding them. We provide experimental work demonstrating the strengths of our approach, which lead us to substantially outperform the related work. Our main contributions are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>A simple yet effective unsupervised noise label modeling based on each sample loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A loss correction approach that exploits the unsupervised label noise model to correct each sample loss, thus preventing overfitting to label noise.</p><p>3. Pushing the state-of-the-art one step forward by combining our approach with mixup data augmentation <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref>.</p><p>4. Guiding mixup data augmentation to achieve convergence even under extreme label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Recent efforts to deal with label noise address two scenarios <ref type="bibr">(Wang et al., 2018b)</ref>: closed-set and open-set label noise.</p><p>In the closed set scenario, the set of possible labels S is known and fixed. All samples, including noisy ones, have their true label in this set. In the open set scenario, the true label of a noisy sample x i may be outside S; i.e. x i may be an out-of-distribution sample <ref type="bibr" target="#b13">(Liang et al., 2018)</ref>. The remainder of this section briefly reviews related work in the closed-set scenario considered in <ref type="bibr">(Zhang et al., 2017)</ref>, upon which we base our approach.</p><p>Several types of noise can be studied in the closed-set scenario, namely uniform or non-uniform random label noise. The former is also known as symmetric label noise and implies ground-truth labels flipped to a different class with uniform random probability. Non-uniform or class-conditional label noise, on the other hand, has different flipping probabilities for each class <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>. Previous research <ref type="bibr">(Patrini et al., 2017)</ref> suggests that uniform label noise is more challenging than non-uniform.</p><p>A simple approach to dealing with label noise is to remove the corrupted data. This is not only challenging because difficult samples may be confused with noisy ones <ref type="bibr">(Wang et al., 2018b)</ref>, but also implies not exploiting the noisy samples for representation learning. It has, however, recently been demonstrated <ref type="bibr" target="#b5">(Ding et al., 2018)</ref> that it is useful to discard samples with a high probability of being incorrectly labeled and still use these samples in a semi-supervised setup.</p><p>Other approaches seek to relabel the noisy samples by modeling their noise through directed graphical models <ref type="bibr" target="#b32">(Xiao et al., 2015)</ref>, Conditional Random Fields <ref type="bibr" target="#b26">(Vahdat, 2017)</ref>, or CNNs <ref type="bibr" target="#b27">(Veit et al., 2017)</ref>. Unfortunately, to predict the true label, these approaches rely on the assumption that a small set of clean samples is always available, which limits their applicability. <ref type="bibr">Tanaka et al. (Tanaka et al., 2018)</ref> have, however, recently demonstrated that it is possible to do unsupervised sample relabeling using the network predictions to predict hard or soft labels.</p><p>Loss correction approaches <ref type="bibr">(Reed et al., 2015;</ref><ref type="bibr">Jiang et al., 2018b;</ref><ref type="bibr">Patrini et al., 2017;</ref><ref type="bibr" target="#b7">Zhang et al., 2018)</ref> modify either the loss directly, or the probabilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples. <ref type="bibr">(Reed et al., 2015)</ref> extend the loss with a perceptual term that introduces a certain reliance on the model prediction. Their approach is, however, limited in that the noise label always affects the objective. <ref type="bibr">(Patrini et al., 2017)</ref> propose a backward method that weights the loss of each sample using the inverse of a noise transition matrix T , which specifies the probability of one label being flipped to another. <ref type="bibr">(Patrini et al., 2017)</ref> presents a forward method that, instead of operating directly on the loss, goes back to the predicted probabilities to correct them by multiplying by the T matrix. <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref> corrects the predicted probabilities using a corruption matrix computed using a model trained on a clean set of samples and their prediction on the corrupted data. Other approaches focus on re-weighting the contribution of noisy samples on the loss. <ref type="bibr">(Jiang et al., 2018b)</ref> proposes an alternating minimization framework in which a mentor network learns a curriculum (i.e. a weight for each sample) to guide a student network that learns under label noise conditions. Similarly, <ref type="bibr" target="#b7">(Guo et al., 2018</ref>) present a curriculum learning approach based on an unsupervised estimation on data complexity through its distribution in a feature space that benefits from training with both clean and noisy samples. <ref type="bibr">(Ren et al., 2018)</ref> weights each sample in the loss based on the gradient directions in training compared to those on validation (i.e. in a clean set). Note that, as for relabeling approaches, the assumption of clean data availability limits the application of many of these approaches. Conversely, approaches like <ref type="bibr">(Wang et al., 2018b)</ref> do not rely on clean data by performing unsupervised noise label detection to help re-weighting the loss, while not discarding noisy samples that are exploited in a similarity learning framework to pull their representations away from true samples of each class.</p><p>In contrast to the aforementioned literature, we propose to deal with noisy labels using exclusively the training loss of each sample without consulting any clean set. Specifically, we fit a two-component beta mixture model to the training loss of each sample to model clean and noisy samples. We use this unsupervised model to implement a loss correction approach that benefits both from bootstrapping <ref type="bibr">(Reed et al., 2015)</ref> and mixup data augmentation <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref> to deal with the closed-set label noise scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning with label noise</head><p>Image classification can be formulated as the problem of learning a model h ? (x) from a set of training examples</p><formula xml:id="formula_0">D = {(x i , y i )} N i=1 with y i ? {0, 1}</formula><p>C being the one-hot encoding ground-truth label corresponding to x i . In our case, h ? is a CNN and ? represents the model parameters (weights and biases). As we are considering classification under label noise, the label y i can be noisy (i.e. x i is a noisy sample). The parameters ? are fit by optimizing a loss function, e.g. categorical cross-entropy:</p><formula xml:id="formula_1">(?) = N i=1 i (?) = ? N i=1 y T i log (h ? (x i )) ,<label>(1)</label></formula><p>where h ? (x) are the softmax probabilities produced by the model and log(?) is applied elementwise. The remainder of this section describes our noisy sample modeling technique and how to extend the loss in Eq. (1) based on this model to handle label noise. For notational simplicity, we use i (?) = i and h ? (x i ) = h i in the remainder of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Label noise modeling</head><p>We aim to identify the noisy samples in the dataset D so that we can implement a loss correction approach (see Subsections 3.2 and 3.3). Our essential observation is simple: random labels take longer to learn than clean labels, meaning that noisy samples have higher loss during the early epochs of training (see <ref type="figure">Figure 1</ref>), allowing clean and noisy samples to be distinguished from the loss distribution alone (see <ref type="figure" target="#fig_1">Figure 2</ref>). Modern CNNs trained with stochastic gra- dient methods typically do not fit the noisy examples until substantial progress has been made in fitting the clean ones. Therefore, one can infer from the loss value if a sample is more likely to be clean or noisy. We propose to use a mixture distribution model for this purpose.</p><p>Mixture models are a widely used unsupervised modeling technique <ref type="bibr" target="#b24">(Stauffer &amp; Grimson, 1999;</ref><ref type="bibr" target="#b20">Permuter et al., 2006;</ref><ref type="bibr" target="#b15">Ma &amp; Leijon, 2011)</ref>, with the Gaussian Mixture Model (GMM) <ref type="bibr" target="#b20">(Permuter et al., 2006)</ref> being the most popular. The probability density function (pdf) of a mixture model of K components on the loss is defined as:</p><formula xml:id="formula_2">p( ) = K k=1 ? k p( | k) ,<label>(2)</label></formula><p>where ? k are the mixing coefficients for the convex combination of each individual pdf p( | k). In our case, we can fit a two components GMM (i.e. K = 2 and ? N (? k , k )) to model the distribution of clean and noisy samples <ref type="figure" target="#fig_1">(Figure 2)</ref>. Unfortunately, the Gaussian is a poor approximation to the clean set distribution, which exhibits high skew toward zero. The more flexible beta distribution <ref type="bibr" target="#b15">(Ma &amp; Leijon, 2011)</ref> allows modelling both symmetric and skewed distributions over <ref type="bibr">[0,</ref><ref type="bibr" target="#b13">1]</ref>; the beta mixture model (BMM) better approximates the loss distribution for mixtures of clean and noisy samples ( <ref type="figure" target="#fig_1">Figure 2</ref>). Empirically, we also found the BMM improves ROC-AUC for clean-noisy label classification over the GMM by around 5 points for 80% label noise in CIFAR-10 when using the training objective in Section 3.3 (see Appendix A). The beta distribution over a (max) normalized loss ? [0, 1] is defined to have pdf:</p><formula xml:id="formula_3">p( | ?, ?) = ?(? + ?) ?(?) ?(?) ??1 (1 ? ) ??1 ,<label>(3)</label></formula><p>where ?, ? &gt; 0 and ?(?) is the Gamma function, and the mixture pdf is given by substituting the above into Eq.</p><p>(2).</p><p>We use an Expectation Maximization (EM) procedure to fit the BMM to the observations. Specifically, we introduce latent variables ? k ( ) = p(k| ) which are defined to be the posterior probability of the point having been generated by mixture component k. In the E-step we fix the parameters ? k , ? k , ? k and update the latent variables using Bayes rule:</p><formula xml:id="formula_4">? k ( ) = ? k p( | ? k , ? k ) K j=1 ? j p( | ? j , ? j ) .<label>(4)</label></formula><p>Given fixed ? k ( ), the M-step estimates the distribution parameters ? k , ? k using a weighted version of the method of moments:</p><formula xml:id="formula_5">? k = ? k 1 ?? k ? k , ? k =? k ? k 1 ?? k s 2 k ? 1 (5) with? k being a weighted average of the losses { i } N i=1 cor- responding to each training sample {x i } N i=1</formula><p>, and s 2 k being a weighted variance estimate:</p><formula xml:id="formula_6">k = N i=1 ? k ( i ) i N i=1 ? k ( i ) ,<label>(6)</label></formula><formula xml:id="formula_7">s 2 k = N i=1 ? k ( i ) i ?? k 2 N i=1 ? k ( i ) .<label>(7)</label></formula><p>The updated mixing coefficients ? k are then calculated in the usual way:</p><formula xml:id="formula_8">? k = 1 N N i=1 ? k ( i ).<label>(8)</label></formula><p>The above E and M-steps are then iterated until convergence or a maximum number of iterations (10 in our experiments) are reached. Note that the above algorithm becomes numerically unstable when the observations are very near zero and one. Our implementation simply sidesteps this issue by bounding the observations in [ , 1 ? ] instead of [0, 1] ( = 10 ?4 in our experiments).</p><p>Finally, we obtain the probability of a sample being clean or noisy through the posterior probability:</p><formula xml:id="formula_9">p(k | i ) = p(k) p( i | k) p( i ) ,<label>(9)</label></formula><p>where k = 0 (1) denotes clean (noisy) classes.</p><p>Note that the loss used to estimate the mixture distribution is always the standard cross-entropy loss <ref type="figure">(Figure 1</ref>) for all samples after every epoch. This not necessarily the loss used for training, which may contain a corrective component to deal with label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Noise model for label correction</head><p>Carefully selecting a loss function to guide the learning process is of particular importance under label noise. Standard categorical cross-entropy loss (Eq. <ref type="formula" target="#formula_1">(1)</ref>) is ill-suited to the task as it encourages fitting label noise <ref type="bibr">(Zhang et al., 2017)</ref>. The static hard bootstrapping loss proposed in <ref type="bibr">(Reed et al., 2015)</ref> provides a mechanism to deal with label noise by adding a perceptual term to the standard cross-entropy loss that helps to correct the training objective:</p><formula xml:id="formula_10">B = ? N i=1 ((1 ? w i ) y i + w i z i ) T log (h i ) ,<label>(10)</label></formula><p>where w i weights the model prediction z i in the loss function. <ref type="bibr">(Reed et al., 2015)</ref> use w i = 0.2, ?i. We refer to this approach as static hard bootstrapping. <ref type="bibr">(Reed et al., 2015)</ref> also proposed a static soft bootstrapping loss (w i = 0.05, ?i) that uses the predicted softmax probabilities h i instead of the class prediction z i . Unfortunately, using a fixed weight for all samples does not prevent fitting the noisy ones (Table 1 in Subsection 4.2) and, more importantly, applying a small fixed weight w i to the prediction (probabilities) z i (h i ) limits the correction of a hypothetical noisy label y i .</p><p>We propose dynamic hard and soft bootstrapping losses by using our noise model to individually weight each sample; i.e., w i is dynamically set to p(k = 1 | i ) and the BMM model is estimated after each training epoch using the crossentropy loss for each sample i . Therefore, clean samples rely on their ground-truth label y i (1 ? w i is large), while noisy ones let their loss being dominated by their class prediction z i or their predicted probabilities h i (w i is large), respectively, for hard and soft alternatives. Note that in mature stages of training the CNN model should provide a good estimation of the true class for noisy samples. Subsection 4.2 compares static and dynamic bootstrapping, showing that dynamic bootstrapping gives superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint label correction and mixup data augmentation</head><p>Recently <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref> proposed a data augmentation technique named mixup that exhibits strong robustness to label noise. This technique trains on convex combinations of sample pairs (x p and x q ) and corresponding labels (y p and y q ):</p><formula xml:id="formula_11">x = ?x p + (1 ? ?)x q ,<label>(11)</label></formula><formula xml:id="formula_12">= ? p + (1 ? ?) q ,<label>(12)</label></formula><p>where ? is randomly sampled from a beta distribution Be (?, ?), with ? = ? set to high values when learning with label noise so that ? tends to be close to 0.5. This combination regularizes the network to favor simple linear behavior between training samples, which reduces oscillations in regions far from them. Regarding label noise, mixup provides a mechanism to combine clean and noisy samples, computing a more representative loss to guide the training process. Even when combining two noisy samples the loss computed can still be useful as one of the noisy samples may (by chance) contain the true label of the other one. As for preventing overfitting to noisy samples, the fact that samples and their labels are mixed favors learning structured data, while hindering learning the unstructured noise.</p><p>Mixup achieves robustness to label noise by appropriate combinations of training examples. Under high-levels of noise mixing samples that both have incorrect labels is prevalent, which reduces the effectiveness of the method. We propose to fuse mixup and our dynamic bootstrapping to implement a robust per-sample loss correction approach:</p><formula xml:id="formula_13">* = ?? ((1 ? w p ) y p + w p z p ) T log (h) ? (1 ? ?) ((1 ? w q ) y q + w q z q ) T log (h) ,<label>(13)</label></formula><p>The loss * defines the hard alternative, while the soft one can be easily defined by replacing z p and z q by h p and h q . These hard and soft losses exploit mixup's advantages while correcting the labels through dynamic bootstrapping, i.e. the weights w p and w q that control the confidence in the ground-truth labels and network predictions are inferred from our unsupervised noise model: w p = p(k = 1 | p ) and w q = p(k = 1 | q ). We compute h p , z p , h q and z q by doing an extra forward pass, as it is not straightforward to obtain the predictions for samples p and q from the mixed probabilities h.</p><p>Ideally, the proposed loss * would lead to a better model by trusting in progressively better predictions during training. For high-levels of label noise, however, the network predictions are unreliable and dynamic bootstrapping may not converge when combined with the complex signal that mixup provides. This is reasonable as under high levels of noise most of the samples are guided by the network's prediction in the bootstrapping loss, encouraging the network to predict the same class to minimize the loss. We apply the regularization term used in <ref type="bibr">(Tanaka et al., 2018)</ref>, which seeks preventing the assignment of all samples to a single class, to overcome this issue:</p><formula xml:id="formula_14">R = C c=1 p c log p c h c ,<label>(14)</label></formula><p>where p c denotes the prior probability distribution for class c and h c is the mean softmax probability of the model for class c across all samples in the dataset. Note that we assume a uniform distribution for the prior probabilities (i.e. p c = 1/C), while approximating h c using mini-batches as done in <ref type="bibr">(Tanaka et al., 2018)</ref>. We add the term ?R to * (Eq. <ref type="formula" target="#formula_1">(13)</ref>) with ? being the regularization coefficient (set to one in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and implementation details</head><p>We thoroughly validate our approach in two well-known image classification datasets: CIFAR-10 and CIFAR-100. The former contains 10 classes, while the latter has 100 classes. Both have 50K color images for training and 10K for validation with resolution 32?32. We use a PreAct ResNet-18 <ref type="bibr">(He et al., 2016)</ref> and train it using SGD and batch size of 128. We use two different schemes for the learning rate policy and number of epochs depending on whether mixup is used (see Appendix B for further details). We further experiment on TinyImageNet (subset of ImageNet <ref type="bibr">(Deng et al., 2009)</ref>) and Clothing1M <ref type="bibr" target="#b32">(Xiao et al., 2015)</ref> datasets to test the generality of our approach far from CIFAR data (Subsection 4.6). We follow <ref type="bibr">(Zhang et al., 2017;</ref><ref type="bibr">Tanaka et al., 2018)</ref> criterion for label noise addition, which consists of randomly selecting labels for a percentage of the training data using all possible labels (i.e. the true label could be randomly maintained). Note that there is another popular label noise criterion <ref type="bibr">(Jiang et al., 2018b;</ref><ref type="bibr">Wang et al., 2018b)</ref> in which the true label is not selected when performing random labeling. We also run our proposed approach under these conditions in Subsection 4.5 for comparison.  <ref type="bibr">(Reed et al., 2015)</ref>. The overall results demonstrate that applying persample weights (DY) benefits training by allowing to fully correct noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Joint mixup and dynamic loss correction</head><p>The proposed dynamic hard bootstrapping exhibits better performance than the state-of-the-art static version <ref type="bibr">(Reed et al., 2015)</ref>. It is, however, not better than the performance of mixup data augmentation, which exhibits excellent robustness to label noise (M in <ref type="table" target="#tab_3">Table 2</ref>). The fusion approach from Eq. (13) (M-DYR-H) and its soft alternative (M-DYR-S), which combines the per-sample weighting of dynamic bootstrapping and robustness to fitting noise labels of mixup, achieves a remarkable improvement in accuracy under high noise levels.  It is important to highlight that we achieve quite similar best and last performance for all levels of label noise in CIFAR datasets, indicating that the proposed method is robust to varying noise levels. <ref type="figure" target="#fig_2">Figure 3</ref> shows uniform manifold approximation and projection (UMAP) embeddings <ref type="bibr" target="#b16">(McInnes et al., 2018)</ref> of the 512 features in the penultimate fully-connected layer of PreAct ResNet-18 trained using our method, and compares them with those found using cross-entropy and mixup. The separation among classes appears visually more distinct using the proposed objective. When clean and noisy samples are combined by mixup they are given the same importance of approximately ? = 0.5 (as ? = ? = 32). While noisy samples benefit from mixing with clean ones, clean samples are contaminated by noisy ones, whose training objective is incorrectly modified. We propose a dynamic mixup strategy in the input that uses a different ? for each sample to reduce the contribution of noisy samples when they are mixed with clean ones:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">On the limits of the proposed approach</head><formula xml:id="formula_15">x = ? p ? p + ? q x p + ? q ? p + ? q x q ,<label>(15)</label></formula><p>where ? p = p(k = 0 | p ) and ? q = p(k = 0 | q ), i.e. we use the noise probability from our BMM to guide mixup in the input. Note that for clean-clean and noisy-noisy cases, the behavior remains similar to mixup with ? = ? = 32, which leads to ? ? 0.5 (i.e. ? p ? ? q ? ? p /(? p +? q ) ? 0.5). This configuration simplifies the input to the network when mixing a sample whose label is potentially useless, while retaining the strengths of mixup for clean-clean and noisynoisy combinations. This is used with the original mixup strategy (Eq. (13)) to benefit from the regularization that an additional label provides. <ref type="table" target="#tab_5">Table 3</ref> presents the results of this approach (MD-DYR-H), which exhibits more stable convergence for 90% label noise in both datasets. <ref type="table" target="#tab_3">Table 2</ref> reported that hard bootstrapping works better than the soft alternative. Unfortunately, hard bootstrapping under high levels of label noise causes large variations in the loss that lead to drops in performance. To ameliorate such instabilities, we propose a decreasing softmax technique <ref type="bibr" target="#b28">(Vermorel &amp; Mohri, 2005)</ref> to progressively move from a soft to a hard dynamic bootstrapping. This is implemented by modifying the softmax temperature T in:</p><formula xml:id="formula_16">h ij = exp(s ij /T ) N k=1 exp(s ik /T ) ,<label>(16)</label></formula><p>where s ij denotes the score obtained in the last layer of the CNN model class j of sample x i . By default T = 1 gives the soft alternative of Eq. (13). To move from soft to hard bootstrapping we linearly reduce the temperature for h p and h q until we reach a final temperature in a certain epoch (T = 0.001 and epoch 200 in our experiments). We experimented with linear, logarithmic, tanh, and step-down temperature decays with similar results. This decreasing softmax MD-DYR-SH obtains much improved accuracy for 90% of label noise (69.1 for CIFAR-10 and 24.3 for CIFAR-100), while slightly decreasing accuracy compared to M-DYR-H and MD-DYR-H at lower noise levels. Note that we significantly outperform the best state-of-the-art we are aware for 90% of label noise, which is 58.3% and 58.0% for best and last validation accuracies (reported in <ref type="bibr">(Tanaka et al., 2018)</ref> with a PreAct ResNet-32 on CIFAR-10). The training process is slightly modified to introduce dynamic mixup (epoch 106) before bootstrapping (epoch 111) for MD-DYR-H and MD-DYR-SH. <ref type="table" target="#tab_7">Table 4</ref> compares with related works for different levels of label noise using a common architecture and the 300 epochs training scheme (see Subsection 4.1) . We introduce bootstrapping in epoch 105 for <ref type="bibr">(Reed et al., 2015)</ref> for the proposed methods, estimate the T matrix of <ref type="bibr">(Patrini et al., 2017)</ref> in epoch 75 (as done in <ref type="bibr" target="#b9">(Hendrycks et al., 2018)</ref>), and use the configuration reported in <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref> for mixup. We outperform the related work in the presence of label noise, obtaining remarkable improvements for high levels of noise (80% and 90%) where the compared approaches do not learn as well from the noisy samples (see best accuracy) and do not prevent fitting noisy labels (see last accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with related approaches</head><p>As noted in Subsection 4.1, when introducing label noise the true label can be excluded from the candidates. In this case label noise is defined as the percentage of incorrect labels instead of random ones (i.e. the criterion followed in previous experiments), a criterion adopted by several other authors <ref type="bibr">(Jiang et al., 2018b;</ref><ref type="bibr" target="#b14">Ma et al., 2018;</ref><ref type="bibr">Ren et al., 2018;</ref><ref type="bibr">Wang et al., 2018b)</ref>. We also run our proposed approach under this setup to allow quantitative comparison ( <ref type="table" target="#tab_8">Table 5</ref>). The proposed method outperforms all related work in CIFAR-10 and CIFAR-100 with MD-DYR-SH, while the results for M-DYR-H are slightly below those of <ref type="bibr">(Jiang et al., 2018b)</ref> for low label noise levels in CIFAR-100. Nevertheless, these results should be interpreted with care due to the different architectures employed and the use of sets of clean data during training in <ref type="bibr">(Jiang et al., 2018b)</ref> and <ref type="bibr">(Ren et al., 2018)</ref>.</p><p>4.6. Generalization of the proposed approach <ref type="table">Table 6</ref> shows the results of the proposed approaches M-DYR-H and MD-DYR-SH compared to mixup <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref> on TinyImageNet to demonstrate that our approach is useful far from CIFAR data. The proposed approach clearly outperforms <ref type="bibr" target="#b7">(Zhang et al., 2018)</ref> for different levels of label noise, obtaining consistent results with the CIFAR experiments. Note that we use the same network, hyperparameters, and learning rate policy as with CIFAR. Furthermore, we tested our approach in real-world label noise by evaluating our method on Clothing1M <ref type="bibr" target="#b32">(Xiao et al., 2015)</ref>, which contains non-uniform label noise with label flips concentrated in classes sharing similar visual patterns with the true class. We followed a similar network and procedure as <ref type="bibr">(Tanaka et al., 2018)</ref> with ImageNet pre-trained weights and ResNet-50, obtaining over 71% test accuracy, which falls short of the state-of-the-art (72.23% <ref type="bibr">(Tanaka et al., 2018)</ref>). We found that finetuning a pre-trained network for one epoch, as done in <ref type="bibr">(Tanaka et al., 2018)</ref>, easily fits label noise limiting our unsupervised label noise model. We believe this occurs due to the structured noise and the small learning rate. Training with cross-entropy alone gives test accuracy over 69%, suggesting that the configurations used might be suboptimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper presented a novel approach on training under label noise with CNNs that does not require any set of clean data. We proposed to fit a beta mixture model to the crossentropy loss of each sample and model label noise in an unsupervised way. This model is used to implement a dynamic bootstrapping loss that relies either on the network prediction or the ground-truth (and potentially noisy) labels depending on the mixture model. We combined this dynamic bootstrapping with mixup data augmentation to implement an incredibly robust loss correction approach. We conducted extensive experiments on CIFAR-10 and CIFAR-100 to show the strengths and weaknesses of our approach demonstrating outstanding performance. We further proposed to use our beta mixture model to guide the combination of mixup data augmentation to assure reliable convergence under extreme noise levels. The approach generalizes well to TinyImageNet but shows some limitations under non-uniform noise in Clothing1M that we will explore in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for the paper "Unsupervised Label Noise Modeling and Loss Correction"</head><p>A. Beta Mixture Model (BMM)</p><p>This section extends the discussion of the proposed unsupervised BMM in the main paper providing detail on several more aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BMM performance under low levels of label noise</head><p>We seek robust representation learning in the presence of label noise, which may occur when images are automatically labeled. Performance will likely drop in carefully annotated datasets with near 0% noise because the loss distribution is not a two-component mixture. In this situation the BMM classifies almost all samples as clean, but some estimation errors may occur, which lead to a reliance on the sometimes incorrect network prediction instead of the true clean label. Nevertheless, for 20% noise, we outperform the compared state-of-the-art at the end of the training, demonstrating improved robustness for low noise levels.</p><p>BMM parameter estimation frequency The BMM parameters are re-estimated after every epoch once the loss correction begins (i.e. there is an initial warm-up as noted in Subsection 4.1 with no loss correction) by computing the cross-entropy loss from a forward pass with the original (potentially noisy) labels. We also tested our approach M-DYR-H (CIFAR-10, 80% of label noise) changing the estimation period to 5 and 0.5 epochs, observing no decrease in accuracy. While the original configuration presented in <ref type="figure" target="#fig_4">Figure 4</ref>(a) reaches 86.8 (86.6) for best (last), every 5 epochs leads to (86.9) 86.8 and every 0.5 to 88.0 (87.5). <ref type="figure" target="#fig_4">Figure  4(b)</ref> shows the clean/noisy classification capabilities of the BMM in terms of Area Under the Curve (AUC) evolution during training, demonstrating that performance and robustness are consistent across noise levels. In particular, the experiment on CIFAR-10 with M-DYR-H exceeds 0.98 AUC for 20, 50 and 80% label noise. AUC increases during training and increases faster for lower noise levels, showing increasingly better clean/noisy discrimination related to consistent BMM predictions over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BMM classification accuracy and robustness</head><p>Effect of BMM classification accuracy on image classification accuracy BMM prediction accuracy is essential for high image classification accuracy, as demonstrated by the tendency for both image classification and BMM accuracy to increase together in <ref type="figure" target="#fig_4">Figure 4</ref>  pecially for higher noise levels. <ref type="figure" target="#fig_4">Figure 4</ref>(c) further verifies this relationship by comparing the BMM with a GMM (Gaussian Mixture Model) on CIFAR-10 with M-DYR-H and 80% label noise. The GMM gives both less accurate clean/noisy discrimination and worse image classification results (clean/noisy AUC drops from 0.98 to 0.94, while image classification accuracy drops from 86.6 to 83.5).</p><p>Performance attributable to the BMM Incorporating the BMM results in a loss that goes beyond mere regularization. This can be verified by removing the BMM and assigning fixed weights in the bootstrapping loss (0.8 to GT and 0.2 to network prediction, keeping mixup for robustness). This leads to a drop from 86.6 for M-DYR-H to 74.6 in the last epoch (80% of label noise on CIFAR10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperparameters</head><p>We stress that experiments across all datasets share the same hyperparameter configuration and lead to consistent improvements over the state-of-the-art, demonstrating that the general approach does not require carefully tuned hyperparams. Indeed, we are likely reporting suboptimal results that could be improved with a label noise free validation set, though availability of this set is not assumed in this paper.</p><p>Starting training with high learning rates is important: training more epochs leads to better performance, as mixup together with a high learning rate helps prevent fitting label noise. This warm-up learns the structured data (mainly associated to clean samples) and helps separate the losses between clean/noisy samples for a better BMM fit.</p><p>Experiment details All experiments used the following setup and hyperparameter configuration:</p><p>Preprocessing Images are normalized and augmented by random horizontal flipping. We use 32?32 random crops after zero padding with 4 pixels on each side.</p><p>Network A PreAct ResNet-18 is trained from scratch using PyTorch 0.4.1. Default PyTorch initialization is used on all layers.</p><p>Optimizer SGD with momentum (0.9), weight decay of 10 ?4 , and batch size 128.</p><p>Training schedule without mixup Training for 120 epochs in total. We reduce the initial learning rate (0.1) by a factor of 10 after 30, 80, and 110 epochs. Warm-up for 30 epochs, i.e. bootstrapping (when used) starts in epoch 31. This configuration is used in all experiments in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Training schedule with mixup Training for 300 epochs in total. We reduce the initial learning rate (0.1) by a factor of 10 after 100 and 250 epochs. Warm-up for 105 epochs, i.e. bootstrapping starts in epoch 106 when used (note: the warmup period can be much longer when using mixup because it mitigates fitting label noise. Mixup ? = 32. This configuration is used for all experiments excluding those in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Regarding BMM parameter estimation: parameters are fit automatically using 10 EM iterations as noted in the paper. We also ran M-DYR-H (80% of label noise, CIFAR-10) using 5 and 20 EM iterations, obtaining 87.4 (87.2) and 86.9 (86.3) for best (last) epoch, suggesting that the method is relatively robust to this hyperparameter.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Empirical PDF and estimated GMM and BMM models for 50% label noise in CIFAR-10 after 10 epochs with standard cross-entropy loss and learning rate of 0.1 (remaining hyperparameters see in Subsection 4.1). Clean and noisy samples are colored for illustrative purposes. The BMM model better fits the skew toward zero loss of the noisy samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>UMAP (McInnes et al., 2018) embeddings for training (top) with 80% of label noise and validation (bottom) on CIFAR-10 with (a)(d) cross-entropy loss from Eq. 1, (b)(e) mixup (Zhang et al., 2018) and (c)(f) our proposed M-DYR-H. et al., 2018) in best (last) accuracy of 71.6 (46.7) in CIFAR-10 and 30.8 (17.6) in CIFAR-100 to 86.8 (86.6) and 48.2 (47.2) using the hard alternative (M-DYR-H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) and (b), es-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>M-DYR-H results on CIFAR-10 for (a) image classification and (b) clean/noisy classification of the BMM. (c) comparison of GMM and BMM for clean/noisy classification with 80% label noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Insight Centre for Data Analytics, Dublin City University (DCU), Dublin, Ireland. Correspondence to: Eric Arazo &lt;eric.arazo@insight-centre.org&gt;, Diego Ortego &lt;diego.ortego@insight-centre.org&gt;. Proceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Validation accuracy on CIFAR-10 for static bootstrapping and the proposed dynamic bootstrapping. Key: CE (cross-entropy loss), ST (static bootstrapping), DY (dynamic bootstrapping), S (soft), and H (hard). Bold indicates best performance.</figDesc><table><row><cell>Alg./Noise level (%)</cell><cell>0</cell><cell>20</cell><cell>50</cell><cell>80</cell></row><row><cell>CE</cell><cell cols="4">Best 93.8 89.7 84.8 67.8 Last 93.7 81.8 55.9 25.3</cell></row><row><cell>ST-S</cell><cell cols="4">Best 93.9 89.7 84.8 67.8 Last 93.9 81.7 55.9 24.8</cell></row><row><cell>ST-H</cell><cell cols="4">Best 93.8 89.7 84.8 68.0 Last 93.8 81.4 56.4 25.7</cell></row><row><cell>DY-S</cell><cell cols="4">Best 93.6 89.7 84.8 67.8 Last 93.4 83.3 57.0 27.8</cell></row><row><cell>DY-H</cell><cell cols="4">Best 93.3 89.7 84.8 71.7 Last 92.9 83.4 65.0 64.2</cell></row><row><cell cols="5">all the experiments). Subsection 4.3 presents the results of</cell></row><row><cell cols="5">this approach and Subsection 4.5 demonstrates its superior</cell></row><row><cell cols="4">performance in comparison to the state-of-the-art.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Validation accuracy on CIFAR-10 (top) and CIFAR-100 (bottom) for joint mixup and bootstrapping. Key: CE (crossentropy), M (mixup), DYR (dynamic bootstrapping + regularization from Eq. 14), S (soft), and H (hard). Bold indicates best performance.</figDesc><table><row><cell>Alg./Noise level (%)</cell><cell>0</cell><cell>20</cell><cell>50</cell><cell>80</cell></row><row><cell>CE</cell><cell cols="4">Best 94.7 86.8 79.8 63.3 Last 94.6 82.9 58.4 26.3</cell></row><row><cell>M (Zhang et al., 2018)</cell><cell cols="4">Best 95.3 95.6 87.1 71.6 Last 95.2 92.3 77.6 46.7</cell></row><row><cell>M-DYR-S</cell><cell cols="4">Best 93.3 93.5 89.7 77.3 Last 93.0 93.1 89.3 74.1</cell></row><row><cell>M-DYR-H</cell><cell cols="4">Best 93.6 94.0 92.0 86.8 Last 93.4 93.8 91.9 86.6</cell></row><row><cell>Alg./Noise level (%)</cell><cell>0</cell><cell>20</cell><cell>50</cell><cell>80</cell></row><row><cell>CE</cell><cell cols="4">Best 76.1 62.0 46.6 19.9 Last 75.9 62.0 37.7 8.9</cell></row><row><cell>M (Zhang et al., 2018)</cell><cell cols="4">Best 74.8 67.8 57.3 30.8 Last 74.4 66.0 46.6 17.6</cell></row><row><cell>M-DYR-S</cell><cell cols="4">Best 71.9 67.9 61.7 38.8 Last 67.4 67.5 58.9 34.0</cell></row><row><cell>M-DYR-H</cell><cell cols="4">Best 70.3 68.7 61.7 48.2 Last 66.2 68.5 58.8 47.6</cell></row><row><cell cols="3">4.2. Static and dynamic loss correction</cell><cell></cell><cell></cell></row><row><cell cols="5">Table 1 presents the results for static (ST) and dynamic (DY)</cell></row><row><cell cols="5">bootstrapping in CIFAR-10. Although ST achieves perfor-</cell></row><row><cell cols="5">mance comparable to DY (except for 80% noise where DY</cell></row><row><cell cols="5">is much better), after the final epoch (last) the performance</cell></row><row><cell cols="5">of DY outperforms ST. The improvements are particularly</cell></row><row><cell cols="5">remarkable for 80% of label noise (from 25.7% of ST-H to</cell></row><row><cell cols="5">64.2 of DY-H). Comparing soft and hard alternatives: hard</cell></row><row><cell cols="5">bootstrapping gives superior performance, which is con-</cell></row><row><cell cols="3">sistent with the findings of the original paper</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>reports outstanding accuracy for 80%</cell></row><row><cell>of label noise, a case where we improve upon mixup (Zhang</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>explores convergence under extreme label noise</cell></row><row><cell>conditions, showing that the proposed approach M-DYR-H</cell></row><row><cell>fails to converge in CIFAR-10 with 90% label noise. Here</cell></row><row><cell>we propose minor modifications to achieve convergence.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Validation accuracy on CIFAR-10 (top) and CIFAR-100 (bottom) with extreme label noise. Key: M (mixup), MD (dynamic mixup), DYR (dynamic bootstrapping + reg. from Eq.(14)), H (hard), and SH (soft to hard). (*) denotes that we have run the algorithm. Bold indicates best performance.</figDesc><table><row><cell>Alg./Noise level (%)</cell><cell>70</cell><cell>80</cell><cell>85</cell><cell>90</cell></row><row><cell>M-DYR-H</cell><cell cols="4">Best 89.6 86.8 71.6 40.8 Last 89.6 86.6 71.4 9.9</cell></row><row><cell>MD-DYR-H</cell><cell cols="4">Best 86.6 83.2 79.4 56.7 Last 85.2 80.5 77.3 50.0</cell></row><row><cell>MD-DYR-SH</cell><cell cols="4">Best 84.6 82.4 79.1 69.1 Last 80.8 77.8 73.9 68.7</cell></row><row><cell>Alg./Noise level (%)</cell><cell>70</cell><cell>80</cell><cell>85</cell><cell>90</cell></row><row><cell>M-DYR-H</cell><cell cols="4">Best 54.4 48.2 29.9 12.5 Last 52.5 47.6 29.4 8.6</cell></row><row><cell>MD-DYR-H</cell><cell cols="4">Best 54.4 47.7 19.8 13.5 Last 50.8 41.7 8.3 3.9</cell></row><row><cell>MD-DYR-SH</cell><cell cols="4">Best 53.1 41.6 28.8 24.3 Last 47.7 35.4 24.4 20.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the state-of-the-art in terms of validation accuracy on CIFAR-10 (top) and CIFAR-100 (bottom). Key: M (mixup), MD (dynamic mixup), DYR (dynamic bootstrapping + reg. from Eq. 14), H (hard) and SH (soft to hard). (*) denotes that we have run the algorithm. Bold indicates best performance.</figDesc><table><row><cell>Alg./Noise level (%)</cell><cell>0</cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>90</cell></row><row><cell>(Reed et al., 2015)*</cell><cell cols="5">Best 94.7 86.8 79.8 63.3 42.9 Last 94.6 82.9 58.4 26.8 17.0</cell></row><row><cell>(Patrini et al., 2017)*</cell><cell cols="5">Best 94.7 86.8 79.8 63.3 42.9 Last 94.6 83.1 59.4 26.2 18.8</cell></row><row><cell>(Zhang et al., 2018)*</cell><cell cols="5">Best 95.3 95.6 87.1 71.6 52.2 Last 95.2 92.3 77.6 46.7 43.9</cell></row><row><cell>M-DYR-H</cell><cell cols="5">Best 93.6 94.0 92.0 86.8 40.8 Last 93.4 93.8 91.9 86.6 9.9</cell></row><row><cell>MD-DYR-SH</cell><cell cols="5">Best 93.6 93.8 90.6 82.4 69.1 Last 92.7 93.6 90.3 77.8 68.7</cell></row><row><cell>Alg./Noise level (%)</cell><cell>0</cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>90</cell></row><row><cell>(Reed et al., 2015)*</cell><cell cols="5">Best 76.1 62.1 46.6 19.9 10.2 Last 75.9 62.0 37.9 8.9 3.8</cell></row><row><cell>(Patrini et al., 2017)*</cell><cell cols="5">Best 75.4 61.5 46.6 19.9 10.2 Last 75.2 61.4 37.3 9.0 3.4</cell></row><row><cell>(Zhang et al., 2018)*</cell><cell cols="5">Best 74.8 67.8 57.3 30.8 14.6 Last 74.4 66.0 46.6 17.6 8.1</cell></row><row><cell>M-DYR-H</cell><cell cols="5">Best 70.3 68.7 61.7 48.2 12.5 Last 66.2 68.5 58.8 47.6 8.6</cell></row><row><cell>MD-DYR-SH</cell><cell cols="5">Best 73.3 73.9 66.1 41.6 24.3 Last 71.3 73.4 65.4 35.4 20.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 .</head><label>5</label><figDesc>Comparison with the state-of-the-art in terms of validation accuracy on CIFAR-10 (top) and CIFAR-100 (bottom). Key: M (mixup), MD (dynamic mixup), DYR (dynamic bootstrapping + reg. from Eq. 14), H (hard), SH (soft to hard), WRN (Wide ResNet), PRN (PreActivation ResNet, and GCNN (Generic CNN).</figDesc><table><row><cell cols="3">Bold indicates best performance.</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell cols="2">Architecture</cell><cell>20</cell><cell cols="2">Noise level (%) 40 60</cell><cell>80</cell></row><row><cell>(Jiang et al., 2018b)</cell><cell cols="2">WRN-101</cell><cell cols="2">92.0 89.0</cell><cell>-49.0</cell></row><row><cell>(Ma et al., 2018)</cell><cell cols="2">GCNN-12</cell><cell cols="3">85.1 83.4 72.8</cell><cell>-</cell></row><row><cell>(Ren et al., 2018)</cell><cell cols="2">WRN-28</cell><cell cols="2">-86.9</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">(Wang et al., 2018b) GCNN-7</cell><cell cols="2">81.4 78.2</cell><cell>-</cell><cell>-</cell></row><row><cell>M-DYR-H</cell><cell cols="2">PRN-18</cell><cell cols="3">94.0 92.8 90.3 46.3</cell></row><row><cell>MD-DYR-SH</cell><cell cols="2">PRN-18</cell><cell cols="3">93.8 92.3 86.1 74.1</cell></row><row><cell>Algorithm</cell><cell cols="2">Architecture</cell><cell>20</cell><cell cols="2">Noise level (%) 40 60</cell><cell>80</cell></row><row><cell>(Jiang et al., 2018b)</cell><cell cols="2">WRN-101</cell><cell cols="2">73.0 68.0</cell><cell>-35.0</cell></row><row><cell>(Ma et al., 2018)</cell><cell>RN-44</cell><cell></cell><cell cols="3">62.2 52.0 42.3</cell><cell>-</cell></row><row><cell>(Ren et al., 2018)</cell><cell cols="2">WRN-28</cell><cell cols="2">-61.3</cell><cell>-</cell><cell>-</cell></row><row><cell>M-DYR-H</cell><cell cols="2">PRN-18</cell><cell cols="3">70.0 64.4 58.1 45.5</cell></row><row><cell>MD-DYR-SH</cell><cell cols="2">PRN-18</cell><cell cols="3">73.7 70.1 59.5 39.5</cell></row><row><cell cols="6">Table 6. Comparison of test accuracy on TinyImageNet. Key: M</cell></row><row><cell cols="6">(mixup) , DYR (dynamic bootstrapping + reg. from Eq. 14), H</cell></row><row><cell cols="6">(hard), and SH (soft to hard). (*) denotes that we have run the</cell></row><row><cell cols="5">algorithm. Bold indicates best performance.</cell></row><row><cell cols="2">Alg./Noise level (%)</cell><cell></cell><cell>20</cell><cell>50</cell><cell>80</cell></row><row><cell cols="2">(Zhang et al., 2018)*</cell><cell>Best Last</cell><cell>53.2 49.4</cell><cell>41.7 31.1</cell><cell>18.9 8.7</cell></row><row><cell>M-DYR-H</cell><cell></cell><cell>Best Last</cell><cell>51.8 51.6</cell><cell>44.4 43.6</cell><cell>18.3 17.7</cell></row><row><cell>MD-DYR-SH</cell><cell></cell><cell>Best Last</cell><cell>60.0 59.8</cell><cell>50.4 50.0</cell><cell>24.4 19.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Science Foundation Ireland (SFI) under grant numbers SFI/15/SIRG/3283 and SFI/12/RC/2289.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nellaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02169</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Power of Ensembles for Active Learning in Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>N?rnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>2009. 4.1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>2016. 4.1</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005-04-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hyperspectral Image Classification in the Presence of Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Men-Tornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
	<note>2018b. 1, 1, 2, 4.1, 4.4, 4.5</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense-Captioning Events in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dimensionality-Driven Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>4.4, 4.5</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian Estimation of Beta Mixture Models with Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leijon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2160" to="2173" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>3.1, 3.1</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UMAP: uniform manifold approximation and projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gro?berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>. 3, 4.3</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lf-Net</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09662</idno>
		<title level="m">Learning Local Features from Images</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Features by Watching Objects Move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishna Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
	<note>2017. 1, 2, 4.4, 4.5</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study of Gaussian mixture models of color and texture features for image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Permuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Francos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jermyn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="706" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
	<note>2015. 1, 2, 3.2, 3.2, 4.2, 4.3, 4.4, 4.5</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to Reweight Examples for Robust Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint/>
	</monogr>
	<note>2018. 1, 1, 2, 4.4, 4.5</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint Optimization Framework for Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
	<note>2018. 2, 3.3, 3.3, 4.1, 4.4, 4.6</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning From Noisy Large-Scale Datasets With Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-armed Bandit Algorithms and Empirical Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermorel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning (ECML)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>4.4</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Devil of Face Recognition is in the Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative Learning With Open-Set Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
	<note>2018b. 1, 2, 4.1, 4.4, 4.5</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Light CNN for Deep Face Representation With Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2884" to="2896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>2, 4.1, 4.6</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires re-thinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
	<note>2017. 1, 2, 3.2, 4.1</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint/>
	</monogr>
	<note>2018. 1, 3, 2, 3.3, 4.1, 4.2, 4.3, 3, 4.4, 4.5, 4.6</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid Scene Parsing Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the Importance of Label Quality for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zlateski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jaroensri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durand</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
