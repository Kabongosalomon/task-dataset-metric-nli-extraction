<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relaxed Attention for Transformer Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-20">20 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Lohrenz</surname></persName>
							<email>t.lohrenz@tu-braunschweig.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>M?ller</surname></persName>
							<email>bjoern.moeller@tu-braunschweig.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
							<email>t.fingscheidt@tu-braunschweig.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universit?t Braunschweig</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relaxed Attention for Transformer Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-20">20 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and-for natural language processing tasks-lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models. In this paper, we explore relaxed attention, a simple and easy-to-implement smoothing of the attention weights, yielding a two-fold improvement to the general transformer architecture: First, relaxed attention provides regularization when applied to the self-attention layers in the encoder. Second, we show that it naturally supports the integration of an external language model as it suppresses the implicitly learned internal language model by relaxing the cross attention in the decoder. We demonstrate the benefit of relaxed attention across several tasks with clear improvement in combination with recent benchmark approaches. Specifically, we exceed the former state-of-the-art performance of 26.90% word error rate on the largest public lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as we achieve a top-performing BLEU score of 37.67 on the IWSLT14 (DE ? EN) machine translation task without external language models and virtually no additional model parameters. Code and models will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Early encoder-decoder models emerged from machine translation, where the encoder compressed the entire source language sentence into a fixed-length embedding vector <ref type="bibr" target="#b13">[14]</ref>. This is particularly difficult for very long sentences <ref type="bibr" target="#b12">[13]</ref>, as the fixed-length embedding vector is only a limitedcapacity representation. The use of attention, introduced in <ref type="bibr" target="#b5">[6]</ref>, enabled the computation of variablelength weight distributions over the input sequence and soon turned out to be advantageous for far more applications than just neural machine translation (NMT), e.g., automatic speech recognition (ASR) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5]</ref>, language modeling and understanding <ref type="bibr" target="#b18">[19]</ref>, object detection <ref type="bibr" target="#b7">[8]</ref>, and image classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. Soon the most prominent attention-based encoder-decoder (AED) model emerged, namely the transformer <ref type="bibr" target="#b54">[55]</ref> model. Without the use of any recurrency, it entirely relies on self-attention in the encoder to model temporal dependencies in the input and cross attention in the decoder to extract relevant timesteps thereof during the autoregressive decoding process. While transformers in language modeling tasks are well-suited for upscaling the model size and depth without any saturation when large amounts of data are present <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22]</ref>, they are also susceptible to overfit and require strong regularization to learn at all <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53]</ref>. In a study exclusively on ASR <ref type="bibr" target="#b36">[37]</ref>, it was shown that regularization by smoothing the attention weights in the decoder's cross attention, dubbed relaxed attention, improves performance when the transformer model is combined with an external language model but, for reasons yet to be explored, does not help without a language model.</p><p>In this work, we take on the idea of relaxed attention to expand it to the self-attention layers in the encoder, regularizing already the encoder. Thereby, we increase the method's versatility as it becomes applicable to encoder-only transformers, which are common in several non-sequence tasks such as image classification or pre-trained bidirectional encoder representation by transformer (BERT, <ref type="bibr" target="#b18">[19]</ref>) models. Our main contributions are summarized as follows:</p><p>? We introduce relaxed self-attention in the transformer encoder to improve generalization and develop fuzzy relaxation as a variant thereof.</p><p>? Beyond relaxed self-attention, we extensively investigate the capability of relaxed cross attention in the decoder of sequence-to-sequence transformer models and show that the improvement is due to better external language model integration as it suppresses the influence of the internal language model.</p><p>? We show improvements of the relaxed attention approaches on a variety of tasks including automatic speech recognition, lip-reading, machine translation, and image classification. On the lip-reading and machine translation task we report a new state of the art and topperforming result, respectively.</p><p>The paper is structured as follows: After a summary of related work in Section 2, we introduce the relaxed attention approach in Section 3, followed by the experimental evaluation including results and discussion in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Regularization of transformer models In this work, we introduce a regularization method to the self-attention function <ref type="bibr" target="#b54">[55]</ref>, which is fundamental to transformer models. Several regularization approaches proposed for such networks in the past are related to the network output of transformer models by modifying the loss computation, either through label smoothing <ref type="bibr" target="#b41">[42]</ref>, or by introducing additional loss terms. This could be a CTC-loss computed on the encoder outputs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12]</ref> for monotonous tasks such as ASR, or a divergence term between output softmax distributions of two forward passes with different dropout masks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>. Related to the network input, several-mostly application-dependent-data augmentation approaches such as spectral augmentation for ASR <ref type="bibr" target="#b45">[46]</ref>, or cutoff for machine translation <ref type="bibr" target="#b47">[48]</ref> have proven to be effective. Another set of regularization methods, specific to transformer models, adds a loss term to encourage attention heads to yield diverse outputs <ref type="bibr" target="#b32">[33]</ref> or is based on the dropout technique <ref type="bibr" target="#b50">[51]</ref> and randomly masks attention heads <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b53">54]</ref> or entire en-/decoder block layers (LayerDrop) <ref type="bibr" target="#b20">[21]</ref> during training. Relaxed attention in <ref type="bibr" target="#b36">[37]</ref> was used to prevent too narrow attention weight distributions in the cross attention during training which only yielded improvements with an external language model in ASR. We, however, apply this approach to the self-attention function to reduce over-fitting already in the encoder and show that relaxed self-attention also helps when applied during both training and test. In addition we include a variety of the aforementioned-proven to be effective-regularization methods as baselines and show that relaxed attention is able to further improve performance yielding complementarity to other regularization methods. Very early once attention-based encoder-decoder networks were introduced to ASR, Chorowski et al. <ref type="bibr" target="#b14">[15]</ref> proposed a modified softmax function to smooth the attention weights in the cross attention between encoder and decoder by replacing the exponential function in the standard softmax function with a sigmoid. Thereby, they compressed the probability-like outputs, but didn't take into account the input sequence length, despite the authors' observation that longer sentences require less smoothing of the attention weights. Even though this method dubbed smoothed focus was so far only applied to recurrent neural network (RNN)-based AED models, we include it as a reference method in our simulations as it is the closest to the relaxed attention approach.</p><p>Internal language model handling For many sequence-to-sequence tasks the integration of language models (LMs) to AED models is of dual use: First, LMs leverage the use of large amounts of additional text-only data to improve performance. Second, LMs can be utilized to adapt acoustic models to domains which differ from the original acoustic training data domain. Several techniques exist to combine language models with AED models, such as shallow fusion <ref type="bibr" target="#b22">[23]</ref>, deep fusion <ref type="bibr" target="#b22">[23]</ref>, and cold fusion <ref type="bibr" target="#b49">[50]</ref>, whereas shallow fusion still is the most common solution due to its simplicity and flexibility. However, AED models tend to learn an internal language model in the autoregressive decoder <ref type="bibr" target="#b39">[40]</ref>, which can either be suppressed by subtracting an additional LM trained only on text transcriptions from the acoustic training data (e.g., density ratio fusion <ref type="bibr" target="#b39">[40]</ref>  more recently-can be adapted to a new domain <ref type="bibr" target="#b40">[41]</ref> requiring additional retraining. For the specific application of automatic speech recognition, in a small study, the authors of <ref type="bibr" target="#b36">[37]</ref> have investigated relaxed cross attention, whereby performance improvements were only achieved with external language models. In our work, we investigate the hypothesis that relaxed cross attention successfully suppresses the internal language model, which in contrast to the aforementioned methods does notapart from a single hyperparameter-require any additional models ( <ref type="bibr" target="#b39">[40]</ref>), parameters ( <ref type="bibr" target="#b49">[50]</ref>), or adaptation trainings ( <ref type="bibr" target="#b40">[41]</ref>), but weakens the internal language model during training of the transformer thus supporting shallow fusion <ref type="bibr" target="#b22">[23]</ref>. In addition, we will introduce relaxed self-attention, which improves performance in many applications even without the use of an explicit LM.</p><formula xml:id="formula_0">1 ? d K B ?T ?d Q B ?L?d V B ?T ?d B ?L?d/4 B ?d/4?T Y i 4B ?T ?d/4 G i 4B ?L?T G i 4B ?L?T 4B ?L?d/4 Z i B ?L?d B ?L?d Z</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed relaxed attention</head><p>Scaled dot-product multi-head attention (MHA, see <ref type="figure" target="#fig_0">Figure 1</ref>) is typically used in two variants in the encoder-decoder transformer model <ref type="bibr" target="#b54">[55]</ref>: First, it is used in the encoder as self-attention to model positional (e.g., temporal) dependencies in the preprocessed input sequence indexed with t ? {1, . . . , T } of length T . Second, it is used in the decoder as cross attention (also often referred to as encoder-decoder or source target attention), which draws the decoder's attention to relevant parts in the encoded input sequence h T 1 ? R T ?d for decoding at output sequence index ? ? {1, . . . , L} with model dimension d. In case of self-attention, all MHA inputs (key K, value V, query Q) are the same, i.e., K V Q, with query input Q ? RL ?d of lengthL = T . For cross attention, key and value inputs, K ? R T ?d and V ? R T ?d , respectively, stem from the encoder output h T 1 yielding K V h T 1 , while the query input Q comes from the previous decoder layer withL 1 during inference andL L during training, where for the latter all L tokens of the target sequence are processed in parallel. Details of the entire typical encoder-decoder transformer architectures are recapitulated in Appendix A.1. The attention weights G i (Q, K) ? IL ?T for the scaled dot-product MHA sum up to one across the query input lengthL after the softmax activation function and thus can be interpreted as a probabilistic weighting applied to the value input projection Y i ? R T ?d/N h , with N h being the number of attention heads each indexed with i ? N h . To prevent overly sharp attention distributions applied to the encoded input sequence, i.e., to introduce some stress into the (training) process, our relaxed attention weights for scaled dot-product attention are defined as simple as (see <ref type="figure" target="#fig_0">Figure 1</ref>, red box)</p><formula xml:id="formula_1">G i (Q, K) = (1 ? ?)G i (Q, K) + ? 1 T , i ? N h ,<label>(1)</label></formula><p>gradually injecting a uniform distribution (with 1 here being anL?T matrix of ones) into the standard attention weights, controlled by a relaxation coefficient ? ? [0, 1], which is a constant single hyperparameter for all respective attention layers. While the authors of <ref type="bibr" target="#b36">[37]</ref> only investigated relaxed cross attention, only for automatic speech recognition and only during training, in our work (i) we propose relaxed cross attention and self-attention, (ii) during training and during inference (matched inference), (iii) we investigate their application to automatic speech recognition, lip-reading, machine translation, and image classification, and (iv) we introduce fuzzy relaxation for the image classification task, where we randomly draw the relaxation coefficient from a normal distribution ? ? N (x; ? ? 0 , ? 2 ), with the initially set ? 0 being the mean ?. For this specific task, the variable sequence length T in equation <ref type="formula" target="#formula_1">(1)</ref> is substituted by a constant number of image patch tokens M 2 (see equation <ref type="formula" target="#formula_8">(3)</ref> in Appendix A.2), thereby omitting a certain natural variation. Fuzzy relaxation reestablishes this variation of the relaxation by randomizing ? during training, while for the matched inference case, the relaxation coefficient is kept fixed at ? ? ? 0 during inference. Details for the encoder-only transformer used for the image classification task are given in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental validation and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Application to automatic speech recognition</head><p>Task and datasets Automatic speech recognition transforms recorded speech signals into a sequence of text tokens. We investigate our relaxed attention method on the Librispeech dataset <ref type="bibr" target="#b42">[43]</ref> with the clean and other conditions of the dev and test subsets. We measure system performance in terms of word error rate WER = 1 ? N ?D?I?S N , depending on the number of words N , deletions D, insertions I and substitutions S. All raw speech signals are sampled at 16 kHz and analyzed with a 25 ms window at a frame shift of 10 ms. As common in ASR, we also use an external language model trained on the text labels of the 960 h training set as well as on the text-only Librispeech language model training corpus, the latter containing sentences from a total amount of 14,500 books from project Gutenberg <ref type="bibr" target="#b42">[43]</ref> which are accessible under public-domain. The Librispeech ASR corpus is available under the very permissive Creative Commons BY 4.0 license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and training</head><p>For training with 100 h and 960 h of training data, we use the standard encoder-decoder transformer model from <ref type="bibr" target="#b54">[55]</ref> with a small and a large configuration, comprising 19.3M and 69.8M parameters, respectively. As common for ASR, filterbank features are extracted for each time frame t and then preprocessed by a four-layer convolutional neural network, each using 3?3 filter kernels (cf. preprocessing block in <ref type="figure" target="#fig_5">Figure 2</ref>, Appendix A.1). All hyperparameters were set according to the recipes available in the fairseq based espresso toolkit 1 <ref type="bibr" target="#b56">[57]</ref> except the relaxation coefficients ?, which have been tuned on the joint clean and other portions of the dev set for both, relaxed cross attention and relaxed self-attention. As additional regularization we use SpecAugment <ref type="bibr" target="#b44">[45]</ref>, label smoothing <ref type="bibr" target="#b41">[42]</ref> and dropout <ref type="bibr" target="#b50">[51]</ref> during training. See Appendix A.3.1 for more details.</p><p>Results and discussion For both, the 100 h and 960 h training data cases in <ref type="table">Table 1</ref>, the resimulated baselines (using training scripts from <ref type="bibr" target="#b56">[57]</ref>) yield similar results as in <ref type="bibr" target="#b36">[37]</ref> using a standard transformer approach. The smoothed focus method <ref type="bibr" target="#b14">[15]</ref> has a higher WER compared to the baseline on the small training data case, but yields small improvements on some clean settings for the 960 h training data case. Compared to smoothed focus, relaxed selfand cross attention adapt to the length T of the input sequence, with the latter yielding solid WER reduction across all dev and test conditions when an LM is used (right-hand side of <ref type="table">Table 1</ref>), thereby confirming the results of Lohrenz et al. <ref type="bibr" target="#b36">[37]</ref>. In Appendix A.4, we show that the strong improvement with LM using relaxed cross attention is due to improved internal language model suppression. <ref type="bibr">Without</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Application to lip-reading</head><p>Task and datasets Automatic lip-reading strives to process an image sequence from recordings of talking faces. We evaluate lip-reading performance in terms of WER on the test partition of the Lip Reading Sentences 3 (LRS3) dataset consisting of a total of 1,321 recorded videos of English utterances sourced from TED talks <ref type="bibr" target="#b2">[3]</ref>. To investigate the performance of the relaxed attention approach on recently successful self-supervised learning approaches, we closely follow the training setup from <ref type="bibr" target="#b48">[49]</ref> and use audio-visual hidden unit BERT (AV-HuBERT) encoder models pre-trained on the English subset of the Voxceleb2 dataset <ref type="bibr" target="#b15">[16]</ref>, containing a total amount of 1,326 hours of unlabeled video recordings. For some experiments we also use an external language model trained on the joint text data from LRS3 and the Librispeech language model training corpus. LRS3 is publicly available under the TED terms of use as well as the Creative Commons BY-NC-ND 4.0 license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and training</head><p>We use AV-HuBERT models 2 , introduced recently in <ref type="bibr" target="#b48">[49]</ref>, which receive image and acoustic frames for pre-training by unlabeled training data to iteratively learn contextualized feature representations h T 1 . For fine-tuning and inference, only the video input is used and preprocessed (cf. preprocessing layer in <ref type="figure" target="#fig_5">Figure 2</ref>, Appendix A.1) with a 3D convolutional layer and a subsequent ResNet-18 <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52]</ref> architecture. The models fine-tuned on 30 h of LRS3 training data use the base configuration of the downloaded AV-HuBERT encoder and have a total of 160M parameters. Models fine-tuned on 433 h of LRS3 training data (with or without self-training) use the large AV-HuBERT encoder and comprise 477M parameters in total. As additional regularization methods we use label smoothing <ref type="bibr" target="#b41">[42]</ref>, LayerDrop <ref type="bibr" target="#b20">[21]</ref>, as well as dropout <ref type="bibr" target="#b50">[51]</ref>. For final experiments, we use the self-training <ref type="bibr" target="#b63">[64]</ref>   <ref type="table">Table 2</ref>: Automatic lip-reading results in terms of WER (%) on the LRS3 task using various sequence-to-sequence topologies (top segment baselines) or AV-HuBERT encoders (lower three segments) pre-trained on unlabeled English data from Voxceleb2 and fine-tuned with a joint transformer decoder on the given amount of fine-tuning training data. We also use self-training (bottom segment) by creating pseudo-labels for the 1,326 h of unlabeled data and using these for fine-tuning. Attention relaxation is applied in training only, except for "matched inference" (attention relaxation in training and test). Best results for each of the three fine-tuning setups are in bold font.</p><p>is inferred to generate pseudo-labels for the 1,326 h of unlabeled Voxceleb2 data. These were then used together with the true labels from the LRS3 training data to fine-tune the pre-trained AV-HuBERT model. Relaxed attention was only used during this final fine-tuning, and relaxation coefficient ? of each relaxed attention approach was optimized on the development set for each corresponding amount of fine-tuning data. See Appendix A.3.2 for more details.</p><p>Results and discussion The upper segment of <ref type="table">Table 2</ref>    <ref type="bibr" target="#b25">26</ref>.50%, respectively). We show that this is also caused by the improved internal language model handling for this task in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Application to machine translation</head><p>Task and datasets Neural machine translation (NMT) models use neural networks that translate an input text sequence from a source language to a different target language. For our particular experiments on relaxed attention we use data from the well-known IWSLT14 translation challenge <ref type="bibr" target="#b9">[10]</ref>, choosing the German-to-English (DE? EN) subtask and report performance in terms of BLEU <ref type="bibr" target="#b43">[44]</ref> scores. For training of an external LM we either use the IWSLT14 target language transcripts (160k utterances) or the MuST-C dataset, the latter contains 47% additional transcripts (236k utterances) from TED talks and is available under the Creative Commons BY-NC-ND 4.0 international license <ref type="bibr" target="#b8">[9]</ref>.</p><p>Models and training For training we use the standard encoder-decoder transformer model from <ref type="bibr" target="#b54">[55]</ref> in the base configuration with 36.7M parameters and apply cutoff augmentation, which first randomly masks input positions and feature dimensions of the embedded input tokens and second uses a divergence loss to minimize the difference in predictions for different input masks 3 <ref type="bibr" target="#b47">[48]</ref>. The joint dictionary for source and target language comprises 10k tokens generated with SentencePiece <ref type="bibr" target="#b30">[31]</ref> and preprocessed with an embedding layer (cf. preprocessing layer in <ref type="figure" target="#fig_5">Figure 2</ref>, Appendix A.1). As in the previous tasks, to investigate relaxed attention with LM, we trained two transformer LMs of equal size: One LM trained with IWSLT14 training transcripts and an extended LM trained on the MuST-C dataset, respectively. For both, relaxed cross attention and relaxed self-attention, the relaxation coefficient ? has been tuned on the development set. See Appendix A.3.3 for more details.</p><p>Results and discussion In the upper segment of <ref type="table" target="#tab_4">Table 3</ref>, we report BLEU scores for recent transformer-based approaches to NMT, whereof we choose the strong approach from Shen et al. <ref type="bibr" target="#b47">[48]</ref> using cutoff augmentation as a baseline and report a somewhat lower BLEU score of 37.42 in our resimulation. Smoothed focus here achieves comparable performance to that baseline with small gains when LMs are used. We observe that a LM trained only with the target language training transcripts of the translation model yields no additional information compared to the internally learned language model and thus does not improve performance for most approaches, even the relaxed cross attention that has been strong (with LM) in previous tasks. However, in case of a strong extended LM trained with additional data, relaxed cross attention (only during training again) yields the best performance of 37.85 BLEU, as it suppresses the internal LM. The best performance for the common case without LM is achieved with our relaxed self-attention approach applied during training and test, slightly outperforming the previous state-of-the-art BLEU score without additional training data (37.60, Shen et al. <ref type="bibr" target="#b47">[48]</ref>), with a score of 37.67, exceeding the resimulated baseline even by 0.25 BLEU. We note, that in <ref type="bibr" target="#b26">[27]</ref> (only available as preprint) the authors also chose the model of Shen et al. <ref type="bibr" target="#b47">[48]</ref> as baseline but were able to reproduce the result of 37.60 BLEU. They report a BLEU score of 37.78 by simply applying a modified learning rate schedule achieving a somewhat smaller improvement of 0.18 BLEU absolute vs. their baseline. Without claiming a new state of the art, we note that both, our and their method are top-performing on the IWSLT14 task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Application to image classification</head><p>Task and datasets Image classification is a fundamental task in computer vision aiming at recognizing the primary content of images and differs significantly from the previous sequence-tosequence tasks as it uses a type of an attention-based encoder-only (decoder-less) transformer model, which recently dominate vision benchmarks. To investigate if relaxed attention is also applicable to such tasks, we evaluate performance in terms of classification accuracy (%) on the computationally less demanding CIFAR-100 dataset <ref type="bibr" target="#b0">[1]</ref>. For each of its 100 classes, it contains 500 and 100 images for training and test, respectively, and is publicly available without a specified license. As initialization, we use a model pre-trained on the ImageNet-1k dataset <ref type="bibr" target="#b17">[18]</ref>, which contains 1.28M training images from 1,000 classes and is also available for research purposes upon agreement of the terms of access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and training</head><p>For our experiments we use the vanilla Swin-T transformer model <ref type="bibr" target="#b35">[36]</ref> as baseline-a recently established vision transformer comprising 29M parameters using localized attention. Details on the architecture (including figures) are given in Appendix A.2. For training settings we follow <ref type="bibr" target="#b35">[36]</ref>. For some experiments we downloaded the official ImageNet-1k pre-trained model <ref type="bibr" target="#b3">4</ref> and report results after fine-tuning for 100 epochs on CIFAR-100 training data. With or without pre-training, relaxed self-attention is applied only during fine-tuning. We investigate the interaction of our relaxed self-attention approach with other regularization methods by omitting already employed (i.e., the well-known stochastic depth method <ref type="bibr" target="#b25">[26]</ref>) or adding recently proposed (i.e., the dense relative localization loss L drloc <ref type="bibr" target="#b34">[35]</ref>) approaches. For fair comparison and following common practice as in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56]</ref>, we report results of our relaxed self-attention approaches after roughly optimizing test accuracy with a small grid search over ? values (and ? 2 for fuzzy relaxation after the optimal ? 0 was found) separately for each batch size (1024 and 128) with pre-training, applying the found values to experiments without pre-training. See Appendix A.3.4 for more details.</p><p>Results and discussion The first segment of <ref type="table" target="#tab_6">Table 4</ref> shows results for reference vision transformer models ranging from 87.10% accuracy for the pure attention-based ViT-S-16 <ref type="bibr" target="#b19">[20]</ref>   that relaxed self-attention doesn't help. This might be due to the limited number of training epochs and a slower convergence caused by the additional relaxed self-attention regularization, similar to the effect of stochastic depth in the resimulated baseline. When applying relaxed attention after pretraining, however, relaxed self-attention alone slightly outperforms the baseline but achieves even higher accuracies when used with matched inference (88.73% vs. 88.53%) and (89.39% vs. 89.16%) for the large and small batch sizes, respectively. Matched inference turned out to be advantageous on this task in most cases, thus we continue to report based thereon. Also, we note that the combination with stochastic depth seems to be beneficial for relaxed self-attention. Our new fuzzy relaxation with matched inference turns out to be useful only on smaller batch sizes after pre-training, achieving a strong accuracy of 89.60% outperforming the baseline ( <ref type="bibr" target="#b35">[36]</ref>, resimulated) at 89.16%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work we broadly explored the idea of relaxed attention for transformer architectures, a simple smoothing method of the attention weights in the attention layers. We confirmed the advantage of relaxed cross attention when combined with strong external language models and introduced relaxed self-attention, thereby regularizing already in the encoder and increasing the versatility of relaxed attention to different transformer variants. We show improvements when applying relaxed attention to automatic speech recognition, lip-reading, machine translation, and image classification. On the LRS3 lip-reading task in particular we achieve a word error rate of 26.31% (vs. the former state of the art of 26.90%) as well as a top-performing BLEU score of 37.67 on the IWSLT14 machine translation task.  <ref type="figure" target="#fig_5">Figure 2</ref>: Standard encoder-decoder transformer during inference as used for sequence-tosequence tasks (i.e., automatic speech recognition, lip-reading, and machine translation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model specifics for the sequence-to-sequence transformer</head><p>In this section we briefly review the original transformer architecture from <ref type="bibr" target="#b54">[55]</ref> consisting of encoder and decoder as shown in <ref type="figure" target="#fig_5">Figure 2</ref>. Please note that here we describe the transformer architecture exactly as used for the investigated sequence-to-sequence tasks (i.e., automatic speech recognition, lipreading, and machine translation), employing task-dependent individual preprocessing steps, while the encoder-only Swin transformer, used for the image classification task, is separately described and shown in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Encoder-decoder transformer</head><p>The input sequence xT 1 of lengthT (and more optional dimensions, e.g., for lip-reading: video channel, height, and width, or for ASR: acoustic feature dimension F ) is entirely fed to the transformer's encoder and auto-regressively transformed (by the decoder model D( )) into an output token sequence c L 1 = (c 1 , c 2 , . . . , c L ) with c ? ? C = {c <ref type="bibr" target="#b0">(1)</ref> , c <ref type="bibr" target="#b1">(2)</ref> , . . . , c (D) } being a single output token (i.e., grapheme-based characters or (sub-) word units <ref type="bibr" target="#b30">[31]</ref>) at output sequence index ? ? {1, . . . , L} from a vocabulary of size D. Specifically, the original input sequence xT 1 is first subject to a taskdependent preprocessing that outputs a feature sequence of t ? {1, . . . , T } frames, optionally subsampled with T ?T . For each decoding step (starting at ? = 1), the transformer decoder uses the entire encoded input sequence h T 1 and the previous output token c ??1 to finally output a vector P ? = D(h T 1 , c ??1 ) comprising probabilities of all D possible output tokens. These probabilities are then subject to a beam search algorithm which, step-by-step, invokes the decoder until an endof-sentence (EOS) threshold is exceeded and the final set of hypotheses is emitted. Considering regularization, the standard encoder-decoder transformer model employs three different variants of dropout <ref type="bibr" target="#b50">[51]</ref>: Residual dropout applied to sub-layer outputs before the residual connection is added, activation dropout applied after the rectified linear unit (ReLU) activation, and attention dropout which is applied to the attention weights inside the MHA function (all layers, where dropout might be applied to the respective outputs, are shown as dashed boxes in <ref type="figure" target="#fig_0">Figures 1 and 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Norm</head><p>Multi-Head Attention + Layer Norm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fully Connected + ReLU</head><p>Fully Connected</p><formula xml:id="formula_2">+ B ? T ? d Q K V B ? T ? d B ? T ? d B ? T ? 4d B ? T ? d</formula><p>Encoder Block    <ref type="figure" target="#fig_0">Figure 1</ref>. Layers, where dropout <ref type="bibr" target="#b50">[51]</ref> might be applied to the outputs, are depicted as dashed-line boxes.</p><formula xml:id="formula_3">+ B ?1?d Q ? K ? V ? B ?1?d Q B ?1?d K V from encoder B ? T ? d Z B ?1?d B ?1?d B ?1?4d B ? 1 ? d Decoder Block</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Scaled dot-product attention</head><p>Besides other variants of the original attention function introduced in <ref type="bibr" target="#b5">[6]</ref>, in this work we focus on scaled dot-product multi-head attention (MHA), introduced together with the orignal encoderdecoder transformer model <ref type="bibr" target="#b54">[55]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (without the red block), the standard MHA employs multiple (i.e., N h ) attention heads</p><formula xml:id="formula_4">Z i (Q, K, V) = softmax ? ? ? QW (Q) i KW (K) i T ? d ? ? ? attention weights =Gi(Q,K) ? VW (V) i value projections =Yi(V) ? RL ? d N h (2) with W (Q) i , W (K) i , W (V) i ? R d? d</formula><p>N h being linear projection weight matrices for the query Q, key K, and value V inputs, i ? N h = {1 . . . N h } being the index of the in total N h attention heads, and d is the feature vector size being used in most layers of the transformer model often referred to as the model dimension. Considering cross attention, key and value inputs stem from the encoder's last layer, yielding K V h T 1 and the entries in each of theL = L rows of the attention weight matrix G i (Q, K) ? IL ?T , with I = [0, 1], sum up to one and are treated as probabilities that correspond to the relevance of a time frame t to the ?-th or t-th position in the query input for cross attention or self-attention, respectively. The outputs Z i of all N h separate attention heads are concatenated and subject to a fully connected output layer, yielding the MHA output Z ? RL ?d . Note that for brevity of notation the attention dropout commonly applied to the attention weights in transformer models is not shown in (2). </p><formula xml:id="formula_5">B ? H ? W ? 3 B ? H 4 ? W 4 ? C B ? H 4 ? W 4 ? C B ? H 8 ? W 8 ? 2C B ? H 8 ? W 8 ? 2C B ? H 16 ? W 16 ? 4C B ? H 16 ? W 16 ? 4C B ? H 32 ? W 32 ? 8C B ? H 32 ? W 32 ? 8C h B ? 8C B ? D P 2? 2? N ? 2?</formula><p>input image output posterior vector <ref type="figure">Figure 4</ref>: Swin transformer as used for the image classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Model specifics for the vision transformer</head><p>As attention-based model for the image classification task we employ the recently successful encoder-only transformer, dubbed the Swin transformer <ref type="bibr" target="#b35">[36]</ref>, as shown in <ref type="figure">Figure 4</ref>. The Swin transformer is a hierarchical vision transformer, which uses a shifting window scheme to compute its feature representations h and can be used as a general purpose backbone for various vision tasks. In contrast to the sequence-based tasks, where a whole decoder is employed to yield sequential output, here, a single fully connected layer with softmax activation (and preceding layer normalization and adaptive average pooling) is used after the Swin transformer blocks to assign probabilities P to the D classes for each image inside a batch B.    layer with a stride of (4, 4) and is thereby embedded into a feature representation of dimension C. The hierarchical structure of the Swin transformer consists then of four stages each depicted as a dashed box in <ref type="figure">Figure 4</ref>. In each stage, the patch merging modules first reduce the spatial resolution and double the feature dimensionality (n ? C ? 2n ? C), while dimensions remain constant for the subsequent processing of that specific stage through the specified number of Swin transformer blocks for that specific stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As input, the</head><formula xml:id="formula_6">+ B ? (h ? w) ? c B ? (h ? w) ? c B ? (h ? w) ? 4c B ? (h ? w) ? c Swin Transformer Block</formula><formula xml:id="formula_7">B ?(w ? h)?c hw M 2 B ? M 2 ? c 1 ? c/4 hw M 2 B ?M 2 ?c/4 hw M 2 B ?M 2 ?c/4 hw M 2 B ? M 2 ? M 2 Rpos 4 hw M 2 B ?M 2 ?c/4 V K Q G i (Q, K) G i 4 hw M 2 B ?M 2 ?M 2 4 hw M 2 B ? M 2 ?c/4 hw M 2 B ?M 2 ?c</formula><p>The Swin transformer block, shown in <ref type="figure" target="#fig_6">Figure 5a</ref>, is based on the original standard transformer block ( <ref type="bibr" target="#b54">[55]</ref>, see also <ref type="figure" target="#fig_3">Figure 3a</ref>), but replaces the ReLU activation with a Gaussian error linear unit (GELU) activation function after the first fully connected layer and, more importantly, uses a (shifted) window-based multi-head attention module, shown in <ref type="figure" target="#fig_6">Figure 5b</ref>. There, the window partitioning limits the self-attention computation to non-overlapping local M ? M windows (M = 7), which are shifted in position every other Swin transformer block. Once the features are split into windows, they are treated as separate batch instances yielding a temporary batch size of hw M 2 B with B being the original batch size. Different to the standard multi-head attention, a relative position bias R pos ? R M 2 ?M 2 is added before softmax activation. The attention weights G i (Q, K) ? R M 2 ?M 2 inside the shifted window-based MHA contain probabilities for relevant entries in these windows and are then subject to the herein investigated relaxation (see red box in <ref type="figure" target="#fig_6">Figure 5b</ref>), yielding</p><formula xml:id="formula_8">G i (Q, K) = (1 ? ?)G i (Q, K) + ? 1 M 2 , i ? N h ,<label>(3)</label></formula><p>with M 2 being the fixed amount of features in a single window. See Section 3 for the sequence-based relaxed attention approach as well as for the fuzzy relaxation which randomly varies the relaxation coefficient ? to compensate for the now constant M 2 term in (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Experimental details</head><p>In this section we will list additional experimental details for all of the investigated tasks, thereby starting with general settings that apply to multiple tasks and then providing details for the experiments of each individual task that has been investigated in this work. Please note that for all tasks we used publicly available code as baselines and did not change any hyper-parameters unless explicitly mentioned (e.g., for ablation studies).</p><p>In experiments where an additional language model was included, we used the common shallow fusion method <ref type="bibr" target="#b22">[23]</ref> for language model fusion. Specifically, shallow fusion combines the output token probability vector P ? at the output of the transformer model (cf. <ref type="figure" target="#fig_5">Figure 2)</ref> for each decoding timestep ? with the same D-length output token probabilities P (LM) ? in the logarithmic domain to gather a joint output token probability logP ? = log P ? + ? log P (LM) ? . The language model weight is used to steer the influence of the language model during decoding and is gathered individually for each task.</p><p>In all investigated tasks, the smoothed focus method from <ref type="bibr" target="#b14">[15]</ref> is applied as a reference method that smoothes the attention weights in the cross attention layer by modyifing the softmax function. Defining the scaled dot-product of query and key projections, which is input to the softmax function in <ref type="bibr" target="#b1">(2)</ref>, as</p><formula xml:id="formula_9">E i = 1 ? d QW (Q) i KW (K) i T</formula><p>= (e i,?,t ) ? R L?T with e i,?,t being elements thereof, the single elements g i,?,t of the attention weights G i (Q, K) with smoothed focus are computed as</p><formula xml:id="formula_10">g i,?,t (Q, K) = ? sig (e i,?,t ) T t 1 ? sig (e i,?,t ) ,<label>(4)</label></formula><p>with ? sig being the sigmoid function, which for smoothed focus replaces the unbounded exponential function from the standard softmax function. Please note that for the Swin transformer the softmax input can be defined analogously as</p><formula xml:id="formula_11">E i = R pos + 1 ? c/4 QW (Q) i KW (K) i T ? R M 2 ?M 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Automatic speech recognition</head><p>The specific model architecture for the trainings with 100 h and 960 h of training data, are standard encoder-decoder transformer models with a small (N e N d 6, N h 4, d 512) and a large (N e 12, N d 6, N h 4, d 512) configuration, respectively, with N e , N d , and N h being the number of encoder blocks, decoder blocks, and attention heads, respectively, and d is the model dimension (i.e., the amount of nodes used in most layers of the model). The external RNN language model consists of shared input and output embedding layers with four LSTM layer in between each comprising 800 nodes yielding a total of 24.5M parameters.</p><p>Training of ASR models was done using the espresso toolkit, which is an extension of the PyTorchbased fairseq toolkit. We performed a small grid search on the joint dev clean and dev other datasets among values {0.0001, 0.001, 0.01, 0.05, 0.1} and {0.1, 0.15, 0.2, 0.25, 0.3} for the relaxation coefficient ? and found ? = 0.01 and ? = 0.25 to be optimal for relaxed self-attention and relaxed cross attention, respectively. Optimal values were used for both, 100 h and 960 h training data configurations. All remaining training hyper-parameters were adopted from the recipes available at https://github.com/freewym/espresso with commit id 390ad6f. Specifically, we train all models for 100 epochs using the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with a learning rate of 0.001. All dropout layers (i.e., residual, activation, and attention dropout) used the dropout rate p 0.2 and the label smoothing coefficient was set to ? = 0.1. Models for 100 h of training data were trained using a single RTX2080ti GPU, while larger models on 960 h of training data were trained on a single A100 GPU.</p><p>Inference was done using a beam search with beam size of 60 and the language model weight ? was fixed at 0.4, following recipes from <ref type="bibr" target="#b56">[57]</ref> for all experiments with LM, without further optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Lip-reading</head><p>The specific model architecture for fine-tuning with 30 h of labeled data, is a pre-trained base AV-HuBERT encoder model with an appended standard transformer decoder model <ref type="figure" target="#fig_0">(N e 12, N d 6, N h 12, d 768)</ref> while for the 433 h and 433 h + 1,326 h setups a large AV-HuBERT encoder with a larger decoder was used (N e <ref type="figure" target="#fig_0">24, N d 9, N h 16, d 1024)</ref>. The external transformer language model comprises 16 decoder blocks with d = 512 (cf. <ref type="figure" target="#fig_3">Figure 3b</ref>, but without the cross attention layer) and uses a shared input/output embedding of the in total D = 1000 subword units, resulting in a language model size of 51M parameters.</p><p>Training of lip-reading models was done using the PyTorch-based fairseq toolkit. We performed a small grid search on the development dataset among values {0.005, 0.01, 0.02, 0.05, 0.1} and {0.1, 0.15, 0.2, 0.25, 0.3} for the relaxation coefficient of relaxed self-attention and relaxed cross attention, respectively. For the 30 h fine-tuning case we found ? = 0.001 and ? = 0.25, for 433 h we found ? = 0.005 and ? = 0.25, and for the 433 h+1,326 h case we found ? = 0.005 and ? = 0.2 to be optimal. All remaining training hyper-parameters were adopted from the recipes available at https://github.com/facebookresearch/av _ hubert with commit id cd1fd24. Residual, activation, and attention dropout layers were using a dropout rate p of 0.1, 0.1, and 0.0, respectively. The label smoothing coefficient was set to ? = 0.1. Models for the smaller 30 h fine-tuning data setup were trained using a single RTX3080 GPU, while for all other settings a single A100 GPU was used for training.</p><p>Inference was done using a beam search with beam size of 50 and the language model weight ? was optimzied for each approach by searching optimal values on the development data among values of {0.05, 0.1, 0.15, 0.2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Machine translation</head><p>The standard encoder-decoder transformer from <ref type="bibr" target="#b54">[55]</ref> was used in the base configuration (N e N d 6, N h 4, d 512). The external transformer language model consists of shared input and output embedding layers of the in total D = 10000 subword units with 6 decoder blocks (cf. <ref type="figure" target="#fig_3">Figure 3b</ref>, but without the cross attention layer) in between and comprises 24.1M parameters.</p><p>Training of the machine translation transformer models was done using the PyTorch-based fairseq toolkit. We performed a small grid search on the development dataset among values {0.005, 0.01, 0.02, 0.05, 0.1} and {0.1, 0.15, 0.2, 0.25, 0.3} for the relaxation coefficient and found ? = 0.05 and ? = 0.1 optimal for relaxed self-attention and relaxed cross attention, respectively. For approaches with LM the language model weight ? was tuned among values {0, 0.05, 0.1, 0.15, 0.2}. All remaining training hyper-parameters were adopted from the recipes available at https://github.com/dinghanshen/Cutoff with commit id 4978563. Residual, activation, and attention dropout layers were set to 0.3, 0.1, and 0.1, respectively. All models were trained using a single RTX2080ti GPU.</p><p>Inference was done using a beam search with beam size of 10 and the language model weight ? was optimized for each approach by searching optimal values on the development dataset among values of {0.05, 0.1, 0.15, 0.2}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.4 Image classification</head><p>We chose the Swin transformer as the specific model architecture for trainings with and without pretraining. It is a multi-purpose backbone for various vision tasks and can be configured in terms of size and complexity. Specifically, we use the tiny configuration of the model dubbed Swin-T, which is defined by an initial feature embedding dimensionality C = 96 and comprises N = 6 Swin transformer blocks in the third stage, resulting in a total of N e = 12 Swin transformer blocks. The num-ber of attention heads N h doubles with each consecutive stage, yielding an amount of {3, 6, 12, 24} attention heads for the respective stages. In total, the model comprises 29M parameters.</p><p>Training of image classification models was done using the PyTorch toolkit. We performed a small grid search among values {0.005, 0.01, 0.05, 0.1, 0.15, 0.2} and {0.01, 0.02, 0.03} for the relaxation coefficient of relaxed self-attention and ? 2 for fuzzy relaxation, respectively. Following common practice on the CIFAR-100 task (see, e.g., <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56]</ref>), parameter search was conducted on the test dataset. For the training with pre-training we found ? = 0.1 and ? = 0.03 to be optimal. Both found relaxation hyper-parameters were also applied for experiments without pre-training. All remaining training hyper-parameters were adopted from the recipes available at https://github.com/microsoft/Swin-Transformer with commit id 5d2aede. For some trainings we use an auxiliary dense relative localization loss L drloc , which encourages vision transformers to learn spatial information between image patches and thereby boosts convergence, especially for small datasets <ref type="bibr" target="#b34">[35]</ref>. For the L drloc loss, we adopted the official Swin-based code from https://github.com/yhlleo/VTs-Drloc with commit id b69adb6. Specifically we train all models for 100 epochs using the Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with a learning rate of 0.000125 for a batch size of 128 and 0.001 for a batch size of 1024. Stochastic depth <ref type="bibr" target="#b25">[26]</ref>, which randomly drops layers in the transformer block, is a standard method for training the baseline model and was used with a drop probability of 0.2. Label smoothing was used with a value of 0.1. All Swin transformer models were trained using a single RTX2080ti GPU.  <ref type="table">Table 5</ref>: Automatic speech recognition results in terms of WER (%) on the Librispeech task using standard encoder-decoder transformer models. The 960 h training dataset is used, see also <ref type="table">Table 1</ref>. <ref type="table">Table 1</ref> for automatic speech recognition, we achieved superior results with relaxed cross attention only when the transformer was combined with an external language model that is trained with large amounts of additional text-only data. This finding is in line with Lohrenz et al. <ref type="bibr" target="#b36">[37]</ref>, but <ref type="bibr" target="#b36">[37]</ref> does not provide a sound reason for such behavior. Different to hybrid ASR approaches, the output token posterior P ? of a trained transformer model cannot technically be decomposed into an acoustic model P(x T 1 |c L 1 ) and language model P(c L 1 ), since the latter is also implicitly learned on the training transcripts by the transformer decoder that in addition to the encoder output autoregressively receives previous output tokens as it is the case for language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Internal language model suppression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Here, we investigate whether the improvement by relaxed cross attention might be due to a suppression of the internal language model. To accomplish this, in <ref type="table">Table 5</ref>, we measure the WER improvement achieved by using an LM when the transformer was trained with and without relaxed cross attention, respectively. Both trained transformer models are combined with two language models, one trained only from the text transcripts of the acoustic training data, and one trained with additional text-only data. Note that both, resimulated baseline results and results for the LM with additional data, are taken from <ref type="table">Table 1</ref>. We observe that for both, the baseline and the relaxed cross attention model, the improvements with the training transcript only LM (rows 2 and 5) vs. the no LM methods are about equally small. In contrast, when combined with the LM model trained on additional data, the model trained with relaxed cross attention yields far more WER reduction as if this strong LM would be used with the baseline. In any case it exceeds an absolute reduction of 0.5% (nowhere reached with the baseline), and for the (noisy) other condition it is more than 1% absolute WER reduction if relaxed cross attention is employed.   <ref type="table">Table 2</ref>.</p><p>For the automatic lip-reading task we observe similar behavior in <ref type="table" target="#tab_12">Table 6</ref>. Here the integration of the training transcripts only LM is even harmful for the baseline model (row 2), while for the relaxed cross attention approach, WERs remain roughly the same compared to the relaxed cross attention-trained model without LM (row 4 vs. 5). In combination with the strong LM, both baseline and relaxed cross attention models take profit on the dev set, while on the test set, relaxed cross attention yields a more than four-fold WER reduction by LM fusion (1.01% absolute) compared to the baseline approach (0.23% absolute).</p><p>Overall, we observe that relaxed cross attention does not yet help when the LM was trained only with the text transcript data that was already exposed to the ASR transformer model during training of acoustic data. We conclude, however, that relaxed cross attention particularly helps when the LM has been trained with additional text data and seems to suppress the internal model bias, thus suppressing the influence of the internally learned (usually poor) language model. Note that the same behavior is observed in <ref type="table" target="#tab_4">Table 3</ref> for neural machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Robustness to different initialization seeds</head><p>In <ref type="table" target="#tab_14">Table 7</ref>, we investigate the influence of different initialization seeds on our experiments. While for the main experiments in Section 4 we experimented on an unchanged and non-optimized seed for random number generation, here-since both of our SOTA contributions are based on the novel self-attention-we analyze the best relaxed self-attention schemes of each task w.r.t. statistical significance when using 5 different random seeds.</p><p>We note that in these experiments, we achieve significant improvement for all three sequence-based tasks including those where we claim state-of-the-art and top performance (i.e., lip-reading and machine translation). In addition, not shown here, the relaxed cross attention method yielded even better performance on all three sequence-based tasks, outperforming relaxed self-attention, but we do not formulate performance claims in this particular analysis as it implies extra computational complexity due to the requirement of a language model as well as additional unpaired text training data. For the image classification task, note that we reach a clear improvement using the non-optimized standard seed for initialization of our main experiments (see <ref type="table" target="#tab_6">Table 4</ref>). Here, however, with additional seeds for initialization, we observe the baseline and the fuzzy relaxation approach to differ without statistical significance. We suspect this is due to non-deterministic operations in the original baseline code from <ref type="bibr" target="#b35">[36]</ref>, which might have flawed the tuning process for the relaxation coefficients for fuzzy relaxation. However, as the average accuracy with fuzzy relaxation is still higher (89.45% vs. 89.29%), we feel encouraged to further expand the relaxed self-attention approach to attention-based approaches for computer vision tasks.   <ref type="table">Table 8</ref>: Ablation study on attention dropout <ref type="bibr" target="#b50">[51]</ref> for exemplary automatic speech recognition and neural machine translation tasks. Best results across approaches are in bold font and arrows (??) point to the direction of better metric values for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Ablation study on attention dropout</head><p>Depicted as dashed boxes in <ref type="figure" target="#fig_0">Figures 1 and 3</ref>, the well-known dropout method <ref type="bibr" target="#b50">[51]</ref> is employed to the standard encoder-decoder transformer in three different variations: Residual dropout, activation dropout, and-most relevant for our study-attention dropout, where the latter is either applied to the attention weights G i after the softmax layer (baseline) or to the modified attention weightsG i after the relaxation operation (relaxed attention approaches, see <ref type="formula" target="#formula_1">(1)</ref>). In <ref type="table">Table 8</ref>, we investigate how these regularization operations interfere with each other for two different tasks that incorporate attention dropout during training. Therefore, in this ablation, we removed attention dropout throughout the encoder and the decoder of the transformer model for both approaches with and without specified types of relaxed attention. Note that the employed models for the lip-reading and image recognition tasks did not use attention dropout (following the respective baseline recipes from <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b35">[36]</ref>, see experimental details in Appendices A.3.2 and A.3.4) and are thus omitted for this ablation. Specific values for attention dropout are given for each task in Appendix A.3.</p><p>We note that relaxed attention nicely combines with attention dropout <ref type="bibr" target="#b50">[51]</ref> as in the test conditions of both tasks the combination of relaxed self-/cross attention with attention dropout yields the best results, which are also reported in the main experiments for both specific tasks in Section 4. Interestingly, attention dropout did even harm the baseline performance for machine translation, as omitting it yields an 0.09 absolute increase in BLEU score, while it improves the advantageous relaxed selfattention even further. In summary, we observe that both proposed relaxed attention approaches seem to go along with other regularization approaches, such as attention dropout <ref type="bibr" target="#b50">[51]</ref>, providing complementary regularization to the attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Sensitiveness of relaxation coefficient ?</head><p>In <ref type="figure">Figures 6 and 7</ref> we investigate the sensitiveness of the relaxation coefficient ? for the automatic speech recognition and for the neural machine translation task respectively. As introduced in Section 3 the constant relaxation coefficient ? ? [0, 1] is a single hyperparameter to control the addition of an uniform distribution to the unmodified attention weights over the temporal dimension of the input sequence. For both exemplary tasks we investigate the influence of ? for relaxing either the encoder self-attention layers <ref type="figure">(Figures 6a and 7a</ref>) or the decoder cross attention layers <ref type="figure">(Figures 6b  and 7b</ref>). Both, <ref type="figure">Figures 6 and 7</ref> show the task-specific performance on the respective development sets, that were used for optimization of the ? hyperparameter. In all shown cases, we make the following observations: (i) While relaxed self-attention performs best with smaller ? values, relaxed cross attention reaches best performance with somewhat higher values, (ii) there is a smooth and substantial range where relaxed self-and cross attention improves over the resimulated baselines with ? = 0 thus showing that the contribution of our method is insensitive with respect to the choice of ? in these ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Statement on potential negative societal impacts</head><p>Our method itself applies to the general transformer model and is-as we have demonstratedapplicable to a variety of applications. Out of these applications, we identify that automatic lipreading can be used for malicious purposes such as eavesdropping on private civilian conversation in video surveillance footage. The dataset we use for automatic lip-reading consists of professionally recorded speakers that are aware of being recorded, are at a close distance, have well illuminated faces while speaking, and are mostly facing towards the camera. These conditions are very unlikely in a malicious surveillance scenario where it is unlikely that the methods and models we developed in our work are of large benefit. In addition, we believe that the positive impact of lip-reading applications clearly outweighs the possible negative applications. Examples of such applications are (i) improving speech recognition in case audio is corrupted, (ii) helping in crime investigations, (iii) enabling people suffering from aphonia to communicate, (iv) silence dictations, and (v) uttering silent alarms or passphrases for security.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Multi-head attention (MHA) as used in encoder and decoder blocks of transformer models with N h = 4 attention heads. The proposed relaxed attention (red block) is presented in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(b) Single decoder block with cross attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Encoder and decoder blocks as used in the transformer model (Figure 2) during inference. Multi-head attention blocks which may exhibit relaxed attention are colored yellow. Details thereof are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Single Swin transformer block as used in the Swin transformer model (cf.Figure 4) during training and inference. Instead of dropout in the encoder-decoder transformer, here we apply stochastic-depth [26] to randomly drop layers parallel to the residual connections during training. (Shifted) window-based multi-head attention blocks which may exhibit relaxed attention are colored yellow. Details thereof are shown in Figure 5b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>hw M 2 B</head><label>2</label><figDesc>?M 2 ?c B ?(w ? h)?c (Shifted) Window-based Multi-Head Attention (b) (Shifted) window-based multi-head attention (MHA) as used in the Swin transformer block Figure 5a with N h = 4 attention heads. The proposed relaxed attention (red block) is presented in Section A.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Details of a Swin transformer block and the (shifted) window-based multi-head attention (MHA), where relaxed attention (red block) is applied for the image classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Relaxed self-attention, without LM 0 .05 .1 .15 .2 .25 .Relaxed cross attention, with LM Sensitiveness of the automatic speech recognition results with respect to the relaxation coefficient ? in terms of combined WER (%) on the joint clean and other portions of the dev dataset of the Librispeech task. Relaxed self-attention, w/o LM, matched in-Relaxed cross attention, with LM Sensitiveness of the neural machine translation results with respect to the relaxation coefficient ? in terms of BLEU scores on the development dataset of the IWSLT14 task (DE? EN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, resimulated) 13.98 28.71 14.82 29.31 10.62 24.19 12.06 25.56 + smoothed focus [15] 14.60 28.73 15.50 30.78 10.83 24.86 12.11 26.46</figDesc><table><row><cell></cell><cell></cell><cell cols="2">without LM</cell><cell cols="2">with LM</cell></row><row><cell>Training data</cell><cell>Approach</cell><cell>dev clean other</cell><cell cols="2">test clean other clean other dev</cell><cell>test clean other</cell></row><row><cell>100 h</cell><cell>Baseline ([37]+ relaxed cross attention + matched inference</cell><cell cols="4">13.91 28.70 14.70 30.10 9.33 22.16 10.62 23.04 14.30 29.03 15.15 30.09 11.04 25.19 12.16 26.36</cell></row><row><cell></cell><cell>+ relaxed self-attention</cell><cell cols="4">13.48 27.87 14.20 28.96 10.22 23.53 11.04 24.55</cell></row><row><cell></cell><cell>+ matched inference</cell><cell cols="4">13.43 28.00 14.46 29.23 10.01 23.96 11.19 25.32</cell></row><row><cell></cell><cell cols="2">Baseline ([37], resimulated) 3.92 9.00</cell><cell>4.47 9.23</cell><cell>3.73 8.52</cell><cell>4.40 8.95</cell></row><row><cell></cell><cell>+ smoothed focus [15]</cell><cell>4.11 9.42</cell><cell>4.35 9.63</cell><cell>3.70 9.18</cell><cell>4.31 9.33</cell></row><row><cell>960 h</cell><cell>+ relaxed cross attention + matched inference</cell><cell>3.95 9.33 3.96 9.29</cell><cell>4.28 9.45 4.20 9.40</cell><cell>3.44 7.74 3.69 8.95</cell><cell>3.58 8.35 4.21 9.46</cell></row><row><cell></cell><cell>+ relaxed self-attention</cell><cell>3.82 8.50</cell><cell>4.05 8.71</cell><cell>3.52 8.03</cell><cell>4.17 8.51</cell></row><row><cell></cell><cell>+ matched inference</cell><cell>3.79 9.12</cell><cell>4.09 9.07</cell><cell>3.35 8.28</cell><cell>3.91 8.50</cell></row><row><cell cols="6">Table 1: Automatic speech recognition results in terms of WER (%) on the Librispeech task using</cell></row><row><cell cols="6">standard encoder-decoder transformer models. Attention relaxation is applied in training only,</cell></row><row><cell cols="6">except for "matched inference" (attention relaxation in training and test). We separately use the</cell></row><row><cell cols="6">100 h and 960 h training datasets and highlight the respective best results for each size in bold font.</cell></row><row><cell cols="6">relaxed self-attention in all dev and test conditions for both training data cases. Specifically, the</cell></row><row><cell cols="6">WER across the test conditions of the 960 h case for relaxed self-attention improved by a relative</cell></row><row><cell cols="6">9% (clean) and 5% (other) compared to the resimulated baseline, yielding complementary regular-</cell></row><row><cell cols="6">ization of our method to the other employed regularization methods. Note that in all aforementioned</cell></row><row><cell cols="6">cases, relaxed attention is best when used only in training. Only in a very specific case on the dev</cell></row><row><cell cols="6">set, however, "matched inference", i.e., relaxed self-attention in training and test, is slightly ahead</cell></row><row><cell cols="2">of using it in training only.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>an LM,</cell></row><row><cell cols="6">both the resimulated baseline and relaxed cross attention approaches are outperformed by our new</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>shows various baselines on LRS3, whereby Makino et al. [39] reached 33.60% WER w/o LM, using 33,000 h of YouTube training data, and Ma et al. [38] achieved 43.40% with LM and 157 h of additional data from the Lip Reading in the Wild dataset [17]. By leveraging pre-training of AV-HuBERT models, Shi et al. [49] report state of the art so far on LRS3 in three cases with 1,326 h unlabeled pre-training data plus 30 h, plus 433 h,</figDesc><table><row><cell>Approach</cell><cell>without LM</cell><cell>with LM (training transcripts only)</cell><cell>with extended LM (additional data)</cell></row><row><cell></cell><cell>test</cell><cell>test</cell><cell>test</cell></row><row><cell>Vaswani et al. [55]</cell><cell>34.40</cell><cell>-</cell><cell>-</cell></row><row><cell>Fan et al. [21]</cell><cell>34.50</cell><cell>-</cell><cell>-</cell></row><row><cell>Wu et al. [58]</cell><cell>35.20</cell><cell>-</cell><cell>-</cell></row><row><cell>Wu et al. [59]</cell><cell>36.88</cell><cell>-</cell><cell>-</cell></row><row><cell>Liang et al. [34]</cell><cell>37.25</cell><cell>-</cell><cell>-</cell></row><row><cell>Shen et al. [48]</cell><cell>37.60</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline ([48], resimulated)</cell><cell>37.42</cell><cell>37.42</cell><cell>37.62</cell></row><row><cell>+ smoothed focus [15]</cell><cell>37.42</cell><cell>37.52</cell><cell>37.67</cell></row><row><cell>+ relaxed cross attention</cell><cell>37.56</cell><cell>37.53</cell><cell>37.85</cell></row><row><cell>+ matched inference</cell><cell>37.60</cell><cell>37.64</cell><cell>37.57</cell></row><row><cell>+ relaxed self-attention</cell><cell>37.57</cell><cell>37.49</cell><cell>37.74</cell></row><row><cell>+ matched inference</cell><cell>37.67</cell><cell>37.67</cell><cell>37.71</cell></row></table><note>plus 433 h + 1,326 h of fine-tuning data, respectively, the latter using self-training to leverage the pre-training data using pseudo-labels. See also our resimulated numbers of that approach. Note that as it is common practice on the LRS3 task to not even report performance on dev condition, we also formulate performance claims on the test set. Smoothed focus [15] helps a bit in 4 out of the 6 totaltest conditions. Without a language model-adding virtually no parameters and only marginally</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Neural machine translation results in terms of BLEU scores on the IWSLT14 task (DE ? EN) using encoder-decoder transformer models with cutoff augmentation<ref type="bibr" target="#b47">[48]</ref>. Attention relaxation is applied in training only, except for "matched inference" (attention relaxation in training and test). Best results across all approaches are in bold font, second best underlined. more complexity during training-our relaxed self-attention achieves WERs of 45.04% vs. 45.90% resimulated<ref type="bibr" target="#b48">[49]</ref>, and 28.84% vs. 29.52% resimulated<ref type="bibr" target="#b48">[49]</ref> in the 30 h and 433 h fine-tuning cases, respectively, with matched inference (relaxation in training and test). With self-training (433 h + 1,326 h), relaxed self-attention without matched inference even achieves 26.31% WER compared to the best lip-reading WER of 26.90% from<ref type="bibr" target="#b48">[49]</ref> thus setting a new state of the art for LRS3. With an additional LM, similar to the ASR task in Section 4.1, relaxed cross attention yields consistent improvement on the test set compared to the resimulated baseline in all three fine-tuning cases (i.e., 42.68% vs. 45.33%, 28.05% vs. 28.97%, and 25.51% vs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>to 91.70% accuracy for the convolution attention hybrid model CMT-S<ref type="bibr" target="#b23">[24]</ref>. The second table segment presents baselines and experimental results for Swin-T transformer models where we chose the vanilla architecture<ref type="bibr" target="#b35">[36]</ref> to resimulate a baseline for our experiments. Omitting stochastic depth<ref type="bibr" target="#b25">[26]</ref> causes a severe loss of performance with pre-training but clearly helps when training from scratch. For the dense relative localization loss L</figDesc><table><row><cell></cell><cell></cell><cell cols="2">w/ pre-training</cell><cell cols="2">w/o pre-training</cell></row><row><cell>Approach</cell><cell></cell><cell cols="2">batch size 1024 128</cell><cell cols="2">batch size 1024 128</cell></row><row><cell></cell><cell></cell><cell>test</cell><cell>test</cell><cell>test</cell><cell>test</cell></row><row><cell>Other transformers</cell><cell cols="2">Dosovitskiy et al. [20], ViT-S-16 87.10 Yuan et al. [62], T2T-ViT-14 88.40 Guo et al. [24], CMT-S 91.70</cell><cell>---</cell><cell>---</cell><cell>---</cell></row><row><cell></cell><cell>Liu et al. [36], Swin-T (vanilla)</cell><cell>88.22</cell><cell>-</cell><cell>53.28</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Liu et al. [35], Swin-T (+ L drloc ) 88.40</cell><cell>-</cell><cell>66.23</cell><cell>-</cell></row><row><cell></cell><cell>Baseline ([36], resimulated)</cell><cell>88.53</cell><cell>89.16</cell><cell cols="2">53.12 64.10</cell></row><row><cell></cell><cell>-stochastic depth [26]</cell><cell>87.62</cell><cell>88.42</cell><cell cols="2">57.62 68.08</cell></row><row><cell>Swin-T transformers</cell><cell>+ L drloc [35] + smoothed focus [15]</cell><cell>88.63 88.44</cell><cell>89.27 89.53</cell><cell cols="2">61.72 66.29 57.02 64.15</cell></row><row><cell></cell><cell>+ relaxed self-attention</cell><cell>88.64</cell><cell>89.21</cell><cell cols="2">52.89 63.72</cell></row><row><cell></cell><cell>+ matched inference</cell><cell>88.73</cell><cell>89.39</cell><cell cols="2">53.15 63.52</cell></row><row><cell></cell><cell>-stochastic depth [26]</cell><cell>87.49</cell><cell>88.42</cell><cell cols="2">56.99 67.91</cell></row><row><cell></cell><cell>+ L drloc [35] + fuzzy relaxation</cell><cell>88.55 88.63</cell><cell>89.29 89.60</cell><cell cols="2">61.37 65.90 52.51 63.58</cell></row></table><note>drloc [35], we confirm performance gains with and especially without pre-training. Smoothed focus helps for the small batch size using pre-training and performs remarkably good for a large batch size when training from scratch. Without pre-training we observe</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Image classification results in terms of accuracy (%) on the CIFAR-100 task using encoder- only transformer models. Relaxed self-attention is applied in training only, except for "matched inference" (relaxation in training and test). All reference methods have roughly the same model size and complexity. Best results across all Swin-T approaches are in bold font, second best underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Automatic lip-reading results in terms of WER (%) on the LRS3 task using standard encoder-decoder transformer models with pre-trained AV-HuBERT encoders. For fine-tuning 433 h + 1,326 h of labeled data are used, see also</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Relaxed self-attention 14.25?0.29 28.63?0.54 26.36?0.22 37.66?0.02 89.45?0.17</figDesc><table><row><cell></cell><cell>Automatic</cell><cell>Automatic</cell><cell>Machine</cell><cell>Image</cell></row><row><cell>Task</cell><cell>Speech Recognition</cell><cell>Lip-Reading</cell><cell>Translation</cell><cell>Classification</cell></row><row><cell></cell><cell>(Librispeech)</cell><cell>(LRS3)</cell><cell>(IWSLT14)</cell><cell>(CIFAR-100)</cell></row><row><cell></cell><cell>100 h</cell><cell>433 h + 1,326 h</cell><cell>w/o LM,</cell><cell>w/ pre-training,</cell></row><row><cell>Setting</cell><cell>training data w/o LM</cell><cell>labeled data w/o LM</cell><cell>matched inference</cell><cell>batchsize 128 fuzzy relaxation</cell></row><row><cell>Metric</cell><cell>WER (%)</cell><cell>WER (%)</cell><cell>BLEU</cell><cell>Acc. (%)</cell></row><row><cell>Data subset</cell><cell>test clean test other</cell><cell>test</cell><cell>test</cell><cell>test</cell></row><row><cell cols="2">Baseline (resimulated) 14.89?0.17 29.66?0.34</cell><cell>26.92?0.21</cell><cell>37.49?0.10</cell><cell>89.29?0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Sensitivity to different initialization of the various tasks. Training of the models for the baseline and best relaxed self-attention approach was repeated 5 times. Results are shown in terms of average and standard deviation values of the respective metrics.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Automatic</cell><cell></cell><cell>Machine</cell></row><row><cell>Task</cell><cell></cell><cell cols="3">Speech Recognition</cell><cell>Translation</cell></row><row><cell></cell><cell></cell><cell cols="2">(Librispeech)</cell><cell></cell><cell>(IWSLT14)</cell></row><row><cell>Setting</cell><cell></cell><cell cols="3">w/ LM 100 h training data</cell><cell>w/o LM,</cell></row><row><cell>Relaxation type (*)</cell><cell></cell><cell cols="2">cross attention</cell><cell></cell><cell>self-attention matched inference</cell></row><row><cell>Metric</cell><cell></cell><cell cols="2">WER (%) ?</cell><cell></cell><cell>BLEU ?</cell></row><row><cell></cell><cell>dev</cell><cell>dev</cell><cell>test</cell><cell>test</cell><cell></cell></row><row><cell>Data subset</cell><cell>clean</cell><cell>other</cell><cell>clean</cell><cell>other</cell><cell>test</cell></row><row><cell cols="5">Baseline (resimulated) 10.62 24.19 12.06 25.56</cell><cell>37.42</cell></row><row><cell>-attention dropout</cell><cell cols="4">11.02 25.24 11.89 26.88</cell><cell>37.51</cell></row><row><cell cols="2">+ relaxed (*) attention 9.33</cell><cell cols="3">22.16 10.62 23.04</cell><cell>37.67</cell></row><row><cell>-attention dropout</cell><cell>9.68</cell><cell cols="3">21.38 10.91 23.16</cell><cell>37.47</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">ASR training recipes at https://github.com/freewym/espresso</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Code available from https://github.com/dinghanshen/cutoff</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">ImageNet-1k pre-trained Swin transformer models and fine-tuning code downloaded from https://github.com/microsoft/Swin-Transformer .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research leading to these results has received funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) for project number 414091002, as well as from the Bundesministerium f?r Wirtschaft und Energie (BMWi) under funding code 01MK20011T.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Audio-Visual Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (Early Access)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00496</idno>
		<title level="m">LRS3-TED: A Large-Scale Dataset for Visual Speech Recognition</title>
		<imprint>
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ASR is All You Need: Cross-Modal Distillation for Lip Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page" from="2143" to="2147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-End Attention-Based Large Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection With Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MuST-C: A Multilingual Corpus for End-to-End Speech Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Di Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>St?ker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IWSLT</title>
		<meeting>of the IWSLT<address><addrLine>Lake Tahoe, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="page" from="2" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Listen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="4960" to="4964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focus on the Present: A Regularization Method for the ASR Source-Target Attention Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="5994" to="5998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SSST</title>
		<meeting>of SSST<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-Based Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lip Reading in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACCV</title>
		<meeting>of ACCV<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2021-05" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing Transformer Depth on Demand With Structured Dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Switch Transformers: Scaling to Trillion Parameter Models With Simple and Efficient Sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2" to="40" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On Using Monolingual Corpora in Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">CMT: Convolutional Neural Networks Meet Vision Transformers</title>
		<imprint>
			<date type="published" when="2021-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Networks With Stochastic Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV<address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thejas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sivathanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03977</idno>
		<title level="m">Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling Laws for Neural Language Models</title>
		<imprint>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Transformer-Based End-to-End Speech Recognition With Connectionist Temporal Classification and Language Model Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="1408" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<title level="m">SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing</title>
		<imprint>
			<date type="published" when="2018-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Asam: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="page" from="5905" to="5914" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Head Attention With Disagreement Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="2897" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">R-Drop: Regularized Dropout for Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021-12" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient Training of Visual Transformers With Small Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Nadai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021-12" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relaxed Attention: A Simple Method to Boost Performance of End-to-End Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lohrenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Cartagena, Colombia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-12" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-To-End Audio-Visual Speech Recognition With Conformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="7613" to="7617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Transducer for Audio-Visual Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Siohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Density Ratio Approach to Language Model Fusion in End-To-End Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Variani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Internal Language Model Adaptation With Text-Only Data for End-to-End Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05354</idno>
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
		<title level="m">When Does Label Smoothing Help</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR Corpus Based on Public Domain Audio Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP<address><addrLine>South Brisbane, QLD, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relational Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training Tips for the Transformer Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="43" to="70" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13818</idno>
		<title level="m">A Simple But Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2022-04" />
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cold Fusion: Training Seq2Seq Models Together With Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="387" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Combining Residual Networks With LSTMs for Lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10270</idno>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Alleviating the Inequality of Attention Heads for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09672</idno>
		<imprint>
			<date type="published" when="2020-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Residual Attention Network for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Honulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Espresso: A Fast End-to-End Neural Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pay Less Attention With Lightweight and Dynamic Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">UniDrop: A Simple Yet Effective Technique to Improve Transformer Without Extra Cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="3865" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Discriminative Multi-Modality Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="14421" to="14430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep Noise Suppression With Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech<address><addrLine>Brno, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08" />
			<biblScope unit="page" from="2806" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tokensto-Token ViT: Training Vision Transformers from Scratch on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E H</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCV</title>
		<meeting>of ICCV</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="538" to="547" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scheduled DropHead: A Regularization Method for Transformer Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rethinking Pre-Training and Self-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020-12" />
			<biblScope unit="page" from="3833" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
