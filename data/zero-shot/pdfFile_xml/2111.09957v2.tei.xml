<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethink Dilated Convolution for Real-time Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Gao</surname></persName>
							<email>roland.gao@mail.utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethink Dilated Convolution for Real-time Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in semantic segmentation generally adapt an ImageNet pretrained backbone with a special context module after it to quickly increase the field-of-view. Although successful, the backbone, in which most of the computation lies, does not have a large enough field-of-view to make the best decisions. Some recent advances tackle this problem by rapidly downsampling the resolution in the backbone while also having one or more parallel branches with higher resolutions. We take a different approach by designing a ResNeXt inspired block structure that uses two parallel 3 ? 3 convolutional layers with different dilation rates to increase the field-of-view while also preserving the local details. By repeating this block structure in the backbone, we do not need to append any special context module after it. In addition, we propose a lightweight decoder that restores local information better than common alternatives. To demonstrate the effectiveness of our approach, our model RegSeg achieves state-of-the-art results on real-time Cityscapes and CamVid datasets. Using a T4 GPU with mixed precision, RegSeg achieves 78.3 mIOU on Cityscapes test set at 30 FPS, and 80.9 mIOU on CamVid test set at 70 FPS, both without ImageNet pretraining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is the task of assigning a class to every pixel in the input image. Applications of it include autonomous driving, natural scene understanding, and robotics. It is also the groundwork for the bottom-up approach <ref type="bibr" target="#b6">[7]</ref> of panoptic segmentation, which, in addition to assigning a class to every pixel, separates instances of the same class.</p><p>Previous advances in semantic segmentation generally adapt ImageNet <ref type="bibr" target="#b9">[10]</ref> pretrained backbones and add a context module with large average poolings like PPM <ref type="bibr" target="#b49">[50]</ref> or large dilation rates like ASPP <ref type="bibr" target="#b3">[4]</ref> to quickly increase the field-of-view. They take advantage of the ImageNet pretrained weights for faster convergence and for higher accuracy on smaller datasets like PASCAL VOC 2012 <ref type="bibr" target="#b11">[12]</ref>,  where training from scratch may not be possible. Such approaches have two potential problems. ImageNet backbones usually have a large number of channels in the last few convolutional layers because they are meant to label images to one of the 1000 classes in ImageNet. For example, ResNet18 <ref type="bibr" target="#b15">[16]</ref> ends with 512 channels, and ResNet50 with 2048 channels. The authors of Mobilenetv3 <ref type="bibr" target="#b18">[19]</ref> find that halving the number of channels in the last convolutional layer does not reduce the accuracy when adapted for semantic segmentation, hinting at the channel-redundancy of Ima-geNet models. Second, ImageNet models are tuned to take input images with resolution around 224 ? 224, but the images in semantic segmentation are much larger. For example, Cityscapes <ref type="bibr" target="#b7">[8]</ref> has images with resolution 1024 ? 2048, and CamVid <ref type="bibr" target="#b0">[1]</ref> with 720 ? 960. ImageNet models lack the field-of-view to encode such large images.</p><p>These two problems have inspired us to design a backbone specifically made for semantic segmentation. We increase the field-of-view directly in the backbone by introducing a novel dilated block structure called the D block, and we keep the number of channels in the backbone low. We take inspiration from the ResNeXt <ref type="bibr" target="#b44">[45]</ref> block structure, which uses group convolution in the traditional ResNet block to improve its accuracy while maintaining similar run time complexity. RegNet <ref type="bibr" target="#b35">[36]</ref> takes the ResNeXt block and provides better baselines across a wide range of FLOP regimes. We adapt the fast RegNetY-600MF for semantic segmentation by swapping out the original Y block with our D block. In particular, when doing group conv, the D block uses one dilation rate for half of the groups and another dilation rate for the other half. By repeating the D block in our RegSeg's backbone, we can easily increase the field-ofview without losing the local details. RegSeg's backbone uses dilation rates as high as 14, and since it has enough field-of-view, we do not append any context modules such as ASPP or PPM.</p><p>Many recent works -such as Auto-DeepLab <ref type="bibr" target="#b29">[30]</ref>, dilated SpineNet <ref type="bibr" target="#b36">[37]</ref>, and DetectoRS <ref type="bibr" target="#b34">[35]</ref> -are hesitant to include dilated convolution with large dilation rates in their architecture design space and still rely on context modules such as ASPP or PPM to increase the field-of-view. We attribute this to the fact that dilated conv leaves holes in between the weights. We solve this problem by starting with small dilation rates and always setting the dilation rate to 1 in one of the branches of the D block. We hope that this work can inspire future researchers to try larger dilation rates in their models.</p><p>We also propose a lightweight decoder that effectively restores the local details lost in the backbone. Previous decoders such as the one in DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref> are too slow to run in real-time, and common lightweight alternatives such as LRASPP <ref type="bibr" target="#b18">[19]</ref> are not as effective. Our decoder is 1.0% better than LRASPP under the same training setting.</p><p>RegSeg runs in real-time. Using a T4 GPU with mixed precision, RegSeg runs at 30 FPS on Cityscapes and 70 FPS on CamVid. Many tasks require the model to run in real-time, such as autonomous driving or mobile deployment. Real-time models are more efficient than non-realtime models, and they have the potential to beat the stateof-the-art when scaled up to the same computational complexity. For example, EfficientNet <ref type="bibr" target="#b40">[41]</ref> previously achieved state-of-the-art results on ImageNet by scaling up a lowcompute model that they found using neural architecture search.</p><p>In summary, our contributions are:</p><p>? We propose a novel dilated block structure (D block) that can easily increase the field-of-view of the backbone while maintaining the local details. By repeating the D block in RegSeg's backbone, we can control the field-of-view without extra computation.</p><p>? We introduce a lightweight decoder that performs better than common alternatives.</p><p>? We conduct extensive experiments to show the effectiveness of our approach. RegSeg achieves 78.3 mIOU   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Network design</head><p>The models found on ImageNet play an important role in general network design, and their improvements often transfer to other domains such as semantic segmentation. RegNet <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref> finds many improvements to the ResNeXt <ref type="bibr" target="#b44">[45]</ref> architecture by using random search to run numerous experiments and analyzing trends to reduce the search space. They provide models across a wide range of flop regimes, and the models outperform EfficientNet <ref type="bibr" target="#b40">[41]</ref> under comparable training settings. EfficientNetV2 <ref type="bibr" target="#b41">[42]</ref> is the improved version of EfficientNet and trains faster by using regular convs instead of depthwise convs at the higher resolutions. In our paper, we take inspiration from RegNet by adapting their block structure for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic segmentation</head><p>Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref> are shown to beat traditional approaches in the task of segmentation. DeepLabv3 <ref type="bibr" target="#b3">[4]</ref> uses dilated conv in the ImageNet pretrained backbone to reduce the output stride to 16 or 8 instead of the usual 32, and increases the receptive field by proposing the Atrous Spatial Pyramid Pooling module (ASPP), which applies parallel branches of convolutional layers with different dilation rates. <ref type="figure" target="#fig_3">Fig. 2</ref> shows an example of dilated conv. PSPNet <ref type="bibr" target="#b49">[50]</ref> proposes the Pyramid Pooling Module (PPM), which applies parallel branches of convolutional layers with different input resolutions by first applying average poolings. In our paper, we propose the dilated block (D block) with a similar structure to ASPP and make it the building block of our backbone, instead of attaching one at the end. DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref> builds on top of DeepLabv3 by adding a simple decoder with two 3 ? 3 convs at output stride 4 to improve the segmentation quality around boundaries. HR-NetV2 <ref type="bibr" target="#b43">[44]</ref> keeps parallel branches with different resolutions right in the backbone, with the finest one at output stride 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Real-time semantic segmentation</head><p>MobilenetV3 uses the lightweight decoder LRASPP <ref type="bibr" target="#b18">[19]</ref> to adapt the fast ImageNet model for semantic segmentation. BiSeNetV1 <ref type="bibr" target="#b47">[48]</ref> and BiSeNetV2 <ref type="bibr" target="#b46">[47]</ref> have two branches in the backbone (Spatial Path and Context Path) and merge them at the end to achieve good accuracy and performance without ImageNet pretraining. SFNet <ref type="bibr" target="#b25">[26]</ref> proposes the Flow Alignment Module (FAM) to upsample low resolution features better than bilinear interpolation. STDC <ref type="bibr" target="#b12">[13]</ref> rethinks the BiSeNet architecture by removing the Spatial Path and designing a better backbone. HarD-Net <ref type="bibr" target="#b2">[3]</ref> reduces GPU memory traffic consumption by using mostly 3 ? 3 convs and barely any 1 ? 1 convs. DDRNet-23 <ref type="bibr" target="#b17">[18]</ref> uses two branches with multiple bilateral fusions between them and appends a new context module called the Deep Aggregation Pyramid Pooling Module (DAPPM) at the end of the backbone. DDRNet-23 is a concurrent work that has not been peer-reviewed. DDRNet-23 is currently the state of the art on real-time Cityscapes semantic segmentation, and we show that RegSeg outperforms DDRNet-23 when trained under the same training setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Field-of-view</head><p>We are interested in the field-of-view (FOV), also known as the receptive field, of our model gained through convolutions. For example, a composition of two 3 ? 3 convs is equal in kernel size and stride to a 5 ? 5 conv, and we simply say that the field-of-view is 5. More generally, the field-of-view of a composition of convs can be calculated iteratively as described in FCN <ref type="bibr" target="#b30">[31]</ref>. Suppose the composition of convs up to the current point is equal in kernel size and stride to one k ? k conv with stride s, and we compose it with a k ? k conv with stride s . We update k and s by</p><formula xml:id="formula_0">k ? k + (k ? 1) * s (1) s ? s * s<label>(2)</label></formula><p>The field-of-view is the final value of k.</p><p>There are two main ways to efficiently increase the fieldof-view. One is to downsample early on with stride 2 convs or average poolings. The other is to use dilated conv. A 3?3 conv with dilation rate r is equal in field-of-view to a conv with kernel size 2r + 1. However, to not leave any holes in between the weights, we need k/s ? r, where k and s are calculated using the composition of convs up to the current point, as described in the previous paragraph. This serves as an upper bound on r, and, in practice, we choose dilation rates much lower than the upper bounds.</p><p>The relationship between the field-of-view and the input image size highly influences the accuracy of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dilated block</head><p>Our dilated block (D block) takes inspiration from the Y block of RegNet <ref type="bibr" target="#b35">[36]</ref>, also known as the SE-ResNeXt block <ref type="bibr" target="#b19">[20]</ref>. The Y block and our new D block utilize group convolutions. Suppose input channels = output channels = w, which is always true for the 3 ? 3 convs in the Y block and the D block. A group conv has an attribute called the group width g, and g must divide w. During the forward pass, the input with w channels are split into w/g groups with g channels each, and a regular conv is applied to each group, and the outputs are concatenated together to form the w channels again.</p><p>Since there is a conv for each group, we can apply different dilation rates to different groups to extract multi-scale features. For example, we can apply dilation rate 1 to half of the groups, and dilation rate 10 to the other half. This is the key to our D block. <ref type="figure">Fig. 3a</ref> shows the Y block. <ref type="figure">Fig. 3b</ref> shows our D block. When d1 = d2 = 1, the D block is equivalent to the Y block. In Sec. 4.4, we experiment with some D blocks that have 4 branches of different dilation rates, but find that they are no better than D blocks that have 2 branches. <ref type="figure">Fig. 4</ref> shows the D block when stride = 2. Similar to the ResNet D-variant <ref type="bibr" target="#b16">[17]</ref>, we apply a 2 ? 2 average pooling on the shortcut branch when the block's stride = 2. BatchNorm <ref type="bibr" target="#b22">[23]</ref> and ReLU immediately follow each conv, except that the ReLUs right before the summation are replaced with one after the summation. We use an SE reduction ratio of 1/4.</p><p>Modern deep learning frameworks support group conv where each group applies the same dilation rate. Since the D block uses different dilation rates for different groups, we have to manually split the input, apply conv, and then concatenate. We use (d1, d2) to denote the dilation rates in a D block. In Tab. 1, we show that the manual split and concatenation and the use of dilated conv impact the speed. As a result, the Y block is slightly faster than the D block even though they have the same FLOP complexity and parameter count. When we use D block(1, 1) in practice, we do not have to manually split and concatenate since both branches use the same dilation rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backbone</head><p>Our backbone is built by repeating the D block, in a style similar to RegNet. The backbone starts with one 32-channel 3?3 conv with stride 2. Then it has one 48-channel D block at 1/4 resolution, three 128-channel D block at 1/8, thirteen 256-channel D block at 1/16, ending with one 320-channel  <ref type="table">Table 2</ref>. Backbone. #Channels is the number of output channels, and the number of input channels is inferred from the previous block. When stride = 2 and #repeat &gt; 1, the first block has stride 2 and the rest have stride 1. D block at 1/16. Group width g = 16 for all D blocks. We do not downsample to 1/32. We increase the dilation rates for the thirteen stride 1 blocks at 1/16: one (1, 1), one (1, 2), four <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref>, and seven <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b13">14)</ref>. As a shorthand, we denote the dilation rates as (1, 1) + (1, 2) + 4 * (1, 4) + 7 * <ref type="figure" target="#fig_1">(1, 14)</ref>. We use dilation rates (1, 1) for all the other blocks, making them equivalent to the Y block, except for the 2?2 avg pooling when stride = 2. In a format similar to EfficientNetV2 <ref type="bibr" target="#b41">[42]</ref>, we display the backbone of RegSeg in Tab. 2. Choosing the right dilation rates for the last thirteen blocks is nontrivial, and we experiment with the dilation rates in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Decoder</head><p>The decoder's job is to restore the local details lost in the backbone. Similar to DeepLabv3+ <ref type="bibr" target="#b4">[5]</ref>, we use [k ? k, c] to denote a k ? k conv with c output channels. We take the backbone's last 1/4, 1/8, and 1/16 feature maps as inputs. We apply a <ref type="bibr">[</ref>  <ref type="bibr" target="#b22">[23]</ref> and ReLU. The decoder is shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. This simple decoder performs better than many existing decoders that have similar latencies. We experiment with different decoder designs in Sec. 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Cityscapes <ref type="bibr" target="#b7">[8]</ref> is a large-scale dataset focused on street scene parsing. It contains 2975 images for training, 500 for validation, and 1525 for testing. We do not use the 20000 coarsely labeled images. There are 19 classes and ignore label = 255. The image size is 1024 ? 2048.</p><p>CamVid <ref type="bibr" target="#b0">[1]</ref> is another street scene dataset similar to Cityscapes. It contains 367 images for training, 101 for validation, and 233 for testing. Following previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48]</ref>, we use only 11 classes and set all other classes to the ignore label = 255. We train on the trainval set and evaluate on the test set. The image size is 720 ? 960.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Train setting</head><p>On Cityscapes, we use SGD with momentum = 0.9, initial learning rate = 0.05, weight decay = 0.0001, but we do not decay BatchNorm parameters. We use the poly learning rate scheduler that sets the current learning rate to the initial learning rate multiplied by (1? cur iter total iter ) 0.9 . We also apply a linear warmup <ref type="bibr" target="#b14">[15]</ref> from 0.1lr to lr for the first 3000 iterations. During training, we apply random horizontal flipping, random scaling of [400, 1600], and random cropping of 768 ? 768. We use a reduced set of RandAug <ref type="bibr" target="#b8">[9]</ref> operations (auto contrast, equalize, rotate, color, contrast, brightness, sharpness). For each image, we apply 2 random operations of magnitude 0.2 (out of 1). We also use class uniform sampling <ref type="bibr" target="#b50">[51]</ref> with class uniform percent = 0.5. We use cross-entropy loss and batch size = 8. RegSeg is trained from randomly initialized weights using PyTorch's <ref type="bibr" target="#b33">[34]</ref> default initialization. We train for 1000 epochs on a single T4 GPU. To speed up training without any loss in accuracy, we use mixed precision training. When submitting to the test server, we train on the trainval set and additionally use 1024 ? 1024 crop size and Online Hard Example Mining loss <ref type="bibr" target="#b38">[39]</ref> (OHEM). OHEM loss, also known as bootstrapped loss, averages the pixel losses that are over 0.3, or averages the top 1/16 pixel losses if the original proportion is less than 1/16.</p><p>When doing ablation studies, we train for only 500 epochs. To halve the number of CPUs required when doing ablation studies, we store the images at half resolution on  <ref type="table">Table 3</ref>. Reproducibility. The classes truck, bus, and train all vary a lot across runs while the other non-shown classes do not vary as much. By removing these three classes from the metric, we can achieve lower variation.</p><p>disk and resize them back to full resolution when loading unless specified otherwise. We store and load the images at full resolution when comparing against DDRNet-23 <ref type="bibr" target="#b17">[18]</ref> and when we submit to the test server. On CamVid, the training setting is similar to that in Cityscapes. Because we do not have ImageNet pretraining and CamVid is small, we use Cityscapes pretrained models. We use random horizontal flipping, random scaling of <ref type="bibr">[288,</ref><ref type="bibr">1152]</ref>, and random cropping of 720 ? 960 with batch size 12. We do not use RandAug or class uniform sampling. We train for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Reproducibility</head><p>To make our ablation studies possible, we need the results to be reproducible. We sort the training images by their filenames to prevent different orders caused by different file systems. Before randomly initializing the model weights, we set the random seed to 0. At the start of each epoch, we set the random seed to the current epoch. By doing so, we eliminate the problem of being in different states of the random number generator during training, caused by initializing different models or by resuming the model training after an incomplete training session. Furthermore, because random shuffling of the filenames happens at the start of each epoch, we can guarantee the same order of images even under different data augmentations.</p><p>Even after this careful reproducibility procedure, the variation on Cityscapes is still very larger, as shown Tab. 3, where we train the same model 4 times and measure the standard deviation. This is likely because of class imbalance and an out-of-distribution validation set. Class imbalance exists because some classes show up less and some classes have smaller instances. The validation set is out-ofdistribution because the images in the training set and validation set are taken from different cities. These problems could potentially be alleviated by pretraining on a larger dataset, but we focus on training from scratch. Unsatisfied by the large variation, we inspect the IOU of individual classes and find that a few classes (truck, bus, and train) vary a lot more than the other classes. We produce a new metric called reduced mIOU (mIOU R ), where we remove these</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Row Dilation rates</head><p>Field-of-view mIOU R  <ref type="bibr" target="#b18">[19]</ref> 74.85 SFNetDecoder <ref type="bibr" target="#b25">[26]</ref> 74.80 BiSeNetDecoder <ref type="bibr" target="#b47">[48]</ref> 74.68 <ref type="table">Table 5</ref>. Decoder Ablation Studies. Our best decoder performs better than common alternatives. three classes before taking the mean. As shown Tab. 3, its variation is much smaller, allowing our ablation studies to hold some weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Backbone ablation studies</head><p>In Tab. 4, we experiment with the dilation rates in the last 13 blocks and show their field-of-view while fixing everything else. We always keep d1 = 1 to preserve the local details. We see that (a) having 4 branches (row 3, 5, and 9) is not necessarily better than 2 branches, (b) dilation rates should be small early on (row 4 vs row 8), and (c) the best field-of-view is around 3800 (row 1 and 2 vs row 4, 6, and 7). Our best backbone is 2.6% better than the dilated RegNetY-600MF <ref type="bibr" target="#b35">[36]</ref> with output stride = 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Decoder ablation studies</head><p>In Tab. 5, we experiment with the decoder design while fixing the backbone architecture. We take the backbone's last 1/4, 1/8, and 1/16 feature maps as inputs. We always apply a [1 ? 1, 128] conv to 1/16 and bilinearly upsample it by a factor of 2. We apply a <ref type="bibr">[</ref> 3 ? 3 conv, or a Y block, all with output channels = 128, to further decode the features, before the final 1 ? 1 conv to 19 channels. All convs are immediately followed by Batch-Norm <ref type="bibr" target="#b22">[23]</ref> and ReLU, except the final conv to 19 channels. All these decoders have output stride = 8 except for our best decoder. We see that 3 ? 3 conv is 0.8% better than 1 ? 1 conv when using summation. Summation and concatenation are similar, and 3 ? 3 conv and the Y block are similar. The best decoder is the one described in Sec. 3.4, which additionally uses 1/4 features. Existing decoders <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref> perform much worse than our best decoder, potentially because they are designed for backbones that do not have a large field-of-view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Training technique ablation studies</head><p>In Tab. 6, we experiment with random resize and random crop hyperparameters. We find that the popular [512, 2048] random resize performs worse than the [400, 1600] resize, across many crop sizes. [400, 1600] is centered around 1000, while [512, 2048] is centered around 1280, so the [400, 1600] random resize is more aligned with the our validation size of 1024. This is in line with the recent Copy-Paste paper <ref type="bibr" target="#b13">[14]</ref>, where they use [0.1 * valsize, 2.0 * valsize] random resize on COCO <ref type="bibr" target="#b28">[29]</ref>, and the FixRes <ref type="bibr" target="#b42">[43]</ref>  where they fix the train-test discrepancy by using a larger test size or a smaller train size on ImageNet. In <ref type="figure">Fig. 6</ref>, we show that [400, 1600] performs better with smaller validation sizes, while [512, 2048] performs better with larger validation sizes.</p><p>In Tab. 7, we combine the best backbone and decoder that we have found in the previous ablation studies and experiment with more training settings. For the experiments in this table, we use images stored at full resolution instead of half resolution. We reconfirm that [400, 1600] resize is helpful. Training for 1000 epochs instead of 500 epochs and using OHEM loss give huge performance gains. We do not use 1024 ? 1024 random crop and OHEM loss except when we submit to the test server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Timing</head><p>We time RegSeg using a single T4 GPU with mixed precision. We use PyTorch 1.9 <ref type="bibr" target="#b33">[34]</ref>  The results show that RegSeg may generalize better than DDRNet-23. Note that two models' FPS are not directly comparable since they might have used different timing setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Comparison on Cityscapes</head><p>We compare against other real-time models, whether they are pretrained on ImageNet or not. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref> and <ref type="figure">Fig. 7</ref>, RegSeg achieves the best parameter-accuracy and flops-accuracy trade-offs. In Tab. 9, we show the accuracy and speed comparison on Cityscapes. Again, note that the FPS across models are not directly comparable. RegSeg outperforms HarDNet <ref type="bibr" target="#b2">[3]</ref>, which is the previous SOTA model without extra data, by 1.5%, and outperforms SFNet(DF2) <ref type="bibr" target="#b25">[26]</ref>, which has the best peer-reviewed results, by 0.5%. RegSeg also outperforms the popular BiSeNetV2- Although the concurrent work of DDRNet-23 <ref type="bibr" target="#b17">[18]</ref> achieves higher test set accuracy than our model, we argue that their success is due to their ImageNet pretraining, while also noting that RegSeg has much lower params and flops. To illustrate our point, we show that RegSeg outperforms DDRNet-23 when trained using the exact same training settings. In Tab. 10, we train each model 3 times and display the mean ? one standard deviation. Using the training settings explained in Sec. 4.2, RegSeg outperforms DDRNet-23 by 0.5% and has better parameters and flops efficiency. The FPS of both models are calculated using Sec. 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we are interested in increasing the fieldof-view of the backbone by using dilated convolution with large dilation rates, while aiming for real-time performance. We introduce the novel D block that can increase the fieldof-view without losing the local details, and experiment with the dilation rates in RegSeg's backbone. We also pro-pose a lightweight decoder that performs better than common alternatives. Together, RegSeg pushes the state-ofthe-art on real-time semantic segmentation datasets such as Cityscapes and CamVid, without ImageNet pretraining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Params vs mIOU on Cityscapes test set. Our model is in red, while other models are in blue. We achieve SOTA paramsaccuracy trade-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Dilated Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Y block and D block. When d1 = d2 = 1, the D block is the same as the Y block. D block when stride s = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For example, if we use the ResNet [16] architecture on Ima-geNet with a testing crop size of 224 ? 224 and look at the feature maps right before the global average pooling, the model needs a field-of-view of at least 224 * 2 ? 1 = 447 for the top-left pixel to see the entire image. Similarly, on Cityscapes with image size 1024 ? 2048, the model needs a field-of-view of 2047 for the top-left pixel of the output to see the bottom-left pixel of the input image, and a fieldof-view of 4095 to see the bottom-right pixel of the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Decoder. w shows the number of output channels. All convs except the final one are followed by BatchNorm<ref type="bibr" target="#b22">[23]</ref> and ReLU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Block Latency. The manual split and concatenation brings a 0.1 ms delay and the use of dilated conv brings another 0.1ms delay. For timing purposes, we use w = 256 and g = 16 for all blocks, and 1 ? 256 ? 64 ? 128 as input. Sec. 4.7 explains the timing setup in more detail.</figDesc><table><row><cell cols="2">Block structure ms</cell></row><row><cell>Y block</cell><cell>1.0</cell></row><row><cell>D block(1,1)</cell><cell>1.1</cell></row><row><cell>D block(1,4)</cell><cell>1.2</cell></row><row><cell>D block(1,10)</cell><cell>1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 ? 1, 128] conv to 1/16, a [1 ? 1, 128] conv to 1/8 and a [1 ? 1, 8] conv to 1/4. We upsample the 1/16, sum it with the 1/8, and apply a [3?3, 64] conv. We upsample again, concatenate with the 1/4, and apply a [3 ? 3, 64] conv, before the final [1 ? 1, 19] conv. All convs except the final one are followed by BatchNorm</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>truck bus train mIoU mIOU R 68.11 73.55 35.77 72.71 75.26 77.35 79.05 53.18 74.63 75.52 73.17 77.78 58.77 74.65 75.54 71.31 81.73 74.27 75.46 75.41</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Backbone Ablation Studies. Our best backbone has small dilation rates early on and large dilation rates later.</figDesc><table><row><cell>1</cell><cell cols="2">(1,1)+(1,2)+4*(1,4)+7*(1,14)</cell><cell>3807</cell><cell>75.85</cell></row><row><cell>2</cell><cell cols="2">(1,1)+(1,2)+(1,4)+(1,6)+(1,8)+(1,10)+7*(1,12)</cell><cell>3743</cell><cell>75.75</cell></row><row><cell>3</cell><cell cols="2">(1,1)+(1,2)+(1,4)+(1,6)+(1,8)+(1,10)+7*(1,3,6,12)</cell><cell>3743</cell><cell>75.69</cell></row><row><cell>4</cell><cell cols="2">(1,1)+(1,2)+(1,4)+(1,6)+(1,8)+8*(1,10)</cell><cell>3295</cell><cell>75.58</cell></row><row><cell>5</cell><cell cols="2">(1,1)+(1,2)+6*(1,4)+5*(1,6,12,18)</cell><cell>3807</cell><cell>75.54</cell></row><row><cell>6</cell><cell cols="2">(1,1)+(1,2)+(1,4)+10*(1,6)</cell><cell>2207</cell><cell>75.53</cell></row><row><cell>7</cell><cell cols="2">(1,1)+(1,2)+(1,4)+(1,6)+(1,8)+(1,10)+(1,12)+6*(1,14)</cell><cell>4127</cell><cell>75.45</cell></row><row><cell>8</cell><cell cols="2">5*(1,4)+8*(1,10)</cell><cell>3263</cell><cell>75.44</cell></row><row><cell>9</cell><cell cols="2">(1,1)+(1,2)+(1,4)+(1,6)+(1,8)+(1,10)+7*(1,4,8,12)</cell><cell>3743</cell><cell>75.17</cell></row><row><cell>10</cell><cell cols="2">Dilated RegNetY-600MF 8*(1,1)+3*(2,2)</cell><cell>607</cell><cell>73.25</cell></row><row><cell>Decoder</cell><cell></cell><cell>mIOU R</cell><cell></cell></row><row><cell cols="2">Sec. 3.4 decoder</cell><cell>75.84</cell><cell></cell></row><row><cell cols="2">sum+3x3 conv</cell><cell>75.75</cell><cell></cell></row><row><cell cols="2">concat+Y block</cell><cell>75.70</cell><cell></cell></row><row><cell cols="2">concat+3x3 conv</cell><cell>75.62</cell><cell></cell></row><row><cell cols="2">sum+1x1 conv</cell><cell>74.93</cell><cell></cell></row><row><cell>LRASPP</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1 ? 1, 128] conv to 1/8 if it will be summed with 1/16, or a [1 ? 1, 32] conv if it will be concatenated with 1/16. After 1/8 and 1/16 are either summed or concatenated, we can use a 1 ? 1 conv, a</figDesc><table><row><cell cols="3">Random Resize Random Crop mIOU R</cell></row><row><cell>[400,1600]</cell><cell>768x1536</cell><cell>76.25</cell></row><row><cell>[400,1600]</cell><cell>1024x1024</cell><cell>76.15</cell></row><row><cell>[400,1600]</cell><cell>512x1024</cell><cell>76.10</cell></row><row><cell>[400,1600]</cell><cell>768x768</cell><cell>75.82</cell></row><row><cell>[512,2048]</cell><cell>1024x1024</cell><cell>75.78</cell></row><row><cell>[512,2048]</cell><cell>768x768</cell><cell>75.17</cell></row><row><cell>[512,2048]</cell><cell>512x1024</cell><cell>75.03</cell></row><row><cell cols="3">Table 6. Random Resize and Random Crop. [400, 1600] random</cell></row><row><cell cols="3">resize performs better than [512, 2048], and 1024x1024 better than</cell></row><row><cell>other common crop settings.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>More training settings. We see that [400, 1600], OHEM, and longer training are helpful.</figDesc><table><row><cell>paper,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>with CUDA 10.2. The input size is 1 ? 3 ? 1024 ? 2048 on Cityscapes, and 1 ? 3 ? 720 ? 960 on CamVid. After 10 iterations of warm up, we average the model's time over the next 100 iterations. We set torch.backends.cudnn.benchmark=True and use torch.cuda.synchronize().4.8. Comparison on CamVidAs shown in Tab. 8, RegSeg achieves 80.9 mIOU on CamVid test set at 70 FPS. It outperforms the previous SOTA DDRNet-23 by 0.8%, and BiSeNetV2-L by 2.4%. GFLOPs vs mIOU on the Cityscapes test set. Our model is in red, while other models are in blue. We achieve SOTA flopsaccuracy trade-off.</figDesc><table><row><cell>mIOU</cell><cell>75 76 77 78 79</cell><cell></cell><cell cols="6">BiSeNetV2-L SwiftNetRN-18 STDC2-Seg75 MSFNet CABiNet FC-HarDNet-70 HyperSeg-M SFNet(DF2) DDRNet-23 RegSeg</cell></row><row><cell></cell><cell>74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>71 72 73</cell><cell>0</cell><cell>20 DFANet A FasterSeg 40 MobileNetV3</cell><cell>60</cell><cell>80 GFLOPs</cell><cell>100</cell><cell>120</cell><cell>140</cell></row><row><cell cols="4">Figure 7. Model</cell><cell cols="2">Extra data</cell><cell cols="2">mIOU</cell><cell>FPS</cell></row><row><cell cols="4">STDC2-Seg [13]</cell><cell></cell><cell>IM</cell><cell>73.9</cell><cell></cell><cell>152.2</cell></row><row><cell cols="4">GAS [28]</cell><cell></cell><cell>-</cell><cell>72.8</cell><cell></cell><cell>153.1</cell></row><row><cell cols="4">CAS [49]</cell><cell></cell><cell>-</cell><cell>71.2</cell><cell></cell><cell>169</cell></row><row><cell cols="4">SFNet(DF2) [26]</cell><cell></cell><cell>IM</cell><cell>70.4</cell><cell></cell><cell>134</cell></row><row><cell cols="4">SFNet(ResNet-18) [26]</cell><cell></cell><cell>IM</cell><cell>73.8</cell><cell></cell><cell>36</cell></row><row><cell cols="4">MSFNet [40]</cell><cell></cell><cell>IM</cell><cell>75.4</cell><cell></cell><cell>91</cell></row><row><cell cols="4">HyperSeg-S [32]</cell><cell></cell><cell>IM</cell><cell>78.4</cell><cell></cell><cell>38.0</cell></row><row><cell cols="4">TD4-PSP18 [21]</cell><cell></cell><cell>IM</cell><cell>72.6</cell><cell></cell><cell>25.0</cell></row><row><cell cols="4">VideoGCRF [2]</cell><cell></cell><cell>C</cell><cell>75.2</cell><cell></cell><cell>-</cell></row><row><cell cols="4">BiSeNetV2 [47]</cell><cell></cell><cell>C</cell><cell>76.7</cell><cell></cell><cell>124</cell></row><row><cell cols="4">BiSeNetV2-L [47]</cell><cell></cell><cell>C</cell><cell>78.5</cell><cell></cell><cell>33</cell></row><row><cell cols="4">CCNet3D [22]</cell><cell></cell><cell>C</cell><cell>79.1</cell><cell></cell><cell>-</cell></row><row><cell cols="4">DDRNet-23 [18]</cell><cell></cell><cell>C</cell><cell cols="2">80.1?0.4</cell><cell>94</cell></row><row><cell cols="4">RegSeg</cell><cell></cell><cell>C</cell><cell cols="2">80.9?0.07</cell><cell>70</cell></row><row><cell cols="9">Table 8. Accuracy and speed comparison on CamVid, IM: Ima-</cell></row><row><cell cols="4">geNet, C: Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Comparison against DDRNet-23 under the exact same training setting. FPS of both models are calculated using Sec. 4.7. RegSeg achieves higher accuracy and better parameters and flops efficiency, while running at similar speeds. L [47] by 3.0%, and MobileNetV3+LRASPP [19] by 5.7%.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="3">Extra data</cell><cell>val mIOU</cell><cell cols="2">test mIOU FPS</cell><cell>Resolution Params (M) GFLOPs</cell></row><row><cell cols="2">CAS [49]</cell><cell></cell><cell>IM</cell><cell></cell><cell>71.6</cell><cell>70.5</cell><cell>108</cell><cell>768x1536</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">DFANet A [24]</cell><cell>IM</cell><cell></cell><cell>-</cell><cell>71.3</cell><cell>100</cell><cell>1024x1024</cell><cell>7.8</cell><cell>3.4</cell></row><row><cell cols="2">FasterSeg [6]</cell><cell></cell><cell>None</cell><cell></cell><cell>73.1</cell><cell>71.5</cell><cell cols="2">163.9 1024x2048</cell><cell>4.4</cell><cell>28.2</cell></row><row><cell cols="2">GAS [28]</cell><cell></cell><cell>IM</cell><cell></cell><cell>72.4</cell><cell>71.8</cell><cell cols="2">108.4 769x1537</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">MobileNetV3 [19]</cell><cell>None</cell><cell></cell><cell>72.36</cell><cell>72.6</cell><cell>-</cell><cell>1024x2048</cell><cell>1.51</cell><cell>9.74</cell></row><row><cell cols="2">HMSeg [25]</cell><cell></cell><cell>None</cell><cell></cell><cell>-</cell><cell>74.3</cell><cell>83.2</cell><cell>768x1536</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">TD4-Bise18 [21]</cell><cell>IM</cell><cell></cell><cell>75.0</cell><cell>74.9</cell><cell cols="2">47.6 1024x2048</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">BiSeNetV2-L [47]</cell><cell>None</cell><cell></cell><cell>75.8</cell><cell>75.3</cell><cell>47.3</cell><cell>512x1024</cell><cell>4.59</cell><cell>139</cell></row><row><cell cols="3">DF2-Seg2 [27]</cell><cell>IM</cell><cell></cell><cell>76.9</cell><cell>75.3</cell><cell cols="2">56.3 1024x2048</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">SwiftNetRN-18 [33]</cell><cell>IM</cell><cell></cell><cell>-</cell><cell>75.5</cell><cell cols="2">39.9 1024x2048</cell><cell>11.8</cell><cell>104</cell></row><row><cell cols="3">HyperSeg-M [32]</cell><cell>IM</cell><cell></cell><cell>76.2</cell><cell>75.8</cell><cell>36.9</cell><cell>512x1024</cell><cell>10.3</cell><cell>8.4</cell></row><row><cell cols="3">CABiNet [46]</cell><cell>IM</cell><cell></cell><cell>76.6</cell><cell>75.9</cell><cell cols="2">76.5 1024x2048</cell><cell>2.64</cell><cell>12</cell></row><row><cell cols="3">FC-HarDNet-70 [3]</cell><cell>None</cell><cell></cell><cell>77.7</cell><cell>76.0</cell><cell>53</cell><cell>1024x2048</cell><cell>4.12</cell><cell>35.6</cell></row><row><cell cols="3">STDC2-Seg75 [13]</cell><cell>IM</cell><cell></cell><cell>77.0</cell><cell>76.8</cell><cell>97.0</cell><cell>768x1536</cell><cell>16.1</cell><cell>54.9</cell></row><row><cell cols="2">MSFNet [40]</cell><cell></cell><cell>IM</cell><cell></cell><cell>-</cell><cell>77.1</cell><cell>41</cell><cell>1024x2048</cell><cell>-</cell><cell>96.8</cell></row><row><cell cols="3">SFNet(DF2) [26]</cell><cell>IM</cell><cell></cell><cell>-</cell><cell>77.8</cell><cell>53</cell><cell>1024x2048</cell><cell>17.9</cell><cell>80.4</cell></row><row><cell cols="3">DDRNet-23 [18]</cell><cell>IM</cell><cell></cell><cell>79.1?0.3</cell><cell>79.4</cell><cell cols="2">37.1 1024x2048</cell><cell>20.1</cell><cell>143.1</cell></row><row><cell cols="2">RegSeg</cell><cell></cell><cell>None</cell><cell></cell><cell>78.13?0.48</cell><cell>78.3</cell><cell>30</cell><cell>1024x2048</cell><cell>3.34</cell><cell>39.1</cell></row><row><cell></cell><cell></cell><cell cols="7">Table 9. Accuracy and speed comparison on Cityscapes, IM: ImageNet</cell></row><row><cell>Model</cell><cell>FPS</cell><cell>val mIOU</cell><cell cols="3">Params (M) GFLOPs</cell><cell></cell><cell></cell></row><row><cell>DDRNet-23</cell><cell>30</cell><cell cols="2">77.59 ? 0.09</cell><cell>20.1</cell><cell>143.1</cell><cell></cell><cell></cell></row><row><cell>RegSeg</cell><cell>30</cell><cell cols="2">78.13 ? 0.48</cell><cell>3.34</cell><cell>39.1</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">and Iasonas Kokkinos. Deep spatio-temporal random fields for efficient video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8915" to="8924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yang</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn-Long</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fasterseg: Searching for faster real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking bisenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9716" to="9725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep dual-resolution networks for real-time and accurate semantic segmentation of road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanduo</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06085</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Temporally distributed networks for fast video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="8818" to="8827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dfanet: Deep feature aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">When humans meet machines: Towards efficient segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="775" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Partial order pruning: for best speed/accuracy trade-off in neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9145" to="9153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph-guided architecture search for realtime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperseg: Patchwise hypernetwork for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4061" to="4070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10213" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Dilated spinenet for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12270</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Real-time semantic segmentation via multiply spatial fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07217</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficientnetv2: Smaller models and faster training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep high-resolution representation learning for visual recognition. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with context aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumya</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Kumaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="124" to="134" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Customizable architecture search for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11641" to="11650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
