<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEPSTRUCT: Pretraining of Language Models for Structure Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
							<email>chenguangwang@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu ? ?</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zui</forename><surname>Chen ?</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Haoyun</roleName><forename type="first">Hong</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
							<email>dawnsong@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DEEPSTRUCT: Pretraining of Language Models for Structure Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models on a collection of task-agnostic corpora to generate structures from text. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. 1 -Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In NAACL-HLT, pages 3554-3565.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models (LMs) have revolutionized NLP over the last few years <ref type="bibr" target="#b1">(Peters et al., 2018;</ref><ref type="bibr">Devlin et al., 2019;</ref><ref type="bibr" target="#b6">Radford et al., 2019b)</ref>, increasingly adept in performing the flexible and task-agnostic downstream transfer. Their transfer performance is less studied in structure prediction tasks, however. Well-studied tasks mainly focus on understanding one particular aspect of the text, such as predicting the next word that comes after as in language modeling. Unlike those downstream tasks, structure prediction requires the structural understanding of the text for further integrating multiple relevant aspects into a structure. For instance, a typical structure prediction task, called open information extraction, seeks the entire structural in- ? Equal contribution. <ref type="bibr">1</ref> The code and datasets are available at https:// github.com/cgraywang/deepstruct. ...... <ref type="figure">Figure 1</ref>: Summary of our approach and results. Upper: an overview of DEEPSTRUCT and the proposed structure pretraining. Lower: performance of our 10B DEEPSTRUCT zero-shot and multi-task, compared with 175B GPT-3 zero-shot.</p><p>formation in a sentence ( <ref type="figure" target="#fig_2">Figure 2</ref>). Different from traditional NLP tasks, structure prediction takes one step further and serves as a natural testbed for the structural understanding competence of LMs. It is non-trivial to transfer LMs to downstream structure prediction tasks. While the structure prediction requires structural understanding, the LMs are pretrained to understand an independent aspect. For example, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> is trained to predict the next word, and <ref type="bibr">BERT (Devlin et al., 2019)</ref> is trained to recover the masked tokens. Recent work has made efforts in bridging the gap in transferring pretrained models to structure prediction tasks with a focus on two directions. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, first, task-specific architectures are proposed to model the structures for different structure prediction tasks <ref type="bibr">(Stanovsky et al., 2018;</ref><ref type="bibr">Soares et al., 2019)</ref>. Second, task-specific data augmentation <ref type="bibr">(Paolini et al., 2021;</ref><ref type="bibr" target="#b18">Wang et al., 2021;</ref><ref type="bibr">Wei et al., 2021)</ref> is introduced, aiming to enrich text format with structure information. These approaches involve custom-designed task augmentations, impeding their usability in general structure prediction tasks.</p><p>(1) <ref type="bibr">Born in 1951</ref>  Predicts an independent aspect (e.g., word(s) and label(s)) Predicts a structure that integrates multiple relevant aspects In this paper, we improve the structural understanding capabilities of LMs. In contrast to previous approaches relying on task augmentations, we introduce structure pretraining, which systematically teaches LMs to better understand structures of text beyond independent aspects in a pretraining phase <ref type="figure">(Figure 1</ref>). This enables the zero-shot transfer of knowledge that LMs learned about structures during our pretraining to downstream structure prediction tasks. For example, our zero-shot 10B parameter LM significantly outperforms the zero-shot GPT-3 (175B) on a structure prediction benchmark dataset ( <ref type="figure">Figure 1</ref>). We accomplish this by reformulating structure prediction as a series of unit taskstriple prediction tasks. We then train LMs on a collection of task-agnostic structural corpora to generate triples from text. The design of triple representation is important: it unifies a wide set of standard structure prediction tasks into the same task format. We apply our pretrained model DEEPSTRUCT to 28 datasets spanning 10 structure prediction tasks, including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with multiple downstream structure prediction training sets and obtain state-of-the-art performance on 21 of 28 datasets. Our contributions are as follows:</p><p>? We improve structural understanding abilities of pretrained LMs. Compared to traditional NLP tasks that only consider the understanding of an independent aspect of the text, structural understanding takes a step further that requires the ability to integrate multiple relevant aspects into a structure. We argue that it is important for LMs to go beyond traditional understanding toward structural understanding, as it requires a higher level of intelligent competence and is more challenging. It can also benefit a wide spectrum of NLP tasks that require structure-level understanding capability.</p><p>? We propose structure pretraining, which pretrains the LMs to understand structures in the text. The basic intuition is that the standard pretraining helps LMs to understand individual aspects of the information in the text, our method learns to integrate those individual aspects into structures. Compared to existing approaches, this method enables the zero-shot transfer of LMs to structure prediction tasks. For instance, our 10B LM produces superior zero-shot performance compared to 175B GPT-3 on a representative structure prediction task.</p><p>? We further equip our pretraining with multi-task learning and apply our method to 28 structure prediction datasets across 10 tasks. We achieve state-of-the-art performance on 21 of 28 datasets that we evaluate. We hope this can help facilitate the structural understanding research in the NLP community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structure Pretraining</head><p>Pretrained LM</p><p>(1) Task augmented pretrain-finetune Task augmentation Finetune on task 0</p><p>Inference on task 0</p><p>Pretrained LM Structure-pretrain on task-agnostic tasks Inference on task 0,1,2,3,...</p><p>(2) Structure pretraining (ours)</p><p>Requires task-specific architectures or data augmentation Requires task-specific examples A task-specific model for each task LM learns to generate structural triples from text Inference on multiple tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero-shot</head><p>Pretrained LM Structure-pretrain on task-agnostic tasks and multiple tasks: 0,1,2,3,... Inference on task 0,1,2,3,...</p><p>LM learns to generate structural triples from text with additional task-specific examples</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task</head><p>Inference on multiple tasks The goal of our method is to improve the structural understanding capabilities of language models (LMs), i.e., understanding the structures of text. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, instead of using the standard pretrain-finetune paradigm for each task, we introduce structure pretraining that aims to teach LMs to correspond to structures in a wide spectrum of tasks at the same time. We evaluate the structural understanding ability on multiple structure prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Pretraining</head><p>While the LM is pretrained to understand a single aspect of the text, structural understanding aims to recover the entire structure of the text <ref type="figure" target="#fig_2">(Figure 2</ref>). Structure pretraining is designed to bridge the gap via guiding LMs to produce structures from the text. It is ideal to generate arbitrary structures as needed. However, this is infeasible due to the highly complex nature of such structures.</p><p>As an alternative, we reformulate the structure prediction as a combination of triple generation tasks. We refer to a triple as (head entity; relation; tail entity) describing relations between entities. We design three pretraining tasks with a focus on predicting the entities, relations, and triples respectively. As shown in <ref type="figure">Figure 1</ref>, (i) entity prediction aims to output triples regarding the entities and their types in an input sentence. We implement this via prepending "entity:" as a prefix in the input. (ii) Relation prediction aims to recover the relations and corresponding types in the input as a triple. Similarly, we add "relation:" including a task separator ":" to each input. (iii) Triple prediction outputs the entire triple structure from the input. We attach "triple:" to indicate this task. These pretraining tasks are task-agnostic to downstream tasks, enabling the zero-shot downstream transfer (Sec. 2.3).</p><p>Although the triple formulation is straightforward, we find that it is very flexible and able to model all structure prediction tasks we consider. A structure prediction task can be generally decomposed into generating the entities, relations, or triples. For example, named entity recognition predicts the entities and their types. It can be naturally represented as an entity prediction problem. Besides, traditional structure prediction tasks focusing on relations (e.g., relation classification) or triples (e.g., open information extraction) can be formulated as relation or triple prediction tasks re-   spectively. A summary of all downstream tasks is described in Sec. 2.2. We frame the pretraining as a conditional generation task where the input corresponds to text x, and the output y is a sequence of triples. Our pretraining can be expressed as estimating a conditional distribution p(y|x) in a probabilistic framework. We use an autoregressive LM to model p(y|x).</p><p>Pretraining Data We train the model on a collection of task-agnostic corpora including prebuilt large-scale alignments between text and triples. In particular, we use T-REx <ref type="bibr">(ElSahar et al., 2018)</ref>, <ref type="bibr">TEKGEN and</ref><ref type="bibr">KELM (Agarwal et al., 2021), WebNLG (Gardent et al., 2017)</ref>, <ref type="bibr">Concept-Net (Speer and Havasi, 2012)</ref>. These corpora align text to triples consisting of high-quality entities and relations in knowledge graphs (e.g., Wikidata), which are used for entity and relation prediction tasks. In addition, for triple prediction tasks, we use OPIEC <ref type="bibr">(Gashteovski et al., 2019</ref>) that provides open schema triples. The pretraining data statistics and the corresponding pretraining tasks are shown in <ref type="table" target="#tab_1">Table 1</ref>. Appendix A.1 shows additional details of our pretraining data. <ref type="figure" target="#fig_4">Figure 4</ref> shows an example of the training procedure for the entity prediction task based on the input and output sample below.</p><p>Input entity: Iago is born in 1951 Output (Iago; instance of; person)</p><p>where the input text and output triple are aligned, and the alignment is provided by our pretraining data. Tokens are predicted autoregressively starting with &lt;s&gt; token and ending with &lt;e&gt; token. The head entity (i.e., Iago) and the tail entity (i.e.,  <ref type="figure">Figure 5</ref>: Summary of tasks and datasets. Blue: entity prediction task; Red: relation prediction task; Purple: entity and relation prediction task; Yellow: triple prediction task. person) of the output triple then serve as the predictions (i.e., entity mention and entity type) of named entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open information extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tasks</head><p>It is resource-intensive to create large-scale structural understanding datasets from scratch. Therefore, we collect existing datasets in the field of structure prediction for evaluation. We consider 28 datasets spanning 10 structure prediction tasks as shown in <ref type="figure">Figure 5</ref>. Detailed descriptions and sizes of datasets are shown in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Zero-Shot</head><p>The zero-shot DEEPSTRUCT refers to the setting where the pretrained model is used without any task-specific training at inference time. This differs from prior fully supervised methods. This setting is challenging as it might be difficult for humans to understand the tasks without prior examples. For example, if we are asked about "semantic role labeling" that aims to recover the predicate-argument structure, it is hard to understand what this really means. Nevertheless, the existing zero-shot setting resonates with human behaviors. For example, for named entity recognition, a human can understand and follow the instruction.</p><p>We enable the zero-shot transfer to structure prediction tasks via converting the downstream tasks to one or a combination of the pretraining tasks. As shown in <ref type="figure">Figure 5</ref>, at inference time, seven structure prediction tasks are formulated as entity prediction with the prefix "entity:" attached to the input example (in blue), while one task is cast as relation prediction with the prefix "relation:" (in red). In addition, open information extraction is a triple prediction task with the prefix "triple:" (in yellow). Joint entity and relation extraction (JER) is a combination of entity and relation prediction (in purple). For the entity and relation prediction, we use the prefix "entity:" and "relation:" respectively. Besides, for each dataset, we build a schema alignment between the pretraining dataset and downstream dataset (details are described in Sec. 5). The output triples are then decoded as corresponding structure predictions based on the pre-built schema alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-Task</head><p>However, the distribution of the pretraining data is not perfectly aligned with the distribution of downstream datasets. This results in a shift in the output distribution of the pretrained model. The zero-shot setting cannot perform at its best on several out-ofdistribution tasks including dialogue state tracking. The reason is that its desired output is a dialogue state, which is lacking in our task-agnostic pretraining corpora. To mitigate this, we integrate multiple structure prediction datasets into the pretraining corpora, and train our method on the mixture of the datasets. We list an example input and output format for each task in <ref type="figure">Figure 5</ref>. For all datasets of a particular task, we adopt the same input and output format. We also add task name and dataset name followed by the separator ":" as a prefix to each input example. For example, we add "jer ade:" to indicate one of the JER datasets, ADE. More examples of each task and dataset are shown in <ref type="table" target="#tab_1">Table 16</ref>. In contrast to fully pretrain-finetuned models that store a copy of parameters for each task, this setting is a lightweight alternative and produces a single model for all tasks, improving parameter sharing.</p><p>After multi-task training, we further finetune our method on the task-specific dataset for each task. The intuition is that finetuning is the de facto way to leverage pretrained LMs to perform downstream tasks. We aim to test an upper bound of the transfer performance of our structure pretraining via the additional finetuning phase. For both multi-task settings, we use the same data format with the training at test time. Basically, we add the task name and dataset name followed by the separator to the input example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we show that DEEPSTRUCT successfully transfers to the structure prediction tasks considered and obtain state-of-the-art results on 21 of 28 datasets we evaluate. All results are obtained via structure pretraining a pretrained 10B parameter LM, <ref type="bibr">GLM (Du et al., 2021)</ref>. The details of the experimental setup, datasets, and comparison methods are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Results</head><p>We have two settings as described in Sec. 2: zeroshot and multi-task. We also finetune the multi-task version on each downstream dataset. In total, we have three versions of DEEPSTRUCT. For comparison: we report the performance of TANL <ref type="bibr">(Paolini et al., 2021)</ref> when available. We also show the best performance among the task-specific models that are described in Appendix A. <ref type="table" target="#tab_4">Table 2</ref> reports the results.</p><p>With the zero-shot setting, a single model is used to perform on multiple tasks without the need of any task-specific training. This is in contrast to previous approaches that rely on task-specific models and datasets for each task. In <ref type="table">Table 3</ref>, we also report the zero-shot performance of GPT-3 175B (Brown et al., 2020) on CoNLL04 and ADE (JER) via formulating the task as a question answering problem through prompting (details of the formulation are described in Sec. 5). JER requires the model to extract a set of entities and a set of relations between pairs of entities from the input text. Each predicted entity or relation has to be also assigned to an entity or a relation type. Zero-shot DEEPSTRUCT 10B outperforms zero-shot GPT-3 175B by a large margin without any prompt engineering. As shown in <ref type="table" target="#tab_4">Table 2</ref>, overall, DEEPSTRUCT's zero-shot performance is still far from that of task-specific supervised models on most tasks. The only exception is that the zero-shot setting obtains the new state-of-the-art performance on the factual probe with averaging 20% P@1 improvement. This is because the taskspecific method is also zero-shot. Note that we have removed the overlapped sentences with the T-REx test sets (factual probe) from our pretraining data. The result indicates that the structure pretraining is able to adapt the LM knowledge to the tasks by making LM aware of symbolic knowledge in the pretraining corpora. Besides, the zeroshot approach generalizes well to all task-agnostic pretraining tasks including entity prediction (e.g., named entity recognition), relation prediction (e.g., relation classification), and triple prediction (e.g., open information extraction).</p><p>Similar to the zero-shot setup, we only train a single model to conduct all the downstream tasks under the multi-task setting. This is different from the supervised models that use task-specific models and parameters. We achieve state-of-the-art performance on three datasets. For the other datasets, we obtain a competitive performance within a few points of the best-compared methods. Notably, most structure prediction tasks show a large gain from zero-shot to multi-task. This suggests that most tasks are out-of-distribution of our structure pretrained model. Nevertheless, our method appears to be able to adapt to the downstream distributions, presenting a fair and strong performance with multi-task learning. Another explanation is that multi-task examples help the model better understand the downstream tasks, such as the output format of each task. We also observe strong multi-task performance on FewRel, which is a low-resource structure prediction benchmark. This suggests that the multi-task setting is beneficial in low-resource regimes via transferring knowledge from similar tasks. For our multi-task training, we leave out the ACE2005 named entity recognition dataset due to the overlap between train and test splits for different tasks. After finetuning, we obtain state-of-theart performance on 21 datasets. For instance, we obtain a +8.0 absolute improvement and a +2.9 absolute improvement on CoNLL05 Brown (semantic role labeling) and TACRED (relation classification) compared to the state-of-the-art methods.</p><p>All above results are based on a pretrained 10B parameter LM, GLM. GLM is an autoregressive LM. In addition, the context x is encoded by a   <ref type="table">Table 3</ref>: Compare DEEPSTRUCT to GPT-3 (Brown et al., 2020) 175B zero-shot on CoNLL04 and ADE datasets (joint entity and relation extraction). Ent. and Rel. denote entity F1 and relation F1 respectively. bidirectional encoder. In principle, generative LMs, such as T5 <ref type="bibr" target="#b7">(Raffel et al., 2019)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, can also be used with the proposed structure pretraining for the structure prediction tasks as well. We leave this as one of the future investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Studies</head><p>Pretraining Strategies As the key question of our work is to investigate how structure pretraining improves the structural understanding ability of LMs, we examine how different pretraining strategies impact the downstream performance. We evaluate the below settings on the CoNLL04 (JER). The first two settings examine the relative importance of the pretraining data: (i) With example-proportional mixing: We follow <ref type="bibr" target="#b7">(Raffel et al., 2019)</ref> with a mixing rate maximum of 10K to balance the different sizes of datasets. All other components are kept the same with DEEPSTRUCT multi-task with finetuning. (ii) With entity and relation augmentation: We add special tokens "[]" to indicate the positions of the entities and relations in a sentence. Additional details are shown in Appendix A.5. (iii) No pretrain, finetune: We remove structure pretraining, and only finetune the LM on CoNLL04. (iv) Zero-shot: We only use the task-agnostic datasets and exclude the multi-task datasets in the pretraining. (v) Multi-task: We use the multi-task model without finetuning. (iv) and (v) are the same with the zero-shot and multi-task settings in Sec. 2. (vi) Finetune: The multiple downstream datasets are excluded in the structure pretraining, but the model is finetuned on CoNLL04. <ref type="table" target="#tab_6">Table 4</ref> shows the results. First, the distribution of pretraining data does not significantly shift from that of most tasks. This limits the impact of the balanced strategy (i). The data augmentation (ii) does not bring additional benefits to the downstream performance. This confirms that the key to the success of structure prediction is our formulation that narrows down a complex structure to a set of triple prediction tasks. This allows the pretraining to capture the entities and relations that are important for tasks. Second, removing the structure pretraining (iii) provides the most direct ablation of how much structure pretraining helps. Structure pretraining significantly improves the LM in structure prediction. This is due to the gap between LM pretraining and downstream structural understanding. For example, the distribution of structure prediction datasets is different from or is considered as out-of-distribution for the pretraining data. Structure pretraining improves the adaptation to those datasets. Next, similar to the findings in <ref type="table" target="#tab_4">Table 2</ref>, we find that both task-agnostic training sets (iv) and multi-tasks datasets (v) contribute to the strength of structure pretraining. In particular, finetuning is still very important to improve the downstream performance <ref type="bibr">(IV et al., 2021)</ref>. However, it produces a task-specific model for each dataset instead of a unified model for all tasks as in our zero-shot or multitask setup. Compared to only finetuning the model on a downstream dataset (vi), the multi-task setting obtains sizable improvements. This is because if the downstream dataset sizes are small such as of CoNLL04, multi-task learning can be extremely helpful in the low-resource regimes <ref type="bibr">(Paolini et al., 2021)</ref>. We conduct the above ablation studies using a base version of DEEPSTRUCT with 220M parameters.</p><p>Scaling Laws As it is often the case that larger models substantially improve the transferring capabilities of LMs <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr">Wei et al., 2021)</ref>, we explore how model scaling benefits the structure pretraining. We evaluate the effect on models with 110M, 220M, 2B, 10B parameters on  JER datasets with multi-task and multi-task finetuned DEEPSTRUCT ( <ref type="figure" target="#fig_5">Figure 6</ref>). As expected, average performance across the datasets improves as models grow larger. We find that when the models reach the order of 10B parameters, structure pretraining obtains the best performance. The 10B parameter model significantly improves the results compared to the 110M parameter model. One reason is that for small-scale models, learning across 28 structure prediction datasets during the structure pretraining may exceed the model capacity. For larger models, structure pretraining fully utilizes the model capacity and also teaches the models to generate triples according to the downstream tasks, allowing them to generalize well to most tasks with the rest capacity. It is also interesting that the performance does not seem to significantly saturate, indicating that the performance may further improve with larger-scale models. Under both setups, we observe similar trends. We also see that the model size matters more to the multi-task setting than to the finetuned version, suggesting finetuning is able to help specifically adapt to a task given a limited model size. The main pitfall is its generalization to more tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Pretrained LMs <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr" target="#b6">Radford et al., 2019b;</ref><ref type="bibr" target="#b26">Yang et al., 2019)</ref> are the key ingredients in contemporary NLP. Sequence-to-sequence (seq2seq) LMs target conditional generation, such as T5 <ref type="bibr" target="#b7">(Raffel et al., 2019)</ref>, <ref type="bibr">BART (Lewis et al., 2020)</ref> and <ref type="bibr">GLM (Du et al., 2021)</ref>. These models have benefited a wide range of nature language generation tasks such as summarization  and text infilling <ref type="bibr">(Zhu et al., 2019;</ref><ref type="bibr" target="#b17">Shen et al., 2020)</ref>. Recent attempts of generative prediction <ref type="bibr">(Paolini et al., 2021;</ref><ref type="bibr" target="#b16">Schick and Sch?tze, 2021;</ref><ref type="bibr">Lester et al., 2021)</ref> have found that seq2seq models are able to provide a unified solution for modeling a wide set of NLP tasks. While existing approaches focus on text-to-text generation, DEEP-STRUCT aims to perform text-to-triple generation.</p><p>Multi-task learning <ref type="bibr">(Caruana, 1997)</ref> aims to train a model for multiple tasks simultaneously. In deep learning, it is usually categorized into hard weight sharing and soft weight constraints <ref type="bibr" target="#b11">(Ruder, 2017)</ref>. In the context of NLP, weight sharing has been adopted in <ref type="bibr">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b27">Yang et al., 2016;</ref>. Since the emergence of large pretrained LMs <ref type="bibr" target="#b5">(Radford et al., 2019a;</ref><ref type="bibr">Devlin et al., 2019;</ref><ref type="bibr" target="#b26">Yang et al., 2019)</ref>, multi-task training has been shown effective to enhance LMs' transferability to downstream tasks <ref type="bibr" target="#b7">(Raffel et al., 2019)</ref>. Recent studies <ref type="bibr">(Wei et al., 2021)</ref> also show that pretrained models finetuned with abundant downstream tasks can conduct effective zero-shot learning. The main difference is that DEEPSTRUCT trains across multiple structure prediction datasets in structure pretraining with task-agnostic corpora, where we cast all datasets into triple formats.</p><p>Structure prediction is a long-standing challenge that relates to many NLP applications such as open information extraction (Gashteovski et al., 2019), named entity recognition <ref type="bibr" target="#b14">(Sang and Meulder, 2003;</ref><ref type="bibr" target="#b21">Weischedel et al., 2013)</ref>, and relation classification <ref type="bibr">(Zhang et al., 2017;</ref><ref type="bibr">Han et al., 2018;</ref><ref type="bibr">Gao et al., 2019)</ref>. To handle different structure prediction problems, prior work presents a variety of task-specific models in the form of sequence tagging <ref type="bibr">(Stanovsky et al., 2018;</ref><ref type="bibr" target="#b7">Li et al., 2019)</ref>, machine reading comprehension <ref type="bibr" target="#b32">(Zhao et al., 2020)</ref> and text classification <ref type="bibr">(Soares et al., 2019)</ref>, which hinders the knowledge transfer across different tasks. TANL <ref type="bibr">(Paolini et al., 2021)</ref> proposes a translation-based approach to unify different structure prediction tasks with task-specific data augmentation. By contrast, our DEEPSTRUCT unifies more structure prediction tasks via a single model and a uniform data format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Related Models Recent studies have provided unified solutions for structural prediction tasks. We focus on the comparison between our DEEP-STRUCT to the state-of-the-art TANL <ref type="bibr">(Paolini et al., 2021)</ref> and DeepEx <ref type="bibr" target="#b18">(Wang et al., 2021)</ref>. TANL <ref type="bibr">(Paolini et al., 2021)</ref> proposes task-specific data augmentation (i.e., augmented natural language) that annotates task information and predictions in the input and output respectively for each structure prediction task. The main difference is that DEEPSTRUCT decomposes the structure prediction tasks into a collection of triple generation tasks. The triple format serves as the unified representation for all considered structure prediction tasks without the need of introducing new data augmentation as in TANL. While TANL mainly works in the multi-task setting, we additionally enable the zero-shot transfer via the task-agnostic structure pretraining. DeepEx <ref type="bibr" target="#b18">(Wang et al., 2021)</ref> explores the attention matrices of pretrained LMs via beam search to generate triples for information extraction tasks. Following the search, DeepEx introduces an extra ranking stage to improve the quality of the triples. Differently, DEEPSTRUCT aims to generate the triples for a wide set structure prediction tasks in an end-to-end fashion thanks to the proposed structure pretraining.</p><p>Besides, both TANL and DeepEx explore relatively small-scale pretrained LMs. Instead, DEEP-STRUCT scales up to 10 billion parameters. <ref type="bibr">Figure 6</ref> shows that the performance improvements follow the scaling law <ref type="bibr" target="#b7">(Raffel et al., 2019;</ref><ref type="bibr">Lester et al., 2021;</ref><ref type="bibr">Wei et al., 2021;</ref><ref type="bibr">Sanh et al., 2021</ref>; Liu query entity: (Iago; instance of; person) Born in 1951 in Tbilisi , Iago is a Georgian artist . Q: Does the entity "Iago" in the above sentence belongs to type "person"? A: Yes. Q: Does the relation between "Iago" and "Tbilisi" in the above sentence belongs to type "lives in"? A: Yes. query relation: (Iago; lives in; Tbilisi) predict entity: person predict relation: lives in <ref type="figure">Figure 7</ref>: An example of GPT-3 zero-shot setting. To predict entities, we convert the gold entity triple (Iago; instance of; person) to an entity based true-or-false question. Similarly, to predict relations, the gold relation triple (Iago; lives in; Tbilisi) is turned into a relation based true-or-false question. The task predictions are correct if the answers are "yes". <ref type="bibr">et al., 2021)</ref>. Based on our results, DEEPSTRUCT generalizes better to more structure prediction tasks compared to TANL and DeepEx.</p><p>Zero-Shot Setup For our zero-shot setup, we follow the zero-shot usage in recent pretrained LM studies <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr">Wei et al., 2021;</ref><ref type="bibr">Sanh et al., 2021)</ref>. It refers to the setting where a pretrained model is used to directly perform downstream tasks without including downstream training sets in its own pretraining data. For DEEP-STRUCT, our pretraining data is task-agnostic. For each task, we build an offline alignment between the schema of the pretraining data and the task dataset based on co-occurrence information in the pretraining data and downstream data <ref type="bibr">(Angeli et al., 2015)</ref>. We then manually curate the alignment. The resulting schema alignment is part of our release 1 . At test time, we convert each task to one or a combination of the pretraining tasks based on <ref type="figure">Figure 5</ref>: entity, relation, or triple prediction. After producing the triples, we use the pre-built schema alignment to obtain the task predictions.</p><p>For GPT-3 zero-shot setting, we follow the prompting method introduced by GPT-3 <ref type="bibr">(Brown et al., 2020)</ref>. In more details, we aim to test the upper bound performance of GPT-3 for structure prediction, in particular the JER task. Therefore, instead of using standard prompts in the form of question answering, we design the prompts for "trueor-false" questions based on the ground truth. In this case, GPT-3 only answers with "yes" or "no" to produce a task prediction <ref type="figure">(Figure 7)</ref>, which is apparently an easier task compared to generating the structures from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We improve structural understanding capabilities of language models. We evaluate it on a wide set of structure prediction tasks including 10 tasks and 28 datasets, and successfully transfer pretrained language models to them through the proposed structure pretraining, which teaches language models to output triples from the text. We enable both zero-shot and multi-task transfer learning. DEEP-STRUCT obtains state-of-the-art results on 21 of 28 datasets. The result shows that pretrained language models can handle higher-level understanding (e.g., structural understanding), which may benefit more NLP tasks. We hope it will foster future research along the language structural understanding direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethical Considerations</head><p>We hereby acknowledge that all of the co-authors of this work are aware of the provided ACM Code of Ethics and honor the code of conduct. This work is mainly about the pretraining and multitask learning of LMs for structural prediction. The followings give the aspects of both our ethical considerations and our potential impacts to the community. This work uses LMs, for which the risks and potential harms are discussed in <ref type="bibr">(Brown et al., 2020)</ref>. There are potential undesirable biases that existed in task-agnostic data (e.g., from Wikipedia) and multi-task downstream datasets (mostly created from news articles). We do not anticipate the production of harmful outputs, especially towards vulnerable populations, after using our model or training NLP models on our datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Environmental Considerations</head><p>We adopt the pretrained LMs from the <ref type="bibr">(Du et al., 2021)</ref>, whose energy cost and carbon footprint during pretraining were 80.6 MWh and 4.6 tCO2e, respectively. Additionally, the structure pretraining takes less than 5% gradient-steps of the number of pretraining steps of LMs, and thus the estimated auxiliary cost for energy is comparatively smaller. In addition, training and tuning pretrained LMs on a wide range of tasks and datasets consume a plenitude of energy and increase emissions of carbon dioxide. To alleviate the problem, in this work we make efforts to study the multi-task training, which only involves training on a combination of all datasets at once. Our results (e.g., <ref type="figure" target="#fig_5">Figure 6)</ref> show that, despite the gap between multi-task and multi-task finetune on smaller models, the performance gap becomes minor when the model size scales up to 10 billion parameters. This indicates that we can reduce energy consumption when training large pretrained models via employing multitask training.  <ref type="bibr">, 2021)</ref>. The Pile corpora are regarded as the similar corpora for training GPT-3. GLM outperforms T5 on text summarization, which shares a similar nature with structure prediction tasks. Compared to GPT-3, GLM is a bidirectional model and is able to perform autoregressive generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Pretraining Procedure</head><p>Task-Agnostic Pretraining We conduct the pretraining on 8 NVIDIA DGX-A100 machines using an Adam optimizer with a 5e-6 learning rate and 0.1 weight decay. We train the model with batch size 4 per GPU for 3 epochs and use the checkpoint of the last iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Training</head><p>We conduct the multi-task training on 8 NVIDIA DGX-A100 machines using an Adam optimizer with a 5e-6 learning rate and 0.1 weight decay. We train the model with batch size 4 per GPU for 6 epochs and use the checkpoint with the best performance on the corresponding validation set for each dataset.</p><p>Inference During the inference, length penalty and minimum target length are the most important hyperparameters. Length penalty is a float between 0 and 1 to control the GLM's generation length.</p><p>The larger the length penalty is, the longer the generation length is. In general, for entity prediction tasks (e.g., NER, SRL, event extraction), a larger length penalty is used. For entity and relation prediction or triple prediction tasks (e.g., JER and OIE), a smaller one is used. For other tasks that require a specific number of output triples (e.g., relation classification, intent detection, factual probe), we trim the generation results according to the requirements of different tasks. We show details of the task-specific hyperparameters in Appendix A.2 to Appendix A.11.</p><p>Pretraining Data We apply the task-agnostic pretraining data presented in Sec. 2.1 during structure pretraining. A small portion of T-REx <ref type="bibr">(El-Sahar et al., 2018)</ref> is used in the factual probe task <ref type="bibr" target="#b2">(Petroni et al., 2020)</ref>. To avoid the test leakage, we (i) sample a small portion of the T-REx as our pretraining data, and (ii) remove samples that appeared in the T-REx dataset of the factual probe task from the pretraining data. We integrate WebNLG 2.1 and 3.0 into a WebNLG dataset. For OPIEC, We use its OPIEC-clean version. Similar to T-REx, we also sample a portion of the OPIEC for our pretraining due to its large size.</p><p>The following sections introduce the dataset formats, comparison methods, and training details for all 10 structure prediction tasks. We show additional input and output examples on all datasets in <ref type="table" target="#tab_1">Table 16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Open Information Extraction</head><p>For OIE, given a sentence, we are asked to extract triples consisting of arguments and predicates. An example of the input and output format is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Born in 1951 in Tbilisi, Iago is a Georgian artist.   <ref type="bibr">et al., 2013)</ref>. The statistics of all datasets are shown in <ref type="table" target="#tab_9">Table 5</ref>.  , which proposes to leverage knowledge stored in attention matrices for OIE.</p><p>Training Details We train our model on the OIE2016 training set for 5 epochs during multi-task finetuning. The per GPU batch size is 4. During inference, for OIE2016, we choose a length penalty of 0.8. WEB, NYT, and PENN only contain the test sets. For these datasets, we use a length penalty of 0.5 and trim the outputs to only contain the first triple. We adopt the preprocessed OIE datasets provided by <ref type="bibr">Stanovsky et al. (2018)</ref>.</p><p>Additional Results A detailed comparison between DEEPSTRUCT and compared approaches is shown in <ref type="table" target="#tab_17">Table 6</ref>. On OIE2016, NYT, and PENN datasets, DEEPSTRUCT presents significant improvements compared to the OIE systems. While on WEB, PropS <ref type="bibr">(Stanovsky et al., 2016)</ref> outperforms our method. The reason is that the arguments of WEB are very short and concise (e.g., "( google ; assimilates ; youtube )"), which aligns better with the phrase extraction paradigm of PropS. We also observe that finetuning hurts the performance on WEB and PENN. This is because we are only able to finetune DEEPSTRUCT on the OIE2016 training set and this can lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Relation Classification</head><p>Given head and tail entities in the target sentence, we seek to identify the relation between them. An example is as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input The 1976 Thomas Cup was the tenth edition of Thomas Cup, the world championship of men's international team badminton (its female counterpart is the Uber Cup). The relationship between Uber Cup and badminton is Output (Uber Cup; sport; badminton)</p><p>where "Uber Cup" and "badminton" are the corresponding head and tail entities, and "sport" is a relation from a predefined category. In addition, we augment the input sentence with the task-specific suffix "The relationship between [head entity] and [tail entity] is" following <ref type="bibr">(Paolini et al., 2021)</ref> as shown in the above example.</p><p>Datasets We evaluate on FewRel <ref type="bibr">(Han et al., 2018)</ref> and TACRED <ref type="bibr">(Zhang et al., 2017)</ref>.</p><p>? FewRel is a few-shot N-way K-shot relation classification dataset for meta learning. For all 100 relations, train (64 relations), validation (16 relations), and test set (20 relations) are constructed accordingly. We report the results on the dev set.</p><p>? TACRED is a large-scale benchmark including over 100K samples and 41 relation types. We select the checkpoint on the dev set, and report results on the test set.</p><p>We show the dataset statistics in <ref type="table" target="#tab_9">Table 5</ref>. We use F1 to evaluate the results. We parse every relation type and the corresponding head and tail entities from every original sample and formulate it as the input and output format as shown in the above example. (iii) TANL <ref type="bibr">(Paolini et al., 2021</ref>) is a sequence-tosequence generation model using task-augmented natural languages.</p><p>Training Details We train our model on training sets of TACRED and FewRel for 20 epochs during multi-task finetuning respectively. The per GPU batch size is 4. As shown in the above input and output format, we use the prompt of "The relationship between [head entity] and [tail entity] is" to query the model to generate the relation. For the zero-shot setting, the model is also provided with the prefix "( [head entity];" to generate the relation and tail entity. The prediction is correct only if both the relation and tail entity are correct. The length penalty equals 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head><p>We show the results in <ref type="table" target="#tab_19">Table 7</ref>. DEEPSTRUCT outperforms all supervised methods on both TACRED and FewRel. We find that our task-agnostic pretraining can significantly help improve relation classification. This is vital to few-shot settings (FewRel), where DEEPSTRUCT can achieve almost perfect F1 scores. We also notice that multi-task DEEPSTRUCT outperforms all compared approaches except for the 5-5 FewRel setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Factual Probe</head><p>Given an input sentence, and a gold head entity and relation, the task is to predict the missing tail entity in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Daniel Bowen, born in 1970, is a Melbourne resident best known as the author of the blog, Diary of an Average Australian. Output <ref type="bibr">(Daniel Bowen;</ref><ref type="bibr">date_of_birth;</ref><ref type="bibr">1970)</ref> where "(Daniel Bowen; date of birth; " is provided in the output, and the model is asked to generate "1970)".</p><p>Datasets We use the Google-RE dataset consisting of 3 relations ("place of birth", "place of death", and "date of birth"), and T-REx with 41 relations from the LAMA benchmark <ref type="bibr" target="#b3">(Petroni et al., 2019)</ref>. The task is evaluated using mean precision at one (P@1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare to the following approaches: (i) LAMA <ref type="bibr" target="#b3">(Petroni et al., 2019)</ref> uses only the head and relation to form the query without using the oracle context, and (ii) LAMA-Oracle <ref type="bibr" target="#b2">(Petroni et al., 2020)</ref> takes (at most) five gold sentences as additional context in the query. Both methods are based on BERT LARGE and the query is constructed based on natural language templates. For example, the Wikidata relation "place_of_birth" is aligned with a template "was born in".</p><p>Training Details As the factual probe task is usually performed without training sets, we only report DEEPSTRUCT's results in the zero-shot and multitask setting (without finetuning). We follow the task format of LAMA-Oracle <ref type="bibr" target="#b2">(Petroni et al., 2020)</ref>, which appends the query to five oracle context sentences. We use the relation labels (e.g., "place of birth") rather than the templates (e.g., "was born in") as they align better with our task-agnostic pretraining. Note that we have removed the T-REx data in the LAMA benchmark from the pretraining data.</p><p>Additional Results <ref type="table" target="#tab_20">Table 8</ref> shows the results. DEEPSTRUCT significantly outperforms compared approaches, which is attributed to the larger model size and knowledge-intensive task-agnostic pretraining. Multi-task setting actually hurts the performance due to the difference between the relation schema of downstream datasets and task-agnostic pretraining datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Joint Entity and Relation Extraction</head><p>The goal of the task is to extract entities and relations (with their type information) from a given sentence. We formulate the task as two unit tasks: the first task is entity prediction to generate the entities, while the second task is relation prediction to generate the relations. Our task formalization is different compared to traditional JER, where our two unit tasks are independent. An example is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Blackstone already holds a 50 percent stake in the two parks that make up Universal Orlando. Entity Output (Blackstone; instance of; organization) (parks; instance of; organization) (Universal Orlando; instance of; organization) Relation Output (Blackstone; employer; parks)</p><p>where the entity output contains entity predictions and relation output contains relation predictions. The entity mentions are detected following the procedure below.</p><p>Evaluation Details The conventional entity prediction evaluation is based on extractive span matching. To ensure a fair comparison in situations where there are multiple entities with the same surface, we adopt the following strategy: we match the spans of the generated entities from left to right in the original sentence when they are first mentioned. If there are duplicated entities, they are matched sequentially. For example, the first generated one matches the first mention span, while the second one matches the second mention span, etc. This strategy applies to all tasks that involve entity mention detection such as named entity recognition.</p><p>Datasets We experiment on the following datasets:</p><p>? The CoNLL04 <ref type="bibr" target="#b10">(Roth and Yih, 2004)</ref> dataset: CoNLL04 consists of four types of entities ("location", "organization", "person", "other") and five types of relations ("work for", "kill", "organization based in", "live in", "located in") on sentences taken from WSJ, AP, etc, containing 922 samples for training, 231 samples for validating, and 288 samples for testing. We use the same split as in <ref type="bibr">(Gupta et al., 2016)</ref>. We use the same type names of entities and relations with TANL <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The ADE (Gurulingappa et al., 2012) dataset: ADE contains annotated documents for drugrelated adverse effects over medical case reports corpus. It consists of two entity types ("Adverse-Effect" and "Drug") and one relation type ("(Has-)Adverse-Effect"). We use the same type names as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The NYT <ref type="bibr" target="#b9">(Riedel et al., 2010)</ref> dataset: NYT is a distantly-supervised joint entity and relation extraction dataset based on New York Times corpus. The dataset consists of three entity types ("PER", "ORG", and "LOC") and 24 Freebase relations. We use a preprocessed version of this dataset from <ref type="bibr" target="#b28">(Yu et al., 2020a</ref>) and use the same type names with TANL <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The ACE2005 (Walker and Consortium, 2005) dataset: ACE2005 is based on the ACE 2005 Multilingual Training Corpus. We use a preprocessed version of this dataset in <ref type="bibr">(Luan et al., 2019)</ref>. We make use of seven entity types and six relation types with the same type names as in TANL.</p><p>The dataset statistics are shown in <ref type="table" target="#tab_9">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method DEEPSTRUCT on the four datasets to the following JER methods: (i) SpERT (Eberts and Ulges, 2020):</p><p>The BERT-based model first conducts named entity recognition formulated as sequence tagging, and performs relation classification between recognized entities; (ii) DyGIE <ref type="figure" target="#fig_2">(Luan et al., 2019)</ref>: The general information extraction framework organizes dynamic spans into graphs; (iii) MRC4ERE <ref type="bibr" target="#b32">(Zhao et al., 2020)</ref>: The model formulates the joint entity and relation extraction task as machine reading comprehension; (iv) RSAN :</p><p>The work presents a relation-specific attention network to jointly extract entities and relations; (v) TANL <ref type="bibr">(Paolini et al., 2021)</ref>: It is a sequence-tosequence extraction model using augmented natural languages.</p><p>Training Details We train our model on JER training sets during multi-task finetuning for (i) 10 epochs on CoNLL04, (ii) 10 epochs on ADE, (iii) 3 epochs on NYT, and (iv) 10 epochs on ACE2005.</p><p>We employ less number of epochs on NYT, as its size is much larger compared to other datasets. We find that the relation prediction task and the entity prediction task need different length penalties. Therefore, we split the training sets corresponding to the two tasks. We choose a length penalty of 0.8 for entity prediction and 0.3 for relation prediction during inference. We use the same evaluation scripts as in <ref type="bibr">(Paolini et al., 2021</ref>). As multi-task training and finetuning for 10 folds on ADE is too expensive for DEEPSTRUCT 10B, only the first split of ADE is included in the multi-task training and finetuning. In the ablation study (Sec. 3.2), we also present a model variant with entity and relation augmentation. For this setting, we augment the output with entity boundary information. For example, for the example in Additional Results <ref type="table" target="#tab_22">Table 9</ref> presents the results. DEEPSTRUCT outperforms or is competitive compared to state-of-the-art supervised approaches. We also find the multi-task DEEPSTRUCT performs competitively with previous task-specific approaches on both ADE and NYT. This indicates that multi-task trained models are cost-effective alternatives to per-task finetuned models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Named Entity Recognition</head><p>Compared to joint entity and relation extraction, named entity recognition only focuses on predicting the entities and their corresponding types in the target sentence. We show an example below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input What we need to do is to make sure that state boards, number one, have adequate funding.</p><p>Output (we; instance of; human) (state; instance of; geographical entity) (state boards; instance of; organization)</p><p>where the head entities of these triples are the entity mentions in the given sentence, and tail entities are from a predefined list of entity types.</p><p>Datasets We experiment on the following datasets:</p><p>? The CoNLL03 <ref type="bibr" target="#b14">(Sang and Meulder, 2003)</ref> dataset: CoNLL03 (English) data was taken from the Reuters Corpus, containing 14,041 training samples, 3,250 validating samples and 3,453 testing samples. It consists four entity types ("LOC", "ORG", "PER", and "MISC"). We use the preprocessed version of this dataset from <ref type="bibr" target="#b28">(Li et al., 2020a)</ref>. It consists five entity types ("DNA", "RNA", "cell_line", "cell_type", and "protein"). We use a preprocessed version of this dataset <ref type="bibr" target="#b28">(Li et al., 2020a)</ref>.</p><p>? The ACE2005 (Walker and Consortium, 2005) dataset: ACE2005 contains 7,299 training samples, 971 validating samples, and 1,060 testing samples. Note that it is also processed based on the ACE2005 corpus but with different data splits compared to that of the ACE2005 JER dataset. It includes seven entity types. We use the preprocessed version of this dataset in <ref type="bibr" target="#b28">(Li et al., 2020a)</ref>, and exclude this dataset in the DEEPSTRUCT's multi-task setting due to its overlap with the ACE2005 JER dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method DEEPSTRUCT on the four datasets to the following NER methods: the method uses augmented language for intent detection, slot filling, and named entity recognition, (v) BiaffineLSTM <ref type="bibr" target="#b29">(Yu et al., 2020b)</ref>: the model transforms NER into dependency parsing using biaffine LSTMs, (vi) TANL <ref type="bibr">(Paolini et al., 2021)</ref>:</p><p>it presents a sequence-to-sequence extraction approach using augmented natural languages.</p><p>Training Details We train our model on NER training sets for 15 epochs on every dataset during multi-task finetuning with early stopping. The per GPU batch size is 4. We choose a length penalty of 0.8 during inference. Since some datasets may contain null predictions, we set the minimum target length to 0.</p><p>Additional Results <ref type="table" target="#tab_1">Table 10</ref> shows the results. DEEPSTRUCT achieves comparable performance to task-specific supervised approaches, except for OntoNotes. We suppose that OntoNotes contains a relatively large number of entity types, making it more challenging for models to use labels for considering their semantic meanings. On GENIA and ACE2005, DEEPSTRUCT outperforms state-ofthe-art task-specific methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Semantic Role Labeling</head><p>In semantic role labeling, we seek to identify the corresponding arguments in the form of spans (or the semantic roles) given a certain predicate. Consider an example as follow.</p><p>Input and Output Format The predicate is marked in the input. The model then yields arguments according to the predicate in the output with their corresponding argument types from a predefined set.</p><p>Input Scotty [ accepted ] the decision with indifference and did not enter the arguments. Output (Scotty; instance of; subject) (decision; instance of; object)</p><p>where "[ accepted ]" is the given predicate, and arguments such as "Scotty", "the decision" and their corresponding types are generated in the form of triples.</p><p>Datasets We experiment on the following datasets: CoNLL05 WSJ, CoNLL05 Brown <ref type="bibr">(Carreras and M?rquez, 2005)</ref> and CoNLL12 . <ref type="table" target="#tab_9">Table 5</ref> shows the dataset statistics.</p><p>? The CoNLL05 WSJ and CoNLL05 Brown datasets: CoNLL05 WSJ and CoNLL05 Brown datasets share the same train and validation splits. They have different test sets. For CoNLL05 WSJ and CoNLL05 Brown, the corresponding test datasets are taken from the WSJ and Brown corpus respectively. The datasets consist of seven different types including "V" (verb), "A0" (subject), "A1" (object), "A2", "A3", "AM-MOD", and "AM-NEG". We use the same type names as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>? The CoNLL12 dataset: CoNLL12 dataset is built upon OntoNotes dataset including 39 argument types. We leverage the same type names as in <ref type="bibr">(Paolini et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method DEEPSTRUCT on the datasets to the following SRL models: (i) <ref type="bibr">Dep and Span (Li et al., 2019)</ref>: this model formulates semantic role labeling as an end-to-end dependency parsing task, (ii) BERT SRL (Shi and Lin, 2019): it is a sequence-tagging version of BERT, (iii) TANL <ref type="bibr">(Paolini et al., 2021)</ref>: this is a sequence-to-sequence extraction model using augmented natural languages. Evaluation Details Sentences with multiple target predicates are duplicated during data preprocessing. So, each sentence is only related to one target predicate that is marked by "[]". We adopt the same evaluation scripts as in <ref type="bibr">(Paolini et al., 2021)</ref>. <ref type="table" target="#tab_1">Table 11</ref> shows the results. Both multi-task and multi-task finetuned DEEP-STRUCT outperform task-specific models by a large margin. An important reason is that <ref type="bibr">Prop-Bank (Kingsbury and Palmer, 2003)</ref> is included in the multi-task training. The knowledge of Prop-Bank transfers well to other SRL datasets. We find that the performance gain is significant since the large-scale model has the capacity to capture the PropBank knowledge. We also observe a minor performance drop from multi-task to multi-task finetuned DEEPSTRUCT on CoNLL05 WSJ and CoNLL12 datasets. This might be attributed to overfitting. Besides, the issue can be relieved if a better hyperparameter combination is used in the multi-task finetuning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Event Extraction</head><p>This task contains two sequential subtasks: (i) event triggers identification and classification: this subtask first identifies the trigger words in target sentences that refer to certain types of events, and (ii) trigger arguments identification and classification: this subtask then extracts arguments from the target sentences that can be mapped to certain roles in the event from (i). where "summit" is an extracted trigger and "meet" is its corresponding trigger event. Then, based on the "summit" event, we can further extract the role of "place" in this event as "Saint Petersburg".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Datasets We experiment on the ACE2005 dataset. For detailed dataset statistics, please refer to <ref type="table" target="#tab_9">Table 5</ref>.</p><p>? The ACE2005 dataset (Walker and Consortium, 2005): ACE2005 contains 33 types of event triggers, and each of them corresponds to a set of argument roles. We follow the preprocessing in TANL <ref type="bibr">(Paolini et al., 2021)</ref> and use the same evaluation scripts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method DEEPSTRUCT on the dataset to the following methods: (i) J3EE (Nguyen and Nguyen, 2019): This method presents a joint model based on a recurrent neural network to first extract mention spans for triggers and arguments and then perform pairwise classification, (ii) DyGIE++ (Wadden et al., 2019): the method leverages BERT for sequence tagging to identify mention spans and then classify each mention with triggers in pair for argument roles, (iii) TANL <ref type="bibr">(Paolini et al., 2021)</ref>: this is a sequenceto-sequence extraction approach using augmented natural languages.</p><p>Training Details We train our model on ACE2005 event trigger and argument training sets for 20 epochs during multi-task finetuning. The per GPU batch size is 4. During inference, we choose a length penalty of 0.8. The argument prediction task requires triggers as input to make predictions. An example is shown in <ref type="table" target="#tab_1">Table 16</ref>. For the argument prediction task, we first generate all trigger predictions using our 10B model. If there is more than one trigger in a sentence, we will duplicate the sentence to make sure that every sample corresponds to a single trigger. The ACE2005 dataset is processed similarly to the named entity recognition task.</p><p>Additional Results <ref type="table" target="#tab_1">Table 12</ref> presents the results. DEEPSTRUCT is competitive with the state-of-theart task-specific supervised models on trigger identification and classification, as well as the argument identification task. In the meantime, DEEPSTRUCT outperforms the comparison methods on the argument classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Coreference Resolution</head><p>The coreference resolution aims to identify and cluster mentions in a document that refers to the same entity. An example is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input And deterrents don't work well when an enemy values your death more than his life. Output (an enemy; refer to; his)</p><p>where "an enemy" appears as the target entity and "his" is the mention it refers to. "refer to" is provided as part of the output triple.  : the model formulates coreference resolution as question answering, (iv) TANL <ref type="bibr">(Paolini et al., 2021)</ref>: this is a sequence-to-sequence extraction model using augmented natural languages.</p><p>Training Details We train our model on CoNLL12 coreference resolution training set for 40 epochs during multi-task finetuning. The per GPU batch size is 4. During inference, we choose a length penalty of 0.8. CoNLL12 coreference resolution has different evaluation metrics compared to other structure prediction tasks, including: (i) MUC: a link-based metric that reflects the minimum number of missing mentions in the response chain (Moosavi and Strube, 2016), (ii) B 3 : a singlemention based metric which computes the macro F1 of all entity mentions, and (iii) CEAF ?4 : a similarity metric based on the assumption that the coreference map should be one-to-one. Due to the limited maximum sequence length of language models, the dataset is chunked with a fixed size of 512 during data preprocessing. Following TANL <ref type="bibr">(Paolini et al., 2021)</ref>, only intra-chunk coreferences are preserved. We also use the same evaluation scripts with <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>Additional Results <ref type="table" target="#tab_1">Table 13</ref> shows the results. DEEPSTRUCT presents better results compared to TANL and classic task-specific supervised approaches. However, DEEPSTRUCT fails when compared with the state-of-the-art coreference method.</p><p>The main reason is that this task requires taskspecific model architectures. In the meantime, we argue that it is promising to employ a unified framework for multiple structure prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 Dialogue State Tracking</head><p>We are presented with a dialogue between a user and an agent to identify what information is known given a list of slots by the end of each round of the conversation. An example is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input in which by the end of this conversation, we know that the user wants to get to Pizza Hut Fen Ditton from Saint Johns College, leaving at 17:15, while the taxi's arrival time is unknown. The slots "taxi arrive by", "taxi departure", "taxi destination", and "taxi leave at" are provided for the output.</p><p>Datasets We use the MultiWOZ 2.1 <ref type="bibr" target="#b8">(Budzianowski et al., 2018;</ref><ref type="bibr" target="#b8">Ramadan et al., 2018;</ref><ref type="bibr">Eric et al., 2020;</ref><ref type="bibr" target="#b31">Zang et al., 2020)</ref>, which is a daily dialogue dataset for task-oriented conversations. We follow the preprocessing in . Following TANL <ref type="bibr">(Paolini et al., 2021)</ref>, the "police" and "hospital" slots are excluded from the training set as they are absent in the test set. The resulting training set contains 7,904 samples. Dataset statistics are presented in <ref type="table" target="#tab_9">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method with: (i) TRADE : It is a transferable multi-domain generative dialogue state tracking model, (ii) SimpleTOD <ref type="bibr">(Hosseini-Asl et al., 2020)</ref>: This is a state-of-the-art task-specific model for dialogue state tracking based on <ref type="bibr">GPT-2 (Radford et al., 2019b)</ref>. In addition, we also compare our method with TANL.</p><p>Training Details We finetune for 20 epochs. The maximum sequence length is 512, and the per GPU batch size is 4. Given a domain and all possible slots, DEEPSTRUCT generates triples regarding the slots: if the information is not yet provided, the tail should be "not given". We use the same type names with <ref type="bibr">(Paolini et al., 2021)</ref>. <ref type="table" target="#tab_1">Table 14</ref> shows the results. While DEEPSTRUCT does not outperform the stateof-the-art SimpleTOD, it is still competitive compared to the task-specific supervised models. This demonstrates the effectiveness of DEEPSTRUCT in dealing with different structure prediction tasks under the same architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 Intent Detection</head><p>Intent detection identifies the user's intent in the conversation with the agent based on a predefined list of slots. It resonates with the classical sentence classification task. Below is an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input and Output Format</head><p>Input Show flight and prices from Kansas City to Chicago next Wednesday arriving in Chicago by 7 pm. Output (intent; is; flight and airfare) where our prediction is the "flight and airfare". The head entity "intent" and predicate "is" are given for all outputs.</p><p>Datasets We use two datasets, the ATIS dataset <ref type="bibr">(Hemphill et al., 1990)</ref>, which contains flight and airline-related conversation and queries, and the SNIPS dataset <ref type="bibr">(Coucke et al., 2018)</ref>, which consists of daily queries from the interaction between the users and dialogue agents. The dataset statistics are shown in <ref type="table" target="#tab_9">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>We compare our method to <ref type="bibr">SF-ID (E et al., 2019)</ref> and TANL <ref type="bibr">(Paolini et al., 2021)</ref> in this task.</p><p>Training Details We formulate the label of every sample as "(intent; is; [label])". We parse every intent from every original sample and formulate it into the input and output format as shown above.</p><p>We finetune for 20 epochs. The maximum sequence length is 512, and the per GPU batch size is 4. We report F1 for this task.</p><p>Additional Results <ref type="table" target="#tab_1">Table 15</ref> shows the results. DEEPSTRUCT is comparable to task-specific supervised approaches on both ATIS and SNIPS datasets. For TANL, we produce the results using the released code 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Error Analysis</head><p>We analyze the errors of DEEPSTRUCT 10B multitask in recall on the CoNLL04 (JER) dataset. We specifically investigate the errors in the relation outputs. <ref type="table" target="#tab_1">Table 17</ref> shows the error cases. We find that most errors are caused by minor differences between ground truth entities and predicted entities from entity outputs. For example, the predicated entity has almost the same span as the ground truth entity (e.g., "U.S." and "the U.S."). Besides, we observe some false-positive errors that are due to the noise in the datasets. In such cases, our predictions are reasonable while they are missing due to the incompleteness of human annotations.   <ref type="bibr">(Sainz et al., 2021)</ref> 73.9 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 71.9 93.6?5.4 97.6?3.2 82.2?5.1 89.8?3.6 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 69.    <ref type="bibr" target="#b32">Zhao et al., 2020)</ref> 88.9 71.9 85.5 62.1 RSAN  84.6 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 89.4 71.4 90.2 80.6 94.9 90.8 88.9 63.7 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 90    <ref type="bibr">(Paolini et al., 2021)</ref> 89.3 82.0 87.7 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 89.     <ref type="bibr">) 45.6 SimpleTOD (Hosseini-Asl et al., 2020</ref> 55.7 TANL <ref type="bibr">(Paolini et al., 2021)</ref> 50.5 TANL (multitask) <ref type="bibr">(Paolini et al., 2021)</ref> 51.4</p><p>DEEPSTRUCT multi-task 53.5 w/ finetune 54.2   we list the percentage of missing triples caused by this particular type of error, and an example of this type of error taken from the CoNLL04 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Open information extraction (Iago; Born in; 1951) Joint entity and relation extraction (Iago; instance of; person) (Iago; city_of_birth; Tbilisi) Relation classification (Iago; city_of_birth; Tbilisi) Multi-task corpora (optional) input: jer conll04: An art exhibit is at the Haka Theatre (Haka Theatre; instance of; theatre) output: ner genia: Japan began the defence of their Asian Cup title input: output: Named entity recognition (Iago; instance of; person) (Asian Cup; instance of; race) (Japan; instance of; location) (art exhibit; located in; Haka Theatre) (Iago; is a; Georgian artist) Born in 1951 in Tbilisi, Iago is a Georgian artist DeepStruct input: triple: The couple have a daughter output: (couple; have; a daughter) Task-agnostic corpora ...... input: entity: He played for FIFA (Structure pretraining) input: relation: The book Fly is in English output: (Fly; language; English) output: (He; instance of; human) (FIFA; instance of; club)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between structural understanding and traditional understanding of text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparing structure pretraining with standard pretrain-finetune paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Summary of training procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Model scaling results on joint entity and relation extraction (JER) datasets. Left: entity F1; Right: relation F1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Comparison Methods The compared models are as follows: (i) BERT-PAIR (Gao et al., 2019) is a sentence-level pairwise model that optimizes the similarity between sentences with the same relation. (ii) BERT EM +Matching the Blanks (MTB) (Soares et al., 2019) proposes continual pretraining for relations over a large-scale entity-linked corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 1, "([Iago]; instance of; person) ([Iago]; city_of_birth; [Tbilisi])" are the augmented outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(i) BERT-MRC (Li et al., 2020a): this method formulates NER as a machine reading comprehension problem, (ii) BERT-MRC+DSC (Li et al., 2020b): this model is a dice-loss enhanced version of BERT-MRC, (iii): Cloze-CNN (Baevski et al., 2019): the model leverages cloze-style pretraining on convolutional neural networks for natural languages, (iv) GSL (Athiwaratkun et al., 2020):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Pretraining dataset statistics and corresponding pretraining tasks. #Sent. and #Rel. denote the number of sentences and relations respectively.</figDesc><table><row><cell>entity:</cell><cell>Iago</cell><cell>is</cell><cell cols="2">born in 1951 (Iago; instance</cell><cell>of;</cell><cell>person)</cell><cell>&lt;e&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deepstruct</cell><cell></cell><cell></cell></row><row><cell>&lt;s&gt;</cell><cell>entity:</cell><cell cols="2">Iago is</cell><cell cols="3">born in 1951 (Iago; instance of;</cell><cell>person)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on all tasks. All evaluation scores are higher the better. TANL is introduced in(Paolini et al.,  2021). The bold denotes the best, and the underline indicates the second best.</figDesc><table><row><cell>Model</cell><cell cols="2">CoNLL04</cell><cell>ADE</cell></row><row><cell></cell><cell>Ent.</cell><cell>Rel.</cell><cell>Ent.</cell><cell>Rel.</cell></row><row><cell>GPT-3 175B zero-shot</cell><cell cols="2">34.7 18.1</cell><cell>5.8</cell><cell>1.3</cell></row><row><cell>zero-shot</cell><cell cols="4">48.3 25.8 60.7 10.6</cell></row><row><cell>DEEPSTRUCT multi-task</cell><cell cols="4">87.4 69.6 90.2 83.7</cell></row><row><cell cols="5">w/ finetune 90.7 78.3 91.1 83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation over different facets of structure pretraining on CoNLL04 test set (joint entity and relation extraction). Ent. and Rel. indicate entity F1 and relation F1 respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Chen, Robert Hoehndorf, Mohamed Elhoseiny, and Xiangliang Zhang. 2020. Efficient long-distance relation extraction with dg-spanbert. CoRR. Janara Christensen, Stephen Soderland, and Oren Etzioni. 2011. An analysis of open information extraction based on semantic role labeling. In Proceedings of the sixth international conference on Knowledge capture, pages 113-120. Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In ICML. Shawn Presser, and Connor Leahy. 2021. The pile: An 800gb dataset of diverse text for language modeling. CoRR. Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2019. Fewrel 2.0: Towards more challenging few-shot relation classification. In EMNLP-IJCNLP, pages 6249-6254. Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-tofine inference. In NAACL-HLT, pages 687-692. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045-3059.</figDesc><table><row><cell>Tianyu Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from RDF data. In INLG, pages 124-133. Kiril Gashteovski, Sebastian Wanner, Sven Hertling, Samuel Broscheit, and Rainer Gemulla. 2019. OPIEC: an open information extraction corpus. CoRR. Pankaj Gupta, Hinrich Sch?tze, and Bernt Andrassy.</cell><cell>Kenton Mike Lewis, Yinhan Liu, Naman Goyal, Mar-jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Jun Luciano Del Corro and Rainer Gemulla. 2013. Clausie: 2020. BART: denoising sequence-to-sequence pre-Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-training for natural language generation, translation, geli, and Christopher D. Manning. 2017. Position-and comprehension. In ACL, pages 7871-7880. aware attention and supervised data improve slot fill-Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong ing. In EMNLP, pages 35-45. clause-based open information extraction. In WWW, pages 355-366. Han, Fei Wu, and Jiwei Li. 2020a. A unified MRC Tianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun framework for named entity recognition. In ACL, Li. 2020. Asking effective and diverse questions: pages 5849-5859. A machine reading comprehension based framework</cell></row><row><cell>2016. Table filling multi-task recurrent neural net-</cell><cell>for joint entity-relation extraction. In IJCAI, pages</cell></row><row><cell>work for joint entity and relation extraction. In COL-</cell><cell>3948-3954.</cell></row><row><cell>ING, pages 2537-2547.</cell><cell></cell></row><row><cell></cell><cell>Wanrong Zhu, Zhiting Hu, and Eric P. Xing. 2019. Text</cell></row><row><cell>Harsha Gurulingappa, Abdul Mateen Rajput, Angus</cell><cell>infilling. CoRR.</cell></row><row><cell>Roberts, Juliane Fluck, Martin Hofmann-Apitius,</cell><cell></cell></row><row><cell>and Luca Toldo. 2012. Development of a benchmark corpus to support the automatic extraction of drug-</cell><cell>A Experimental Setup</cell></row><row><cell>related adverse effects from medical case reports. J. Biomed. Informatics, pages 885-892.</cell><cell>A.1 Implementation Details</cell></row><row><cell>Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A large-scale supervised few-shot relation classifi-cation dataset with state-of-the-art evaluation. In EMNLP, pages 4803-4809. Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.</cell><cell>Model Architecture We leverage the General-ized Language Model (GLM) (Du et al., 2021) as our base language model pretrained on autore-Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, gressive blank infilling objectives. GLM follows Xueyun Zhu, Emmanuel Awa, Pengcheng He, an adaptive encoder-decoder architecture. It im-Weizhu Chen, Hoifung Poon, Guihong Cao, and Jianfeng Gao. 2020. The microsoft toolkit of multi-proves the pretrain-finetune consistency via cloze-</cell></row><row><cell>Question-answer driven semantic role labeling: Us-</cell><cell>task deep neural networks for natural language un-style finetuning. GLM adopts the Byte Pair Encod-</cell></row><row><cell>ing natural language to annotate natural language. In EMNLP, pages 643-653. Charles T. Hemphill, John J. Godfrey, and George R.</cell><cell>derstanding. In ACL, pages 118-126. ing (Radford et al., 2019b), covering 50,257 tokens. In this work, we leverage the models in four dif-Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A gen-ferent scales: 110M, 220M, 2B, and 10B 2 . The</cell></row><row><cell>Doddington. 1990. The ATIS spoken language sys-tems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, USA, June 24-27, 1990.</cell><cell>Nafise Sadat Moosavi and Michael Strube. 2016. eral framework for information extraction using dy-110M model is pretrained over English Wikipedia namic span graphs. In NAACL-HLT, pages 3036-and BookCorpus, and the others are pretrained over 3046. the Pile corpora (Gao et al.</cell></row><row><cell>Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,</cell><cell>Which coreference evaluation metric do you trust?</cell></row><row><cell>Semih Yavuz, and Richard Socher. 2020. A sim-</cell><cell>a proposal for a link-based entity aware metric. In</cell></row><row><cell>ple language model for task-oriented dialogue. In</cell><cell>Proceedings of the 54th Annual Meeting of the As-</cell></row><row><cell>NeurIPS.</cell><cell>sociation for Computational Linguistics (Volume 1:</cell></row><row><cell>Robert L. Logan IV, Ivana Balazevic, Eric Wallace,</cell><cell>Long Papers), pages 632-642.</cell></row><row><cell>Pawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Fabio Petroni, Sameer Singh, and Sebastian Riedel.</cell><cell>Trung Minh Nguyen and Thien Huu Nguyen. 2019.</cell></row><row><cell>Tseng, I?igo Casanueva, Stefan Ultes, Osman Ra-2021. Cutting down on prompts and parameters:</cell><cell>One for all: Neural joint modeling of entities and</cell></row><row><cell>madan, and Milica Gasic. 2018. Multiwoz -A large-scale multi-domain wizard-of-oz dataset for Simple few-shot learning with language models. CoRR. task-oriented dialogue modelling. In EMNLP, pages Mandar Joshi, Omer Levy, Luke Zettlemoyer, and 5016-5026. Daniel S. Weld. 2019. BERT for coreference reso-</cell><cell>events. In AAAI, pages 6851-6858. Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002. Anuj Kumar Goyal, Peter Ku, and Dilek Hakkani-The genia corpus: An annotated research abstract T?r. 2020. Multiwoz 2.1: A consolidated multi-corpus in molecular biology domain. In HLT, page</cell></row><row><cell>lution: Baselines and analysis. In EMNLP-IJCNLP,</cell><cell>82-86.</cell></row><row><cell>pages 5802-5807.</cell><cell></cell></row></table><note>Xavier Carreras and Llu?s M?rquez. 2005. Introduc- tion to the conll-2005 shared task: Semantic role la- beling. In CoNLL, pages 152-164. Rich Caruana. 1997. Multitask learning. Mach. Learn., pages 41-75.Alice Coucke, Alaa Saade, Adrien Ball, Th?odore Bluche, Alexandre Caulier, David Leroy, Cl?ment Doumouro, Thibault Gisselbrecht, Francesco Calt- agirone, Thibaut Lavril, Ma?l Primet, and Joseph Dureau. 2018. Snips voice platform: an embedded spoken language understanding system for private- by-design voice interfaces. CoRR. Filipe de S? Mesquita, Jordan Schmidek, and Denilson Barbosa. 2013. Effectiveness and efficiency of open relation extraction. In EMNLP-ACL, pages 447- 457. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language under- standing. In NAACL-HLT, pages 4171-4186. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. All NLP tasks are generation tasks: A general pretrain- ing framework. CoRR. Haihong E, Peiqing Niu, Zhongfu Chen, and Meina Song. 2019. A novel bi-directional interrelated model for joint intent detection and slot filling. In ACL, pages 5467-5471. Markus Eberts and Adrian Ulges. 2020. Span-based joint entity and relation extraction with transformer pre-training. In ECAI-PAIS, pages 2006-2013. Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Fr?d?rique Laforest, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In LREC-ELRA.domain dialogue dataset with state corrections and state tracking baselines. In LREC, pages 422-428. Leo Gao, Stella Biderman, Sid Black, Laurence Gold- ing, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima,Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Tree- banks and lexical Theories, volume 3. Citeseer.Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. 2020b. Dice loss for data- imbalanced NLP tasks. In ACL, pages 465-476. Zuchao Li, Shexia He, Hai Zhao, Yiqing Zhang, Zhu- osheng Zhang, Xi Zhou, and Xiang Zhou. 2019. De- pendency or span, end-to-end uniform semantic role labeling. In AAAI, pages 6730-6737. Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-tuning v2: Prompt tuning can be comparable to fine-tuning uni- versally across scales and tasks. CoRR.Harinder Pal et al. 2016. Demonyms and compound relational nouns in nominal open ie. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction, pages 35-39.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Statistics of downstream datasets.</figDesc><table><row><cell>Output (Iago; Born in; 1951) (Iago; is a;</cell></row><row><cell>Georgian artist)</cell></row><row><cell>where we extract arguments (e.g., Iago) and their</cell></row><row><cell>predicates (e.g., Born in) in the form of triples as</cell></row><row><cell>outputs from the input sentence.</cell></row><row><cell>Datasets We evaluate the performance of the</cell></row><row><cell>compared approaches on OIE benchmark datasets</cell></row><row><cell>including OIE2016 (Stanovsky and Dagan, 2016),</cell></row><row><cell>a dataset converted from QA-SRL (He et al.,</cell></row><row><cell>2015) based on Newswire and Wikipedia; three</cell></row><row><cell>datasets transformed from news corpus, in-</cell></row><row><cell>cluding NYT (de S? Mesquita et al., 2013),</cell></row><row><cell>WEB (de S? Mesquita et al., 2013), PENN (Xu</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 6 :</head><label>6</label><figDesc>Results on open information extraction.</figDesc><table><row><cell></cell><cell>TACRED</cell><cell>5-1</cell><cell>5-5</cell><cell cols="2">FewRel 1.0 10-1</cell><cell>10-5</cell></row><row><cell>BERTEM (Soares et al., 2019)</cell><cell>70.1</cell><cell>88.9</cell><cell>-</cell><cell></cell><cell>82.8</cell><cell>-</cell></row><row><cell>BERTEM+MTB (Soares et al., 2019)</cell><cell>71.5</cell><cell>90.1</cell><cell>-</cell><cell></cell><cell>83.4</cell><cell>-</cell></row><row><cell>DG-SpanBERT (Chen et al., 2020)</cell><cell>71.5</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>BERT-PAIR (Gao et al., 2019)</cell><cell></cell><cell>85.7</cell><cell cols="2">89.5</cell><cell>76.8</cell><cell>81.8</cell></row><row><cell>NLI-DeBERTa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 7 :</head><label>7</label><figDesc>Results on relation classification.</figDesc><table><row><cell>Google-RE</cell><cell>T-Rex</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 8 :</head><label>8</label><figDesc>Results on factual probe.</figDesc><table><row><cell></cell><cell cols="2">CoNLL04</cell><cell cols="2">ADE</cell><cell></cell><cell>NYT</cell><cell cols="2">ACE2005</cell></row><row><cell></cell><cell>Ent</cell><cell>Rel</cell><cell>Ent</cell><cell>Rel</cell><cell>Ent</cell><cell>Rel</cell><cell>Ent</cell><cell>Rel</cell></row><row><cell>SpERT (Eberts and Ulges, 2020)</cell><cell>88.9</cell><cell>71.5</cell><cell>89.3</cell><cell>78.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DyGIE (Luan et al., 2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.4</cell><cell>63.2</cell></row><row><cell>MRC4ERE (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 9 :</head><label>9</label><figDesc>Results on joint entity and relation extraction.</figDesc><table><row><cell></cell><cell></cell><cell>CoNLL03</cell><cell>OntoNotes</cell><cell>GENIA</cell><cell>ACE2005</cell></row><row><cell cols="2">BERT-MRC (Li et al., 2020a)</cell><cell>93.0</cell><cell>91.1</cell><cell>-</cell><cell>86.9</cell></row><row><cell cols="2">BERT-MRC+DSC (Li et al., 2020b)</cell><cell>93.3</cell><cell>92.1</cell><cell></cell><cell></cell></row><row><cell cols="2">Cloze-CNN (Baevski et al., 2019)</cell><cell>93.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">GSL (Athiwaratkun et al., 2020)</cell><cell>90.7</cell><cell>90.2</cell><cell></cell><cell></cell></row><row><cell cols="2">BiaffineLSTM (Yu et al., 2020b)</cell><cell>93.5</cell><cell>91.3</cell><cell>80.5</cell><cell>85.4</cell></row><row><cell cols="2">TANL (Paolini et al., 2021)</cell><cell>91.7</cell><cell>89.8</cell><cell>76.4</cell><cell>84.9</cell></row><row><cell cols="2">TANL (multitask) (Paolini et al., 2021)</cell><cell>91.7</cell><cell>89.4</cell><cell>76.4</cell><cell>-</cell></row><row><cell></cell><cell>zero-shot</cell><cell>44.4</cell><cell>42.5</cell><cell>47.2</cell><cell>28.1</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task</cell><cell>93.1</cell><cell>87.6</cell><cell>80.2</cell><cell>-</cell></row><row><cell></cell><cell>w/ finetune</cell><cell>93.0</cell><cell>87.8</cell><cell>80.8</cell><cell>86.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 10 :</head><label>10</label><figDesc>Results on named entity recognition.</figDesc><table><row><cell>CoNLL05 WSJ</cell><cell>CoNLL05 Brown</cell><cell>CoNLL12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 11 :</head><label>11</label><figDesc>Results on semantic role labeling.</figDesc><table><row><cell></cell><cell></cell><cell>Trigger Id</cell><cell>Trigger Cl</cell><cell>Argument Id</cell><cell>Argument Cl</cell></row><row><cell cols="2">J3EE (Nguyen and Nguyen, 2019)</cell><cell>72.5</cell><cell>69.8</cell><cell>59.9</cell><cell>52.1</cell></row><row><cell cols="2">DyGIE++ (Wadden et al., 2019)</cell><cell></cell><cell>69.7</cell><cell>55.4</cell><cell>52.5</cell></row><row><cell cols="2">TANL (Paolini et al., 2021)</cell><cell>72.9</cell><cell>68.4</cell><cell>50.1</cell><cell>47.6</cell></row><row><cell cols="2">TANL (multitask) (Paolini et al., 2021)</cell><cell>71.8</cell><cell>68.5</cell><cell>48.5</cell><cell>48.5</cell></row><row><cell>DEEPSTRUCT</cell><cell>multi-task w/ finetune</cell><cell>71.7 73.5</cell><cell>67.9 69.8</cell><cell>54.9 59.4</cell><cell>52.7 56.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 12 :</head><label>12</label><figDesc>Results on event extraction (ACE2005).</figDesc><table><row><cell></cell><cell></cell><cell>CoNLL12</cell><cell></cell></row><row><cell>MUC</cell><cell>B 3</cell><cell>CEAF ?4</cell><cell>Avg. F1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 13 :</head><label>13</label><figDesc>Results on coreference resolution.</figDesc><table><row><cell>MultiWOZ 2.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 14 :</head><label>14</label><figDesc>Results on dialogue state tracking.</figDesc><table><row><cell>ATIS</cell><cell>SNIPS</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 15 :</head><label>15</label><figDesc>Results on intent detection.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 17 :</head><label>17</label><figDesc>Analysis of recall errors of DEEPSTRUCT on CoNLL04 joint entity and relation extraction task. For each error type,</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/THUDM/GLM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/dair-iitd/OpenIE-standalone</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/amazon-research/tanl</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank the anonymous reviewers for their suggestions and comments. This material is in part based upon work supported by Berkeley DeepDrive and Berkeley Artificial Intelligence Research. Xiao Liu, Zui Chen, Haoyun Hong, and Jie Tang are supported by the NSFC for Distinguished Young Scholar (61825602) and NSFC (61836013).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">C?cero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">How context affects language models&apos; factual predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2019</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever. OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale multi-domain belief tracking with knowledge sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Osman Ramadan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="432" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-PKDD</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open information extraction from conjunctive sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swarnadeep</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COL-ING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2288" to="2299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ander Barrena, and Eneko Agirre. 2021. Label verbalization and entailment for effective zero and fewshot relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Sainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1199" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanya</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<editor>Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F?vry, Jason Alan Fries, Ryan Teehan</editor>
		<imprint>
			<pubPlace>Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
	<note>Rush. 2021. Multitask prompted training enables zero-shot task generalization. CoRR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Blank language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Quach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5186" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-shot information extraction as a unified text-to-triple translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyun</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1225" to="1238" />
		</imprint>
	</monogr>
	<note>Jie Tang, and Dawn Song</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language models are open knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Du</surname></persName>
		</author>
		<editor>Andrew M. Dai, and Quoc V. Le. 2021</editor>
		<imprint/>
	</monogr>
	<note>Finetuned language models are zero-shot learners</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia</title>
		<meeting><address><addrLine>PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transferable multi-domain state generator for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="808" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corefqa: Coreference resolution as querybased span prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6953" to="6963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open information extraction with tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="868" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zheng Zhang, and Xipeng Qiu. 2021. A unified generative framework for various NER subtasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/IJCNLP</title>
		<imprint>
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-task cross-lingual sequence tagging from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI-PAIS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2282" to="2289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Named entity recognition as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6470" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A relation-specific attention network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiannan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4054" to="4060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multiwoz 2.2 : A dialogue dataset with additional annotation corrections and state tracking baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sunkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11328" to="11339" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
