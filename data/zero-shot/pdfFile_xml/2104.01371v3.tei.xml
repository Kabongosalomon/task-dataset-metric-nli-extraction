<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convex Aggregation for Opinion Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayate</forename><surname>Iso</surname></persName>
							<email>hayate@megagon.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh ? Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolan</forename><surname>Wang</surname></persName>
							<email>xiaolan@megagon.ai</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh ? Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh ? Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
							<email>s.angelidis@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh ? Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
							<email>wangchiew@fb.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh ? Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megagon</forename><surname>Labs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh ? Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convex Aggregation for Opinion Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in text autoencoders have significantly improved the quality of the latent space, which enables models to generate grammatical and consistent text from aggregated latent vectors. As a successful application of this property, unsupervised opinion summarization models generate a summary by decoding the aggregated latent vectors of inputs. More specifically, they perform the aggregation via simple average. However, little is known about how the vector aggregation step affects the generation quality. In this study, we revisit the commonly used simple average approach by examining the latent space and generated summaries. We found that text autoencoders tend to generate overly generic summaries from simply averaged latent vectors due to an unexpected L 2 -norm shrinkage in the aggregated latent vectors, which we refer to as summary vector degeneration. To overcome this issue, we develop a framework COOP, which searches input combinations for the latent vector aggregation using input-output word overlap. Experimental results show that COOP successfully alleviates the summary vector degeneration issue and establishes new state-of-theart performance on two opinion summarization benchmarks. Code is available at https: //github.com/megagonlabs/coop. This is a great place to eat. The food is always fresh and the staff is very friendly. It's a great place to go if you are in the area. The food is always good and the prices are very reasonable.</p><p>This place is great. The food is good and the service is great. The staff is very friendly and attentive. They have a nice selection of drinks and the food is always fresh. The prices are very reasonable.</p><p>First time here and it was really good. The service was great, the food was delicious, and the portions were very large. This is a great place to go for Chinese food. This is a great place to eat. The food was delicious and the staff was very attentive. The catfish was tender and tasty. The hush puppies were amazing. The Mac and cheese was very good. They have a great beer selection as well.</p><p>Text space x avg <ref type="figure">Figure 1</ref>: Illustration of the latent space Z and text space X . The de facto standard approach in unsupervised opinion summarization uses the simple average of input review vectors z review (?) to obtain the summary vector z avg (?). The simply averaged vector z avg tends to be close to the center (i.e., has a small L 2 -norm) in the latent space, and a generated summary x avg (?) tends to become overly generic. Our proposed framework COOP finds a better aggregated vector to generate a more specific summary x COOP (?) from the latent vector z COOP (?). Input Reviews: Great service and clean restaurant. Tonkotsu was excellant. Nice thick broth and with a little chili oil really hit the spot. Gyoza was excellent and not overfried like some other places. Will return! &lt;/s&gt; I recommend the hachi special ramen, the broth was delicious and the noodles cooked just right. We also tried the chashu fried rice which we'll definitely be ordering again. &lt;/s&gt; This place is great! Small place but so good! The chef taught us about ramen and what he learned from studying ramen in japan! Really interesting! Definitely coming back!!! &lt;/s&gt; The best ramen in phoenix. They feature tonkotsu, miso and soyu flavored soups and delicious pork in ramen. The owner has trained in japan before coming to arizona and the quality rarely sway dramatically compared to other ramen restaurants when the owner is away. &lt;/s&gt; Best ramen i've had in phoenix for a very long time. Tradition tonkotsu ramen, shoyu, and a fantastic miso broth are on the menu. The goyza is perfect. &lt;/s&gt; Hachi ramen is delicious! It is just like being at a small ramen shop in japan. They focus on their broths creating complex and amazing flavors. I have tried two of the ramen flavors, their small plates and desserts and have been floored each time. This is the best ramen in the state and i highly recommend it. &lt;/s&gt; The food here was just ok. The broth was amazing, but my noodles weren't done right. Some were cooked perfectly but others were chewy. Probably will not come back &lt;/s&gt; Tonkatsu ramen is the bomb! No msg and broth is so good! The pork is melt in your mouth and not too fatty. The egg has a little infusion of soy, ginger marinade that is extra special! Owner talks to customers and takes great pride! I will be back! I'd take a picture but I ate it too fast! &lt;/s&gt; SimpleAvg z avg summary : This place is a great place to eat. The food is delicious and the staff is very friendly. They have a great selection of dishes and the prices are very reasonable. The service is good and the food is always fresh. It's a great place to go for lunch or dinner.</p><p>COOP z coop summary : Great service and delicious food. It's a small restaurant but the staff is very friendly and attentive. The ramen was delicious and the broth was really good. Will definitely be back.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The unprecedented growth of online review platforms and the recent success of neural summarization techniques <ref type="bibr" target="#b9">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b39">See et al., 2017;</ref><ref type="bibr" target="#b28">Liu and Lapata, 2019)</ref>, spurred significant interest in research on multi-document opinion summarization <ref type="bibr" target="#b3">(Angelidis and Lapata, 2018;</ref><ref type="bibr" target="#b10">Chu and Liu, 2019;</ref><ref type="bibr" target="#b7">Bra?inskas et al., 2020;</ref><ref type="bibr" target="#b40">Suhara et al., 2020;</ref><ref type="bibr" target="#b1">Amplayo and Lapata, 2020;</ref>. The goal of multi-document opinion summarization is to generate a summary that represents salient opinions in the input reviews. * Work done while at Megagon Labs.</p><p>Research on multi-document opinion summarization is challenging because of the lack of goldstandard summaries, which are difficult to collect at scale. This is in contrast to single-document summarization, where there exists an abundant annotated datasets <ref type="bibr" target="#b37">(Sandhaus, 2008;</ref><ref type="bibr" target="#b17">Hermann et al., 2015;</ref><ref type="bibr" target="#b36">Rush et al., 2015;</ref><ref type="bibr" target="#b31">Narayan et al., 2018)</ref>. Consequently, the primary approach is to employ text autoencoders for unsupervised opinion summarization <ref type="bibr" target="#b10">(Chu and Liu, 2019;</ref><ref type="bibr" target="#b7">Bra?inskas et al., 2020)</ref>. Text autoencoders, especially variational autoencoders (VAEs), are known for the ability to generate grammatical and consistent text by aggregating multiple latent vectors <ref type="bibr" target="#b6">(Bowman et al., 2016)</ref>. Unsupervised opinion summarization models leverage this property to generate a summary by first aggregating the latent vectors of input reviews via simple average, and then decoding the summary from the aggregated vector.</p><p>However, it has not been verified if the simple average is the best choice for summary generation. Furthermore, little is known about the relationship between the latent vector and the generation quality. In this paper, we report that text autoencoder models with the simple average vector aggregation tend to generate overly generic summaries, which we refer to as summary vector degeneration. For example, as shown in <ref type="figure">Figure 1</ref>, with simply averaged latent vectors, the generated summaries of two distinct entities are almost identical. We further discovered two factors that cause summary vector degeneration: (1) simply averaged latent vectors cause unexpected L 2 -norm shrinkage, and (2) latent vectors with smaller L 2 -norm are decoded into less informative summaries (e.g., contain only general information.)</p><p>To address the summary vector degeneration issue, we develop COOP, a latent vector aggregation framework. In essence, COOP considers convex combinations of the latent vectors of input reviews for better summary generation. More specifi-cally, we focus on searching for a convex combination that maximizes the input-output word overlap between input reviews and a generated summary. This optimization strategy helps the model generate summaries that are more consistent with input reviews, thus improving the quality of summarization for unsupervised opinion summarization models.</p><p>Our contributions are summarized as follows:</p><p>? We report that the commonly used simple average vector aggregation method causes summary vector degeneration, which makes the decoder generate less informative and overly generic summaries. ? We formalize latent vector aggregation as an optimization problem, which considers the convex combination of input review latent vectors. We propose a solution, COOP, to approximate the optimal latent vector with linear time complexity. To the best of our knowledge, this is the first work that optimizes latent vector aggregation for opinion summarization. ? We conduct comparative experiments against existing methods <ref type="bibr" target="#b10">(Chu and Liu, 2019;</ref><ref type="bibr" target="#b7">Bra?inskas et al., 2020)</ref>, which implement more sophisticated techniques. Our experiments demonstrate that by coupling with COOP, two opinion summarization models (BIMEANVAE and Optimus) establish new state-of-the-art performance on both Yelp and Amazon datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Let us denote R = {x i } |R| i=1 as a dataset of customer reviews of the same domain (e.g., restaurant or product), where each review is a sequence of words x = (x 1 , ..., x ?x? ) in the text space X . Given an entity e and its reviews R e ? R, the goal of the multi-document opinion summarization task is to generate an abstractive summary s e such that the salient opinions in R e are included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Opinion Summarization</head><p>Existing unsupervised opinion summarization models <ref type="bibr" target="#b10">(Chu and Liu, 2019;</ref><ref type="bibr" target="#b7">Bra?inskas et al., 2020)</ref> use the autoencoder, where an encoder E ? X ? ? Z mapping from the text space X to latent space Z, and a decoder G ? Z ? ? X that generates texts from latent vectors. Encoder E: Given an entity e and its reviews R e , the encoder E essentially maps every review x i ? R e into the latent space: z i = E(x i ), where z i is the latent vector of review x i . Decoder G: The other core component is the decoder G, which generate a new textx = (x 1 , ...,x ?x? ) from a given latent vector z:x = G(z). Training: At the training phase, the autoencoder model is trained to generate the review. While various training methods have been proposed, the simplest approach is aimed to reconstruct the input review from the corresponding latent vector. Generation: At the generation phase, given a set of input review latent vectors Z e = {z 1 , ..., z |R e | }, existing opinion summarization models use simple average to create the latent vector of the summary (summary vector) z avg summary = 1 |R e | ? |R e | i=1 z i , which is then decoded into the summary. In this paper, our focus is to analyze and improve the latent vector aggregation for the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational Autoencoders</head><p>In this study, we use variational autoencoders (VAEs) as the text autoencoder since it provides a smooth latent space, which allows to produce grammatical and consistent text from aggregated latent vectors <ref type="bibr" target="#b23">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b6">Bowman et al., 2016)</ref>. More specifically, we tested two VAE variations, namely BIMEANVAE and Optimus . BIMEANVAE uses bidirectional LSTM as the encoder, LSTM as the decoder, and applies a mean pooling layer to the BiLSTM layer to obtain the latent vector. OPTI-MUS  is a Transformer-based VAE model that uses BERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> as the encoder and GPT-2 <ref type="bibr" target="#b34">(Radford et al., 2019)</ref> as the decoder. Unlike MeanSum <ref type="bibr" target="#b10">(Chu and Liu, 2019)</ref>, both BIMEANVAE and Optimus do not use any additional objectives but the basic VAE objective (i.e., the reconstruction loss with KL regularization):</p><formula xml:id="formula_0">L(?, ?) = L rec + ?L KL L rec (?, ?) = ?E q ? (z|x) [log p ? (x|z)] L KL (?) = D KL (q ? (z|x)?p ? (z)),</formula><p>where ? and ? are the parameters of the encoder E and decoder G. ? is a hyper-parameter that controls the strength of the KL regularization L KL (?). We choose the standard Gaussian distribution N (0, I) as the prior distribution p ? (z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revisiting Simple Average Approach</head><p>In this section, we revisit the commonly used simple average approach (SimpleAvg) and examine the relations between the aggregated latent vector and the quality of generated summaries.</p><p>Taking a simple average is an intuitive way to optimize the aggregated vector in the latent space since it minimizes the total distance between input latent vectors and the aggregated vector. Thus, it appears to be a reasonable design choice and has been adopted by multiple unsupervised opinion summarization models as de-facto standard.</p><p>However, we find that only considering the total distance between the input and the aggregated latent vectors does not always render high-quality summaries. This is because SimpleAvg is completely ignorant of the decoder performance and the resulting generation. In fact, we observe that SimpleAvg tends to produce overly generic summaries (as shown in <ref type="figure">Figure 1</ref>), which we refer to as summary vector degeneration. To gain a better understanding of the summary vector degeneration problem, we conducted further analysis and discovered two factors that cause this problem: (1) simply averaging input latent vectors causes L 2 -norm shrinkage, and (2) latent vectors with smaller L 2 -norm tend to be decoded into less informative generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">L 2 -norm Shrinkage in Latent Space</head><p>To understand how simply averaged latent vectors distribute in the latent space, we compared the L 2norm of the latent vectors of input reviews and summary vectors created by SimpleAvg. We conducted analysis using BIMEANVAE on two review datasets, Yelp and Amazon.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the average L 2 -norm of the summary vectors significantly shrinks from 9.97 to 4.10 on Yelp (10.15 to 4.17 on Amazon) as the number of input reviews is increased from 1 (i.e., individual reviews) to 8. The results show that simply averaging multiple latent vectors can cause L 2 -norm shrinkage of the summary vector. As we expect each dimension in the latent space to represent a distinct semantics, L 2 -norm shrinkage may cause some information loss in the summary vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Summary Vector Degeneration</head><p>To investigate the effect of L 2 -norm shrinkage in the latent space, we further analyzed the quality of generated text for each latent vector and conducted correlation analysis against the L 2 -norm. We used two metrics to assess the quality of generated text: (a) text length and (b) information amount. For the information amount, we trained an autoregressive model (RNN-LM) on each dataset and used negative log probabilities of generated summaries (i.e., a higher value means more amount of infor-  <ref type="figure">Figure 3</ref> shows that the L 2 -norm of latent vectors is highly correlated with (a) generated text length and (b) information amount. The results support that latent vectors with smaller (larger) L 2norm are decoded into less (more) informative text. Therefore, we confirm that the commonly used SimpleAvg is a suboptimal solution for latent vector aggregation as it tends to cause summary vector degeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Convex Aggregation in Latent Space</head><p>As discussed above, there are two limitations with the de-facto standard SimpleAvg. First, it causes summary vector degeneration. Second, it is ignorant of the decoder generation (in the text space X ) for a summary vector (in the latent space Z.)</p><p>To address the issues, we consider an optimization problem that searches for the best combination of the latent vectors of input reviews that maximizes the alignment between input reviews and generated summaries. We restrict the search space to the convex combinations of input review representations, so the contribution of each input review is always zero or positive. This is based on the assumption that each review in the input set should 1 Training details are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input reviews</head><p>Generated summary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coop</head><p>Input-output word overlap Latent space <ref type="figure">Figure 4</ref>: COOP searches convex combinations of the latent vectors of input reviews based on the inputoutput word overlap between a generated summary and input reviews. ? denotes the simply averaged vector.</p><p>be either ignored or reflected. Hence, we refer to the latent representation aggregation problem as convex aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">COOP: Convex Aggregation for Opinion Summarization</head><p>We develop a latent vector aggregation framework COOP to solve the convex aggregation problem in <ref type="figure">Figure 4</ref>. COOP optimizes for the input-output word overlap between a generated summary and the input reviews:</p><formula xml:id="formula_1">maximize z Overlap(R e , G(z)) subject to z = |R e | i=1 w i z i |R e | i=1 w i = 1, ?w i ? R + .</formula><p>The input-output word overlap (Overlap) evaluates the consistency between input reviews and a generated summary, and it can naturally penalize hallucinated generations. Specifically, we use the ROUGE-1 F1 score as the input-output word overlap metric <ref type="bibr" target="#b27">(Lin, 2004)</ref> 2 . Note that the method does not use gold-standard summaries or any information in the test set but uses the input reviews for calculating word overlap information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Space</head><p>Following the intuition that some input reviews are useful and others are not, we narrow down the search space to the power set of an input review set R e . The summary vector is then calculated as the average of the latent representations of the "selected" reviews: z</p><formula xml:id="formula_2">power summary = 1 |R ? e | ? |R ? e | i=1 z i , where R ? e ? 2</formula><p>R e \ {?} is non-empty subsets in the power set 2 R e . We also tested black-box optimization techniques <ref type="bibr" target="#b4">(Audet and Hare, 2017)</ref> to search the entire continuous space, but we did not observe improvements despite the extra optimization cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Dataset: For our experiments, we used two publicly available datasets, Yelp <ref type="bibr" target="#b10">(Chu and Liu, 2019)</ref> and Amazon <ref type="bibr" target="#b7">(Bra?inskas et al., 2020)</ref>. Besides reviews used for training, these two datasets also contain gold-standard summaries for 200 and 60 sampled entities, respectively. For both datasets, the summaries are manually created from 8 input reviews, and we used the same dev/test split, 100/100 for Yelp and 28/32 for Amazon, released by their authors for our experiments.</p><p>Experimental settings: We used Adam optimizer (Kingma and Ba, 2015) with a linear scheduler, whose initial learning rate is set to 10 ?3 (10 ?5 )</p><p>for BIMEANVAE (Optimus.) To mitigate the KL vanishing issue, we also applied KL annealing during the training <ref type="bibr" target="#b22">(Kingma et al., 2016;</ref><ref type="bibr" target="#b15">Fu et al., 2019;</ref>. For generation, we used beam search with a size of 4. In order to generate summary-like texts, we introduce a technique, first-person pronoun blocking, that prohibits to generate first-person pronouns (e.g., I, my, me) during summary generations. We report the ROUGE-1/2/L F1 scores for the automatic evaluation <ref type="bibr" target="#b27">(Lin, 2004)</ref> 3 .</p><p>Comparative methods: We compared our models (i.e., BIMEANVAE and Optimus with COOP) against state-of-the-art opinion summarization models that use SimpleAvg for latent vector aggregation, namely TextVAE (Bowman et al., 2016), MeanSum <ref type="bibr" target="#b10">(Chu and Liu, 2019)</ref>. We also coupled BIMEANVAE and Optimus with SimpleAvg to verify the effectiveness of COOP. In addition, we report the performance of other abstractive, extractive or weakly-supervised opinion summarization models.</p><p>Besides the unsupervised summarization models, we also report two types of oracle methods. Oracle (single): This oracle method selects a single input review that takes the highest ROUGE-1 3 https://pypi.org/project/py-rouge/ F1 score on the gold-standard summary. Oracle (comb.): This oracle method selects the best set of input reviews from the power set 2 R e \ {?} of input review set R e so that it achieves the highest ROUGE-1 F1 score on the gold-standard summary when BIMEANVAE is used as the summarization model. This can also be interpreted as the upper-bound performance of BIMEANVAE.</p><p>More details about our evaluation can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Automatic Evaluation</head><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, COOP is able to improve both summarization models, BIMEANVAE and Optimus, by a large margin. With COOP, BIMEANVAE and Optimus obtain the new stateof-the-art performance on both benchmark datasets. Besides the summarization performance, we also show the model sizes in <ref type="table" target="#tab_0">Table 1</ref>. Note that BIMEANVAE performs competitively well against Optimus, which is trained on top of large pretrained language models and has approximately 20x more parameters than BIMEANVAE. We believe this is due to the simple yet important configuration in the model architecture, which uses a BiLSTM encoder (vs. unidirectional LSTM in TextVAE) and a mean-pooling layer (vs. last hidden state).</p><p>Meanwhile, BIMEANVAE and Optimus with COOP outperforms Oracle (single), which selects the single review that takes the highest ROUGE score. The results indicate that our aggregation framework takes the quality of unsupervised multidocument opinion summarization to the next stage.</p><p>It is worthwhile to note that both VAE variations with the conventional simple average aggregation competitively perform well against the state-of-the-art performance on opinion summarization benchmarks as shown in <ref type="table" target="#tab_0">Table 1</ref>. In contrast to previous study <ref type="bibr" target="#b7">(Bra?inskas et al., 2020)</ref>, which showed that text VAE performs poorly on the opinion summarization, our modified configuration makes BIMEANVAE a competitive baseline for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluation</head><p>We conducted human evaluation to assess the quality of generated summaries. More specifically, we collected the generated summaries for entities in the Yelp test set with four different models, COOP (BIMEANVAE), SimpleAvg  we asked three human judges to evaluate the summaries with two criteria: informativeness and content support.</p><p>We first asked human judges to evaluate the informativeness of the generated summaries by the Best-Worst Scaling <ref type="bibr" target="#b29">(Louviere et al., 2015)</ref>, which scores each summarization method with values ranging from -100 (unanimously worst) to +100 (unanimously best). We then asked human judges to evaluate the content support of the generated summaries. For each sentence in the generated summary, the judges chose an option from (a) fully supported, (b) partially supported, or (c) not.</p><p>We present the human evaluation results in Table 2. As shown, summaries generated by COOP are more informative than SimpleAvg 4 and the other baseline models. Meanwhile, COOP also behaves well on content support as it generates more sentences with full/partial content support than the other methods. These results indicate that COOP is able to generate more informative and less hallucinated summaries. Combined with the automatic evaluation results, we conclude that COOP meaningfully improves the quality of summarization generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section, we conduct a series of additional analysis to verify the effectiveness and efficiency of COOP. We also provide detailed descriptions of the setups and additional analysis in the Appendix.  <ref type="figure">Figure 5</ref>: L 2 -norm distributions of latent vectors of input reviews and aggregated vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Summary Vector Analysis</head><p>Does COOP avoid L 2 -norm shrinkage? To verify if COOP alleviates the summary vector degeneration issue, we compare the L 2 -norm distributions of the summary vectors by SimpleAvg, COOP, Oracle (comb.), and the original reviews (None.) We used BIMEANVAE for SimpleAvg and COOP as the base models. <ref type="figure">Figure 5</ref> shows COOP does not show severe L 2norm shrinkage compared to SimpleAvg. However, the distributions of any aggregation methods, including Oracle, show smaller means of L 2 -norm compared to individual reviews. This is expected, as customer reviews often contain irrelevant (and specific) information that is not suitable for summaries (e.g., personal experience.) Therefore, just preserving the L 2 -norm of input latent vectors does not necessarily lead to high-quality summary vectors.</p><p>We confirm that COOP has a similar distribution to that of Oracle (comb.), which achieves the upper bound performance of COOP. The results indicate that COOP successfully excludes input reviews that contain too much irrelevant information, so it can create high-quality summary vectors without accessing any gold-standard summaries. How good is COOP's summary vector? We verify how good COOP's summary vector selections are with respect to summary generation quality. Specifically, we sorted all summary vector candidates in power set 2 R e \ {?} by the ROUGE-1 score using the generated summary and goldstandard summaries. By doing so, we can use the position of COOP's selection to evaluate the summary vector quality using ranking metrics. We iterated the process for each entity e and used two metrics, namely Mean Reciprocal Rank (MRR) and normalized discounted cumulative gain (nDCG) <ref type="bibr" target="#b38">(Sch?tze et al., 2008)</ref>, for the evaluation.   We conducted the analysis using BIMEANVAE on the test set. We also evaluated random selection and simple average (i.e., selecting all input reviews.) As shown in <ref type="table" target="#tab_3">Table 3</ref>, COOP's selection is significantly better than that of the other methods on both of the benchmarks. This confirms that COOP can find good summary vectors that are decoded into high-quality summaries. According to the MRR values, COOP selects the 7-8th bestranked combination (out of 255 candidates) on average.The summary quality by the simple average is marginally better (worse) than random selection on Yelp (Amazon.) This is aligned with our findings and discussions in ?3, and it further clarifies the negative effects of summary vector degeneration caused by SimpleAvg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Approximate Search</head><p>While we further narrowed down the search space of COOP into power set in ?4.2, the brute-force search becomes intractable for a larger number of input reviews N . Therefore, we tested approximate search algorithms for efficient and effective search.</p><p>The simplest solution is the greedy search, which begins from a single review and progressively adds a review that offers the highest gain in the objective value. The greedy search can be easily generalized to beam search, which stores k candidates for each step. We also consider the "inverse" version of the search algorithms, which begins from all input reviews and removes a review that offers the highest gain by excluding the input review step by step.  We refer to the original and the inverse versions as forward and backward, respectively. <ref type="figure" target="#fig_3">Figure 6</ref> reports the ROUGE-1 performance of BIMEANVAE with COOP using approximate search on the Yelp and Amazon datasets 5 . The greedy search (beam size = 1) still outperforms the SimpleAvg baseline, and increasing the beam size further improves the performance of both search methods. Thus, we confirm that COOP framework still can provide significant gains by using approximate search when the input size is too large to conduct the exact search for the entire space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Input-output Word Overlap</head><p>To investigate the effectiveness of the input-output word overlap as an intrinsic metric, we analyze the Spearman's rank correlation between the inputoutput word overlap and the ROUGE scores on the gold-standard summaries for each summary vector in the search space 2 R e \ {?}.</p><p>Interestingly, as shown in <ref type="table" target="#tab_5">Table 4</ref>, the two datasets show different trends. In contrast to the Amazon dataset, where the input-output word overlap shows high correlation values against the ROUGE scores, the correlation values on the Yelp dataset are much smaller. The results confirm that the effect of the input-output word overlap is not just because it is correlated with ROUGE scores between a generated summary and the gold-standard summaries. <ref type="figure">Figure 7</ref> shows an example of generated summaries using BIMEANVAE with the SimpleAvg and COOP for reviews about a restaurant in the Yelp dataset. This example shows that the summary generated from the SimpleAvg z avg summary contains general opinions (e.g., "the food is delicious."). In contrast, COOP effectively chose a subset of reviews to generate a summary vector z coop summary , which was decoded into a more specific summary. 5 ROUGE-2/L are shown in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Opinion Summarization Multi-document opinion summarization uses the unsupervised approach as it is difficult to collect a sufficient amount of gold-standard summaries for training. Previously, the common approach was extractive summarization, which selects sentences based on the centrality criteria <ref type="bibr" target="#b14">(Erkan and Radev, 2004)</ref>. Due to the recent advances in neural network models, unsupervised abstractive summarization techniques have become the mainstream for opinion summarization.</p><p>Most abstractive unsupervised opinion summarization techniques use a two-stage approach that trains an encoder-decoder model based on the reconstruction objective and generates a summary from the average latent vectors of input reviews using the trained model <ref type="bibr" target="#b10">(Chu and Liu, 2019)</ref>. <ref type="bibr" target="#b1">Amplayo and Lapata (2020)</ref> and  have expanded the approach by creating pseudo review-pairs to train a summarization model.</p><p>Our study revisits this two-stage approach and develop a latent vector aggregation framework, which can be combined with a variety of opinion summarization models.</p><p>Variational Auto-Encoder The VAE is a variant of auto-encoder that learns a regularized latent space. The text VAE <ref type="bibr" target="#b6">(Bowman et al., 2016)</ref>, VAE with autoregressive decoder, has been commonly used for various NLP tasks including text generation <ref type="bibr" target="#b43">(Ye et al., 2020)</ref>, paraphrase generation <ref type="bibr" target="#b5">(Bao et al., 2019)</ref> and text style transfer <ref type="bibr" target="#b18">(Hu et al., 2017;</ref><ref type="bibr" target="#b20">John et al., 2019)</ref>. In contrast to the success of the text VAE in NLP tasks, an earlier attempt for using the text VAE for opinion summarization was not successful; <ref type="bibr" target="#b7">Bra?inskas et al. (2020)</ref> showed that the performance of text VAE was significantly lower than the other baselines. In this paper, we devise BIMEANVAE, a simple variant of the text VAE, which performs competitively well against the previous state-of-theart methods even when coupled with SimpleAvg.</p><p>Recently, a more expressive text VAE model Optimus , which is built on top of pretrained BERT and GPT-2 models, has been developed. The model was originally developed for sentence generation tasks, and we are the first to combine it with a latent vector aggregation framework for unsupervised opinion summarization tasks. <ref type="figure">Figure 7</ref>: Example of summaries generated by BIMEANVAE with SimpleAvg and COOP for reviews about a product on the Yelp dataset. The colors denote the corresponding opinions, and struck-through reviews in gray were not selected by COOP for summary generation (Note that SimpleAvg uses all the input reviews.) Terms that are more specific to the entity are underlined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we revisit the unsupervised opinion summarization architecture and show that the commonly used simple average aggregation is suboptimal since it causes summary vector degeneration and does not consider the difference in the quality of input reviews or decoder generations.</p><p>To address the issues, we develop a latent vector aggregation framework COOP, which searches convex combinations of the latent vectors of input reviews based on the word overlap between input reviews and a generated summary. The strategy helps the model generate summaries that are more consistent with input reviews. To the best of our knowledge, COOP is the first framework that tackles the latent vector aggregation problem for opinion summarization.</p><p>Our experiments have shown that with COOP, two summarization models, BIMEANVAE and Optimus, establish new state-of-the-art performance on two opinion summarization benchmarks. The results demonstrate that our aggregation framework takes the quality of unsupervised opinion summarization to the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Preparation</head><p>Source Dataset: For the Yelp dataset, we used reviews provided in the Yelp Open Dataset 6 . For</p><p>Amazon dataset, we used reviews provided in the Amazon product review dataset <ref type="bibr" target="#b16">(He and McAuley, 2016)</ref>, and we select 4 categories: Electronics; Clothing, Shoes and Jewelry, Home and Kitchen; Health and Personal Care. Preprocessing: We restricted the character set to be ASCII printable for the experiments. We preprocessed the datasets by excluding non-ASCII characters from the reviews and by removing accents from accented characters. Tokenizer:</p><p>For BIMEANVAE, we used sentencepiece <ref type="bibr" target="#b24">(Kudo and Richardson, 2018)</ref> 7 to train a BPE tokenizer with a vocabulary size of 32k, a character coverage of 100%, and a max sentence length of 8,192. For Optimus, we used the pre-trained tokenizers provided by transformers <ref type="bibr" target="#b42">(Wolf et al., 2020)</ref> 8 .</p><p>Since Optimus uses different models for the encoder and decoder, we used different tokenizers for the encoder (bert-base-cased) and the decoder (gpt2), respectively. Training data: We used pre-defined training sets of Yelp and Amazon with additional filtering. We filtered out reviews that consist of more than 128 tokens after tokenization by the BPE tokenizer trained for BIMEANVAE. As a result, the training sets contain 3.8 million and 13.3 million reviews in Yelp and Amazon respectively. We further excluded entities that have less than 10 reviews. The basic statistics of the training data after those filtering steps are shown in <ref type="table" target="#tab_6">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yelp Amazon</head><p># of entity 75,988 244,652 # of reviews 4,658,968 13,053,202 a batch size of 256. The embedding size and the hidden size are set to 512 and the output vocabulary layer is tied with the input embedding layer <ref type="bibr" target="#b33">(Press and Wolf, 2017;</ref><ref type="bibr" target="#b19">Inan et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Additional analysis on latent vector and input text quality</head><p>In ?3, we investigated the relationship between L 2 -norm of latent vectors and the generated text qualities, and found strong positive correlations. In addition to the generated reviewsx, we conduct the same analysis using input reviews x. As shown in <ref type="figure" target="#fig_4">Figure 8</ref>, we confirm the same trends that L 2 -norm of latent vectors is highly correlated with both metrics. Thus, we confirm that less (more) informative text tends to be embedded closer to (more distant from) the origin in the latent space.    To be specific, we tested two KL annealing strategies to control the ? value, i.e., cyclical KL annealing <ref type="bibr" target="#b15">(Fu et al., 2019)</ref> and pretrain-then-anneal with KL thresholding (aka FreeBits) <ref type="bibr" target="#b22">(Kingma et al., 2016;</ref>.</p><p>The cyclic KL annealing repeats the monotonic annealing process of the ? parameter from 0 to 1 multiple times <ref type="bibr" target="#b15">(Fu et al., 2019)</ref>. The pretrainthen-annealing approach has two steps . The first step pre-trains an autoencoder model with the ? parameter fixed to 0. The second step re-initializes the decoder parameter and trains the model with the ? parameter monotonically increased from 0 to 1. In addition to the annealing schedule, we also searched a threshold hyper-parameter for the KL value to control the strength of the KL regularization <ref type="bibr" target="#b22">(Kingma et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Baseline Models</head><p>We considered multiple abstractive and extractive summarization models. <ref type="bibr">TextVAE (Bowman et al., 2016)</ref>: A vanilla text VAE model that has a unidirectional LSTM layer and uses the last hidden state to calculate the parameters of the posterior distribution. The model was tested in <ref type="bibr" target="#b7">Bra?inskas et al. (2020)</ref> but performed poorly.</p><p>MeanSum <ref type="bibr" target="#b10">(Chu and Liu, 2019)</ref>: An unsupervised multi-document abstractive summarization method that minimizes a combination of the reconstruction and similarity loss.</p><p>CopyCat <ref type="bibr" target="#b7">(Bra?inskas et al., 2020)</ref>: An unsupervised opinion summarization model. CopyCat incorporates an additional latent vector c to model an entire review set R e in addition to latent vectors for individual reviews. This hierarchical modeling enables CopyCat to consider both global (entity-level) and local (review-level) information to calculate a latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Performance on Development Set</head><p>We report the performance on the development set in <ref type="table" target="#tab_8">Table 6</ref>. COOP consistently improve the performance on ROUGE scores (except for ROUGE-L on Yelp) on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Baselines for summary vector degeneration</head><p>In this paper, we develop a latent vector aggregation framework based on the input-output word-overlap to address the summary vector degeneration problem. As alternative and reasonable solutions, we tested the following methods and confirm that none of them consistently outperforms SimpleAvg, as shown in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>Extractive This method uses an extractive summarization technique to select k input reviews that best represent the input review set. In the analysis, we used LexRank and set k = 4, which was chosen based on the Oracle (comb.) performance on the dev set. We used the simple average for the latent representation aggregation.</p><p>Inverse-Variance Weighting Weighted average based on importance scores of input reviews is an alternative way for latent representation aggregation. Specifically, we consider the variance parame-  ter of posterior diag ? 2 as the importance score of each input review. We found that generic reviews (i.e., reviews that do not describe entity-specific information) tend to have large variance parameters. To reduce the influence by such kind of generic input reviews, we use the inverse-variance weighting <ref type="bibr" target="#b11">(Cochran, 1954)</ref> to assign larger weights to input reviews that contain more entity-specific information:</p><formula xml:id="formula_4">z ivw summary = ? N e i=1 ? ?1 i ?1 ? N e i=1 ? ?1 i z i .</formula><p>Policy Gradient We also used reinforcement learning to optimize the convex aggregation problem. In particular, we used the self-critical policy gradient <ref type="bibr">(PG;</ref><ref type="bibr" target="#b35">Rennie et al., 2017;</ref><ref type="bibr" target="#b32">Paulus et al., 2018)</ref> to search the weights of input reviews:</p><formula xml:id="formula_5">L PG = (r(y s ) ? r(?)) ? |y s | t=1 log p(y s t |y s &lt;t , R e ))</formula><p>, where the reward function r is the input-output word overlap described in Section 4. For each entity, we froze the encoder-decoder parameters and trained the input review weights with L PG for 10 epochs with an initial learning rate 10 ?2 .</p><p>Re-scaling The last baseline approach is to rescale the aggregated latent vector. Specifically, we first normalize the averaged latent vector z avg summary (Section 2) and then re-scale the normalized vector with a constant value ? ? {1, 5, 10}: We describe the details of the ranking metrics used in ?6.1.  Mean Reciprocal Rank (MRR):</p><formula xml:id="formula_6">MRR = 1 |Z test agg | z?Z test agg 1 rank(z) ,</formula><p>Normalized Discounted Cumulative Gain (nDCG):</p><formula xml:id="formula_7">nDCG = 1 |Z test agg | z?Z test agg 1 log 2 (rank(z) + 1) ,</formula><p>where Z test agg is the set of summery vectors for each aggregation method on test set, and rank(z) denotes the rank of the summary vector z selected by the respective aggregation method in the search space 2 R e \ {?}.</p><p>Runtime (text/sec) Amazon BiMeanVAE Optimus </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Approximate Search</head><p>In addition to the ROUGE-1 scores shown in the main paper, we show the approximated search performance for ROUGE-2/L scores in <ref type="figure" target="#fig_7">Figure 9</ref>. We observe that the ROUGE-2 score shows that backward search is better for the Yelp dataset and forward search is better for the Amazon dataset, similar to the ROUGE-1 score. In contrast to ROUGE-1/2, the ROUGE-L score shows that backward search is always better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Runtime Analysis</head><p>We report the inference runtime of BIMEANVAE and Optimus in <ref type="figure" target="#fig_8">Figure 10</ref>. For the Yelp/Amazon data, BIMEANVAE can generate 220.17/173.59 reviews/sec on average, while Optimus can generate only 0.68/1.16 reviews/sec on average. Optimus is a huge model that uses BERT as the encoder and GPT-2 as the decoder, which makes it more computationally expensive than BIMEANVAE. Nevertheless, the inference time is still acceptable for running summarizers in batch processing. However, due to the GPU memory size limitation, it becomes infeasible for Optimus to take a batch size of more than 8, while BIMEANVAE can process much larger batches within a reasonable time.</p><p>Input Reviews: I usually wear size 37, but found a 38 feels better in this sandal. I absolutely love this sandal. So supportive and comfortable, although at first I did get a blister on my big toe. Do not let this be the deciding factor. It stretched out and is now fabulous. I love it so much that I bought it in three colors. &lt;/s&gt; This is a really cute shoe that feels very comfortable on my high arches. The strap on the instep fits my feet very well, but I have very slim feet. I can see how it would be uncomfortably tight on anyone with more padding on their feet. &lt;/s&gt; I love these sandals. The fit is perfect for my foot, with perfect arch support. I don't think the leather is cheap, and the sandals are very comfortable to walk in. They are very pretty, and pair very well with pants and dresses. &lt;/s&gt; My wife is a nurse and wears dansko shoes. We were excited to try the new crimson sandal and normally order 39 sandal and 40 closed toe. Some other reviews were right about a narrow width and tight toe box. We gave them a try and passed a great pair of shoes to our daughter with her long narrow feet, and she loves them... &lt;/s&gt; Finally, a Dansko sandal that's fashion forward! It was love at first sight! This is my 4th Dansko purchase. Their sizing, quality and comfort is very consistent. I love the stying of this sandal and I'm pleased they are offering bolder colors. Another feature I love is the Dri-Lex topsole -it's soft and keeps feet dry. &lt;/s&gt; I really love these sandals. my only issue is after wearing them for a while my feet started to swell as I have a high instep and they were a little tight across the top. I'm sure they will stretch a bit after a few wears &lt;/s&gt; I have several pairs of Dansko clogs that are all size 39 and fit perfectly. So I felt confident when I ordered the Tasha Sandal in size 39. I don't know if a 40 would be too large but the 39 seems a little small. Otherwise, I love them. They are very cushiony and comfortable! &lt;/s&gt; I own many Dansko shoes and these are among my favorites. They have ALL the support that Dansko offers in its shoes plus they are very attractive. I love the the heel height and instant comfort. They look great with slacks and dresses, dressed up or not... &lt;/s&gt; SimpleAvg z avg summary : This is a great shoe. It is very comfortable, and the fit is perfect. The only issue is that it's a little big on the toe area, but it's not a problem. It is very comfortable to wear and it's very comfortable.</p><p>COOP z coop summary : This is a very nice sandal that is comfortable and supportive. The only problem is that the straps are a little tight in the toe area, but it's not a problem. They are very comfortable and look great with a pair of shoes and dress shoes. Love them! <ref type="figure">Figure 11</ref>: Example of summaries generated by BIMEANVAE with SimpleAvg and COOP for reviews about a product on the Amazon dataset. The colors denote the corresponding opinions, and struck-through reviews in gray were not selected by COOP for summary generation (Note that SimpleAvg uses all the input reviews.) Terms that are more specific to the entity are underlined. Red and struck-through text denotes hallucinated content that has the opposite meaning compared to the input.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Average L 2 -norm of simply averaged summary vectors for different number of input reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>L 2 -norm vs. information amount of generated text Figure 3: Correlation analysis of the L 2 -norm of latent vectors ?z? and the generated text quality: (a) text length and (b) information amount. mation) (Brown et al., 1992; Mielke et al., 2019) 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>ROUGE-1 scores of COOP with approximate search in different configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>L 2 -norm vs. information amount of input text Illustrations of the relationships between L 2norm of latent vectors ?z? and the input review quality: (a) text length and (b) the information content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Approximated search performance of ROUGE-2/L scores with different batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>The inference runtime of BIMEANVAE and Optimus with different batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>BIMEANVAE), CopyCat and PlanSum. Then, ROUGE scores on the benchmarks. Bold-faced and underlined denote the best and second-best scores respectively. COOP significantly improves the performance of two summarization models, BIMEANVAE and Optimus, and achieves new state-of-the-art performance on both of the benchmark datasets. ? means the results are copied from Bra?inskas et al. (2020), ? from Amplayo et al. (2021), ? from Wang et al. (2020), ? from Angelidis et al. (2021), and ? from<ref type="bibr" target="#b40">Suhara et al. (2020)</ref>. Note that this study classifies OpinionDigest and PlanSum as weakly-supervised summarizers since they use additional information other than review text.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Yelp</cell><cell>Amazon</cell></row><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>R1</cell><cell>R2</cell><cell>RL</cell><cell>#Param</cell></row><row><cell></cell><cell cols="2">COOP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">BIMEANVAE</cell><cell>35.37</cell><cell>7.35</cell><cell>19.94 36.57 7.23 21.24</cell><cell>13M</cell></row><row><cell></cell><cell></cell><cell>Optimus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>33.68</cell><cell>7.00</cell><cell>18.95 35.32 6.22 19.84</cell><cell>239M</cell></row><row><cell></cell><cell cols="2">SimpleAvg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">BIMEANVAE</cell><cell>32.87</cell><cell>6.93</cell><cell>19.89 33.60 6.64 20.87</cell><cell>13M</cell></row><row><cell></cell><cell></cell><cell>Optimus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31.23</cell><cell>6.48</cell><cell>18.27 33.54 6.18 19.34</cell><cell>239M</cell></row><row><cell></cell><cell></cell><cell cols="2">TextVAE</cell><cell>?</cell><cell></cell><cell></cell><cell>25.42</cell><cell>3.11</cell><cell>15.04 22.87 2.75 14.46</cell><cell>13M</cell></row><row><cell></cell><cell></cell><cell cols="3">MeanSum</cell><cell>?</cell><cell></cell><cell>28.46</cell><cell>3.66</cell><cell>15.57 29.20 4.70 18.15</cell><cell>28M</cell></row><row><cell></cell><cell cols="2">Abstractive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CopyCat</cell><cell cols="2">?</cell><cell></cell><cell></cell><cell>29.47</cell><cell>5.26</cell><cell>18.09 31.97 5.81 20.16</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">Opinosis  ?</cell><cell></cell><cell></cell><cell>24.88</cell><cell>2.78</cell><cell>14.09 28.42 4.57 15.50</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="4">DenoiseSum  ?</cell><cell></cell><cell>30.14</cell><cell>4.99</cell><cell>17.65</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Extractive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">LexRank  ?</cell><cell></cell><cell></cell><cell>25.01</cell><cell>3.62</cell><cell>14.67 28.74 5.47 16.75</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="5">Spectral-BERT ?</cell><cell>30.20</cell><cell>4.50</cell><cell>17.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>QT ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28.40</cell><cell>3.97</cell><cell>15.27 34.04 7.03 18.08</cell></row><row><cell></cell><cell cols="6">Weakly Supervised</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">PlanSum  ?</cell><cell></cell><cell></cell><cell>34.79</cell><cell>7.01</cell><cell>19.74 32.87 6.12 19.05</cell></row><row><cell></cell><cell></cell><cell cols="5">OpinionDigest ?</cell><cell>29.30</cell><cell>5.77</cell><cell>18.56</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Oracle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>single</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31.73</cell><cell>4.94</cell><cell>16.95 35.44 7.71 20.74</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>comb.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">42.72 10.21 24.00 40.55 8.77 23.33</cell><cell>-</cell></row><row><cell></cell><cell>Info</cell><cell cols="7">Content Support Fully Partially</cell><cell>No</cell></row><row><cell>COOP</cell><cell>28.0</cell><cell cols="4">38.1%</cell><cell cols="2">35.7%</cell><cell>26.2%</cell></row><row><cell>SimpleAvg</cell><cell>18.0</cell><cell cols="3">35.4%</cell><cell></cell><cell cols="2">35.2%</cell><cell>29.4%</cell></row><row><cell>CopyCat</cell><cell cols="4">-52.0 37.6%</cell><cell></cell><cell cols="2">34.2%</cell><cell>28.2%</cell></row><row><cell>PlanSum</cell><cell>6.0</cell><cell cols="3">30.7%</cell><cell></cell><cell cols="2">36.2%</cell><cell>33.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Human evaluation on Yelp dataset. COOP outperforms the other baseline models on both informativeness (Info) and content support.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quality of summary vectors for different aggregation methods. Values are in percentage.</figDesc><table><row><cell></cell><cell>35.5</cell><cell cols="2">Exact Search Yelp</cell><cell>37</cell><cell cols="2">Exact Search Amazon</cell></row><row><cell>ROUGE-1</cell><cell>33.0 34.0 34.5 35.0 33.5</cell><cell cols="2">Forward SimpleAvg Backward</cell><cell>35 36 34</cell><cell cols="2">Forward SimpleAvg Backward</cell></row><row><cell></cell><cell>1 2</cell><cell>4 Beam Size</cell><cell>8</cell><cell>1 2</cell><cell>4 Beam Size</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Spearman correlation values between the input-output word overlap and ROUGE F1 scores on the test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the filtered training data.</figDesc><table><row><cell>B Revisiting Simple Average Approach</cell></row><row><cell>B.1 RNN-LM model for information amount</cell></row><row><cell>We trained single-layer RNN-LMs on Yelp and</cell></row><row><cell>Amazon datasets respectively, in 100k steps with</cell></row><row><cell>sentencepiece</cell></row><row><cell>8 https://github.com/huggingface/</cell></row><row><cell>transformers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>ROUGE scores on the development set of benchmarks. Bold-faced and underlined denote the best and second-best scores respectively.</figDesc><table><row><cell>to be 10 ?3 for BIMEANVAE and 10 ?5 for Opti-</cell></row><row><cell>mus.</cell></row><row><cell>KL annealing: For the VAE training, we adopt</cell></row><row><cell>the KL annealing to avoid the KL vanishing is-</cell></row><row><cell>sue (Bowman et al., 2016).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>ROUGE scores of BIMEANVAE, coupled with different input aggregation methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>BIMEANVAE search space and the best assignments on Yelp and Amazon datasets.</figDesc><table><row><cell cols="2">Computing infrastructure</cell><cell cols="2">TITAN V</cell></row><row><cell cols="2">Training duration</cell><cell cols="2">Yelp: 25 hours, Amazon: 20 hours</cell></row><row><cell>Search strategy</cell><cell></cell><cell cols="2">Manual tuning</cell></row><row><cell cols="2">Model implementation</cell><cell cols="2">https://github.com/megagonlabs/coop</cell></row><row><cell>Hyperparameter</cell><cell></cell><cell>Search space</cell><cell>Best assignment</cell></row><row><cell>number of training steps</cell><cell></cell><cell>500,000</cell><cell>500,000</cell></row><row><cell>batch size</cell><cell></cell><cell>4</cell><cell>4</cell></row><row><cell>encoder</cell><cell></cell><cell>bert-base-cased</cell><cell>bert-base-cased</cell></row><row><cell>decoder</cell><cell></cell><cell>gpt2</cell><cell>gpt2</cell></row><row><cell>prior distribution</cell><cell></cell><cell>N (0, I)</cell><cell>N (0, I)</cell></row><row><cell>size of latent code</cell><cell></cell><cell>512</cell><cell>512</cell></row><row><cell>free bits</cell><cell cols="2">choice[0.0, 0.1, 0.25, 0.5, 1.0, 2.0]</cell><cell>2.0</cell></row><row><cell>KL annealing strategy</cell><cell cols="2">choice[Cyclic, Pretrain+Anneal]</cell><cell>Cyclic</cell></row><row><cell>learning rate scheduler</cell><cell cols="2">linear schedule with warmup</cell><cell>linear schedule with warmup</cell></row><row><cell>learning rate optimizer</cell><cell></cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>Adam ? 1</cell><cell></cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Adam ? 2</cell><cell></cell><cell>0.999</cell><cell>0.999</cell></row><row><cell>learning rate</cell><cell cols="2">choice[1e-5, 1e-4, 1e-3]</cell><cell>1e-5</cell></row><row><cell>gradient norm</cell><cell></cell><cell>1.0</cell><cell>1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Optimus search space and the best assignments on Yelp and Amazon datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We also tested other ROUGE scores such as ROUGE-2/L in the preliminary experiments and found that ROUGE-1 (i.e., word overlap) works most robustly, so we decided to use the most straightforward metric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">BIMEANVAE shows robust performance even combined with SimpleAvg.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://www.yelp.com/dataset 7 https://github.com/google/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Arthur Bra?inskas for his valuable feedback and correcting the description about CopyCat.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised opinion summarization with content planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12489" to="12497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised opinion summarization with noising and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.175</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extractive Opinion Summarization in Quantized Transformer Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinald</forename><surname>Kim Amplayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00366</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="277" to="293" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1403</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3675" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Derivative-Free and Blackbox Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Audet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Hare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating sentences from disentangled syntactic and semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1602</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6008" to="6019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised opinion summarization as copycat-review generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Bra?inskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.461</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5151" to="5169" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An estimate of an upper bound for the entropy of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1046</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1223" to="1232" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Combination of Estimates from Different Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William G Cochran</surname></persName>
		</author>
		<idno type="DOI">10.2307/3001666</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="129" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Show your work: Improved reporting of experimental results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2185" to="2194" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cyclical annealing schedule: A simple approach to mitigating KL vanishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="240" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for non-parallel text style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hareesh</forename><surname>Bahuleyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A surprisingly effective fix for deep latent variable modeling of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3603" to="3614" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimus: Organizing sentences via pre-trained modeling of a latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4678" to="4699" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5070" to="5081" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Best-worst scaling: Theory, methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony Alfred John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What kind of language is hard to language-model?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4975" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Deep Reinforced Model for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Language Models are Unsupervised Multitask Learners. Ope-nAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-critical Sequence Training for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<title level="m">The New York Times Annotated Corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26752</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press Cambridge</publisher>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">OpinionDigest: A simple framework for opinion summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.513</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5789" to="5798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A spectral method for unsupervised multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.32</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Variational Template Machine for Data-to-Text Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
