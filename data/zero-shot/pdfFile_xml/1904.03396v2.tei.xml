<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
							<email>amitmoryossef@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Bar Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Bar Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a long paper in NAACL-2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system's reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. John, who was born in London, works for IBM.</p><p>Other outputs are also possible: 2. John, who works for IBM, was born in London.</p><p>3. London is the birthplace of John, who works for IBM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IBM employs John, who was born in London.</head><p>These variations result from different ways of structuring the information: choosing which fact to mention first, and in which direction to express each fact. Another choice is to split the text into two different sentences, e.g., <ref type="bibr">5</ref>. John works for IBM. John was born in London.</p><p>Overall, the choice of fact ordering, entity ordering, and sentence splits for these facts give rise to 12 different structures, each of them putting the focus on somewhat different aspect of the information. Realistic inputs include more than two facts, greatly increasing the number of possibilities.</p><p>Another axis of variation is in how to verbalize the information for a given structure. For example, (2) can also be verbalized as 2a. John works for IBM and was born in London. and (5) as: 5a. John is employed by IBM. He was born in London.</p><p>We refer to the first set of choices (how to structure the information) as text planning and to the second (how to verbalize a plan) as plan realization. <ref type="bibr">1</ref> The distinction between planning and realization is at the core of classic natural language generation (NLG) works <ref type="bibr" target="#b27">(Reiter and Dale, 2000;</ref><ref type="bibr" target="#b9">Gatt and Krahmer, 2017)</ref>. However, a recent wave of neural NLG systems ignores this distinction 1 Note that the variation from 5 to 5a includes the introduction of a pronoun. This is traditionally referred to as referring expression generation (REG), and falls between the planning and realization stages. We do not treat REG in this work, but our approach allows natural integration REG systems' outputs.</p><p>1 arXiv:1904.03396v2 [cs.CL] 1 May 2019 and treat the problem as a single end-to-end task of learning to map facts from the input to the output text <ref type="bibr" target="#b8">(Gardent et al., 2017;</ref><ref type="bibr" target="#b5">Du?ek et al., 2018)</ref>. These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of <ref type="bibr" target="#b25">Puduppully et al. (2018)</ref>, who introduce a neural content-planning module in the end-to-end architecture.</p><p>While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts <ref type="bibr" target="#b34">(Wiseman et al., 2017)</ref>, struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in adequacy or correctness of the generated text). When compared to templatebased methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input <ref type="bibr" target="#b26">(Puzikov and Gurevych, 2018)</ref>. Also, they do not allow control over the output's structure. We speculate that this is due to demanding too much of the network: while the neural system excels at capturing the language details required for fluent realization, they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner.</p><p>Proposal we propose an explicit, symbolic, text planning stage, whose output is fed into a neural generation system. The text planner determines the information structure and expresses it unambiguously-in our case as a sequence of ordered trees. This stage is performed symbolically and is guaranteed to remain faithful and complete with regards to the input facts. Once the plan is determined, 2 a neural generation system is used to transform it into fluent, natural language text. By being able to follow the plan structure closely, the network is alleviated from the need to determine higher-level structural decisions and can track what was already covered more easily. This allows the network to perform the task it excels in, producing fluent, natural language outputs.</p><p>We demonstrate our approach on the WebNLG corpus and show it results in outputs which are as fluent as neural systems, but more faithful to the input facts. The method also allows explicit control of the output structure and the generation of diverse outputs (some diversity examples are available in the Appendix). We release our code and the corpus extended with matching plans in https://github.com/ AmitMY/chimera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview of the Approach</head><p>Task Description Our method is concerned with the task of generating texts from inputs in the form of RDF sets. Each input can be considered as a graph, where the entities are nodes, and the RDF relations are directed labeled edges. Each input is paired with one or more reference texts describing these triplets. The reference can be either a single sentence or a sequence of sentences. Formally, each input G consists of a set of triplets of the form (s i , r i , o i ), where s i , o i ? V ("subject" and "object") correspond to entities from DBPedia, and r i ? R is a labeled DBPedia relation (V and R are the sets of entities and relations, respectively). For example, <ref type="figure" target="#fig_1">Figure 1a</ref> shows a triplet set G and <ref type="figure" target="#fig_1">Figure 1d</ref> shows a reference text. We consider the data set as a set of input-output pairs <ref type="bibr">(G, ref)</ref>, where the same G may appear in several pairs, each time with a different reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Overview</head><p>We split the generation process into two parts: text planning and sentence realization. Given an input G, we first generate a text plan plan(G) specifying the division of facts to sentences, the order in which the facts are expressed in each sentence, and the ordering of the sentences. This data-to-plan step is non-neural (Section 3). Then, we generate each sentence according to the plan. This plan-to-sentence step is achieved through an NMT system (Section 4). <ref type="figure" target="#fig_1">Figure 1</ref> demonstrates the entire process.</p><p>To facilitate our plan-based architecture, we devise a method to annotate (G, ref ) pairs with the corresponding plans (Section 3.1), and use it to construct a dataset which is used to train the planto-text translation. The same dataset is also used to devise a plan selection method (Section 3.2).</p><p>General Applicability It is worth considering the dataset-specific vs. general applicability aspects of our method. On the low-level details, this  work is very much dataset dependent. We show how to represent plans for specific datasets, and, importantly for this work, how to automatically construct plans for this dataset given inputs and expected natural language outputs. The method of plan construction will likely not generalize "as is" to other datasets, and the plan structure itself may also be found to be lacking for more demanding generation tasks. However, on a higher level, our proposal is very general: intermediary plan structures can be helpful, and one should consider ways of obtaining them, and of using them. In the short term, this will likely take the form of ad-hoc explorations of plan structures for specific tasks, as we do here, to establish their utility. In the longer term, research may evolve to looking into how general-purpose plan are structured. Our main message is that the separation of planning from realization, even in the context of neural generation, is a useful one to be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Planning</head><p>Plan structure Our text plans capture the division of facts to sentences and the ordering of the sentences. Additionally, for each sentence, the plan captures (1) the ordering of facts within the sentence; (2) The ordering of entities within a fact, which we call the direction of the relation. For example, the {A, location, B} relation can be expressed as either A is located in B or B is the location of A; (3) the structure between facts that share an entity, namely chains and sibling struc-  A text plan is modeled as a sequence of sentence plans, to be realized in order. Each sentence plan is modeled as an ordered tree, specifying the structure in which the information should be realized. Structuring each sentence as a tree enables a clear succession between different facts through shared entities. Our text-plan design assumes that each entity is mentioned only once in a sentence, which holds in the WebNLG corpus. The ordering of the entities and relations within a sentence is determined by a pre-order traversal of the tree. <ref type="figure" target="#fig_1">Figure 1b</ref> shows an example of a text plan. Formally, given the input G, a text plan T is a sequences of sentence plans T = s 1 , ..., s N T . A sen-tence plan s is a labeled, ordered tree, with arcs of the form (h, , m), where h, m ? V are head and modifier nodes, each corresponding to an input entity, and = (r, d) is the relation between nodes, where r ? R is the RDF relation, and d ? {?, ?} denotes the direction in which the relation is ex-</p><formula xml:id="formula_0">pressed: d =? if (h, r, m) ? G, and d =? if (m, r, h) ? G. A text plan T is said to match an input G iff every triplet (s, r, o) in G is expressed in T exactly once, either as an edge (s, (r, ?), o) or as an edge (o, (r i , ?), s).</formula><p>Chains (h, 1 , m), (m, 2 , x) represent a succession of facts that share a middle entity ( <ref type="figure" target="#fig_3">Figure  2a</ref>), while siblings -nodes with the same parent -(h, 1 , m 1 ), (h, 2 , m 2 ) represents a succession of facts about the same entity ( <ref type="figure" target="#fig_3">Figure 2b</ref>). Sibling and chain structures can be combined ( <ref type="figure" target="#fig_3">Figure  2c</ref>). An example of an input we addressed in the WebNLG corpus, and matching text plan is given in <ref type="figure" target="#fig_1">Figure 1b</ref>. Exhaustive generation For small-ish input graphs G-such as those in the WebNLG task we consider here-it is trivial to generate all possible plans by first considering all the ways of grouping the input into sets, then from each set generating all possible trees by arranging it as an undirected graph and performing several DFS traversals starting from each node, where each DFS traversal follows a different order of children. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adding Plans to Training Data</head><p>While the input RDFs and references are present in the training dataset, the plans are not. We devise a method to recover the latent plans for most of the input-reference pairs in the training set, constructing a new dataset of (G, ref, T ) triplets of inputs, reference texts, and corresponding plans.</p><p>We define the reference ref , and the text-plan T to be consistent with each other iff (a) they exhibit the same splitting into sentences-the facts in every sentence in ref are grouped as a sentence plan in T , and (b) for each corresponding sentence and sentence-plan, the order of the entities is identical.</p><p>The matching of plans to references is based on the observations that (a) it is relatively easy to identify entities in the reference texts, and a pair of entities in an input is unique to a fact; (b) it is relatively easy to identify sentence splits; (c) a reference text and its matching plan must share the same entities in the same order, and with the same sentence splits. Sentence split consistency We define a set of triplets to be potentially consistent with a sentence iff each triplet contains at least one entity from the sentence (either its subject or object appear in the sentence), and each entity in the sentence is covered by at least one triplet. Given a reference text, we split it into sentences using NLTK <ref type="bibr" target="#b3">(Bird and Loper, 2004)</ref>, and look for divisions of G into disjoint sets such that each set is consistent with a corresponding sentence. For each such division, we consider the exhaustive set of all induced plans. Facts order consistency A natural criterion would be to consider a reference sentence and a sentence-plan originating from the corresponding RDF as matching iff the sets of entities in the sentence and the plan are identical, and all entities appear in the same order. <ref type="bibr">4</ref> Based on this, we could represent each sentence and each plan as a sequence of entities, and verify the sequences match. However, using this criterion is complicated by the fact that it is not trivial to map between the entities in the plan (that originate from the RDF triplets) and the entities in the text. In particular, due to language variability, the same plan entity may appear in several forms in the textual sentences. Some of these variations (i.e. "A.F.C Fylde" vs. "AFC Fylde") can be recognized heuristically, while others require external knowledge ("UK conservative party" vs. "the Tories"), and some are ambiguous and require full-fledged co-reference resolution ("them", "he", "the former"). Hence, we relax our matching criterion to allow for possible unrecognized entities in the text.</p><p>Concretely, we represent each sentence plan as a sequence of its entities (pe 1 , ..., pe k ), and each sentence as the sequence of its entities which we managed to recognize and to match with an input entity (se 1 , ..., se m ), m ? k.</p><p>tions hold: (1) The sentence entities (se 1 , ..., se m ) are a proper sub-sequence of the plan entities (pe 1 , ..., pe k ); and (2) each of the remaining entities in the plan already appeared previously in the plan. The second condition accounts for the fact that most un-identified entities are due to pronouns and similar non-lexicalized referring expressions, and that these only appear after a previous occurrence of the same entity in the text. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Test-time Plan Selection</head><p>To select the plan to be realized, we propose a mechanism for ranking the possible plans. Our plan scoring method is a product-of-experts model, where each expert is a conditional probability estimate for some property of the plan. The conditional probabilities are MLE estimates based on the plans in the training set constructed in section 3.1. Estimates involving relation names are smoothed using Lidstone smoothing to account for unseen relations. We use the following experts: Relation direction For every relation r ? R, we compute its probability to be expressed in the plan in its original order (d =?) or in the reverse order (d =?): p dir (d =? |R). This captures the tendency of certain relations to be realized in the reversed order to how they are defined in the knowledge base. For example, in the WebNLG corpus the relation "manager" is expressed as a variation of "is managed by" instead of one of "is the manager of" in 68% of its occurrences (p dir (d =? |manager) = 0.68). Global direction We find that while the probability of each relation to be realized in a reversed order is usually below 0.5, still in most plans of longer texts there are one or two relations that appear in the reversed order. We capture this tendency using an expert that considers the conditional probability p gd (nr = n| |G|) of observing n reversed edges in an input with |G| triplets. Splitting tendencies For each input size, we keep track of the possible ways in which the set of facts can be split to subsets of particular sizes. That is, we keep track of probabilities such as p s (s = [3, 2, 2] | 7) of realizing an input of 7 RDF triplets as three sentences, each realizing the corresponding number of facts. Relation transitions We consider each sentence plan as a sequence of the relation types expressed in it r 1 , . . . , r k followed by an EOS symbol, and compute the markov transition probabilities over this sequence: p trans (r 1 , r 2 , . . . , r k , EOS) = ? i=1,k p t (r i+1 |r i ). The expert is the product of the transition probabilities of the individual sentence plans in the text plan. This captures the tendencies of relations to follow each other and in particular, the tendencies of related relations such as birth-place and birth-date to group, allowing their aggregation in the generated text (John was born in London on Dec 12th, 1980).</p><p>Each of the possible plans are then scored based on the product of the above quantities. <ref type="bibr">7</ref> The scores work well for separating good from lousy text plans, and we observe a threshold above which most generated plans result in adequate texts. We demonstrate in Section 6 that realizing highly-ranked plans manages to obtain good automatic realization scores. We note that the plan in <ref type="figure" target="#fig_1">Figure 1b</ref> is the one our ranking algorithm ranked first for the input in <ref type="figure" target="#fig_1">Figure 1a</ref>.</p><p>Possible Alternatives In addition to the single plan selection, the explicit planning stage opens up additional possibilities. Instead of choosing and realizing a single plan, we can realize a diverse set of high-scoring plans, or realizing a random high-scoring plan, resulting in a diverse and less templatic set of texts across runs. This relies on the combination of two factors: the ability of the scoring component to select plans that correspond to plausible human-authored texts, and the ability of the neural realizer to faithfully realize the plan into fluent text. While it is challenging to directly evaluate the plans adequacy, we later show an evaluation of the plan realization component. <ref type="figure" target="#fig_5">Figure 3</ref> shows three random plans for the same graph and their realizations. Further examples of the diversity of generation are given in the appendix.</p><p>The explicit and symbolic planning stage also allows for user control over the generated text, either by supplying constraints on the possible plans (e.g., number of sentences, entities to focus on, the order of entities/relations, or others) or by supplying complete plans. We leave these options for future work. <ref type="bibr">7</ref> We note that for an input of n triplets, there are O(2 2n + n * n!) possible plans, making this method prohibitive for even moderately sized input graphs. However, it is sufficient for the WebNLG dataset in which n ? 7. For larger graphs, better plan scoring and more efficient search algorithms should be devised. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><formula xml:id="formula_1">Dessert ? course [ Bionico ? country [ Mexico ] ? ingredient [ Granola ] ? region [ Jalisco ] ]</formula><p>The Dessert Bionico requires Granola as one of its ingredients and originates from the Jalisco region of Mexico .</p><formula xml:id="formula_2">(b) Bionico ? country [ Mexico ] ? region [ Jalisco ] . Dessert ? course [ Bionico ? ingredient [ Granola ] ]</formula><p>Bionico is a food found in the Mexico region Jalisco. The Dessert Bionico requires Granola as an ingredient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c)</head><formula xml:id="formula_3">Bionico ? ingredient [ Granola ] ? course [ Dessert ] . Bionico ? region [ Jalisco ] ? country [ Mexico ]</formula><p>Bionico contains Granola and is served as a Dessert. Bionico is a food found in the region of Jalisco, Mexico </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Plan Realization</head><p>For plan realization, we use an off-the-shelf vanilla neural machine translation (NMT) system to translate plans to texts. The explicit division to sentences in the text plan allows us to realize each sentence plan individually which allows the realizer to follow the plan structure within each (rather short) sentence, reducing the amount of information that the model needs to remember. As a result, we expect a significant reduction in over-and under-generation of facts, which are common when generating longer texts. Currently, this comes at the expense of not modeling discourse structure (i.e., referring expressions). This deficiency may be handled by integrating the discourse into the text plan, or as a post-processing step. 8 . We leave this for future work.</p><p>To use text plans as inputs to the NMT, we linearize each sentence plan by performing a preorder traversal of the tree, while indicating the tree structure with brackets ( <ref type="figure" target="#fig_1">Figure 1c</ref>). The directed relations (r, d) are expressed as a sequence of two or more tokens, the first indicating the direction and the rest expressing the relation. <ref type="bibr">9</ref> Entities that are identified in the reference text are replaced with single, entity-unique tokens. This allows the NMT system to copy such entities from the input rather than generating them. <ref type="figure" target="#fig_1">Figure 1d</ref> is an example of possible text resulting from such linearization.</p><p>Training details We use a standard NMT setup with a copy-attention mechanism <ref type="bibr" target="#b10">(Gulcehre et al., 2016)</ref> 10 and the pre-trained GloVe.6B word em-8 Minimally, each entity occurrence can keep track of the number of times it was already mentioned in the plan. Other alternatives include using a full-fledged referring expression generation system such as NeuralREG (Ferreira et al., 2018) <ref type="bibr">9</ref> We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase. <ref type="bibr">10</ref> Concretely, we use the OpenNMT toolkit <ref type="bibr" target="#b14">(Klein et al., 2017)</ref> with the copy attn flag. Exact parameter values are beddings 11 <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref>. The pretrained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts.</p><p>Generation details We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as Month DAY+ordinal, YEAR (i.e., July 4th, 1776) and for numbers with units (i.e., "5"(minutes)) we remove the parenthesis and quotation marks (5 minutes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>The WebNLG challenge <ref type="bibr" target="#b4">(Colin et al., 2016)</ref> consists of mapping sets of RDF triplets to text including referring expression generation, aggregation, lexicalization, surface realization, and sentence segmentation. It contains sets with up to 7 triplets each along with one or more reference texts for each set. The test set is split into two parts: seen, containing inputs created for entities and relations belonging to DBpedia categories that were seen in the training data, and unseen, containing inputs extracted for entities and relations belonging to 5 unseen categories. While the unseen category is conceptually appealing, we view the seen category as the more relevant setup: generating fluent, adequate and diverse text for a mix of known relation types is enough of a challenge also without requiring the system to invent verbalizations for unknown relation types. Any realistic generation system could afford to provide at least a few verbalizations for each relation of interest. We thus focus our attention mostly on the seen case (though our system does also perform well on the unseen case). Following Section 3.1, we manage to match a detailed in the appendix. 11 nlp.stanford.edu/data/glove.6B.zip consistent plan for 76% of the reference texts and use these plan-text pairs to train the plan realization NMT component. Overall, the WebNLG training set contains 18, 102 RDF-text pairs while our plan-enhanced corpus contains 13, 828 plantext pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>Compared Systems We compare to the best submissions in the WebNLG challenge <ref type="bibr" target="#b8">(Gardent et al., 2017)</ref>: Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe <ref type="bibr" target="#b22">(Mille et al., 2017)</ref>, a classic grammar-based NLG system that scored best in the human evaluation. Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> decoder with attention <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>, a copy-attention mechanism <ref type="bibr" target="#b10">(Gulcehre et al., 2016)</ref> and a neural checklist model <ref type="bibr" target="#b13">(Kiddon et al., 2016)</ref>, as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural.</p><p>6 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Automatic Metrics</head><p>We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref>, Meteor <ref type="bibr" target="#b1">(Banerjee and Lavie, 2005)</ref>, ROUGE L <ref type="bibr" target="#b20">(Lin, 2004)</ref> and CIDEr <ref type="bibr" target="#b32">(Vedantam et al., 2015)</ref>, using the nlg-eval 13 tool <ref type="bibr" target="#b29">(Sharma et al., 2017)</ref> on the entire test set and on each part separately (seen and unseen).</p><p>In the original challenge, the best performing system in automatic metric was based on end-toend NMT (Melbourne </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Manual Evaluation</head><p>Next, we turn to manually evaluate our system's performance regarding faithfulness to the input on the one hand and fluency on the other. We describe here the main points of the manual evaluation setup, with finer details in the appendix.</p><p>Faithfulness As explained in Section 3, the first benefit we expect of our plan-based architecture is to make the neural systems task simpler, helping it to remain faithful to the semantics expressed in the plan which in turn is guaranteed to be faithful to the original RDF input (by faithfulness, we mean expressing all facts in the graph and only facts from the graph: not dropping, repeating or hallucinating facts). We conduct a manual evaluation over the seen portion of the WebNLG human evaluated test set (139 input sets). We compare Best-Plan and StrongNeural. 15 For each output text, we manually mark which relations are expressed in it, which are omitted, and which relations exist with the wrong lexicalization. We also count the number of relations the system over generated, either repeating facts or inventing new facts.</p><p>16 <ref type="table" target="#tab_2">Table 2</ref> shows the results. BestPlan reduces all error types compared to StrongNeural, by 85%, 56% and 90% respectively. While on-par regarding automatic metrics, BestPlan substantially outperforms the new state-of-the-art end-to-end neural system in semantic faithfulness.</p><p>For example, <ref type="figure">Figure 4</ref> compares the output of He was selected by nasa in 1963 and became a crew member on the Apollo 8 flight mission.</p><p>He retired on September 1st, 1969.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Output from StrongNeural</head><p>William Anders was a fighter pilot who joined nasa in 1963 and served as a crew member of Apollo 8.</p><p>William Anders retired on September 1st, 1969 and spent 8820.0 minutes in space.</p><p>William Anders was born in British Hong Kong on october October 17th, 1933.</p><p>(c) Output from BestPlan <ref type="figure">Figure 4</ref>: Comparing end-to-end neural generation with our plan based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StrongNeural (4b) and</head><p>BestPlan (4c) on the last input in the seen test set (4b). While both systems chose three sentences split and aggregated details about birth in one sentence and details about the occupation in another, StrongNeural also expressed the information in chronological order. However, StrongNeural failed to generate facts 3 and 5. BestPlan made a lexicalization mistake in the third sentence by expressing "October" before the actual date, which is probably caused by faulty entity matching for one of the references, and (by design) did not generate any referring expression, which we leave for future work. <ref type="table">Expressed  417  360  Omitted  6  41  Wrong-lexicalization 17</ref> 39 Over-generation 3 29 Fluency Next, we assess whether our systems succeed at maintaining the high-quality fluency of the neural systems. We perform pairwise evaluation via Amazon Mechanical Turk wherein each task the worker is presented with an RDF set (both in a graph form, and textually), and two texts in random order, one from BestPlan, the other from a competing system. We compare Best-Plan against a strong end-to-end neural system (StrongNeural), a grammar-based system which StrongNeural Reference UPF-FORGe BestPlan -0.6% -5.4% +5.1% <ref type="table">Table 3</ref>: MTurk average worker score for BestPlan compared to each system. It is a worse than the reference texts, on-par with the neural end-to-end system, and a better than the previous state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BestPlan StrongNeural</head><p>is the state-of-the-art in human evaluation (UPF-FORGe), and the human-supplied WebNLG references (Reference). The workers were presented with three possible answers: BestPlan text is better (scored as 1), the other text is better (scored as -1), and both texts are equally fluent (scored as 0). <ref type="table">Table 3</ref> shows the average worker score given to each pair divided by the number of texts compared. BestPlan performed on-par with StrongNeural, and surpassed the previous state-of-the-art UPF-FORGe. It, however, scored worse than the reference texts, which is expected given that it does not produce referring expressions. Our approach manages to keep the same fluency level typical to end-to-end neural systems, thanks to the NMT realization component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Plan Realization Consistency</head><p>We test the extent to which the realizer generates texts that are consistent with the plans. For several subsets of ranked plans (best plan, top 1%, and top 10%) for the seen and unseen test sets separately, we realize up to 100 randomly selected text-plans per input. We realize each sentence plan and evaluate using two criteria: (1) Do all entities from the plan appear in the realization;</p><p>(2) Like the consis-  <ref type="table">Table 4</ref>: Surface realizer performance. Entities: Percent of sentence plans that were realized with all the requested entities. Order: of the sentences that were realized with all requested entities, percentage of realizations that followed the requested entity order. tency we defined above, do all entities appear in the same order in the plan and the realization. <ref type="table">Table 4</ref> indicates that for decreasingly probable plans our realizer does worse in the first criterion. However, for both parts of the test set, if the realizer managed to express all of the entities, it expressed them in the requested order, meaning the outputs are consistent with plans. This opens up a potential for user control and diverse outputs, by choosing different plans for realization.</p><p>Finally, we verify that the realization of potentially diverse plans is not only consistent with each given plan but also preserves output quality. For each input, we realize a random plan from the top 10%. We repeat this process three times with different random seeds to generate different outputs, and mark these systems as RandomPlan-1/2/3. <ref type="table">Table 1</ref> shows that these random plans maintain decent quality on the automatic metrics, with a limited performance drop, and the automatic score is stable across random seeds. <ref type="bibr">17</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Text planning is a major component in classic NLG. For example, <ref type="bibr" target="#b31">Stent et al. (2004)</ref> shows a method of producing coherent sentence plans by exhaustively generating as many as 20 sentence plan trees for each document plan, manually tagging them, and learning to rank them using the RankBoost algorithm <ref type="bibr" target="#b28">(Schapire, 1999)</ref>. Our planning approach is similar, but we only have a set of "good" reference plans without internal ranks. While the sentence planning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. <ref type="bibr" target="#b17">Lapata (2003)</ref> devised a probabilistic model for sentence ordering which correlated well with human ordering. Our plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works <ref type="bibr" target="#b2">(Barzilay and Lapata, 2006;</ref><ref type="bibr">Lapata, 2012, 2013)</ref>.</p><p>Many generation systems <ref type="bibr" target="#b8">(Gardent et al., 2017;</ref><ref type="bibr" target="#b5">Du?ek et al., 2018)</ref> are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process.</p><p>Generation from structured data often requires referring to a knowledge base <ref type="bibr" target="#b21">(Mei et al., 2015;</ref><ref type="bibr" target="#b13">Kiddon et al., 2016;</ref><ref type="bibr" target="#b33">Wen et al., 2015)</ref>. This led to input-coverage tracking neural components such as the checklist model <ref type="bibr" target="#b13">(Kiddon et al., 2016)</ref> and copy-mechanism <ref type="bibr" target="#b10">(Gulcehre et al., 2016)</ref>. Such methods are effective for ensuring coverage and reducing the number of over-generated facts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model.</p><p>More complex tasks, like RotoWire <ref type="bibr" target="#b34">(Wiseman et al., 2017)</ref> require modeling also document-level planning. <ref type="bibr" target="#b25">Puduppully et al. (2018)</ref> explored a method to explicitly model document planning using the attention mechanism.</p><p>The neural text generation community has also recently been interested in "controllable" text generation <ref type="bibr" target="#b12">(Hu et al., 2017)</ref>, where various aspects of the text (often sentiment) are manipulated <ref type="bibr" target="#b7">(Ficler and Goldberg, 2017)</ref> or transferred <ref type="bibr" target="#b30">(Shen et al., 2017;</ref><ref type="bibr" target="#b36">Zhao et al., 2017;</ref>. In contrast, like in <ref type="bibr" target="#b35">(Wiseman et al., 2018)</ref>, here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outperforms the end-to-end system regarding faithfulness to the input. Additionally, the planning stage allows explicit user-control and generating diverse sentences, to be pursued in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Diverse Outputs</head><p>We demonstrate the ability of the model to produce diverse outputs by showing examples of generation from graphs with 4, 5 or 6 edges. For each graph, we show every kth plan, where k is chosen so that our 25 examples cover the top 10% of the plans, and order them by the scores assigned to them by the scoring model (the score is shown to the right of each plan, as well as the rank in the list). Higher scoring plans correspond to more natural plans, according to our model, but all of them are viable options. Then, for each plan we show the corresponding text generated by the NMT model. This provides a glimpse of: (1) the quality of the scoring model; (2) the diversity of the plans; (3) the naturalness of the generation.</p><p>For the plans, color boxes indicate entities, and gray boxes around them indicate bracketing. Vertical bars indicate sentence splits. For the generated text, each entity is underlines with the color corresponding to its box.</p><p>A.1 Example: Graph with 4 Edges <ref type="figure" target="#fig_6">Figure 5</ref> shows a random 4-edge graph from the seen part of the test set. <ref type="figure">Figure 6</ref> shows the plans and <ref type="figure">Figure 7</ref> the corresponding texts.  <ref type="figure" target="#fig_7">Figure 8</ref> shows a random 5-edge graph from the seen part of the test set. <ref type="figure">Figure 9</ref> shows the plans and <ref type="figure" target="#fig_1">Figure 10</ref> the corresponding texts.  <ref type="figure" target="#fig_1">Figure 11</ref> shows a random 6-edge graph from the seen part of the test set. <ref type="figure" target="#fig_1">Figure 12</ref> shows the plans and <ref type="figure" target="#fig_1">Figure 13</ref> the corresponding texts. <ref type="figure" target="#fig_1">Figure 11</ref>: Example of a graph with 6 edges B Manual Evaluation Setup When performing pairwise system comparisons, we show the user, for each set of RDFs, the two texts produced by the compared systems in random order, along with the RDF triplets in textual and image forms as a reference. For consistency, both texts are normalized by lower-casing and splitting tokens on punctuation. The same interface is used for turkers (for the fluency task) and local annotators (for the faithfulness task).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Fluency Evaluation by Crowd</head><p>We evaluate on the RDF sets in the original WebNLG manual evaluation setup. The task is performed by mechanical-turk workers. The workers are presented with the question:</p><p>"Which text reads more fluently?" which can be answered by either Text 1, Text 2 or Both are equally good or bad.</p><p>We paid 0.08$ per hit, employing three workers on each. For qualification, workers were required to have over 98% hit approval rate, and over 1000 approved hits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Faithfulness Evaluation by Expert</head><p>To obtain reliable fine-grained evaluation of semantic faithfulness, the first author annotated the system outputs of StrongNeural and BestPlan.</p><p>For each text, we present all the RDF input triplets, and ask the annotator to choose for each triplet one of three options: (1) This triplet is expressed in the text; (2) This triplet is not expressed in the text (ommitted); (3) The text expresses a relation between the two entities that is different than the one specified for them in the RDF triplet (wrong lexicalization). Also, for each text, we ask the annotator to count the number of facts that were wrongly over generated, counting both repeated facts and hallucinated ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Parameters</head><p>For the realization model we use the Open-NMT toolkit <ref type="bibr" target="#b14">(Klein et al., 2017)</ref> with pretrained GloVe.6B word embeddings <ref type="bibr" target="#b24">(Pennington et al., 2014)</ref>, downloaded from http://nlp. stanford.edu/data/glove.6B.zip. We used the default parameters (except for the -copy attn flag). This corresponds to the following values:</p><p>? train steps = 40000 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Example input RDF AIP Advances | editor | A.T. Charlie Johnson A.T. Charlie Johnson | almaMater | Harvard University AIP Advances | ISSN number | "2158-3226" A.T. Charlie Johnson | residence | United States (Linearization of the text plan A.T. Charlie Johnson ? editor [ AIP Advances ? issn number [ 2158-3226 ] ] . A.T. Charlie Johnson ? residence [ United States ] ? alma mater [ Harvard University ] (d) Possible output sentence A.T. Charlie Johnson is the editor of AIP Advances which has the ISSN number 2158-3226. He lives in the United States, and graduated from Harvard University.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Summary of our proposed generation process: the planner takes the input RDF triplets in (a), and generates the explicit plan in (b). The plan is then linearized (c) and passed to a neural generation system, producing the output (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Combination: John lives in London, the capital of England, and works as a bartender.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Fact construction structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Three random linearized plans for the same input graph, and their text realizations. All taken from the top 10% scoring plans. (a) structures the output as a single sentence, while (b) and (c) as two sentences. The second sentence in (b) puts emphasis on Bionico being a dessert, while in (c) the emphasis is on the ingredients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Example of a graph with 4 edges A.2 Example: Graph with 5 Edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Example of a graph with 5 edges A.3 Example: Graph with 6 Edges</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics (Table 1). BestPlan is competitive with StrongNeural in all metrics, with small differences either way per metric. 14 12 Note that this only affects the training stage. At test time, we do not require gold plans, and evaluate on all sentences. StrongNeural can be attributed to its ability to generate referring expressions, which we currently do not support.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">BLEU METEOR ROUGE L CIDEr</cell></row><row><cell cols="3">UPF-FORGe</cell><cell cols="2">?</cell><cell>38.5</cell><cell>0.390</cell><cell>60.9</cell><cell>2.500</cell></row><row><cell cols="2">Melbourne</cell><cell>?</cell><cell></cell><cell></cell><cell>45.0</cell><cell>0.376</cell><cell>63.5</cell><cell>2.814</cell></row><row><cell cols="5">RandomPlan-1 ?</cell><cell>43.3</cell><cell>0.384</cell><cell>57.6</cell><cell>2.342</cell></row><row><cell cols="5">RandomPlan-2 ?</cell><cell>43.5</cell><cell>0.384</cell><cell>57.4</cell><cell>2.332</cell></row><row><cell cols="5">RandomPlan-3 ?</cell><cell>43.5</cell><cell>0.384</cell><cell>57.4</cell><cell>2.303</cell></row><row><cell cols="4">StrongNeural</cell><cell>?</cell><cell>46.5</cell><cell>0.392</cell><cell>65.4</cell><cell>2.866</cell></row><row><cell>BestPlan</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>47.4</cell><cell>0.391</cell><cell>63.1</cell><cell>2.692</cell></row><row><cell cols="7">Table 1: Results for all categories. Team color indicates</cell></row><row><cell cols="7">the type of system used (NMT ? , Rule-Based ? , Rule-</cell></row><row><cell cols="5">Based + NMT</cell><cell></cell></row></table><note>13 https://github.com/Maluuba/nlg-eval 14 At least part of the stronger results for? ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. William Anders | dateOfRetirement | "1969-09-01" 2. William Anders | was selected by NASA | 1963 3. William Anders | timeInSpace | "8820.0"(minutes) 4. William Anders | birthDate | "1933-10-17" 5. William Anders | occupation | Fighter pilot 6. William Anders | birthPlace | British Hong Kong 7. William Anders | was a crew member of | Apollo 8 (a) The last RDF in the seen test-set William Anders was born on October 17th, 1933 in British Hong Kong.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semantic faithfulness of each system regarding 440 RDF triplets from 139 input sets in the seen part of the manually evaluated test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The exact plan can be determined based on a data-driven scoring function that ranks possible suggestions, as in this work, or by other user provided heuristics or a trained ML model. The plans' symbolic nature and precise relation to the input structures allow verification of their correctness.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">If a graph includes a cycle (0.4% of the graphs in the WebNLG corpus contain cycles) we skip it, as it is guaranteed that a different split will result in cycle-free graphs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We then consider a sentence and a sentenceplan to be consistent if the following two condi-4 An additional constraint is that no two triplets in the RDFs set share the same entities. This is to ensure that if two entities appeared in a structure, only one relation could have been expressed there. This almost always holds in the WebNLG corpus, failing on only 15 out of 6,940 input sets.5  We match plan entities to sentence entities using greedy string matching with Levenshtein distance<ref type="bibr" target="#b18">(Levenshtein, 1966)</ref> for each token and a manually tuned threshold for a match. While this approach results in occasional false positives, most cases are detected correctly. We match dates by using the chrono-python package that parses dates from natural language texts.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">A sensible alternative would be to use a coreference resolution system at this stage. In our case it turned out to not help, and even performed somewhat worse.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15">We do not evaluate UPF-FORGe as it is a verifiable grammar-based system that is fully faithful by design.16  This evaluation was conducted by the first author, on a set of shuffled examples from the BestPlan and StrongNeural systems, without knowing which outputs belongs to which system. We further note that evaluating for faithfulness requires careful attention to detail (making it less suitable for crowd-workers), but has a precise task definition which does not involve subjective judgment, making it possible to annotate without annotator biases influencing the results. We release our judgments for this stage together with the code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17">While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aggregation via set partitioning for natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 31. Association for Computational Linguistics</title>
		<meeting>the ACL 2004 on Interactive poster and demonstration sessions, page 31. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from dbpedia data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Mrabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Natural Language Generation conference</title>
		<meeting>the 9th International Natural Language Generation conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="163" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01170</idno>
		<title level="m">Findings of the e2e nlg challenge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neuralreg: An end-to-end approach to referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Thiago Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?kos</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>K?d?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08093</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02633</idno>
		<title level="m">Controlling linguistic style aspects in neural language generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The webnlg challenge: Generating text from rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<idno>abs/1703.09902</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08148</idno>
		<title level="m">Pointing the unknown words</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00955</idno>
		<title level="m">Toward controlled generation of text</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Globally coherent text generation with neural checklist models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chlo?</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02810</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised concept-to-text generation with hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inducing document plans for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1503" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic text structuring: Experiments with sentence ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Delete, retrieve, generate: A simple approach to sentiment and style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06437</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>He He, and Percy Liang</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What to talk about and how? selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00838</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Forge at semeval-2017 task 9: Deep sentence generation based on a sequence of graph transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alicia</forename><surname>Burga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="920" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data-to-text generation with content selection and planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00582</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">E2e nlg challenge: Neural models vs. templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeniy</forename><surname>Puzikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="463" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A brief introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ijcai</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1401" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<idno>abs/1706.09799</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6830" to="6841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Trainable sentence planning for complex information presentation in spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on association for computational linguistics, page 79. Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on association for computational linguistics, page 79. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01745</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08052</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Challenges in data-to-document generation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning neural templates for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10122</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adversarially regularized autoencoders for generating discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1706.04223</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
