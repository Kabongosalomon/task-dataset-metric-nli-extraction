<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional GANs with Auxiliary Discriminative Classifier</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Data Intelligence System Research Center</orgName>
								<orgName type="department" key="dep2">Institute of Com-puting Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Data Intelligence System Research Center</orgName>
								<orgName type="department" key="dep2">Institute of Com-puting Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huawei</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Data Intelligence System Research Center</orgName>
								<orgName type="department" key="dep2">Institute of Com-puting Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Pan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Com-puting Technology</orgName>
								<orgName type="laboratory">CAS Key Labo-ratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional GANs with Auxiliary Discriminative Classifier</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Correspondence to: Huawei Shen &lt;shenhuawei@ict.ac.cn&gt;. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-right 2022 by the author(s).</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional generative models aim to learn the underlying joint distribution of data and labels to achieve conditional data generation. Among them, the auxiliary classifier generative adversarial network (AC-GAN) has been widely used, but suffers from the problem of low intra-class diversity of the generated samples. The fundamental reason pointed out in this paper is that the classifier of AC-GAN is generator-agnostic, which therefore cannot provide informative guidance for the generator to approach the joint distribution, resulting in a minimization of the conditional entropy that decreases the intra-class diversity. Motivated by this understanding, we propose a novel conditional GAN with an auxiliary discriminative classifier (ADC-GAN) to resolve the above problem. Specifically, the proposed auxiliary discriminative classifier becomes generator-aware by recognizing the class-labels of the real data and the generated data discriminatively. Our theoretical analysis reveals that the generator can faithfully learn the joint distribution even without the original discriminator, making the proposed ADC-GAN robust to the value of the coefficient hyperparameter and the selection of the GAN loss, and stable during training. Extensive experimental results on synthetic and real-world datasets demonstrate the superiority of ADC-GAN in conditional generative modeling compared to stateof-the-art classifier-based and projection-based conditional GANs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> have achieved substantial progress in learning high-dimensional, complex data distribution such as images <ref type="bibr" target="#b2">(Brock et al., 2019;</ref><ref type="bibr" target="#b18">Karras et al., 2019;</ref><ref type="bibr" target="#b20">2020b;</ref><ref type="bibr">a;</ref>. Standard GANs consist of a generator network, which transfers latent codes sampled from tractable distributions such as Gaussian in the latent space to data points in the data space, and a discriminator network, which attempts to distinguish real data and generated data. The generator is trained in an adversarial game against the discriminator so that it can learn the data distribution at the Nash equilibrium. Remarkably, training GANs unconditionally is difficult to achieve equilibrium, making the generator prone to mode collapse <ref type="bibr" target="#b37">(Salimans et al., 2016;</ref><ref type="bibr" target="#b27">Lin et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019)</ref>. In addition, practitioners are interested in being able to control in advance the content of the generated samples <ref type="bibr" target="#b44">(Yan et al., 2015;</ref><ref type="bibr" target="#b39">Tan et al., 2020)</ref> in practical applications. A promising solution to these issues is conditioning the generator, leading to conditional GANs.</p><p>Conditional GANs (cGANs) <ref type="bibr" target="#b28">(Mirza &amp; Osindero, 2014</ref>) is a family of variants of GANs that leverages the side information from annotated labels of samples to implement and train a conditional generator for conditional image generation from class-labels <ref type="bibr" target="#b33">(Odena et al., 2017;</ref><ref type="bibr" target="#b29">Miyato &amp; Koyama, 2018;</ref><ref type="bibr" target="#b2">Brock et al., 2019)</ref>. To implement the conditional generator, the common technique nowadays injects the conditional information via conditional batch normalization <ref type="bibr" target="#b6">(de Vries et al., 2017;</ref><ref type="bibr" target="#b14">Hou et al., 2021b)</ref>. To train the conditional generator, a lot of effort put into effectively injecting the conditional information into the discriminator or auxiliary classifier that guides the conditional generator <ref type="bibr" target="#b32">(Odena, 2016;</ref><ref type="bibr" target="#b29">Miyato &amp; Koyama, 2018;</ref><ref type="bibr" target="#b48">Zhou et al., 2018;</ref><ref type="bibr" target="#b21">Kavalerov et al., 2021;</ref><ref type="bibr" target="#b15">Kang &amp; Park, 2020;</ref><ref type="bibr" target="#b47">Zhou et al., 2020)</ref>. Among them, the auxiliary classifier generative adversarial network (AC-GAN) <ref type="bibr" target="#b33">(Odena et al., 2017)</ref> has been widely used due to its simplicity and extensibility. Specifically, AC-GAN utilizes an auxiliary classifier that first attempts to recognize the labels of data and then teaches the generator to produce label-consistent (classifiable) data. However, it has been reported that AC-GAN suffers from the low intra-class diversity problem in the generated samples, especially on datasets with a large number of classes <ref type="bibr" target="#b33">(Odena et al., 2017;</ref><ref type="bibr" target="#b38">Shu et al., 2017;</ref><ref type="bibr" target="#b7">Gong et al., 2019)</ref>. arXiv:2107.10060v5 <ref type="bibr">[cs.</ref>LG] 17 Jun 2022</p><p>In this study, we point out that the fundamental reason for the low intra-class diversity problem of AC-GAN is that the classifier is agnostic to the generated data distribution and thus cannot provide informative guidance for the generator to learn the target distribution. Motivated by this understanding, we propose a novel conditional GAN with an auxiliary discriminative classifier, namely ADC-GAN, to resolve the above problem by enabling the classifier to be aware of the generated data distribution as well as the real data distribution. To this end, the discriminative classifier is trained to distinguish between the real and generated data while recognizing their class-labels. The discriminative capability allows the classifier to provide the discrepancy between the real and generated data distributions like the discriminator, and the classification capability enables it to capture the dependencies between data and labels. We show in theory that the generator of our proposed ADC-GAN can learn the joint data and label distribution under the optimal discriminative classifier even without the discriminator, making the method robust to the value of the coefficient hyperparameter and the selection of the GAN loss and stable during training. We also highlight the superiority of ADC-GAN compared to the two most related works (TAC-GAN <ref type="bibr" target="#b7">(Gong et al., 2019)</ref> and PD-GAN <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018)</ref>) by analyzing their potential issues and limitations. Results on synthetic data clearly show that the proposed ADC-GAN successfully resolves the problem of AC-GAN by faithfully recovering the joint distribution of real data and labels. Extensive experiments based on two popular codebases demonstrate the effectiveness of the proposed ADC-GAN compared with state-of-the-art cGANs in conditional generative modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generative Adversarial Networks</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> consist of two types of neural networks: the generator G : Z ? X that maps a latent code z ? Z endowed with an easily sampled distribution P Z to a data point x ? X , and the discriminator D : X ? [0, 1] that distinguishes between real data that sampled from the real data distribution P X and fake data that sampled from the generated data distribution Q X = G P Z induced by the generator. The goal of the generator is to confuse the discriminator by producing data that are as real as possible. Formally, the objective functions for the discriminator and generator are defined as follows:</p><formula xml:id="formula_0">min G max D V (G, D) = E x?P X [log D(x)] + E x?Q X [log(1 ? D(x))]. (1)</formula><p>Theoretically, learning the generator under the optimal discriminator can be regarded as minimizing the Jensen-Shannon (JS) divergence between the real data distribution and the generated data distribution, i.e., min G JS(P X Q X ).</p><p>This would enable the generator to restore the real data distribution at its optimum. However, the training of GANs on complex natural images is typically unstable <ref type="bibr" target="#b3">(Che et al., 2016)</ref>, especially in the absence of supervision such as conditional information. In addtition, the content of the images generated by GANs cannot be specified in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Base Method: AC-GAN</head><p>Learning GANs with conditional information can not only improve the training stability but also achieve conditional generation. As one of the most representative conditional GANs, AC-GAN <ref type="bibr" target="#b33">(Odena et al., 2017)</ref> utilizes an auxiliary classifier C : X ? Y to learn the dependencies between data and labels endowed with a label prior P Y and then encourages the conditional generator G : Z ? Y ? X to generate as much classifiable data as possible. The objective functions for the discriminator, the auxiliary classifier, and the generator of AC-GAN 1 are defined as follows:</p><formula xml:id="formula_1">max D,C V (G, D) + ? ? E x,y?P X,Y [log C(y|x)] , (2) min G V (G, D) ? ? ? E x,y?Q X,Y [log C(y|x)] ,<label>(3)</label></formula><p>where ? &gt; 0 is a coefficient hyperparameter, P X,Y indicates the joint distribution of real data and labels, and Q X,Y = G (P Z ? P Y ) denotes the joint distribution of the generated data and labels induced by the conditional generator.</p><p>Proposition 2.1. For fixed generator, the optimal classifier of AC-GAN has the form of C * (y|x) = p(x,y) p(x) .</p><p>Theorem 2.2. Given the optimal classifier, at the equilibrium point, optimizing the classification task for the generator of AC-GAN is equivalent to:</p><formula xml:id="formula_2">min G KL(Q X,Y P X,Y ) ? KL(Q X P X ) + H Q (Y |X),<label>(4)</label></formula><p>where H Q (Y |X) = ? y q(x, y) log q(y|x)dx is the conditional entropy of the generated samples.</p><p>The proofs of all theorems are referred to Appendix A. Our Theorem 2.2 exposes two shortcomings of AC-GAN. Firstly, maximization of the KL divergence between the marginal generator and data distributions (max G KL(Q X P X )) contradicts the goal of conditional generative modeling that matches Q X,Y with P X,Y . Although this issue can be mitigated to some extent by the adversarial game between the discriminator and generator that minimizes the JS divergence between the two marginal distributions (min G JS(Q X P X )), we find that it still has a negative impact on training stability and generation performance. Secondly, minimization of the entropy of labels conditioned  <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018)</ref>, AC-GAN <ref type="bibr" target="#b33">(Odena et al., 2017)</ref>, and TAC-GAN <ref type="bibr" target="#b7">(Gong et al., 2019)</ref>) and ADC-GAN. The symbol +/? indicates the GAN labels (real or fake) and y is the class-label of data x. ADC-GAN is different from PD-GAN by explicitly predicting the label and is different from AC-GAN and TAC-GAN in that the classifier C d also distinguishes real from generated, like the discriminator. on data of the generated distribution (min G H Q (Y |X)) will result in the label of the generated data being deterministic. In other words, it forces the generated data for each class away from the classification hyperplane, explaining the low intra-class diversity of the generated samples in AC-GAN, especially when the distributions of different classes have non-negligible overlap, which occurs naturally as the fact that neither state-of-the-art classifiers nor human beings can achieve 100% classification accuracy on real-world datasets <ref type="bibr" target="#b35">(Russakovsky et al., 2015)</ref>. The original AC-GAN, whose classifier is trained from both real and generated samples, suffers from the same issue (cf. Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method: ADC-GAN</head><p>The goal of conditional generative modeling is to faithfully learn the joint distribution of real data and labels regardless of the shape of the joint distribution (whether there is overlap between the distributions of different classes). We first note that the reason why AC-GAN fails to learn the target joint distribution (Theorem 2.2) originates from that the optimal classifier C * (y|x) = p(x,y) p(x) (Proposition 2.1) is agnostic to the density of the generated (marginal or joint) distribution (q(x) or q(x, y)). As a result, the classifier cannot provide the discrepancy between the target distribution and the generated distribution, resulting in a biased learning objective of the generator. Recall that the optimal discriminator D * (x) = p(x) p(x)+q(x) is aware of the real data distribution as well as the generated data distribution <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, and can therefore provide the discrepancy between the real and generated data distributions p(x) q(x) = D * (x) 1?D * (x) for faithful generative modeling of the generator. Intuitively, the distribution-aware ability on both real and generated data is caused by the fact that the discriminator distinguishes between the real and generated data with different labels (real or fake). Motivated by this understanding, we propose to make the classifier capable of classifying the the real and generated data with different class-labels, establishing a discriminative classifier C d : X ? Y + ? Y ? (Y + for real data and Y ? for generated data) that recognizes the label of the real and generated samples discriminatively. The generator is encouraged to produce classifiable real data rather than classifiable fake data. Mathematically, the objective functions for the discriminator, the discriminative classifier, and the generator of ADC-GAN are defined as:</p><formula xml:id="formula_3">max D,C d V (G, D) + ? ? (E x,y?P X,Y [log C d (y + |x)] +E x,y?Q X,Y [log C d (y ? |x)]), (5) min G V (G, D) ? ? ? (E x,y?Q X,Y [log C d (y + |x)] ?E x,y?Q X,Y [log C d (y ? |x)]), (6) where C d (y + |x) = exp(? + (y)??(x)) ? exp(? + (?)??(x))+ ? exp(? ? (?)??(x)) (resp. C d (y ? |x) = exp(? ? (y)??(x)) ? exp(? + (?)??(x))+ ? exp(? ? (?)??(x)) )</formula><p>indicates the probability that a data x is classified as the label y and real (resp. fake) simultaneously by the discriminative classifier. Here, ? : X ? R d is a feature extractor that is shared with the original discriminator in our implementation (D = ? ? ? ? ? with a linear mapping ? : R d ? R and a sigmoid function ? : R ? [0, 1]), and ? + : Y ? R d and ? ? : Y ? R d capture learnable embeddings of labels responsible to the real and generated data, respectively.</p><p>At the first glance, the objective function with the discriminative classifier for the generator seems to be redundant as maximization of log C d (y + |x) implicitly contains the goal of minimization of log C d (y ? |x). However, we show below that the second term is indispensable for accurately learning <ref type="table">Table 1</ref>: Theoretical learning objective for the generator of competing methods under the optimal discriminator and classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHOD THEORETICAL LEARNING OBJECTIVE FOR THE GENERATOR</head><p>AC-GAN (ODENA ET AL., 2017)</p><formula xml:id="formula_4">min G JS(P X Q X ) + ?(KL(Q X,Y P X,Y ) ? KL(Q X P X ) + H Q (Y |X)) TAC-GAN (GONG ET AL., 2019) min G JS(P X Q X ) + ?(KL(Q X,Y P X,Y ) ? KL(Q X P X )) ADC-GAN (OURS) min G JS(P X Q X ) + ?(KL(Q X,Y P X,Y )) PD-GAN (MIYATO &amp; KOYAMA, 2018) min G JS(Q X,Y P X,Y )</formula><p>the real joint data-label distribution. Arguably, maximization of log C d (y + |x) forces the generator to produce only few label-consistent data, facilitating the fidelity but losing the diversity of the generated samples. On the other hand, minimization of log C d (y ? |x) encourages the generator to not synthesis the typically label-consistent data, increasing the diversity but may degrade the fidelity of the generated samples. In general, the two objectives together assist the generator in achieving its goal as we proved below.</p><p>Proposition 3.1. For fixed generator, the optimal discriminative classifier of ADC-GAN has the form of the following:</p><formula xml:id="formula_5">C * d (y + |x) = p(x, y) p(x) + q(x) , C * d (y ? |x) = q(x, y) p(x) + q(x)</formula><p>.</p><p>Proposition 3.1 shows that the optimal discriminative classifier is aware of the densities of the real and generated joint distributions, therefore it is able to provide the discrepancy</p><formula xml:id="formula_6">p(x,y) q(x,y) = C * d (y + |x) C * d (y ? |x)</formula><p>to optimize the generator. Theorem 3.2. Given the optimal discriminative classifier, at the equilibrium point, optimizing the classification task for the generator of ADC-GAN is equivalent to:</p><formula xml:id="formula_7">min G KL(Q X,Y P X,Y ).<label>(7)</label></formula><p>Theorem 3.2 confirms that the discriminative classifier itself can guarantee the generator to restore the real joint distribution at the optimum. In practice, we retain the discriminator to train the generator for better training stability and convergence. The overall learning objective for the generator under the optimal discriminator and discriminative classfier is to minimize the JS divergence between the marginal data distributions and the reversed KL divergence bewteen the joint data-label distributions (min G JS(P X Q X ) + ? ? KL(Q X,Y P X,Y )). Since the optimal solution set for generative modeling contains the optimal solution set for conditional generative modeling (arg min G JS(P X Q X ) ? arg min G KL(Q X,Y P X,Y )), the guidance to the generator provided by discriminator and discriminative classifier are harmonious, which makes ADC-GAN robust to the value of the hyperparameter ? and the selection of the GAN loss V (G, D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis on Competing Methods</head><p>In this section, we analyze the drawbacks of the two competing methods, TAC-GAN <ref type="bibr" target="#b7">(Gong et al., 2019)</ref> and PD-GAN <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018)</ref>, to show the superiority of ADC-GAN. We also analyze AM-GAN <ref type="bibr" target="#b48">(Zhou et al., 2018)</ref> in Appendix C. Before diving into the details, we show diagrams of the discriminator and classifier of these methods in <ref type="figure" target="#fig_0">Figure 1</ref> and summarize the theoretical learning objective for the generator under the optimal discriminator and classifier of these methods in <ref type="table">Table 1</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Competing Method: TAC-GAN</head><p>TAC-GAN <ref type="bibr" target="#b7">(Gong et al., 2019)</ref> addresses the low intra-class diversity problem of AC-GAN by eliminating the conditional entropy of the generated data distribution H Q (Y |X) by learning the generator with another classifier C mi : X ? Y, which is trained with the generated samples. The objective functions for the discriminator, the twin classifiers, and the generator of TAC-GAN are defined as follows:</p><formula xml:id="formula_8">max D,C,Cmi V (G, D) + ? ? (E x,y?P X,Y [log C(y|x)] +E x,y?Q X,Y [log C mi (y|x)]), (8) min G V (G, D) ? ? ? (E x,y?Q X,Y [log C(y|x)] ?E x,y?Q X,Y [log C mi (y|x)]). (9)</formula><p>Theorem 4.1. Given the twin optimal classifiers, at the equilibrium point, optimizing the classification tasks for the generator of TAC-GAN is equivalent to:</p><formula xml:id="formula_9">min G KL(Q X,Y P X,Y ) ? KL(Q X P X ).<label>(10)</label></formula><p>Our Theorem 4.1 reveals that the learning objective of the generator of TAC-GAN, under the twin optimal classifiers, can be regarded as optimizing contradictory divergences, i.e., minimization between joint distributions but maximization between marginal distributions. Although theoretically the JS divergence or others <ref type="bibr" target="#b31">(Nowozin et al., 2016;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017)</ref> introduced through the adversarial training between the discriminator and generator may remedy this issue, it is difficult to obtain the optimal discriminator and classifier in the practical optimization to ensure the elimination of the contradiction. We argue that the training instability of TAC-GAN reported in the literature <ref type="bibr" target="#b22">(Kocaoglu et al., 2018;</ref><ref type="bibr" target="#b10">Han et al., 2020)</ref> and found in our experiments (cf. <ref type="figure" target="#fig_3">Figures 3(a)</ref> and 5) can be explained by this analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Competing Method: PD-GAN</head><p>PD-GAN <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018)</ref> injects the conditional information into the projection discriminator D p : X ?Y ? [0, 1] via the inner-product between the embedding of the label and the representation of the data to calculate the joint discriminative score of the data-label pair. In such a way, PD-GAN inherits the property of convergence point similar to the standard GAN such that it can avoid the low intraclass diversity problem of AC-GAN ideally. Specifically, the objective functions for the projection discriminator and the generator of PD-GAN are defined as follows:</p><formula xml:id="formula_10">min G max Dp V (G, D p ) = E x,y?P X,Y [log D p (x, y)] + E x,y?Q X,Y [log(1 ? D p (x, y))]</formula><p>. <ref type="formula">(11)</ref> Based on this formulation, the optimal projection discriminator has the following form:</p><formula xml:id="formula_11">D * p (x, y) = 1 1 + exp(?d * (x, y)) = p(x, y) p(x, y) + q(x, y) ? d * (x, y) = log p(x, y) q(x, y) = log p(x) q(x) + log p(y|x) q(y|x) ,<label>(12)</label></formula><p>where p(y|x) =</p><formula xml:id="formula_12">exp(? + (y)??(x)) ? exp(? + (?)??(x)) and q(y|x) = exp(? ? (y)??(x)) ? exp(? ? (?)??(x))</formula><p>. And PD-GAN accordingly defines:</p><formula xml:id="formula_13">r(x) := log p(x) q(x) := ?(?(x)), r(y|x) := log p(y|x) q(y|x) := ( ?(y) ? + (y) ? ? ? (y)) ? ?(x) r(y|x) ? (13) log ??Y exp ? + (?) ? ?(x) + log ??Y exp ? ? (?) ? ?(x) a .</formula><p>However, PD-GAN actually ignores the partition term a 2 in Equation 13 and heuristically constructs the logit of the projection discriminator in the form of:</p><formula xml:id="formula_14">d(x, y) = r(x) +r(y|x) = ?(?(x)) + ?(y) ? ?(x). (14)</formula><p>Discarding the partition term would make PD-GAN no longer belong to probability models that are able to model the conditional probabilities p(y|x) and q(y|x), resulting in losing the complete dependencies between data and labels. Particularly, for mismatched data-label pair (x, y) with probabilities of p(x, y) = 0 and q(x, y) = 0, the projection discriminator D * p (x, y) = p(x,y) p(x,y)+q(x,y) = 0 0 is undefined and thus unreliable. Our ADC-GAN can penalize the mismatched data-label pair because C * d (y + |x) = p(x,y) p(x)+q(x) = 0 &gt;0 = 0 (p(x) + q(x) &gt; 0 for valid data x). Moreover, the optimal projection discriminator constructed according to the minimax GAN lacks theoretical guarantees on other GAN loss functions. The proposed ADC-GAN can be flexibly applied to any version of the GAN loss as we do not require a specific form of the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Data</head><p>We first conduct experiments on a one-dimensional synthetic mixture of Gaussians, following the practices of <ref type="bibr" target="#b7">(Gong et al., 2019)</ref>, to qualitatively show the fidelity of distribution learning capability of ADC-GAN. As shown in Without the GAN loss V (G, D), AC-GAN outputs nearly deterministic data for each class. TAC-GAN without the GAN loss also cannot accurately capture the real data distribution, verifying the contradiction in Theorem 4.1. Impressively, the proposed ADC-GAN faithfully restores the real data distribution even without the GAN loss, validating Theorem 3.2 that the discriminative classfier alone can guide the generator to learn the real data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments based on BigGAN-PyTorch</head><p>In this section, we conduct experiments on three common real-world datasets: CIFAR-10, CIFAR-100 <ref type="bibr" target="#b23">(Krizhevsky et al., 2009)</ref>, and Tiny-ImageNet <ref type="bibr" target="#b25">(Le &amp; Yang, 2015)</ref> based on the BigGAN-PyTorch repository 3 with our extensions 4 . The optimizer is Adam with learning rate of 2 ? 10 ?4 on CIFAR-10/100 and 1 ? 10 ?4 for the generator and 4 ? 10 ?4 for the discriminator on Tiny-ImageNet. We train all methods for 1000 and 500 epochs with batch size of 50 and 100  on CIFAR-10/100 and Tiny-ImageNet, respectively. The discriminator/classifier are updated 4 and 2 times per generator update step on CIFAR-10/100 and Tiny-ImageNet, respectively. We follow the practice of <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018;</ref><ref type="bibr" target="#b7">Gong et al., 2019)</ref> to adopt the hinge loss <ref type="bibr" target="#b26">(Lim &amp; Ye, 2017;</ref><ref type="bibr" target="#b40">Tran et al., 2017)</ref> as the implementation of V (G, D). The coefficient hyperparameters of AC-GAN and AM-GAN (Zhou et al., 2018) (cf. Appendix C for analysis) are set as ? = 0.2 as it performs the best. As for TAC-GAN and ADC-GAN, the coefficient hyperparameters are set as ? = 1.0 on CIFAR-10/100 and ? = 0.5 on Tiny-ImageNet.</p><p>Image Generation. We use the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b12">(Heusel et al., 2017)</ref> and Intra-FID <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018)</ref> metrics to measure the overall and intraclass qualities of the generated images, respectively. <ref type="table" target="#tab_0">Table 2</ref> shows that ADC-GAN obtains the best FID and Intra-FID scores on all three datasets, indicating consistent superiority over previous cGANs in conditional image generation.</p><p>Training Stability. We also note that ADC-GAN yields the best training stability according to the FID training curves (cf. <ref type="figure" target="#fig_3">Figures 3(a)</ref> and 5). Even without the discriminator, the training stability ADC-GAN (w/o D) still exceeds that of most competing methods. AC-GAN diverges during training on all three datasets. TAC-GAN also diverges on CIFAR-100 and Tiny-ImageNet and achieves a relatively stable FID training curve only on the simplest dataset, CIFAR-10. We hence report the results of all methods using the best checkpoint. These unstable FID training curves implicitly verify the drawback of existing classifier-based cGANs that optimize contradictory divergences.</p><p>Different Coefficients. To explicitly show the above issues, we set the objective function of classifier-based cGANs as <ref type="figure">C)</ref> is the task between the generator and classifier. As shown in <ref type="figure" target="#fig_3">Figures 3(b)</ref> and 6, ADC-GAN consistently gains superior FID scores across different coefficient hyperparameters even for </p><formula xml:id="formula_15">(1 ? ? )V (G, D) + ? V C (G, C), where V C (G,</formula><formula xml:id="formula_16">(1 ? ? )V (G, D) + ? V C (G, C), where V C (G, C)</formula><p>is the task between the generator and classifier. Data-to-Class Relations. To investigate whether the model captures appropriate data-to-class relations, we conduct image classification experiments based on the learned representations of the discriminator/classifier ?(x). Specifically, we first train a logistic regression classifier using the scikit-learn library with the training data and compute the classification accuracy of the validation data. As reported in <ref type="table" target="#tab_0">Table 2</ref>, ADC-GAN significantly outperforms competing methods on all datasets in terms of the Accuracy metrics. The reason is that the discriminative classifier needs to recognize the labels of data while simultaneously distinguishing between real and fake data, which facilitates the robustness of the classifier in modeling data-to-class relations. Notice that PD-GAN obtains the worst results. By comparing the CIFAR-10 T-SNE (Van der Maaten &amp; Hinton, 2008) visualization results of PD-GAN and ADC-GAN in <ref type="figure">Figure 4</ref>, it is clear that PD-GAN does not have the ability to learn proper data-to-class relations as ADC-GAN does, reflecting the problem caused by the loss of partition terms in PD-GAN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments based on PyTorch-StudioGAN</head><p>In this section, we compare ADC-GAN with state-ofthe-art cGANs using the PyTorch-StudioGAN repository 5 , of which evaluation protocols are different from that of the BigGAN-PyTorch repository that we used in <ref type="table" target="#tab_0">Table 2</ref>. Nonetheless, our comparison is fair because the methods in each experiment follows the same evaluation protocol.</p><p>Image Generation on ImageNet. We first conduct experiments on ImageNet (128 ? 128) following the experimental settings of ReACGAN <ref type="bibr" target="#b16">(Kang et al., 2021)</ref>. <ref type="table" target="#tab_1">Table 3</ref> reports the Inception Score (IS) <ref type="bibr" target="#b37">(Salimans et al., 2016)</ref> and FID results. Our ADC-GAN is comparable with the state-of-theart cGANs, BigGAN and ReACGAN <ref type="bibr" target="#b16">(Kang et al., 2021)</ref>, in the batch size of 256 and 2048, showing effectiveness on large-scale high-resolution image datasets. Notice that, however, we only ran our ADC-GAN once with ? = 1 in each of the two batch size settings, and did not make other attempts due to our limited computational resources. We argue that the results of ADC-GAN can be improved by choosing an appropriate coefficient hyperparameter ?.</p><p>Different GAN Losses. We also investigate the robustness of ADC-GAN with respect to the GAN loss function V (G, D) by adopting different versions. <ref type="table" target="#tab_2">Table 4</ref> report the qualitative results on CIFAR-100 (cf. <ref type="table" target="#tab_3">Table 5</ref> in Appendix D for complete results). Impressively, the proposed ADC-GAN achieves the best iFID (intra-FID), recall <ref type="bibr" target="#b24">(Kynk??nniemi et al., 2019)</ref>, and coverage <ref type="bibr" target="#b30">(Naeem et al., 2020)</ref> scores across the non-saturation <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, WGAN-GP <ref type="bibr" target="#b9">(Gulrajani et al., 2017)</ref>, and hinge <ref type="bibr" target="#b26">(Lim &amp; Ye, 2017)</ref> versions of the GAN loss. The best iFID scores indicate the best conditional generative modeling performance, and the best recall and coverage results reflect the best (intra-class) diversity of the generated samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Efforts on developing cGANs <ref type="bibr" target="#b28">(Mirza &amp; Osindero, 2014)</ref> can be divided into two steps. The first is to study how to implement a conditional generator. Methods in this category are concatenation <ref type="bibr" target="#b28">(Mirza &amp; Osindero, 2014)</ref>, conditional batch normalization <ref type="bibr" target="#b6">(de Vries et al., 2017)</ref>, and conditional convolution layers <ref type="bibr" target="#b36">(Sagong et al., 2019)</ref>. The second is to study how to train the conditional generator to produce labeldependent samples, which can be further divided into two categories, classifier-based and projection-based cGANs.</p><p>Classifier-based cGANs. AC-GAN <ref type="bibr" target="#b33">(Odena et al., 2017)</ref> leveraged an auxiliary classifier to identify consistency between data and labels. MH-GAN <ref type="bibr" target="#b21">(Kavalerov et al., 2021)</ref> improved AC-GAN by replacing the cross-entropy loss of the classifier with the multi-hinge loss. AM-GAN <ref type="bibr" target="#b48">(Zhou et al., 2018)</ref> replaced the discriminator with a K + 1-way classifier with an additional "fake" label. Omni-GAN <ref type="bibr" target="#b47">(Zhou et al., 2020)</ref> combined the discriminator with the classifier to construct a K + 2-dimensional multi-label classifier. TAC-GAN <ref type="bibr" target="#b7">(Gong et al., 2019)</ref> corrected the biased learning objective of AC-GAN by introducing another classifier, which is the multi-class version of Anti-Labeler of CausalGAN <ref type="bibr" target="#b22">(Kocaoglu et al., 2018)</ref>. UAC-GAN <ref type="bibr" target="#b10">(Han et al., 2020)</ref> improved the training stability of TAC-GAN with MINE <ref type="bibr" target="#b1">(Belghazi et al., 2018)</ref>. ECGAN <ref type="bibr" target="#b4">(Chen et al., 2021)</ref> provides a unified view of cGANs with and without classifiers. Orthogonally to our work, ContraGAN <ref type="bibr" target="#b15">(Kang &amp; Park, 2020)</ref> and ReACGAN <ref type="bibr" target="#b16">(Kang et al., 2021)</ref> modeled data-to-data relations as well as data-to-class relations using the conditional contrastive loss and the data-to-data crossentropy loss, respectively. However, they did not solve the low intra-class diversity problem of AC-GAN as they inherited the generator-agnostic classifier.</p><p>Projection-based cGANs. PD-GAN <ref type="bibr" target="#b29">(Miyato &amp; Koyama, 2018)</ref> injected the class information into the discriminator via label projection and achieved the state-of-the-art generation quality of natural images <ref type="bibr" target="#b2">(Brock et al., 2019;</ref><ref type="bibr" target="#b43">Wu et al., 2019;</ref><ref type="bibr" target="#b45">Zhang et al., 2020;</ref><ref type="bibr" target="#b46">Zhao et al., 2021)</ref>. P2GAN <ref type="bibr" target="#b11">(Han et al., 2021)</ref> further improved PD-GAN by compensating the missed partition term in the objective function.</p><p>Discriminative classifiers. <ref type="bibr" target="#b42">Watanabe &amp; Favaro (2021)</ref> exploited the discriminative classifier for training GANs with any level of labeling but different from us with the objective function for the generator, which enables ADC-GAN to faithfully learn the target distribution. SSGAN-LA <ref type="bibr" target="#b13">(Hou et al., 2021a)</ref> presented the similar idea but different loss functions with ADC-GAN (multi-hinge v.s. cross-entropy) to tackle the degraded learning objective of self-supervised GANs, while ADC-GAN is for conditional GANs. Moreover, our analysis of the degradation objective is more accurate and informative than that of SSGAN-LA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we present a novel conditional generative adversarial network with an auxiliary discriminative classifier (ADC-GAN) to achieve faithful conditional generative modeling. We also discuss the differences between ADC-GAN with competing cGANs and analyze their potential issues and limitations. Extensive experimental results validate the theoretical superiority of ADC-GAN compared with stateof-the-art classifier-based and projection-based cGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>A.1. Proof of Proposition 2.1 Proposition 2.1. For fixed generator, the optimal classifier of AC-GAN has the form of C * (y|x) = p(x,y) p(x) .</p><p>Proof.</p><formula xml:id="formula_17">max C E x,y?P X,Y [log C(y|x)] = E x?P X E y?P Y |X [log C(y|x)] (15) ? min C E x?P X E y?P Y |X [? log C(y|x)] = E x?P X [H(p(y|x)) + KL(p(y|x) C(y|x))] (16) ? C * (y|x) = arg min C KL(p(y|x) C(y|x)) = p(y|x) = p(x, y) p(x)<label>(17)</label></formula><p>A.2. Proof of Theorem 2.2 Theorem 2.2. Given the optimal classifier, at the equilibrium point, optimizing the classification task for the generator of AC-GAN is equivalent to: min</p><formula xml:id="formula_18">G KL(Q X,Y P X,Y ) ? KL(Q X P X ) + H Q (Y |X),<label>(4)</label></formula><p>where H Q (Y |X) = ? y q(x, y) log q(y|x)dx is the conditional entropy of the generated samples.</p><p>Proof.</p><formula xml:id="formula_19">max G E x,y?Q X,Y [log C * (y|x)] = E x,y?Q X,Y log p(x, y) p(x) = E x,y?Q X,Y log p(x, y) q(x, y) q(x) p(x) q(x, y) q(x) (18) ? min G E x,y?Q X,Y log q(x, y) p(x, y) ? E x?Q X log q(x) p(x) ? E x,y?Q X,Y log q(x, y) q(x) (19) ? min G KL(Q X,Y P X,Y ) ? KL(Q X P X ) + H Q (Y |X)<label>(20)</label></formula><p>A.3. Proof of Proposition 3.1 Proposition 3.1. For fixed generator, the optimal discriminative classifier of ADC-GAN has the form of the following:</p><formula xml:id="formula_20">C * d (y + |x) = p(x, y) p(x) + q(x) , C * d (y ? |x) = q(x, y) p(x) + q(x) . Proof. max C d E x,y?P X,Y [log C d (y + |x)] + E x,y?Q X,Y [log C d (y ? |x)] ? max C d E x,y?P m X,Y [log C d (y|x)],<label>(21)</label></formula><p>with p m (x, y + ) = 1 2 p(x, y), p m (x, y ? ) = 1 2 q(x, y), and p m (x) = y p m (x, y) = 1 2 p(x) + 1 2 q(x).</p><formula xml:id="formula_21">? max C d E x?P m X E y?P m Y |X [log C d (y|x)] ? min C d E x?P m X E y?P m Y |X [? log C d (y|x)]<label>(22)</label></formula><p>? min</p><formula xml:id="formula_22">C d E x?P m X [H(p m (y|x)) + KL(p m (y|x) C d (y|x))] (23) ? C * d (y|x) = arg min C d KL(p m (y|x) C d (y|x)) = p m (y|x) = p m (x, y) p m (x)<label>(24)</label></formula><p>Therefore, the optimal discriminative classifier of ADC-GAN has the form of C Theorem 3.2. Given the optimal discriminative classifier, at the equilibrium point, optimizing the classification task for the generator of ADC-GAN is equivalent to: min G KL(Q X,Y P X,Y ).</p><p>Proof.</p><formula xml:id="formula_24">max G E x,y?Q X,Y log C * d (y + |x) ? E x,y?Q X,Y log C * d (y ? |x) (25) ? max G E x,y?Q X,Y log p(x, y) p(x) + q(x) ? E x,y?Q X,Y log q(x, y) p(x) + q(x) (26) ? min G E x,y?Q X,Y log q(x, y) p(x, y) ? min G KL(Q X,Y P X,Y )<label>(27)</label></formula><p>A.5. Proof of Theorem 4.1 Proposition A.1. For fixed generator, the twin optimal classifiers of TAC-GAN have the following forms:</p><formula xml:id="formula_25">C * (y|x) = p(x, y) p(x) , C * mi (y|x) = q(x, y) q(x) .<label>(28)</label></formula><p>Proof. The proof is similar to that of Proposition 2.1 in Appendix A.1 by considering C and C mi as two independent classifiers with respect to distribution P and Q, respectively.</p><p>Theorem 4.1. Given the twin optimal classifiers, at the equilibrium point, optimizing the classification tasks for the generator of TAC-GAN is equivalent to:</p><formula xml:id="formula_26">min G KL(Q X,Y P X,Y ) ? KL(Q X P X ).<label>(10)</label></formula><p>Proof.</p><formula xml:id="formula_27">max G E x,y?Q X,Y [log C * (y|x)] ? E x,y?Q X,Y [log C * mi (y|x)] (29) ? max G E x,y?Q X,Y log p(x, y) p(x) ? E x,y?Q X,Y log q(x, y) q(x) (30) ? max G E x,y?Q X,Y log p(x, y) q(x, y) ? E x?Q X log p(x) q(x) (31) ? min G KL(Q X,Y P X,Y ) ? KL(Q X P X )<label>(32)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis on the Original AC-GAN</head><p>In this section, we show that original AC-GAN whose auxiliary classifier is trained with both real and generated samples still suffers from the same issue as we proved in Theorem 2.2. Formally, the full objective function of the original AC-GAN is formulated as the following:</p><formula xml:id="formula_28">max D,C V (G, D) + ? ? E x,y?P X,Y [log C(y|x)] + E x,y?Q X,Y [log C(y|x)] ,<label>(33)</label></formula><formula xml:id="formula_29">min G V (G, D) ? ? ? E x,y?Q X,Y [log C(y|x)] .<label>(34)</label></formula><p>The objective function for training the classifier can be rewritten as:</p><formula xml:id="formula_30">max C E x,y?P X,Y [log C(y|x)] + E x,y?Q X,Y [log C(y|x)] ? max C E x,y?P m X,Y [log C(y|x)],<label>(35)</label></formula><p>with p m (x, y) = 1 2 (p(x, y) + q(x, y)) and p m (x) = y p m (x, y) = 1 2 (p(x) + q(x)). And we can obtain the optimal classifier according to the following:</p><formula xml:id="formula_31">max C E x,y?P m X,Y [log C(y|x)] ? min C E x?P m X ,y?P m Y |X [? log C(y|x)] (36) ? min C E x?P m X [H(p m (y|x)) + KL(p m (y|x) C(y|x))] (37) ? C * (y|x) = p m (y|x) = p(x, y) + q(x, y) p(x) + q(x) .<label>(38)</label></formula><p>Suppose that the conditional generator learns the joint distribution of real data and labels, i.e., q(x, y) = p(x, y) and q(x) = p(x), the optimal classifier C * (y|x) = p(x,y)+q(x,y) p(x)+q(x) = p(x,y) p(x) also provide the objective stated in Theorem 2.2 for the generator, which contains the conditional entropy of the generated samples H Q (Y |X) that reduces the intra-class diversity of the generated samples. In other words, the original classifier does not allow the generator to remain on the desired distribution because it still provides momentum to update the generator, resulting in a biased learning objective for the generator in the original version of AC-GAN. The essential reason is that the classifier of the original AC-GAN is incapable of distinguishing the real data from the generated data. Therefore, the classifier of the original AC-GAN cannot provide the difference between the real and generated joint distributions to optimize the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis on AM-GAN</head><p>AM-GAN <ref type="bibr" target="#b48">(Zhou et al., 2018)</ref> optimizes the following objectives with an label-extended discriminator D + : X ? Y ? {0}:</p><formula xml:id="formula_32">max D+ E x,y?P X,Y [log D + (y|x)] + E x,y?Q X,Y [log D + (0|x)],<label>(39)</label></formula><formula xml:id="formula_33">min G E x,y?Q X,Y [log D + (y|x)].<label>(40)</label></formula><p>The objective function for training the discriminator D + can be rewritten as:</p><formula xml:id="formula_34">max D+ E x,y?P X,Y [log D + (y|x)] + E x,y?Q X,Y [log D + (0|x)] ? max D+ E x,y?P m X,Y [log D + (y|x)],<label>(41)</label></formula><p>where p m (x, y) = 1 2 p(x, y), ?y ? Y, p m (x, 0) = 1 2 q(x), and p m (x) = y p m (x, y) = 1 2 (p(x) + q(x)). Then we have:</p><formula xml:id="formula_35">max D+ E x,y?P m X,Y [log D + (y|x)] ? min D+ E x?P m X ,y?P m Y |X [? log D + (y|x)]<label>(42)</label></formula><p>? min</p><formula xml:id="formula_36">D+ E x?P m X [H(p m (y|x)) + KL(p m (y|x) D + (y|x))] ? D * + (y|x) = p m (y|x) = p(x, y) p(x) + q(x)</formula><p>, ?y ? Y. <ref type="formula" target="#formula_1">(43)</ref> Under the optimal discriminator D * + , the generator of AM-GAN can be regarded as optimizing the following:</p><formula xml:id="formula_37">max G E x,y?Q X,Y [log D * + (y|x)] ? max G E x,y?Q X,Y log p(x, y) p(x) + q(x) (44) ? min G E x,y?Q X,Y log q(x, y) p(x, y) p(x) + q(x) q(x, y) = E x,y?Q X,Y log q(x, y) p(x, y) + log p(x) + q(x) 2 ? log q(x, y) + log 2 (45) ? min G E x,y?Q X,Y log q(x, y) p(x, y) + 1 2 log p(x) + 1 2 log q(x) ? log q(x, y) + log 2 (46) ? min G E x,y?Q X,Y log q(x, y) p(x, y) ? 1 2 log q(x) p(x) ? log q(x, y) q(x) + log 2 (47) ? min G KL(Q X,Y P X,Y ) ? 1 2 KL(Q X P X ) + H Q (Y |X) + log 2.<label>(48)</label></formula><p>In summary, AM-GAN with the original discriminator remained (compared in our experiments) can be considered to be minimizing an upper bound of JS(Q X P X ) + KL(Q X,Y P X,Y ) ? 1 2 KL(Q X P X ) + H Q (Y |X) + log 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results</head><formula xml:id="formula_38">1 ? ? )V (G, D) + ? V C (G, C), where V C (G, C)</formula><p>is the task between the generator and classifier. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of discriminators/classifiers of existing cGANs (PD-GAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2(a), the real data distribution consists of three classes with non-negligible overlaps. Figures 2(b) to 2(d) show the learned distributions, which are estimated by kernel density estimation (KDE) (Parzen, 1962) on the generated data of AC-GAN, TAC-GAN, and ADC-GAN without the original GAN loss V (G, D), respectively. Figures 2(e) to 2(h) show the KDE results of PD-GAN, AC-GAN, TAC-GAN, and ADC-GAN trained with the non-saturating GAN loss (Goodfellow et al., 2014), respectively. AC-GAN tends to generate classifiable data so that it decreases the intra-class diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative comparison of distribution modeling results on the one-dimensional synthetic data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) FID curves during GAN training on CIFAR-100. (b) FID scores of classifier-based cGANs with different ? on CIFAR-100. The objective function in this experiment is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>T-SNE visualization of CIFAR-10 validation data based on learned representations extracted from the penultimate layer in the discriminator/classifier ?(x). Different colors indicate different classes. ? = 1.0 (i.e., without the discriminator), showing strong robustness with respect to? , while AC-GAN and TAC-GAN perform substantially worse when ? becomes larger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>* d (y + |x) = p m (x,y + ) p m (x) = p(x,y) p(x)+q(x) and C * d (y ? |x) = p m (x,y ? ) p m (x) = q(x,y) p(x)+q(x) that conclude the proof. A.4. Proof of Theorem 3.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) FID curves on CIFAR-10 (b) FID curves on Tiny-ImageNet Figure 5: FID curves during GAN training on CIFAR-10 and Tiny-ImageNet, respectively. (a) FID with different ? on CIFAR-10 (b) FID with different ? on Tiny-ImageNet Figure 6: FID comparisons of classifier-based cGANs with different coefficient hyperparameters ? on CIFAR-10 and Tiny-ImageNet, respectively. The objective function in this experiment is (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>DATASETS</cell><cell>METRICS</cell><cell cols="5">PD-GAN AC-GAN AM-GAN TAC-GAN ADC-GAN</cell></row><row><cell></cell><cell>FID (?)</cell><cell>6.23</cell><cell>6.50</cell><cell>6.81</cell><cell>5.83</cell><cell>5.66</cell></row><row><cell>CIFAR-10</cell><cell>INTRA-FID (?)</cell><cell>48.90</cell><cell>57.67</cell><cell>69.31</cell><cell>56.67</cell><cell>40.45</cell></row><row><cell></cell><cell>ACCURACY (?)</cell><cell>66.22</cell><cell>84.69</cell><cell>83.63</cell><cell>88.27</cell><cell>89.51</cell></row><row><cell></cell><cell>FID (?)</cell><cell>8.70</cell><cell>11.24</cell><cell>10.42</cell><cell>10.38</cell><cell>8.12</cell></row><row><cell>CIFAR-100</cell><cell>INTRA-FID (?)</cell><cell>51.15</cell><cell>83.06</cell><cell>78.11</cell><cell>79.59</cell><cell>49.24</cell></row><row><cell></cell><cell>ACCURACY (?)</cell><cell>37.89</cell><cell>55.26</cell><cell>55.77</cell><cell>60.03</cell><cell>64.24</cell></row><row><cell></cell><cell>FID (?)</cell><cell>26.10</cell><cell>25.02</cell><cell>21.34</cell><cell>21.12</cell><cell>19.02</cell></row><row><cell>TINY-IMAGENET</cell><cell>INTRA-FID (?)</cell><cell>66.23</cell><cell>99.04</cell><cell>90.56</cell><cell>95.48</cell><cell>63.05</cell></row><row><cell></cell><cell>ACCURACY (?)</cell><cell>27.79</cell><cell>44.59</cell><cell>44.67</cell><cell>44.44</cell><cell>48.89</cell></row></table><note>FID and Intra-FID and Accuracy (%) comparisons on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>FID and IS comparisons on ImageNet (128 ? 128). B.S. means the batch size and Iters. means the training iterations. Results of BigGAN and ReACGAN are copied from the ReACGAN paper<ref type="bibr" target="#b16">(Kang et al., 2021)</ref>.</figDesc><table><row><cell cols="2">B.S. ITERS.</cell><cell>METHODS</cell><cell>IS (?)</cell><cell>FID (?)</cell></row><row><cell></cell><cell></cell><cell>BIGGAN</cell><cell>43.97</cell><cell>16.36</cell></row><row><cell>256</cell><cell>500K</cell><cell>REACGAN</cell><cell>68.27</cell><cell>13.98</cell></row><row><cell></cell><cell></cell><cell>ADC-GAN</cell><cell>66.96</cell><cell>11.65</cell></row><row><cell></cell><cell></cell><cell>BIGGAN</cell><cell>99.71</cell><cell>7.89</cell></row><row><cell>2048</cell><cell>200K</cell><cell>REACGAN ADC-GAN</cell><cell>92.74 97.47</cell><cell>8.23 9.46</cell></row><row><cell></cell><cell>500K</cell><cell cols="2">ADC-GAN 108.10</cell><cell>8.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>IS, FID, iFID, Precision, Recall, Density, and Coverage comparisons with state-of-the-art methods under different GAN loss functions on CIFAR-100, respectively. The best results are bold and the second best are underlined.</figDesc><table><row><cell>GAN LOSS</cell><cell>METHODS</cell><cell>IS ?</cell><cell>FID ?</cell><cell>IFID ?</cell><cell cols="4">PRECISION ? RECALL ? DENSITY ? COVERAGE ?</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>11.48</cell><cell>11.59</cell><cell>105.38</cell><cell>0.7337</cell><cell>0.6804</cell><cell>0.8646</cell><cell>0.8513</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>7.98</cell><cell>49.46</cell><cell>207.56</cell><cell>0.7322</cell><cell>0.0793</cell><cell>0.6225</cell><cell>0.4112</cell></row><row><cell>NON-SATURATION</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="3">11.34 11.88 11.07 104.21 14.47 131.90</cell><cell>0.7429 0.7379</cell><cell>0.6077 0.6972</cell><cell>0.8324 0.8521</cell><cell>0.7887 0.8609</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>11.15</cell><cell>13.54</cell><cell>146.86</cell><cell>0.7390</cell><cell>0.6155</cell><cell>0.8481</cell><cell>0.7729</cell></row><row><cell></cell><cell>REACGAN</cell><cell>11.79</cell><cell>13.72</cell><cell>125.21</cell><cell>0.7541</cell><cell>0.5861</cell><cell>0.8695</cell><cell>0.8005</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>5.66</cell><cell>69.48</cell><cell>?</cell><cell>0.5976</cell><cell>0.1603</cell><cell>0.4310</cell><cell>0.2649</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>10.97</cell><cell>19.30</cell><cell>148.40</cell><cell>0.6880</cell><cell>0.5444</cell><cell>0.6770</cell><cell>0.7242</cell></row><row><cell>W-GP</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="3">11.04 15.56 11.01 14.02 101.14 121.23</cell><cell>0.7023 0.7058</cell><cell>0.6474 0.6804</cell><cell>0.7048 0.7549</cell><cell>0.7535 0.7956</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>6.72</cell><cell>49.77</cell><cell>147.22</cell><cell>0.6498</cell><cell>0.2834</cell><cell>0.5827</cell><cell>0.3549</cell></row><row><cell></cell><cell>REACGAN</cell><cell>6.67</cell><cell>47.74</cell><cell>150.7</cell><cell>0.6188</cell><cell>0.3104</cell><cell>0.4806</cell><cell>0.3396</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>11.76</cell><cell>10.96</cell><cell>108.08</cell><cell>0.7436</cell><cell>0.6812</cell><cell>0.8790</cell><cell>0.8609</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>11.66</cell><cell>21.65</cell><cell>168.87</cell><cell>0.7577</cell><cell>0.3649</cell><cell>0.8297</cell><cell>0.7225</cell></row><row><cell>HINGE</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="3">12.07 12.56 11.82 10.73 103.78 134.75</cell><cell>0.7572 0.7387</cell><cell>0.6020 0.7023</cell><cell>0.8957 0.8721</cell><cell>0.8400 0.8707</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>10.08</cell><cell>13.22</cell><cell>128.50</cell><cell>0.7372</cell><cell>0.6251</cell><cell>0.8356</cell><cell>0.7790</cell></row><row><cell></cell><cell>REACGAN</cell><cell>11.80</cell><cell>12.52</cell><cell>140.47</cell><cell>0.7510</cell><cell>0.5982</cell><cell>0.9300</cell><cell>0.8327</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>IS, FID, iFID, Precision, Recall, Density, and Coverage comparisons of competing methods under different GAN loss functions on CIFAR-10 and CIFAR-100, respectively. The best results are bold and the second best are underlined.</figDesc><table><row><cell>CIFAR-10</cell><cell>METHODS</cell><cell>IS ?</cell><cell>FID ?</cell><cell>IFID ?</cell><cell cols="4">PRECISION ? RECALL ? DENSITY ? COVERAGE ?</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>9.68</cell><cell>8.93</cell><cell>81.30</cell><cell>0.7581</cell><cell>0.6718</cell><cell>1.0622</cell><cell>0.9208</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>9.74</cell><cell>9.21</cell><cell>87.76</cell><cell>0.7592</cell><cell>0.6484</cell><cell>1.0491</cell><cell>0.9147</cell></row><row><cell>NON-SATURATION</cell><cell>TAC-GAN ADC-GAN</cell><cell>9.61 9.87</cell><cell>9.31 8.47</cell><cell>81.04 77.69</cell><cell>0.7349 0.7497</cell><cell>0.6717 0.6912</cell><cell>0.9575 0.9968</cell><cell>0.8990 0.9202</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>9.60</cell><cell>8.87</cell><cell>120.45</cell><cell>0.7598</cell><cell>0.6595</cell><cell>1.0025</cell><cell>0.9061</cell></row><row><cell></cell><cell>REACGAN</cell><cell>9.69</cell><cell>8.51</cell><cell>113.23</cell><cell>0.7648</cell><cell>0.6594</cell><cell>1.0532</cell><cell>0.9242</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>9.99</cell><cell>8.72</cell><cell>80.11</cell><cell>0.7525</cell><cell>0.6771</cell><cell>1.0395</cell><cell>0.9182</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>5.01</cell><cell>81.93</cell><cell>176.24</cell><cell>0.7389</cell><cell>0.0037</cell><cell>0.7484</cell><cell>0.2129</cell></row><row><cell>LEAST SQUARE</cell><cell>TAC-GAN ADC-GAN</cell><cell>9.41 9.89</cell><cell>10.67 8.61</cell><cell>80.92 75.86</cell><cell>0.7386 0.7405</cell><cell>0.6520 0.6919</cell><cell>0.9159 0.9944</cell><cell>0.8657 0.9223</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>9.10</cell><cell>12.93</cell><cell>135.75</cell><cell>0.7661</cell><cell>0.5761</cell><cell>1.0236</cell><cell>0.8262</cell></row><row><cell></cell><cell>REACGAN</cell><cell>9.80</cell><cell>9.52</cell><cell>125.83</cell><cell>0.7772</cell><cell>0.5988</cell><cell>1.1008</cell><cell>0.9138</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>5.27</cell><cell>75.24</cell><cell>104.15</cell><cell>0.5569</cell><cell>0.2132</cell><cell>0.3678</cell><cell>0.2141</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>8.88</cell><cell>14.77</cell><cell>88.02</cell><cell>0.7015</cell><cell>0.6477</cell><cell>0.7421</cell><cell>0.7798</cell></row><row><cell>W-GP</cell><cell>TAC-GAN ADC-GAN</cell><cell>8.93 9.49</cell><cell>13.26 11.25</cell><cell>76.93 74.98</cell><cell>0.6847 0.6996</cell><cell>0.6705 0.7019</cell><cell>0.7454 0.8182</cell><cell>0.8127 0.8517</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>6.38</cell><cell>51.43</cell><cell>137.17</cell><cell>0.5640</cell><cell>0.3995</cell><cell>0.4040</cell><cell>0.2931</cell></row><row><cell></cell><cell>REACGAN</cell><cell>6.60</cell><cell>44.62</cell><cell>117.25</cell><cell>0.5813</cell><cell>0.4333</cell><cell>0.4559</cell><cell>0.3287</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>9.79</cell><cell>8.45</cell><cell>79.40</cell><cell>0.7464</cell><cell>0.6853</cell><cell>1.0083</cell><cell>0.9158</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>9.96</cell><cell>8.97</cell><cell>88.40</cell><cell>0.7681</cell><cell>0.6523</cell><cell>1.0250</cell><cell>0.9168</cell></row><row><cell>HINGE</cell><cell>TAC-GAN ADC-GAN</cell><cell>9.78 9.63</cell><cell>8.80 8.42</cell><cell>81.30 75.50</cell><cell>0.7446 0.7447</cell><cell>0.6749 0.6882</cell><cell>1.0026 0.9854</cell><cell>0.9103 0.9193</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>9.63</cell><cell>8.89</cell><cell>85.39</cell><cell>0.7582</cell><cell>0.6538</cell><cell>1.0411</cell><cell>0.9098</cell></row><row><cell></cell><cell>REACGAN</cell><cell>9.83</cell><cell>8.84</cell><cell>78.07</cell><cell>0.7623</cell><cell>0.6675</cell><cell>1.0003</cell><cell>0.9158</cell></row><row><cell>CIFAR-100</cell><cell>METHODS</cell><cell>IS ?</cell><cell>FID ?</cell><cell>IFID ?</cell><cell cols="4">PRECISION ? RECALL ? DENSITY ? COVERAGE ?</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>11.48</cell><cell>11.59</cell><cell>105.38</cell><cell>0.7337</cell><cell>0.6804</cell><cell>0.8646</cell><cell>0.8513</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>7.98</cell><cell>49.46</cell><cell>207.56</cell><cell>0.7322</cell><cell>0.0793</cell><cell>0.6225</cell><cell>0.4112</cell></row><row><cell>NON-SATURATION</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="3">11.34 11.88 11.07 104.21 14.47 131.90</cell><cell>0.7429 0.7379</cell><cell>0.6077 0.6972</cell><cell>0.8324 0.8521</cell><cell>0.7887 0.8609</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>11.15</cell><cell>13.54</cell><cell>146.86</cell><cell>0.7390</cell><cell>0.6155</cell><cell>0.8481</cell><cell>0.7729</cell></row><row><cell></cell><cell>REACGAN</cell><cell>11.79</cell><cell>13.72</cell><cell>125.21</cell><cell>0.7541</cell><cell>0.5861</cell><cell>0.8695</cell><cell>0.8005</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>11.32</cell><cell cols="2">12.19 101.92</cell><cell>0.7263</cell><cell>0.6903</cell><cell>0.8318</cell><cell>0.8471</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>4.93</cell><cell>87.70</cell><cell>252.85</cell><cell>0.7087</cell><cell>0.0007</cell><cell>0.5836</cell><cell>0.2220</cell></row><row><cell>LEAST SQUARE</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="2">7.27 11.56 11.85 49.08</cell><cell>162.58 103.06</cell><cell>0.7427 0.7334</cell><cell>0.2114 0.6949</cell><cell>0.7210 0.8145</cell><cell>0.4438 0.8526</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>12.59</cell><cell>15.62</cell><cell>122.71</cell><cell>0.7866</cell><cell>0.4642</cell><cell>1.0109</cell><cell>0.7863</cell></row><row><cell></cell><cell>REACGAN</cell><cell cols="2">12.90 15.09</cell><cell>164.93</cell><cell>0.7827</cell><cell>0.4672</cell><cell>1.0454</cell><cell>0.8282</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>5.66</cell><cell>69.48</cell><cell>?</cell><cell>0.5976</cell><cell>0.1603</cell><cell>0.4310</cell><cell>0.2649</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>10.97</cell><cell>19.30</cell><cell>148.40</cell><cell>0.6880</cell><cell>0.5444</cell><cell>0.6770</cell><cell>0.7242</cell></row><row><cell>W-GP</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="3">11.04 15.56 11.01 14.02 101.14 121.23</cell><cell>0.7023 0.7058</cell><cell>0.6474 0.6804</cell><cell>0.7048 0.7549</cell><cell>0.7535 0.7956</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>6.72</cell><cell>49.77</cell><cell>147.22</cell><cell>0.6498</cell><cell>0.2834</cell><cell>0.5827</cell><cell>0.3549</cell></row><row><cell></cell><cell>REACGAN</cell><cell>6.67</cell><cell>47.74</cell><cell>150.7</cell><cell>0.6188</cell><cell>0.3104</cell><cell>0.4806</cell><cell>0.3396</cell></row><row><cell></cell><cell>PD-GAN</cell><cell>11.76</cell><cell>10.96</cell><cell>108.08</cell><cell>0.7436</cell><cell>0.6812</cell><cell>0.8790</cell><cell>0.8609</cell></row><row><cell></cell><cell>AC-GAN</cell><cell>11.66</cell><cell>21.65</cell><cell>168.87</cell><cell>0.7577</cell><cell>0.3649</cell><cell>0.8297</cell><cell>0.7225</cell></row><row><cell>HINGE</cell><cell>TAC-GAN ADC-GAN</cell><cell cols="3">12.07 12.56 11.82 10.73 103.78 134.75</cell><cell>0.7572 0.7387</cell><cell>0.6020 0.7023</cell><cell>0.8957 0.8721</cell><cell>0.8400 0.8707</cell></row><row><cell></cell><cell>CONTRAGAN</cell><cell>10.08</cell><cell>13.22</cell><cell>128.50</cell><cell>0.7372</cell><cell>0.6251</cell><cell>0.8356</cell><cell>0.7790</cell></row><row><cell></cell><cell>REACGAN</cell><cell>11.80</cell><cell>12.52</cell><cell>140.47</cell><cell>0.7510</cell><cell>0.5982</cell><cell>0.9300</cell><cell>0.8327</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We follow the common practice in the literature to adopt the stable version instead of the original one. We also provide an analysis of the original AC-GAN in Appendix B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">PD-GAN discards a in implementing the projection discriminator based on the hypothesis that a can be merged into r(x). However, r(x) does not model any label information, which should be involved by a . Therefore, it is unreasonable to do this.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ajbrock/ BigGAN-PyTorch 4 https://github.com/houliangict/adcgan</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/POSTECH-CVLab/ PyTorch-StudioGAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is funded by the National Natural Science Foundation of China under Grant Nos. 62102402, U21B2046, and National Key R&amp;D Program of China (2020AAA0105200). Huawei Shen is also supported by Beijing Academy of Artificial Intelligence (BAAI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02136</idno>
		<title level="m">Mode regularized generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified view of cGANs with and without classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W.</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Twin auxilary classifiers gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unbiased auxiliary classifier gans with mine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07567</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual projection generative adversarial networks for conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stathopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-supervised GANs with label augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Slimmable generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7746" to="7753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive learning for conditional image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Contragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auxiliary classifier GANs with stable training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acgan</forename><surname>Rebooting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W.</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multi-class hinge loss for conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021-01" />
			<biblScope unit="page" from="1290" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning causal implicit generative models with adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Causalgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The power of two samples in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pacgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">cGANs with projection discriminator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Sagong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Ko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00709</idno>
		<title level="m">cgans with conditional convolution layer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ac-gan learns a biased distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Michigan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16417</idno>
		<title level="m">Multi-inputconditioned hair image generation for portrait editing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A unified generative adversarial network training via self-labeling and self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00953</idno>
		<title level="m">Latent optimisation for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00570</idno>
		<title level="m">Conditional image generation from visual attributes</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved consistency regularization for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11033" to="11041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Omnigan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13074</idno>
		<title level="m">On the secrets of cgans and beyond</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Activation maximization generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
