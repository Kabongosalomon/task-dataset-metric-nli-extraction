<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Orthogonal Classifier for Improving the Adversarial Robustness of Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Information Sciences</orgName>
								<orgName type="institution">Yantai University</orgName>
								<address>
									<postCode>264005</postCode>
									<settlement>Yantai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Software Engineering Institute</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics and Information Sciences</orgName>
								<orgName type="institution">Yantai University</orgName>
								<address>
									<postCode>264005</postCode>
									<settlement>Yantai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Orthogonal Classifier for Improving the Adversarial Robustness of Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial robustness</term>
					<term>Classification layer</term>
					<term>Dense</term>
					<term>Orthogonal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural networks are susceptible to artificially designed adversarial perturbations. Recent efforts have shown that imposing certain modifications on classification layer can improve the robustness of the neural networks. In this paper, we explicitly construct a dense orthogonal weight matrix whose entries have the same magnitude, thereby leading to a novel robust classifier. The proposed classifier avoids the undesired structural redundancy issue in previous work. Applying this classifier in standard training on clean data is sufficient to ensure the high accuracy and good robustness of the model. Moreover, when extra adversarial samples are used, better robustness can be further obtained with the help of a special worst-case loss. Experimental results show that our method is efficient and competitive to many state-of-the-art defensive approaches. Our code is available at https://github.com/MTandHJ/roboc.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural networks have achieved impressive performance in many challenging computer vision tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref>. But it was soon realized that they are vulnerable to artificial adversarial perturbations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. Imposing human imperceptible perturbations on clean data could deceive the networks and cause incorrect classification. In order to improve the robustness of neural networks, a large number of studies on defense mechanisms have appeared; see, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref>. Although these methods have achieved good performance, the training process is usually computationally expensive and not scalable <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>It was pointed out by <ref type="bibr" target="#b17">[18]</ref> that the feature representations learned by standard training tend to concentrate on decision boundaries, an undesirable property resulting in vulnera-bility of networks. By analyzing the relationship between robustness and feature distribution, some recent studies intend to use the modified classifiers to improve the robustness of the networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. The corresponding approaches seem more scalable and easier to implement.</p><p>In particular, starting from the Max-Mahalanobis distribution, Pang et al. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> proposed an algorithm called GenerateOptMeans to explicitly build a linear classifier, which brings a significant improvement in adversarial robustness with very little additional computational cost. However, the classifier generated by GenerateOptMeans suffers from the problem of structural redundancy (see detailed discussions in Section 3.2 and experimental results in Section 4.2), which may lead to underfitting in some cases.</p><p>In this article, we are to introduce a class of dense orthogonal vectors to construct a novel robust classifier. Compared to the GenerateOptMeans algorithm used in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, the classifier weights calculation procedure we give is much simpler and can avoid the undesired structural redundancy issue. In addition, orthogonal weights are also conducive to learning more discriminative features while ensuring a stable training <ref type="bibr" target="#b32">[33]</ref>. Experimental results show that using the proposed classifier in standard training on clean data is sufficient to ensure the high accuracy and good robustness of the model. Moreover, if extra adversarial samples are available, the robustness can be further improved with the help of a worst-case intra-class compactness loss. Comparative empirical results on MNIST, FashionMNIST and CIFAR-10 datasets show that the proposed approach achieves the state-of-the-art robustness under various attacks, while still maintaining high accuracy on clean data.</p><p>The main contributions of the work are summarized as follows:</p><p>(1) We use a special type of dense orthogonal matrix, whose entries have the same magnitude, to construct a novel classifier to improve the robustness of neural networks. The proposed classifier encourages the encoder to learn the feature representations that are compact within the class and dispersed between classes.</p><p>(2) We provide a step by step procedure to explicitly determine the weights of the classifier. Compared to the GenerateOptMeans algorithm developed in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, our procedure is much simpler and avoids the structural redundancy issue.</p><p>(3) Our classifier can be used in two ways. One is only to perform standard training on clean data. The other is to use extra adversarial samples and a corresponding worst-case intra-class compactness loss for adversarial training. The former keeps a good balance between the efficiency, accuracy and robustness. The latter guarantees a better robustness performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Orthogonal neural networks</head><p>There has been a lot of work on orthogonal neural networks, see e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>. The use of orthogonal weights in neural networks can encourage the model to learn diverse and expressive features, and simultaneously improve the training stability <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>. A common method of introducing orthogonality into networks is to construct specific regularized terms in the optimization objective.</p><p>In this paper, we aim to design an orthogonal classifier to improve the adversarial robustness of the networks. The main difference between our work and the previous research lies in two aspects. First, the previous research basically considered the orthogonality of the entire network, whereas this article only focuses on the classifier part, i.e., the penultimate layer. Second, instead of using the common soft regularization approach, we present an explicit procedure to construct the required orthogonal weights directly. And the orthogonal weights are frozen during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attack models and defense methods</head><p>Since the discovery that neural networks are vulnerable to artificial perturbations, a series of attack methods have emerged to evaluate the robustness of networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. These attack models design small perturbations to clean samples to generate various adversarial examples. FGSM <ref type="bibr" target="#b7">[8]</ref> is a simple but effective attack method that utilizes the sign of the gradients of the loss function to generate adversarial samples. PGD <ref type="bibr" target="#b18">[19]</ref> is a more powerful iterative attack that starts from a random position in the neighborhood of a clean input and then applies FGSM for several iterations. DeepFool <ref type="bibr" target="#b17">[18]</ref> is a gradient-based attack algorithm that iteratively linearizes the classifier to generate the smallest perturbation sufficient to change the classification label. C&amp;W <ref type="bibr" target="#b2">[3]</ref> is one of the most powerful attack to detect adversarial samples in 2 norm. SLIDE <ref type="bibr" target="#b29">[30]</ref> is a more efficient model that overcomes the inefficiency of PGD in searching for 1 perturbations. Recently, Croce et al. <ref type="bibr" target="#b5">[6]</ref> combined four diverse attacks into a more aggressive one, AutoAttack, as a new benchmark for empirical robustness evaluation.</p><p>With the development of attack models, corresponding defense mechanisms have been continuously strengthened; see, e.g., adversarial training <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>, certified robustness <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref> and detection methods <ref type="bibr" target="#b8">[9]</ref>. Among them, adversarial training <ref type="bibr" target="#b18">[19]</ref>, which minimizes the worst-case loss in the perturbation region, is considered to be one of the most powerful defenses. However, this defense is difficult to apply to large-scale problems due to its heavy computational cost. Another problem with adversarial training is that improved robustness often comes at the expense of the prediction accuracy of clean data <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>. Some recent work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref> resorted to modify the linear classifier part to improve the robustness of the networks, without bringing too much extra computation. Particularly, Pang et al. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> provided a procedure called GenerateOptMeans algorithm to explicitly preset the weights of the classification layer, thereby helping improve the robustness of the trained model. However, the classifier derived in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> suffers from the problem of structural redundancy, which may lead to underfitting in some cases (see discussions in Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Orthogonal Classifier for Robustness Improvement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The relationship between robustness and feature distribution</head><p>In this section, we first briefly discuss the relationship between robustness and feature distribution from an intuitive perspective, and then derive the corresponding optimization objectives.</p><p>Let P denote the dimension of the feature representations, and K be the number of categories. A typical feed-forward neural network includes a nonlinear encoder f that transforms the input x to its feature representation f (x) ? R P , and a multi-class classifier c given by</p><formula xml:id="formula_0">c(x) = W T f (x) + b, (3.1)</formula><p>where W ? R P ?K and b ? R K denote the weight matrix and bias, respectively.</p><p>Since the bias has little impact on the prediction, we will assume that b is zero for brevity in the analysis below. For 1 ? i ? K, define the classification domain:</p><formula xml:id="formula_1">A i := {f ? R P : (w i ? w j ) T f ? 0, j = i}, (3.2)</formula><p>where w i is the i-th column of the weight matrix W . If the feature representation of a sample is located in domain A i , then we think the sample belongs to class i. Thus, for a given sample x with the ground-truth label y, in order to correctly classify it, the following estimate</p><formula xml:id="formula_2">w T y f ? w T j f, ?j = y (3.3)</formula><p>should hold true. That is ||w y || 2 cos ? y ? ||w j || 2 cos ? j , ?j = y, <ref type="bibr">(3.4)</ref> where ? y and ? j denote the angles between the feature vector f (x) and the corresponding weight vectors, respectively. Intuitively, the larger the length of w y , a smaller cos ? y can ensure that (3.4) holds. Nevertheless, a smaller cos ? y means that the learned feature f (x) is closer to the decision boundary. However, it has already been pointed out in <ref type="bibr" target="#b17">[18]</ref> that the main reason why neural networks are vulnerable to attacks is that the learned features are too close to the decision boundary. Therefore, in order to guarantee that all features are not close to the decision boundary, we do not want to see any longer weight vectors. The most ideal situation is that all vectors have the same length. Thus, we set</p><formula xml:id="formula_3">||w i || 2 = s, ?1 ? i ? K,<label>(3.</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>where s is a positive hyperparameter. Consequently, the label of a sample x is determined by</p><formula xml:id="formula_4">arg max 1?i?K w T i f (x) := arg min 1?i?K f (x) ? w i 2 2 ,</formula><p>where we have used the condition (3.5) in the equality. Then, we come to the first optimization objective</p><formula xml:id="formula_5">min f E (x,y) f (x) ? w y 2 2 , (3.6)</formula><p>which encourages the intra-class compactness of the feature representations.</p><p>On the other hand, it is obvious that the greater the distance between the class centers, the harder it is to confuse the classification results. Therefore, in order to further improve the robustness, we expect the distance between any two class centers to be as large as possible. To this end, we formulate the second optimization objective as follows</p><formula xml:id="formula_6">max W min 1?i&lt;j?K w i ? w j 2 2 , (3.7)</formula><p>which encourages the inter-class diversity of the feature representations. So far, from an intuitive perspective, we have derived the optimization objectives (3.6) and (3.7) that are subject to the constraint (3.5), which are same as those in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The construction of orthogonal robust classifier</head><p>According to the arguments in Section 3.1, the weights of a robust classifier should satisfy (3.5) and (3.7).</p><p>It has been proved in <ref type="bibr" target="#b23">[24]</ref> that under the constraint (3.5), the maximal value of (3.7) is 2Ks 2 /(K ? 1), where K is the number of classes. Moreover, an algorithm named as GenerateOptMeans was used in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> to explicitly construct a specific upper triangular matrix W to satisfy (3.5) and (3.7). After then, the classification layer is frozen and only the encoder part is trained to satisfy the intra-class loss (3.6).</p><p>However, we point out here that the upper triangular structure adopted in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> may cause some problems. Consider a simple neural network with only one linear encoder f (x) = V T x. Let W ? R P ?K denote the corresponding classification layer. When W is an upper triangular matrix and the feature dimension P is larger than the category number K (a very common case in practice), the classifier can be simplified as</p><formula xml:id="formula_7">c(x) = W T V T x := W T V T x,</formula><p>where W is a submarix constituted by the first K rows of W , and V only includes the first K columns of V . Thus, there are (P ? K) features unused for classification at all, which can lead to underfitting in some situations (see Section 4.2 for related experiments). We call the above phenomenon as structural redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Construction of a Dense Orthogonal Classifier</head><p>Input: The number of classes K, the length s and the feature dimension P .</p><p>Set T = log 2 P and M (0) = 2 ?T /2 s. for t = 1, 2, ? ? ? , T do Apply the formula (3.8) to recursively construct a series of M (t) . Choose the first K columns of M (T ) to build the weight matrix W . Output: The weight matrix W for classifier. Remark 1. Though Algorithm 1 takes the number of classes K, the length s and the feature dimension P as inputs, K and P are directly determined by the learning task and the neural network, respectively. Only the vector length s is left as a hyperparameter to be given.</p><p>In order to overcome the problem of structural redundancy, we next propose a step by step procedure to construct a fully dense weight matrix W that approximately satisfies the constraint (3.7). For simplicity, we assume that the feature dimension satisfies P = 2 T , where T is a given integer.</p><p>Set</p><formula xml:id="formula_8">M (0) = 2 ?T /2 s. For t = 1, ? ? ? , T , we recursively define M (t) = M (t?1) ?M (t?1) M (t?1) M (t?1) . (3.8)</formula><p>In view of the hierarchical block structure of M (t) , it is easy to verify that</p><formula xml:id="formula_9">M (t) = 2 ?T /2 s m i,j 2 t ?2 t ,<label>(3.9)</label></formula><p>where |m i,j | = 1, which means that M (t) is a fully dense matrix with all entries of the same magnitude. Furthermore, by an inductive argument, we have</p><formula xml:id="formula_10">(M (t) ) T M (t) = 2 t?T s 2 I, ?1 ? t ? T,</formula><p>which means that the column vectors of M (t) are orthogonal to each other. Finally, we choose the first K columns of M (T ) to formulate the desired weight matrix W for the classifier.</p><p>Remark 2. Our approach has several advantages. First, the recursive construction process (3.8) is simple and easy to implement. Second, because all entries of the constructed weight matrix are non-zero (|m i,j | = 1), there is no problem of structural redundancy. Third, the magnitude of all entries is the same, which forces the encoder to learn all feature representations unbiased.</p><formula xml:id="formula_11">Remark 3. It follows from orthogonality that w i ? w j 2 2 = 2(s 2 ? w T i w j ) = 2s 2 for all 1 ? i &lt; j ? K.</formula><p>The value 2s 2 is smaller than the optimal value 2Ks 2 /(K ? 1) of (3.7). Nonetheless, for a large K, which is common in practice, such difference is negligible. Thus, the proposed weight matrix can be regarded as an approximate solution of the problem (3.7).</p><p>After constructing the desired classifier, one just need to fix the classifier and train the encoder part by minimizing the intra-class loss <ref type="bibr">(3.6)</ref>. In this way, a standard training on clean data is sufficient to learn discriminative features (see <ref type="figure" target="#fig_0">Figure 1</ref>), thereby ensuring the accuracy and certain robustness of the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Using adversarial samples to further improve robustness</head><p>In the previous section, we only use clean data for learning. In order to further improve the robustness, we can turn to adversarial samples <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> and introduce a special worstcase loss.</p><p>More specifically, for any clean sample x, let x denote the corresponding adversarial sample. We expect that the feature representation of any adversarial sample should still satisfy the intra-class compactness and inter-class dispersion properties discussed in Section 3.1. This means that after determining the classifier weight matrix W from Algorithm 1, we shall solve the following optimization problem</p><formula xml:id="formula_12">min f 1 P E (x,y) ? ? f (x) ? w y 2 2 + (1 ? ?) ? max x ?x ? f (x ) ? w y 2 2 ,<label>(3.10)</label></formula><p>where ? denotes ? norm, is a perturbation budget, and the hyperparameter ? ? (0, 1] controls the effects of the clean samples and adversarial samples. The second item in (3.10) means that the worst-case feature representations should also be compact around the corresponding classification centers, thus leading to a stronger robustness. Obviously, the previous optimization objective (3.6) is a special case of ? = 1. We explicitly multiply the coefficient 1 P , where P denotes the output feature dimension of the network, to ensure consistent performance over different architectures.</p><p>Remark 4. It is infeasible to exactly solve the second optimization term in (3.10). However, as suggested by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref>, we can approximately solve this problem by searching the worst-case adversarial samples on the fly, e.g., applying PGD attack for 10 iterations with a step size 0.25 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Datasets. In this section, we evaluate the performance of our approach on three commonly used datasets, MNIST <ref type="bibr" target="#b16">[17]</ref>, FashionMNIST <ref type="bibr" target="#b35">[36]</ref> and CIFAR-10 <ref type="bibr" target="#b14">[15]</ref>. MNIST and FashionMNIST are collections of grayscale handwritten digits and clothes, respectively. Both of them consist of 60000 training samples and 10000 test samples, namely 7000 28 ? 28 samples per class. FashionMNIST is a more challenging task than MNIST and thus can be served as a direct drop-in replacement for it. The CIFAR-10 dataset contains 60000 32 ? 32 color images, which are divided into a training set of 50000 images and a test set of 10000 images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacks.</head><p>To reliably evaluate the adversarial robustness of defense methods, several benchmark adversarial attacks including FGSM <ref type="bibr" target="#b7">[8]</ref>, PGD <ref type="bibr" target="#b18">[19]</ref>, DeepFool <ref type="bibr" target="#b17">[18]</ref>, C&amp;W <ref type="bibr" target="#b2">[3]</ref>, SLIDE <ref type="bibr" target="#b29">[30]</ref> and AutoAttack <ref type="bibr" target="#b5">[6]</ref> are adopted. Except that AutoAttack is due to the source code from <ref type="bibr" target="#b5">[6]</ref>, all implementations of them are provided by FoolBox <ref type="bibr" target="#b25">[26]</ref>. The major settings of these attacks are listed in <ref type="table" target="#tab_0">Table 1</ref>, wherein step size denotes the relative step size of PGD and SLIDE, the learning rate of C&amp;W, and the overshoot of DeepFool, respectively. For brevity, refer to PGD20 as the shorthand of PGD with 20 iterations.</p><p>Baseline defenses. We select 5 benchmark defenses, FGSM-AT <ref type="bibr" target="#b7">[8]</ref>, PGD-AT <ref type="bibr" target="#b18">[19]</ref>, ALP <ref type="bibr" target="#b15">[16]</ref>, TRADES <ref type="bibr" target="#b41">[42]</ref> and MMC <ref type="bibr" target="#b24">[25]</ref>, for comparison. We implement these defense methods following the default settings of original papers. In particular, we train FGSM-AT and ALP using the same setup as PGD-AT. All methods performed on FashionMNIST follow the same training settings for MNIST. When adversarial samples are required in the procedure of training, FGSM-AT crafts perturbations on the fly by FGSM attack, while others utilize PGD10 attack within = 8/255 on CIFAR-10 and PGD40 attack within = 0.3 on MNIST and FashionMNIST. Note that although we mainly focus on ? distance in this article, we also report adversarial robustness over 1 , 2 , ? norms for completeness.</p><p>Architectures. Following MMC <ref type="bibr" target="#b24">[25]</ref>, we verify the effectiveness of our approach on CIFAR-10 using ResNet32, of which the dimension is P = 64 and accordingly T = log 2 P = 6. When training on CIFAR-10, we use the augmentation popularized by Residual Networks: 4 pixels are reflection padded on each side, and a 32 ? 32 crop is randomly sampled from the padded image or its horizontal flip. According to the suggestion in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref>, we adopt a small CNN for evaluation on MNIST and FashionMNIST, which comprises four convolutional layers followed by three linear layers. In order to satisfy the condition P = 2 T , we adjust its original dimension P = 200 to P = 256, and accordingly T = log 2 P = 8. In addition, we employ PReLU <ref type="bibr" target="#b11">[12]</ref> as the activation function instead of widely used ReLU <ref type="bibr" target="#b20">[21]</ref>, because the latter only outputs positive signals, leading to an unbalanced distribution of features.</p><p>Hyperparameters. As suggested in <ref type="bibr" target="#b23">[24]</ref>, the length of weight vectors is set as s = 10. By searching ? with a step size of 0.05, we find the model is optimal as long as ? is roughly between 0.1 and 0.4. We train ResNet32 for 180 epochs using the SGD optimizer with an initial learning rate 0.1 which is decayed by a factor 10 at epoch 100, 150 and 175. The MNIST and FashionMNIST datasets are simpler than CIFAR-10, so it is sufficient to run for 80 epochs through the SGD optimizer with an initial learning rate 0.1 which is decayed by a factor 10 at epoch 35, 60 and 75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with GenerateOptMeans</head><p>In this section, we compare Algorithm 1 with GenerateOptMeans <ref type="bibr" target="#b24">[25]</ref> in three aspects: the speed of weight matrix construction, structural redundancy issue and robustness performance.</p><p>Construction speed. We first compare the CPU Times required by the two construction algorithms. All CPU Times (in seconds) are averaged over a total of 10 trials. It can be seen from <ref type="figure" target="#fig_1">Figure 2</ref> that when the number of classes is fixed at K = 10, although both algorithms only require milliseconds, our construction algorithm is still much faster than GenerateOptMeans <ref type="bibr" target="#b24">[25]</ref>. Moreover, in an artificial case of K = 2 T , we find that the speed advantage of Algorithm 1 becomes more apparent as the number of categories increases.</p><p>Structural redundancy issue. We perform a toy experiment on MNIST to illustrate this problem. We take the first 2000 images from MNIST for training and use the entire test set for evaluation. A linear fully-connected network with only one hidden layer is adopted here. The input of the encoder part is a 16-dimensional vector (that is, the image is resized to 4 ? 4 and then flattened into a vector) and the output dimension of the encoder is set to P = 2 T . We utilize Algorithm 1 and the GenerateOptMeans algorithm <ref type="bibr" target="#b23">[24]</ref> to calculate the classifier, respectively. Then a standard training on encoder part is conducted for 10 epochs with a learning rate 0.1. The training and test accuracies averaged over 5 independent experiments are depicted in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Under a normal situation, as the dimensionality of feature representation increases, the representation capability of the encoder will be improved accordingly, which ultimately  leads to an improvement in the final classification accuracy. However, we observe from <ref type="figure" target="#fig_2">Figure 3</ref> that with the increase of T , the GenerateOptMeans-based networks maintain an abnormally constant classification accuracy, while our approach yields a normal and better performance. Such phenomenon shows that the upper triangular weight matrix constructed in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> will indeed cause the problem of underfitting.</p><p>Robustness. Finally, we use ResNet32 to compare the accuracy and robustness of our approach with MMC [25] on the CIFAR-10 dataset. We take bold type to indicate the best result, and underline type to indicate the second best result. As shown in <ref type="table" target="#tab_1">Table  2</ref>, our approach of ? = 1 significantly surpasses MMC on natural accuracy and under PGD attack. Moreover, our approach of ? = 0.2 still outperforms MMC when exposed to the adversarial samples crafted by AutoAttack, the most powerful attack by far. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Robustness evaluation</head><p>In this section, we compare our approach with several baseline defenses, including FGSM-AT <ref type="bibr" target="#b7">[8]</ref>, PGD-AT <ref type="bibr" target="#b18">[19]</ref>, ALP <ref type="bibr" target="#b15">[16]</ref> and TRADES <ref type="bibr" target="#b41">[42]</ref>, on CIFAR-10 and Fashion-MNIST datasets. Recall that all adversarial samples are crafted in ? distance, but for more reliable evaluation, we also provide the robustness in other norms.</p><p>CIFAR-10. We first report the results under widely used perturbations 8/255 and 16/255 in ? norm. <ref type="table" target="#tab_2">Table 3</ref> shows a consistent improvement on both natural accuracy and adversarial robustness. Even under AutoAttack, one of the most aggressive attacks, our method of ? = 0.15 can still outperform PGD-AT by nearly 1.1% and 1.4% within = 8/255 and = 16/255, respectively. TRADES achieves the strongest robustness within = 16/255, whereas it comes at the expense of natural accuracy. It manifests that our approach can make a better trade-off, enhancing the robustness while preserving high natural accuracy. To further demonstrate the reliability of the robustness brought by our modified orthogonal classifier, we investigate the performance under 1 and 2 perturbations. The results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. Although our approach is adversarially trained under  FashionMNIST. <ref type="table" target="#tab_4">Table 5</ref> shows the results on the FashionMNIST dataset. Although TRADES of 1/? = 1 and ALP achieve better natural accuracy, they perform poorly under AutoAttack, 18% and 24% less than ours, respectively. TRADES (1/? = 6) does put more emphasis on robustness, but at the cost of 8% natural accuracy and lower robustness in 2 norm. Because FGSM-AT injects robustness by the simplest one-step attack, it performs as poorly as the Standard Training when exposed to multi-step attacks. But interestingly, its classification accuracy under FGSM attack is significantly higher than others, and even exceeds the best natural accuracy. This phenomenon may stem from the fact that adversarial samples could be regarded as special augmentations. Moreover, the FGSM attack is so simple that the adversarially trained model could fit the underlying distribution easily. In fact, even more aggressive attacks such as PGD10 can also be used to improve the image recognition <ref type="bibr" target="#b39">[40]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Sensitivity analysis</head><p>Hyperparameter analysis. We first examine the effect of the hyperparameters s and ?, respectively. Experimental results are summarized in <ref type="table" target="#tab_5">Table 6</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>. As the length s increases from 7 to 13, both the natural accuracy and robustness change very little, which means that the proposed method is stable w.r.t. the length of the weight vectors. On the other hand, the balance coefficient ?, which controls the effects of clean samples and adversarial samples, is a relatively crucial factor. A larger ? generally leads to better natural accuracy but less robustness. Nevertheless, when it is roughly between 0.1 and 0.4, the proposed method is able to maintain stable and extraordinary performances.</p><p>Perturbation budgets. Although all methods are adversarially trained within = 8/255, it would be better if they could extrapolate to lager perturbations. <ref type="figure" target="#fig_4">Figure 5</ref> depicts the relationship between classification accuracy and increasing perturbation budgets under PGD10 attack. Except for the subplot at the upper right, the proposed approach (? = 0.3 on MNIST and FasionMNIST, and ? = 0.15 on CIFAR-10) consistently surpasses other defenses, which demonstrates a better scalability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computational cost</head><p>In this section, we list the CPU Times of different defensive approaches. As shown in <ref type="figure">Figure 6</ref>, the proposed approach (? = 1) only takes a few minutes for training, while other defensive methods usually need hours at least. Thus, our approach is a very efficient temporary defense when only clean samples are available.</p><p>On the other hand, if a more reliable defense is required, we have to use extra adversarial samples, which inevitably leads to more computational costs. In this case, our approach (? &lt; 1) has similar time complexity to other benchmark defenses. It is worth noting that although FGSM-AT is a fast adversarial training method, it can only effectively defend against a simple one-step attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>In this paper, by analyzing the relationship between robustness and feature distributions, we deduced several crucial conditions w.r.t. the intra-class compactness and inter-class dispersion, respectively. To explicitly meet these conditions, we then developed a simple step by step procedure to construct a fixed orthogonal classifier, which takes only milliseconds from beginning to end. Because all entries of the constructed matrix are non-zero, there is no problem of structural redundancy. Thanks to the orthogonality of the weight matrix and the fact that all entries are of the same magnitude, only using clean data in a standard training can protect networks against most typical attacks while maintaining a high natural accuracy. To further improve the robustness, we turn to adversarial samples and introduce a special worst-case loss, of which the hyperparameter ? controls the balance between natural accuracy and adversarial robustness. According to the sensitivity analysis, we found this adversarially trained model was able to maintain stable and extraordinary performances as long as ? was roughly between 0.1 and 0.4.</p><p>We leave a number of issues for future research. As is the case in many adversarial training mechanisms, our approach also suffers from the problem of the heavy computational cost when it utilizes adversarial samples for stronger robustness. Thus, exploring simpler and more scalable method deserves a further study in the future. On the other hand, our approach is based on the prior assumption that the robust category features should be scattered and orthogonal to each other. Such a strong assumption will appear unreasonable when dealing with thousands of fine-grained categories. Prior assumptions extracted from the dataset could be more appropriate and scalable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>t-SNE [31] visualization of the feature representations on CIFAR-10, where the index numbers indicate the corresponding classes. Left: Feature distribution obtained by a normal training of the entire network; Right: Feature distribution learned by using the orthogonal classifier constructed by Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the speed of constructing a weight matrix W ? R 2 T ?K under various T , where T = log 2 P and P is the output feature dimension of the encoder. Left: Fixed K = 10; Right: K is increasing with T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Compared accuracy on a toy example with increasing feature dimensions. Left: Training accuracy; Right: Test accuracy. For GenerateOptMeans, the structural redundancy of classifier weights leads to underfitting on natural accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Sensitivity of vector length s and loss weight ? on CIFAR-10. Left: Classification accuracy under various s with fixed ? = 0.15; Right: Classification accuracy under various ? with fixed s = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Evaluation of adversarial robustness under PGD10 attack over different datasets and norms. Each subplot manifests the relationship regarding classification accuracy versus various perturbation budgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Figure 6 :</head><label>46</label><figDesc>Summary of computational cost on different datasets. Here, Ours-ST indicates a standard training using only clean data (? = 1), while Ours-AT means using extra adversarial samples (? &lt; 1). We use TRADES to represent TRADES (1/? = 1) or TRADES (1/? = 6), as the hyperparameter ? has little effect on CPU Time of TRADES.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The basic setup for adversarial attacks in ? , 1 and 2 norms.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell></cell></row><row><cell></cell><cell cols="3">FGSM PGD PGD</cell><cell>PGD</cell><cell>PGD</cell><cell cols="2">DeepFool AutoAttack</cell><cell cols="2">PGD SLIDE</cell><cell cols="4">PGD PGD C&amp;W AutoAttack</cell></row><row><cell>number of iterations</cell><cell>-</cell><cell>10</cell><cell>20</cell><cell>40</cell><cell>50</cell><cell>50</cell><cell>-</cell><cell>50</cell><cell>50</cell><cell>50</cell><cell>100</cell><cell>1000</cell><cell>-</cell></row><row><cell>step size</cell><cell>-</cell><cell>0.25</cell><cell cols="3">0.1 0.033333 0.033333</cell><cell>0.02</cell><cell>-</cell><cell>0.05</cell><cell>0.05</cell><cell>0.1</cell><cell>0.05</cell><cell>0.01</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of classification accuracy (%) between MMC and the proposed method on CIFAR-10. We directly cite the * results of MMC from RobustBench<ref type="bibr" target="#b6">[7]</ref> and others from the original paper.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">? , = 8/255</cell><cell>? , = 16/255</cell></row><row><cell></cell><cell cols="3">Clean PGD50 AutoAttack</cell><cell>PGD50</cell></row><row><cell>MMC [25]</cell><cell>80.89  *</cell><cell>55.00</cell><cell>43.48  *</cell><cell>27.70</cell></row><row><cell cols="3">Ours (? = 1) 92.62 78.85</cell><cell>0.00</cell><cell>61.87</cell></row><row><cell cols="2">Ours (? = 0.2) 81.21</cell><cell>46.39</cell><cell>43.64</cell><cell>24.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of classification accuracy (%) on CIFAR-10 under ? perturbations within = 8/255 or = 16/255.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>? , = 8/255</cell><cell></cell><cell></cell><cell>? , = 16/255</cell><cell></cell></row><row><cell></cell><cell>Clean</cell><cell cols="3">PGD20 DeepFool AutoAttack</cell><cell cols="3">PGD20 DeepFool AutoAttack</cell></row><row><cell>Standard Training</cell><cell>93.27</cell><cell>0.00</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>FGSM-AT [8]</cell><cell>88.23</cell><cell>0.01</cell><cell>0.17</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>PGD-AT [19]</cell><cell>79.42</cell><cell>48.44</cell><cell>48.70</cell><cell>42.99</cell><cell>19.28</cell><cell>25.50</cell><cell>10.92</cell></row><row><cell>ALP [16]</cell><cell>80.70</cell><cell>48.27</cell><cell>49.54</cell><cell>43.23</cell><cell>19.70</cell><cell>27.22</cell><cell>12.27</cell></row><row><cell cols="2">TRADES (1/? = 1) [42] 82.22</cell><cell>39.93</cell><cell>41.83</cell><cell>34.62</cell><cell>10.70</cell><cell>17.29</cell><cell>6.15</cell></row><row><cell cols="2">TRADES (1/? = 6) [42] 74.04</cell><cell>45.90</cell><cell>44.73</cell><cell>40.78</cell><cell>21.06</cell><cell>23.80</cell><cell>14.49</cell></row><row><cell>Ours (? = 1)</cell><cell>92.62</cell><cell>78.68</cell><cell>32.22</cell><cell>0.00</cell><cell>61.36</cell><cell>12.00</cell><cell>0.00</cell></row><row><cell>Ours (? = 0.15)</cell><cell>80.43</cell><cell>48.83</cell><cell>51.31</cell><cell>44.15</cell><cell>27.50</cell><cell>32.24</cell><cell>12.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of classification accuracy (%) on CIFAR-10 under 1 , 2 perturbations within = 12 and = 0.5, respectively.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">1 , = 12</cell><cell></cell><cell>2 , = 0.5</cell><cell></cell></row><row><cell></cell><cell>Clean</cell><cell cols="2">PGD50 SLIDE</cell><cell cols="3">PGD50 C&amp;W AutoAttack</cell></row><row><cell>Standard Training</cell><cell>93.27</cell><cell>0.62</cell><cell>0.69</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>FGSM-AT [8]</cell><cell>88.23</cell><cell>2.08</cell><cell>29.54</cell><cell>0.01</cell><cell>0.01</cell><cell>0.00</cell></row><row><cell>PGD-AT [19]</cell><cell>79.42</cell><cell>57.25</cell><cell>23.08</cell><cell cols="2">56.70 54.67</cell><cell>53.31</cell></row><row><cell>ALP [16]</cell><cell>80.70</cell><cell>55.68</cell><cell>22.16</cell><cell>56.22</cell><cell>54.26</cell><cell>53.34</cell></row><row><cell cols="2">TRADES (1/? = 1) [42] 82.22</cell><cell>58.17</cell><cell>18.26</cell><cell>54.68</cell><cell>51.88</cell><cell>51.26</cell></row><row><cell cols="2">TRADES (1/? = 6) [42] 74.04</cell><cell>54.95</cell><cell>25.24</cell><cell>54.29</cell><cell>51.32</cell><cell>51.10</cell></row><row><cell>Ours (? = 1)</cell><cell>92.62</cell><cell cols="2">89.83 84.35</cell><cell>89.61</cell><cell>0.92</cell><cell>0.00</cell></row><row><cell>Ours (? = 0.15)</cell><cell>80.43</cell><cell>60.41</cell><cell>30.64</cell><cell>59.88</cell><cell>54.59</cell><cell>53.48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of classification accuracy (%) on FashionMNIST over 1 , 2 , ? perturbations within = 10, = 2 and = 0.3, respectively. accuracy comparable to the Standard Training. Recall that it only accesses the clean data and thus requires no extra computation at all. Therefore, it may be an acceptable temporary defense method when dealing with tasks that place more emphasis on classification accuracy or computational load.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">1 , = 10</cell><cell></cell><cell>2 , = 2</cell><cell></cell><cell></cell><cell></cell><cell>? , = 0.3</cell><cell></cell></row><row><cell></cell><cell>Clean</cell><cell cols="2">PGD50 SLIDE</cell><cell cols="3">PGD100 C&amp;W AutoAttack</cell><cell cols="4">FGSM PGD50 DeepFool AutoAttack</cell></row><row><cell>Standard Training</cell><cell>91.82</cell><cell>24.50</cell><cell>8.27</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>2.12</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>FGSM-AT [8]</cell><cell>91.94</cell><cell>20.72</cell><cell>12.75</cell><cell>0.18</cell><cell>0.00</cell><cell>0.00</cell><cell>96.34</cell><cell>0.00</cell><cell>0.02</cell><cell>0.00</cell></row><row><cell>PGD-AT [19]</cell><cell>77.76</cell><cell>67.20</cell><cell>57.99</cell><cell>62.19</cell><cell>48.09</cell><cell>0.19</cell><cell>70.55</cell><cell>61.97</cell><cell>64.04</cell><cell>45.99</cell></row><row><cell>ALP [16]</cell><cell>83.08</cell><cell>65.74</cell><cell>53.65</cell><cell>64.14</cell><cell>25.53</cell><cell>2.35</cell><cell>68.21</cell><cell>61.40</cell><cell>56.35</cell><cell>24.25</cell></row><row><cell cols="2">TRADES (1/? = 1) [42] 86.06</cell><cell>68.88</cell><cell>58.40</cell><cell>65.20</cell><cell>26.61</cell><cell>3.46</cell><cell>67.89</cell><cell>58.07</cell><cell>56.13</cell><cell>29.61</cell></row><row><cell cols="2">TRADES (1/? = 6) [42] 78.05</cell><cell>63.98</cell><cell>50.34</cell><cell>56.30</cell><cell>22.15</cell><cell>1.65</cell><cell>62.49</cell><cell>56.36</cell><cell>54.91</cell><cell>34.22</cell></row><row><cell>Ours (? = 1)</cell><cell>92.56</cell><cell cols="2">89.81 87.90</cell><cell>82.91</cell><cell>0.00</cell><cell>0.00</cell><cell>88.44</cell><cell>34.14</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Ours (? = 0.3)</cell><cell>78.80</cell><cell>70.10</cell><cell>51.81</cell><cell>68.41</cell><cell>43.55</cell><cell>4.20</cell><cell>69.71</cell><cell>62.20</cell><cell>63.28</cell><cell>48.35</cell></row><row><cell>a high natural</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison of classification accuracy (%) across different hyperparameters s and ? on CIFAR-10. ? ) 44.020 43.770 43.930 44.150 43.480 40.060 43.680 AutoAttack( 2 ) 52.580 52.620 53.290 53.480 53.150 52.620 52.910 ? ) 43.890 44.410 44.150 43.640 41.420 39.370 36.430 AutoAttack( 2 ) 52.560 52.520 53.480 53.540 52.270 52.210 51.520</figDesc><table><row><cell>? = 0.15</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? perturbations within = 8/255, it still extrapolates well to other unseen attacks and norms of interest.In addition, note that even the special case (? = 1) which only uses clean data in a standard training can protect the networks against most typical attacks while maintaining</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research is partially supported by National Natural Science Foundation of China (11771257) and Natural Science Foundation of Shandong Province (ZR2018MA008)</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can we gain more from orthogonality regularizations in training deep CNNs?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">Madry A. &amp; Kurakin A. On evaluating adversarial robustness</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Flammarion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robustbench</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09670</idno>
		<title level="m">a standardized adversarial robustness benchmark</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting adversarial examples via prediction difference for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="page" from="182" to="192" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable verified training for provably robust image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dvijotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adversary resistant deep neural networks via advanced feature nullification. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing humanlevel performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Orthogonal weight normalization: solution to optimization over multiple dependent stiefel manifolds in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Techinical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06373</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial logit pairing. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepFool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial defense by restricting the hidden space of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial classification: an adversarial risk analysis approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Naveiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Redondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?os</forename><surname>Insua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ruggeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="133" to="148" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE European Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Max-mahalanobis linear discriminant analysis networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rethinking softmax crossentropy loss for adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference On Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<title level="m">Brendel W. &amp; Bethge M. Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is robustness the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial training for free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial training and robustness for multiple perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rethinking feature distribution for loss functions in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Orthogonal convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving adversarial robustness requires revisiting misclassified examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving adversarial robustness of deep neural networks by using semantic information. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pu S All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Distributed Systems Security Symposium (NDSS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating universal adversarial perturbation with ResNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">537</biblScope>
			<biblScope unit="page" from="302" to="312" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">You only propagate once: Accelerating adversarial training via maximal principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
