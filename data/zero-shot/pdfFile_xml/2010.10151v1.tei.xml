<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coherent Hierarchical Multi-Label Classification Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-20">20 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Giunchiglia</surname></persName>
							<email>eleonora.giunchiglia@cs.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
							<email>thomas.lukasiewicz@cs.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coherent Hierarchical Multi-Label Classification Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-20">20 Oct 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hierarchical multi-label classification (HMC) is a challenging classification task extending standard multi-label classification problems by imposing a hierarchy constraint on the classes. In this paper, we propose C-HMCNN(h), a novel approach for HMC problems, which, given a network h for the underlying multilabel classification problem, exploits the hierarchy information in order to produce predictions coherent with the constraint and improve performance. We conduct an extensive experimental analysis showing the superior performance of C-HMCNN(h) when compared to state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-label classification is a standard machine learning problem in which an object can be associated with multiple labels. A hierarchical multi-label classification (HMC) problem is defined as a multi-label classification problem in which classes are hierarchically organized as a tree or as a directed acyclic graph (DAG), and in which every prediction must be coherent, i.e., respect the hierarchy constraint. The hierarchy constraint states that a datapoint belonging to a given class must also belong to all its ancestors in the hierarchy. HMC problems naturally arise in many domains, such as image classification <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, text categorization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>, and functional genomics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>. They are very challenging for two main reasons: (i) they are normally characterized by a great class imbalance, because the number of datapoints per class is usually much smaller at deeper levels of the hierarchy, and (ii) the predictions must be coherent. Consider, e.g., the task proposed in <ref type="bibr" target="#b12">[13]</ref>, where a radiological image has to be annotated with an IRMA code, which specifies, among others, the biological system examined. In this setting, we expect to have many more "abdomen" images than "lung" images, making the label "lung" harder to predict. Furthermore, the prediction "respiratory system, stomach" should not be possible given the hierarchy constraint stating that "stomach" belongs to "gastrointestinal system". While most of the proposed methods directly output predictions that are coherent with the hierarchy constraint (see, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>), there are models that allow incoherent predictions and, at inference time, require an additional post-processing step to ensure its satisfaction (see, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31]</ref>). Most of the state-of-the-art models based on neural networks belong to the second category (see, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">33]</ref>).</p><p>In this paper, we propose C-HMCNN(h), a novel approach for HMC problems, which, given a network h for the underlying multi-label classification problem, exploits the hierarchy information to produce predictions coherent with the hierarchy constraint and improve performance. C-HMCNN(h) is based on two basic elements: (i) a constraint layer built on top of h, ensuring that the predictions are coherent by construction, and (ii) a loss function teaching C-HMCNN(h) when to exploit the prediction on the lower classes in the hierarchy to make predictions on the upper ones. C-HMCNN(h) has the following four features: (i) its predictions are coherent without any post-processing, (ii) differently from other state-of-the-art models (see, e.g., <ref type="bibr" target="#b32">[33]</ref>), its number of parameters is independent from the number of hierarchical levels, (iii) it can be easily implemented  <ref type="figure">Figure 1</ref>: In all figures, the smaller yellow rectangle corresponds to R 1 , while the bigger yellow one corresponds to R 2 . The first row of figures corresponds to R 1 ? R 2 = R 1 , the second corresponds to R 1 ? R 2 = ?, and the third corresponds to R 1 ? R 2 ? {R 1 , ?}. First 4 columns: decision boundaries of f (resp., g) for classes A and B (resp., A and B \ A). Last 2 columns: decision boundaries of h for classes A and B. In each figure, the darker the blue (resp., red), the more confident a model is that the datapoints in the region belong (do not belong) to the class (see the scale at the end of each row).</p><p>on GPUs using standard libraries, and (iv) it outperforms the state-of-the-art models Clus-Ens <ref type="bibr" target="#b27">[28]</ref>, HMC-LMLP <ref type="bibr" target="#b6">[7]</ref>, HMCN-F, and HMCN-R [33] on 20 commonly used real-world HMC benchmarks.</p><p>The rest of this paper is organized as follows. In Section 2, we introduce the notation and terminology used. Then, in Section 3, we present the core ideas behind C-HMCNN(h) on a simple HMC problem with just two classes, followed by the presentation of the general solution in Section 4. Experimental results are presented in Section 5, while the related work is discussed in Section 6. The last section gives some concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation and terminology</head><p>Consider an arbitrary HMC problem with a given set of classes, which are hierarchically organized as a DAG. If there is a path of length ? 0 from a class A to a class B in the DAG, then we say that B is a subclass of A (every class is thus a subclass of itself). Consider an arbitrary datapoint x ? R D , D ? 1. For each class A and model m, we assume to have a mapping m A :</p><formula xml:id="formula_0">R D ? [0, 1]</formula><p>such that x ? R D is predicted to belong to A whenever m A (x) is bigger than or equal to a userdefined threshold. To guarantee that the hierarchy constraint is always satisfied independently from the threshold, the model m should guarantee that</p><formula xml:id="formula_1">m A (x) ? m B (x), for all x ? R D , whenever A is a subclass of B: if m A (x) &gt; m B (x),</formula><p>for some x ? R D , then we have a hierarchy violation (see, e.g., <ref type="bibr" target="#b32">[33]</ref>). For ease of readability, in the rest of the paper, we always leave implicit the dependence on the considered datapoint x, and write, e.g., m A for m A (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Basic case</head><p>Our goal is to leverage standard neural network approaches for multi-label classification problems and then exploit the hierarchy constraint in order to produce coherent predictions and improve performance. Given our goal, we first present two basic approaches, exemplifying their respective strengths and weaknesses. These are useful to then introduce our solution, which is shown to present their advantages without exhibiting their weaknesses. In this section, we assume to have just two classes A ? R D and B ? R D and the constraint stating that A is a subclass of B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic approaches</head><p>In the first approach, we treat the problem as a standard multi-label classification problem and simply set up a neural network f with one output per class to be learnt: to ensure that no hierarchy violation happens, we need an additional post-processing step. In this simple case, the post-processing could set the output for A to be min(f A , f B ) or the output for B to be max(f B , f A ). In this way, all predictions are always coherent with the hierarchy constraint. Another approach for this case is to build a network g with two outputs, one for A and one for B \ A. To meaningfully ensure that no hierarchy violation happens, we need an additional post-processing step in which the predictions for the class B are given by max(g B\A , g A ). Considering the two above approaches, depending on the specific distribution of the points in A and in B, one solution may be significantly better than the other, and a priori we may not know which one it is.</p><p>To visualize the problem, assume that D = 2, and consider two rectangles R 1 and R 2 with R 1 smaller than R 2 , like the two yellow rectangles in the subfigures of <ref type="figure">Figure 1</ref>. Assume A = R 1 and B = R 1 ? R 2 . Let f + be the model obtained by adding a post-processing step to f setting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref> (analogous considerations hold, if we set</p><formula xml:id="formula_2">f + A = min(f A , f B ) and f + B = f B , as in</formula><formula xml:id="formula_3">f + A = f A and f + B = max(f B , f A ) instead).</formula><p>Intuitively, we expect f + to perform well even with a very limited number of neurons when R 1 ? R 2 = R 1 , as in the first row of <ref type="figure">Figure 1</ref>. However, if R 1 ? R 2 = ?, as in the second row of <ref type="figure">Figure 1</ref>, we expect f + to need more neurons to obtain the same performance. Consider the alternative network g, and let g + be the system obtained by setting g + A = g A and g + B = max(g B\A , g A ). Then, we expect g + to perform well when R 1 ? R 2 = ?. However, if R 1 ? R 2 = R 1 , we expect g + to need more neurons to obtain the same performance. We do not consider the model with one output for B \ A and one for B, since it performs poorly in both cases.</p><p>To test our hypothesis, we implemented f and g as feedforward neural networks with one hidden layer with 4 neurons and tanh nonlinearity. We used the sigmoid non-linearity for the output layer (from here on, we always assume that the last layer of each neural network presents sigmoid nonlinearity). f and g were trained with binary cross-entropy loss using Adam optimization <ref type="bibr" target="#b15">[16]</ref> for 20k epochs with learning rate 10 ?2 (? 1 = 0.9, ? 2 = 0.999). The datasets consisted of 5000 (50/50 train test split) datapoints sampled from a uniform distribution over [0, 1] 2 . The first four columns of <ref type="figure">Figure 1</ref> show the decision boundaries of f and g. Those of f + and g + , reported in Appendix A, can be derived from the plotted ones, while the converse does not hold. These figures highlight that f (resp., g) approximates the two rectangles better than g (resp., f ) when R 1 ? R 2 = R 1 (resp., R 1 ? R 2 = ?). In general, when R 1 ? R 2 ? {R 1 , ?}, we expect that the behavior of f and g depends on the relative position of R 1 and R 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our solution</head><p>Ideally, we would like to build a neural network that is able to have roughly the same performance of f + when R 1 ? R 2 = R 1 , of g + when R 1 ? R 2 = ?, and better than both in any other case. We can achieve this behavior in two steps. In the first step, we build a new neural network consisting of two modules: (i) a bottom module h with two outputs in [0, 1] for A and B, and (ii) an upper module, called max constraint module (MCM), consisting of a single layer that takes as input the output of the bottom module and imposes the hierarchy constraint. We call the obtained neural network coherent hierarchical multi-label classification neural network (C-HMCNN(h)).</p><p>Consider a datapoint x. Let h A and h B be the outputs of h for the classes A and B, respectively, and let y A and y B be the ground truth for the classes A and B, respectively.</p><p>The outputs of MCM (which are also the output of C-HMCNN(h)) are:</p><formula xml:id="formula_4">MCM A = h A , MCM B = max(h B , h A ).<label>(1)</label></formula><p>Notice that the output of C-HMCNN(h) ensures that no hierarchy violation happens, i.e., that for any threshold, it cannot be the case that MCM predicts that a datapoint belongs to A but not to B. In the second step, to exploit the hierarchy constraint during training, C-HMCNN(h) is trained with a novel loss function, called max constraint loss (MCLoss), defined as MCLoss = MCLoss A + MCLoss B , where:</p><formula xml:id="formula_5">MCLoss A = ?y A ln(MCM A ) ? (1 ? y A ) ln(1 ? MCM A ), MCLoss B = ?y B ln(max(h B , h A y A )) ? (1 ? y B ) ln(1 ? MCM B )).<label>(2)</label></formula><p>MCLoss differs from the standard binary cross-entropy loss</p><formula xml:id="formula_6">L = ?y A ln (MCM A ) ? (1 ? y A ) ln (1 ? MCM A ) ? y B ln (MCM B ) ? (1 ? y B ) ln (1 ? MCM B ), iff x ? A (y A = 0), x ? B (y B = 1), and h A &gt; h B .</formula><p>The following example highlights the different behavior of MCLoss compared to L. Example 3.1. Assume h A = 0.3, h B = 0.1, y A = 0, and y B = 1. Then, we obtain:</p><formula xml:id="formula_7">L = ? ln(1 ? MCM A ) ? ln(MCM B ) = ? ln(1 ? h A ) ? ln(h A ) .</formula><p>Given the above, we get:</p><formula xml:id="formula_8">?L ?h A = ? 1 h A ? 1 ? 1 h A ? ?1.9 ?L ?h B = 0 .</formula><p>Hence, if C-HMCNN(h) is trained with L, then it wrongly learns that it needs to increase h A and keep h B . On the other hand, for C-HMCNN(h) (with MCLoss), we obtain:</p><formula xml:id="formula_9">?MCLoss ?h A = ? 1 h A ? 1 ? 1.4 ?MCLoss ?h B = ? 1 h B = ?10 .</formula><p>In this way, C-HMCNN(h) rightly learns that it needs to decrease h A and increase h B .</p><p>Consider the example in <ref type="figure">Figure 1</ref>.</p><p>To check that our model behaves as expected, we implemented h as f , and trained C-HMCNN(h) with MCLoss on the same datasets and in the same way as f and g.</p><p>The last two columns of <ref type="figure">Figure 1</ref> show the decision boundaries of h (those of C-HMCNN(h) can be derived from the plotted ones and are in Appendix A). h's decision boundaries mirror those of f (resp., g) when </p><formula xml:id="formula_10">R 1 ? R 2 = R 1 (resp., R 1 ? R 2 = ?).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">General case</head><p>Consider a generic HMC problem with a set S of n hierarchically structured classes, a datapoint x ? R D , and a generic neural network h with one output for each class in S. Given a class A ? S, D A is the set of subclasses of A in S, 1 y A is the ground truth label for class A and h A ? [0, 1] is the prediction made by h for A. The output MCM A of C-HMCNN(h) for a class A is:</p><formula xml:id="formula_11">MCM A = max B?DA (h B ).<label>(3)</label></formula><p>For each class A ? S, the number of operations performed by MCM A is independent from the depth of the hierarchy, making C-HMCNN(h) a scalable model. Thanks to MCM, C-HMCNN(h) is guaranteed to always output predictions satisfying the hierarchical constraint, as stated by the following theorem, which follows immediately from Eq. (3). For each class A ? S, MCLoss A is defined as:</p><formula xml:id="formula_12">MCLoss A = ?y A ln( max B?DA (y B h B )) ? (1 ? y A ) ln(1 ? MCM A ).</formula><p>The final MCLoss is given by:</p><formula xml:id="formula_13">MCLoss = A?S MCLoss A .<label>(4)</label></formula><p>The importance of using MCLoss instead of the standard binary cross-entropy loss L becomes even more apparent in the general case. Indeed, as highlighted by the following example, the more ancestors a class has, the more likely it is that C-HMCNN(h) trained with L will remain stuck in bad local optima.</p><formula xml:id="formula_14">Example 4.2.</formula><p>Consider a generic HMC problem with n + 1 classes, and a class A ? S being a subclass of A, A 1 , . . . , A n . Suppose h A &gt; h A1 , . . . , h An , y A = 0, and y A1 , . . . , y An = 1. Then, if we use the standard binary cross-entropy loss, we obtain:</p><formula xml:id="formula_15">L = L A + n i=1 L Ai , L = ? ln(1 ? h A ) ? n ln(h A ), ?L ?h A = 1 1 ? h A ? n h A .</formula><p>Since y A = 0, we would like to get ?LA ?hA &gt; 0. However, that is possible only if h A &gt; n n+1 . Let n = 1, then we need h A &gt; 0.5, while if n = 10, we need h A &gt; 10/11 ? 0.91. On the contrary, if we use MCLoss, we obtain:</p><formula xml:id="formula_16">MCLoss = MCLoss A + n i=1 MCLoss Ai , MCLoss = ? ln(1?h A )+ n i=1 MCLoss Ai , ?MCLoss ?h A = 1 1?h A .</formula><p>Thus, no matter the value of h A , we get ?MCLossA ?hA &gt; 0.</p><p>Finally, thanks to both MCM and MCLoss, C-HMCNN(h) has the ability of delegating the prediction on a class A to one of its subclasses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental analysis</head><p>In this section, we first discuss how to effectively implement C-HMCNN(h), leveraging GPU architectures. Then, we present the experimental results of C-HMCNN(h), first considering two synthetic experiments, and then on 20 real-world datasets for which we compare with current state-of-the-art models for HMC problems. Finally, ablation studies highlight the positive impact of both MCM and MCLoss on C-HMCNN(h)'s performance. <ref type="bibr" target="#b1">2</ref> The metric that we use to evaluate models is the area under the average precision and recall curve AU (P RC). The AU (P RC) is computed as the area under the average precision recall curve, whose points (P rec, Rec) are computed as:</p><formula xml:id="formula_17">P rec = n i=1 TP i n i=1 TP i + n i=1 FP i Rec = n i=1 TP i n i=1 TP i + n i=1 FN i ,</formula><p>where TP i , FP i , and FN i are the number of true positives, false positives, and false negatives for class i, respectively. AU (P RC) has the advantage of being independent from the threshold used to predict when a datapoint belongs to a particular class (which is often heavily application-dependent) and is the most used in the HMC literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GPU implementation</head><p>For readability, MCM A and MCLoss A have been defined for a specific class A. However, it is possible to compute MCM and MCLoss for all classes in parallel, leveraging GPU architectures.</p><p>Let H be an n ? n matrix obtained by stacking n times the n outputs of the bottom module h of C-HMCNN(h). Let M be an n ? n matrix such that, for i, j ? {1, . . . , n}, where ? represents the Hadamard product, and given an arbitrary p ? q matrix Q, max(Q, dim = 1) returns a vector of length p whose i-th element is equal to max(Q i1 , . . . , Q iq ). For MCLoss, we can use the same mask M to modify the standard binary cross-entropy loss (BCELoss) that can be found in any available library (e.g., PyTorch). In detail, let y be the ground-truth vector, [h A1 , . . . , h An ] be the output vector of h,h = y ? [h A1 , . . . , h An ],H be the n ? n matrix obtained by stacking n times the vectorh. Then, MCLoss = BCELoss(((1 ? y) ? MCM) + (y ? max(M ?H, dim = 1)), y).</p><formula xml:id="formula_18">M ij = 1 if A j is a subclass of A i (A j ?<label>D</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Synthetic experiment 1</head><p>Consider the generalization of the experiment in Section 4 in which we started with R 1 outside R 2 (as in the second row of <ref type="figure">Figure 1</ref>), and then moved R 1 towards the centre of R 2 (as in the first row of <ref type="figure">Figure 1</ref>) in 9 uniform steps. The last row of <ref type="figure">Figure 1</ref> corresponds to the fifth step, i.e., R 1 was halfway. This experiment is meant to show how the performance of C-HMCNN(h), f + , and g + as in Section 3 vary depending on the relative positions of R 1 and R 2 . Here, f , g, and h were implemented and trained as in Section 3. For each step, we run the experiment 10 times, <ref type="bibr" target="#b2">3</ref> and we plot the mean AU (P RC) together with the standard deviation for C-HMCNN(h), f + , and g + in <ref type="figure" target="#fig_4">Figure 2</ref>.</p><p>As expected, <ref type="figure" target="#fig_4">Figure 2</ref> shows that f + performed poorly in the first three steps when R 1 ?R 2 = ?, it then started to perform better at step 4 when R 1 ? R 2 ? {R 1 , ?}, and it performed well from step 6 when R 1 overlaps significantly with R 2 (at least 65% of its area). Conversely, g + performed well on the first five steps, and its performance started decaying from step 6. C-HMCNN(h) performed well at all steps, as expected, showing robustness with respect to the relative positions of R 1 and R 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Synthetic experiment 2</head><p>In order to prove the importance of using MCLoss instead of L, in this experiment we compare two models: (i) our model C-HMCNN(h), and (ii) h + MCM, i.e., h with MCM built on top and trained with the standard binary cross-entropy loss L. Consider the nine rectangles arranged as in <ref type="figure" target="#fig_5">Figure 3</ref> named R 1 , . . . , R 9 . Assume (i) that we have classes A 1 . . . A 9 , (ii) that a datapoint belongs to A i if it belongs to the i-th rectangle, and (iii) that A 5 (resp., A 3 ) is an ancestor (resp., descendant) of every class. Thus, all points in R 3 belong to all classes, and if a datapoint belongs to a rectangle, then it also belongs to class A 5 . The datasets consisted of 5000 (50/50 train test split) datapoints sampled from a uniform distribution over [0, 1] 2 . Let h be a feedforward neural network with a single hidden layer with 7 neurons. We train both h + MCM and C-HMCNN(h) for 20k epochs using Adam optimization with learning rate 10 ?2 (? 1 = 0.9, ? 2 = 0.999). As expected , the average AU (P RC) (and standard deviation) over 10 runs for h + MCM trained with L is 0.938 (0.038), while h + MCM trained with MCLoss (C-HMCNN(h)) is 0.974 (0.007). Notice that not only h + MCM performs worse, but also, due to the convergence to bad local optima, the standard deviation obtained with h+MCM is 5 times higher than the one of C-HMCNN(h): the (min, median, max) AU (P RC) for h+MCM are (0.871, 0.945, 0.990), while for C-HMCNN(h) are (0.964, 0.975, 0.990). Since h + MCM presents a high standard deviation, the figure shows the decision boundaries of the 6th best performing networks for class A 5 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the state of the art</head><p>We tested our model on 20 real-world datasets commonly used to compare HMC systems (see, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>): 16 are functional genomics datasets <ref type="bibr" target="#b8">[9]</ref>, 2 contain medical images <ref type="bibr" target="#b12">[13]</ref>, 1 contains images of microalgae <ref type="bibr" target="#b13">[14]</ref>, and 1 is a text categorization dataset <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr" target="#b3">4</ref> The characteristics of these datasets are summarized in <ref type="table" target="#tab_1">Table 1</ref>. These datasets are particularly challenging, because their number of training samples is rather limited, and they have a large variation, both in the number of features (from 52 to 1000) and in the number of classes (from 56 to 4130). We applied the same preprocessing to all the datasets. All the categorical features were transformed using one-hot encoding. The missing values were replaced by their mean in the case of numeric features and by a vector of all zeros in the case of categorical ones. All the features were standardized.</p><p>We built h as a feedforward neural network with two hidden layers and ReLU non-linearity. To prove the robustness of C-HMCNN(h), we kept all the hyperparameters fixed except the hidden dimension and the learning rate used for each dataset, which are given in Appendix B and were optimized over the validation sets. In all experiments, the loss was minimized using Adam optimizer with weight decay 10 ?5 , and patience 20 (? 1 = 0.9, ? 2 = 0.999). The dropout rate was set to 70% and the batch size to 4. As in <ref type="bibr" target="#b32">[33]</ref>, we retrained C-HMCNN(h) on both training and validation data for the same number of epochs, as the early stopping procedure determined was optimal in the first pass.</p><p>For each dataset, we run C-HMCNN(h), Clus-Ens <ref type="bibr" target="#b27">[28]</ref>, and HMC-LMLP <ref type="bibr" target="#b6">[7]</ref> 10 times, and the average AU (P RC) is reported in <ref type="table" target="#tab_2">Table 2</ref>. For simplicity, we omit the standard deviations, which for C-HMCNN(h) are in the range [0.5 ? 10 ?3 , 2.6 ? 10 ?3 ], proving that it is a very stable model. As reported in <ref type="bibr" target="#b22">[23]</ref>, Clus-Ens and HMC-LMLP are the current state-of-the-art models with publicly available code. These models were run with the suggested configuration settings on each dataset. <ref type="bibr" target="#b4">5</ref> The results are shown in <ref type="table" target="#tab_2">Table 2</ref>, left side. On the right side, we show the results of HMCN-R and HMCN-F directly taken from <ref type="bibr" target="#b32">[33]</ref>, since the code is not publicly available. We report the results of both systems, because, while HMCN-R has worse results than HMCN-F, the amount of parameters of the latter grows with the number of hierarchical levels. As a consequence, HMCN-R is much lighter in terms of total amount of parameters, and the authors advise that for very large hierarchies, HMCN-R is probably a better choice than HMCN-F considering the trade-off performance vs. com- putational cost <ref type="bibr" target="#b32">[33]</ref>. Note that the number of parameters of C-HMCNN(h) is independent from the number of hierarchical levels.</p><p>As reported in <ref type="table" target="#tab_2">Table  2</ref>, C-HMCNN(h) has the greatest number of wins (it has the best performance on all datasets but 3) and best average ranking <ref type="bibr">(1.25)</ref>.</p><p>We also verified the statistical significance of the results following <ref type="bibr" target="#b10">[11]</ref>. We first executed the Friedman test, obtaining p-value 4.26 ? 10 ?15 . We then performed the post-hoc Nemenyi test, and the resulting critical diagram is shown in <ref type="figure" target="#fig_6">Figure 4</ref>, where the group of methods that do not differ significantly (significance level 0.05) are connected through a horizontal line. The Nemenyi test is powerful enough to conclude that there is a statistical significant difference between the performance of C-HMCNN(h) and all the other models but HMCN-F. Hence, following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>, we compared C-HMCNN(h) and HMCN-F using the Wilcoxon test. This test, contrarily to the Friedman test and the Nemenyi test, takes into account not only the ranking, but also the differences in performance of the two algorithms. The Wilcoxon test allows us to conclude that there is a statistical significant difference between the performance of C-HMCNN(h) and HMCN-F with p-value of 6.01 ? 10 ?3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ablation studies</head><p>To analyze the impact of both MCM and MCLoss, we compared the performance of C-HMCNN(h) on the validation set of the FunCat datasets against the performance of h + , i.e., h with the postprocessing as in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b14">[15]</ref> and h+MCM, i.e., h with MCM built on top. Both these models were trained using the standard binary cross-entropy loss. As it can be seen in <ref type="table" target="#tab_3">Table 3</ref>, MCM by itself already helps to improve the performances on all datasets but Derisi, where h + and h+MCM have the same performance. However, C-HMCNN(h), by exploiting both MCM and MCLoss, always outperforms h + and h + MCM. In <ref type="table" target="#tab_3">Table 3</ref>, we also report after how many epochs the algorithm </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>HMC problems are a generalization of hierarchical classification problems, where the labels are hierarchically organized, and each datapoint can be assigned to one path in the hierarchy (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30]</ref>). Indeed, in HMC problems, each datapoint can be assigned multiple paths in the hierarchy.</p><p>In the literature, HMC methods are traditionally divided into local and global approaches <ref type="bibr" target="#b28">[29]</ref>. Local approaches decompose the problem into smaller classification ones, and then the solutions are combined to solve the main task. Local approaches can be further divided based on the strategy that they deploy to decompose the main task. If a method trains a different classifier for each level of the hierarchy, then we have a local classifier per level as in <ref type="bibr">[5-7, 21, 35]</ref>. The works <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> are extended by <ref type="bibr" target="#b32">[33]</ref>, where HMCN-R and HMCN-F are presented. Since HMCN-R and HMCN-F are trained with both a local loss and a global loss, they are considered hybrid local-global approaches. If a method trains a classifier for each node of the hierarchy, then we have a local classifier per node. In <ref type="bibr" target="#b7">[8]</ref>, a linear classifier is trained for each node with a loss function that captures the hierarchy structure. On the other hand, in <ref type="bibr" target="#b14">[15]</ref>, one multi-layer perceptron for each node is deployed. A different approach is proposed in <ref type="bibr" target="#b2">[3]</ref>, where kernel dependency estimation is employed to project each label to a low-dimensional vector. To preserve the hierarchy structure, a generalized condensing sort and select algorithm is developed, and each vector is then learned singularly using ridge regression. Finally, if a method trains a different classifier per parent node in the hierarchy, then we have a local classifier per parent node. For example, <ref type="bibr" target="#b17">[18]</ref> proposes to train a model for each sub-ontology of the Gene Ontology, combining features automatically learned from the sequences and features based on protein interactions. In <ref type="bibr" target="#b33">[34]</ref>, instead, the authors try to solve the overfitting problem typical of local models by representing the correlation among the labels by the label distribution, and then training each local model to map datapoints to label distributions. Global methods consist of single models able to map objects with their corresponding classes in the hierarchy as a whole. A well-known global method is CLUS-HMC <ref type="bibr" target="#b31">[32]</ref>, consisting of a single predictive clustering tree for the entire hierarchy. This work is extended in <ref type="bibr" target="#b27">[28]</ref>, where Clus-Ens, an ensemble of CLUS-HMC, is proposed. In <ref type="bibr" target="#b21">[22]</ref>, a neural network incorporating the structure of the hierarchy in its architecture is proposed. While this network makes predictions that are coherent with the hierarchy, it also makes the assumption that each parent class is the union of the children. In <ref type="bibr" target="#b3">[4]</ref>, the authors propose a "competitive neural network", whose architecture replicates the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary and outlook</head><p>In this paper, we proposed a new model for HMC problems, called C-HMCNN(h), which is able to (i) leverage the hierarchical information to learn when to delegate the prediction on a superclass to one of its subclasses, (ii) produce predictions coherent by construction, and (iii) outperfom current state-of-the-art models on 20 commonly used real-world HMC benchmarks. Further, its number of parameters does not depend on the number of hierarchical levels, and it can be easily implemented on GPUs using standard libraries. In the future, we will use as h an interpretable model (see, e.g., <ref type="bibr" target="#b18">[19]</ref>), and study how MCM and MCLoss can be modified to improve the interpretability of C-HMCNN(h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In this paper, we proposed a novel model that is shown to outperform the current state-of-the-art models on commonly used HMC benchmarks. We expect our approach to have a large impact on the research community not only because of its positive results but also because it is relatively easy to implement and test using standard libraries, and the code is publicly available. From the application perspective, given the generality of the approach, it is impossible to foresee all the possible impacts in all the different application domains where HMC problems arise. We thus focus on functional genomics, which is the application domain most benchmarks come from.</p><p>The goal in functional genomics is to describe the functions and interactions of genes and their products, RNA and proteins. As stated in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, in recent years, the generation of proteomic data has increased substantially, and annotating all sequences is costly and time-consuming, making it often unfeasible. It is thus necessary to develop methods (like ours) that are able to automatize this process. Having better models for such a task may unlock many possibilities. Indeed, it may (i) allow to better understand the role of proteins in disease pathobiology, (ii) help determine the function of metagenomes offering possibilities to discover novel genes and novel biomolecules, and (iii) facilitate finding drug targets, which is crucial to the success of mechanism-based drug discovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Analysis Details</head><p>In this section, we provide more details about the conducted experimental analysis. As stated in the paper, across the different experiments, we kept all hyperparameters fixed with the exception of the hidden dimension and the learning rate, which are reported in the first two columns of <ref type="table" target="#tab_4">Table 4</ref>. The other hyperparameters were determined by searching the best hyperparameters configuration on the Funcat datasets; we then took the configuration that led to the best results on the  <ref type="table" target="#tab_4">Table 4</ref> shows the average inference time per batch in milliseconds. The average is computed over 500 batches for each dataset. All experiments were run on an Nvidia Titan Xp with 12 GB memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Intuitively, C-HMCNN(h) is able to decide whether to learn B: (i) as a whole (top figure), (ii) as the union of B \ A and A (middle figure), and (iii) as the union of a subset of B and a subset of A (bottom figure). C-HMCNN(h) has learnt when to exploit the prediction on the lower class A to make predictions on the upper class B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Definition 4.3 (Delegate). Let S = {A 1 , . . . , A n } be a set of hierarchically structured classes. Let x ? R D be a datapoint. Let h A1 , . . . , h An ? [0, 1] be the outputs of a neural network h given the input x. Let MCM A1 , . . . , MCM An be defined as in Eq. (3). Consider a class A i ? S and a class A j ? D Ai with i = j. Then, C-HMCNN(h) delegates the prediction on A i to A j for x, if MCM Ai = h Aj and h Aj &gt; h Ai . Consider the basic case in Section 3 and the figures in the last column of Figure 1. Thanks to MCM and MCLoss, C-HMCNN(h) behaves as expected: it delegates the prediction on B to A for (i) 0% of the points in A when R 1 ? R 2 = R 1 (top figure), (ii) 100% of the points in A when R 1 ? R 2 = ? (middle figure), and (iii) 85% of the points in A when R 1 and R 2 are as in the bottom figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ai ), and M ij = 0, otherwise. Then, MCM = max(M ? H, dim = 1) , 2 Link: https://github.com/EGiunchiglia/C-HMCNN/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Mean AU (P RC) with standard deviation of C-HMCNN(h), f + , and g + for each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>From left to right: (i) rectangles disposition, (ii) decision boundaries for A 5 of h + MCM trained with L, and (iii) decision boundaries for A 5 of C-HMCNN(h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Critical diagram for the Nemenyi's statistical test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Figure 5a: Decision boundaries for class B of f + when R 1 ? R 2 = ?.Figure 5b: Decision boundaries for class B of f + when R 1 ? R 2 = R 1 .Figure 5c: Decision boundaries for class B of f + when R 1 ? R 2 ? {?, R 1 }. Figure 6a: Decision boundaries for class B of g + when R 1 ? R 2 = ?.Figure 6b: Decision boundaries for class B of g + when R 1 ? R 2 = R 1 . Figure 6c: Decision boundaries for class B of g + when R 1 ? R 2 ? {?, R 1 }. Figure 7a: Decision boundaries for class B of C-HMCNN(h) when R 1 ? R 2 = ?. Figure 7b: Decision boundaries for class B of C-HMCNN(h) when R 1 ?R 2 = R 1 .Figure 7c: Decision boundaries for class B of C-HMCNN(h) when R 1 ? R 2 ? {?, R 1 }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Theorem 4.1. Let S = {A 1 , . . . , A n } be a set of hierarchically structured classes. Let h be a neural network with outputs h A1 , . . . , h An ? [0, 1]. Let MCM A1 , . . . , MCM An be defined as in Eq. (3). Then, C-HMCNN(h) does not admit hierarchy violations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of the 20 real-world datasets. Number of features (D), number of classes (n), and number of datapoints for each dataset split.</figDesc><table><row><cell>TAXONOMY</cell><cell>DATASET</cell><cell>D</cell><cell>n</cell><cell cols="3">TRAINING VALIDATION TEST</cell></row><row><cell>FUNCAT (FUN)</cell><cell>CELLCYCLE</cell><cell>77</cell><cell>499</cell><cell>1625</cell><cell>848</cell><cell>1281</cell></row><row><cell>FUNCAT (FUN)</cell><cell>DERISI</cell><cell>63</cell><cell>499</cell><cell>1605</cell><cell>842</cell><cell>1272</cell></row><row><cell>FUNCAT (FUN)</cell><cell>EISEN</cell><cell>79</cell><cell>461</cell><cell>1055</cell><cell>529</cell><cell>835</cell></row><row><cell>FUNCAT (FUN)</cell><cell>EXPR</cell><cell>551</cell><cell>499</cell><cell>1636</cell><cell>849</cell><cell>1288</cell></row><row><cell>FUNCAT (FUN)</cell><cell>GASCH1</cell><cell>173</cell><cell>499</cell><cell>1631</cell><cell>846</cell><cell>1281</cell></row><row><cell>FUNCAT (FUN)</cell><cell>GASCH2</cell><cell>52</cell><cell>499</cell><cell>1636</cell><cell>849</cell><cell>1288</cell></row><row><cell>FUNCAT (FUN)</cell><cell>SEQ</cell><cell>478</cell><cell>499</cell><cell>1692</cell><cell>876</cell><cell>1332</cell></row><row><cell>FUNCAT(FUN)</cell><cell>SPO</cell><cell>80</cell><cell>499</cell><cell>1597</cell><cell>837</cell><cell>1263</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) CELLCYCLE</cell><cell>77</cell><cell>4122</cell><cell>1625</cell><cell>848</cell><cell>1281</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) DERISI</cell><cell>63</cell><cell>4116</cell><cell>1605</cell><cell>842</cell><cell>1272</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) EISEN</cell><cell>79</cell><cell>3570</cell><cell>1055</cell><cell>528</cell><cell>835</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) EXPR</cell><cell>551</cell><cell>4128</cell><cell>1636</cell><cell>849</cell><cell>1288</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) GASCH1</cell><cell>173</cell><cell>4122</cell><cell>1631</cell><cell>846</cell><cell>1281</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) GASCH2</cell><cell>52</cell><cell>4128</cell><cell>1636</cell><cell>849</cell><cell>1288</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) SEQ</cell><cell>478</cell><cell>4130</cell><cell>1692</cell><cell>876</cell><cell>1332</cell></row><row><cell cols="2">GENE ONTOLOGY (GO) SPO</cell><cell>80</cell><cell>4166</cell><cell>1597</cell><cell>837</cell><cell>1263</cell></row><row><cell>TREE</cell><cell>DIATOMS</cell><cell>371</cell><cell>398</cell><cell>1085</cell><cell>464</cell><cell>1054</cell></row><row><cell>TREE</cell><cell>ENRON</cell><cell>1000</cell><cell>56</cell><cell>692</cell><cell>296</cell><cell>660</cell></row><row><cell>TREE</cell><cell>IMCLEF07A</cell><cell>80</cell><cell>96</cell><cell>7000</cell><cell>3000</cell><cell>1006</cell></row><row><cell>TREE</cell><cell>IMCLEF07D</cell><cell>80</cell><cell>46</cell><cell>7000</cell><cell>3000</cell><cell>1006</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of C-HMCNN(h) with the other state-of-the-art models. The performance of each system is measured as the AU (P RC) obtained on the test set. The best results are in bold.</figDesc><table><row><cell>Dataset</cell><cell cols="5">C-HMCNN(h) HMC-LMLP CLUS-ENS HMCN-R HMCN-R</cell></row><row><cell>CELLCYCLE FUN</cell><cell>0.255</cell><cell>0.207</cell><cell>0.227</cell><cell>0.247</cell><cell>0.252</cell></row><row><cell>DERISI FUN</cell><cell>0.195</cell><cell>0.182</cell><cell>0.187</cell><cell>0.189</cell><cell>0.193</cell></row><row><cell>EISEN FUN</cell><cell>0.306</cell><cell>0.245</cell><cell>0.286</cell><cell>0.298</cell><cell>0.298</cell></row><row><cell>EXPR FUN</cell><cell>0.302</cell><cell>0.242</cell><cell>0.271</cell><cell>0.300</cell><cell>0.301</cell></row><row><cell>GASCH1 FUN</cell><cell>0.286</cell><cell>0.235</cell><cell>0.267</cell><cell>0.283</cell><cell>0.284</cell></row><row><cell>GASCH2 FUN</cell><cell>0.258</cell><cell>0.211</cell><cell>0.231</cell><cell>0.249</cell><cell>0.254</cell></row><row><cell>SEQ FUN</cell><cell>0.292</cell><cell>0.236</cell><cell>0.284</cell><cell>0.290</cell><cell>0.291</cell></row><row><cell>SPO FUN</cell><cell>0.215</cell><cell>0.186</cell><cell>0.211</cell><cell>0.210</cell><cell>0.211</cell></row><row><cell>CELLCYCLE GO</cell><cell>0.413</cell><cell>0.361</cell><cell>0.387</cell><cell>0.395</cell><cell>0.400</cell></row><row><cell>DERISI GO</cell><cell>0.370</cell><cell>0.343</cell><cell>0.361</cell><cell>0.368</cell><cell>0.369</cell></row><row><cell>EISEN GO</cell><cell>0.455</cell><cell>0.406</cell><cell>0.433</cell><cell>0.435</cell><cell>0.440</cell></row><row><cell>EXPR GO</cell><cell>0.447</cell><cell>0.373</cell><cell>0.422</cell><cell>0.450</cell><cell>0.452</cell></row><row><cell>GASCH1 GO</cell><cell>0.436</cell><cell>0.380</cell><cell>0.415</cell><cell>0.416</cell><cell>0.428</cell></row><row><cell>GASCH2 GO</cell><cell>0.414</cell><cell>0.371</cell><cell>0.395</cell><cell>0.463</cell><cell>0.465</cell></row><row><cell>SEQ GO</cell><cell>0.446</cell><cell>0.370</cell><cell>0.438</cell><cell>0.443</cell><cell>0.447</cell></row><row><cell>SPO GO</cell><cell>0.382</cell><cell>0.342</cell><cell>0.371</cell><cell>0.375</cell><cell>0.376</cell></row><row><cell>DIATOMS</cell><cell>0.758</cell><cell>-</cell><cell>0.501</cell><cell>0.514</cell><cell>0.530</cell></row><row><cell>ENRON</cell><cell>0.756</cell><cell>-</cell><cell>0.696</cell><cell>0.710</cell><cell>0.724</cell></row><row><cell>IMCLEF07A</cell><cell>0.956</cell><cell>-</cell><cell>0.803</cell><cell>0.904</cell><cell>0.950</cell></row><row><cell>IMCLEF07D</cell><cell>0.927</cell><cell>-</cell><cell>0.881</cell><cell>0.897</cell><cell>0.920</cell></row><row><cell>AVERAGE RANKING</cell><cell>1.25</cell><cell>5.00</cell><cell>3.93</cell><cell>2.93</cell><cell>1.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Impact of MCM and MCM+MCLoss on the performance measured as AU (P RC) and on the total number of epochs for the validation set of the Funcat datasets. RC) Epochs AU (P RC) Epochs AU (P RC) Epochs</figDesc><table><row><cell></cell><cell>h +</cell><cell></cell><cell>h+MCM</cell><cell></cell><cell cols="2">C-HMCNN(h)</cell></row><row><cell cols="2">Dataset AU (P CELLCYCLE 0.220</cell><cell>74</cell><cell>0.229</cell><cell>108</cell><cell>0.232</cell><cell>106</cell></row><row><cell>DERISI</cell><cell>0.179</cell><cell>58</cell><cell>0.179</cell><cell>66</cell><cell>0.182</cell><cell>67</cell></row><row><cell>EISEN</cell><cell>0.262</cell><cell>76</cell><cell>0.271</cell><cell>107</cell><cell>0.285</cell><cell>110</cell></row><row><cell>EXPR</cell><cell>0.246</cell><cell>14</cell><cell>0.265</cell><cell>19</cell><cell>0.270</cell><cell>20</cell></row><row><cell>GASCH1</cell><cell>0.239</cell><cell>28</cell><cell>0.258</cell><cell>42</cell><cell>0.261</cell><cell>38</cell></row><row><cell>GASCH2</cell><cell>0.221</cell><cell>103</cell><cell>0.234</cell><cell>132</cell><cell>0.235</cell><cell>131</cell></row><row><cell>SEQ</cell><cell>0.245</cell><cell>8</cell><cell>0.269</cell><cell>13</cell><cell>0.274</cell><cell>13</cell></row><row><cell>SPO</cell><cell>0.186</cell><cell>103</cell><cell>0.189</cell><cell>117</cell><cell>0.190</cell><cell>115</cell></row><row><cell>AVERAGE RANKING</cell><cell>2.94</cell><cell></cell><cell>2.06</cell><cell></cell><cell>1.00</cell><cell></cell></row><row><cell cols="7">stopped training in average. As it can be seen, even though C-HMCNN(h) and h+MCM need more</cell></row><row><cell>epochs than h</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>+ , the numbers are still comparable.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hidden dimension used for each dataset, learning rate used for each dataset, and average inference time per batch in milliseconds (ms). Average computed over 500 batches for each dataset. DATASET Hidden Dimension Learning Rate Time per batch (ms) highest number of datasets. The hyperparameter values taken in consideration were: (i) learning rate: [10 ?3 , 10 ?4 , 10 ?5 ], (ii) batch size: [4, 64, 256], (iii) dropout: [0.6, 0.7], and (iv) weight decay: [10 ?3 , 10 ?5 ]. Concerning the hidden dimension, we took into account all possible dimensions from 250 to 2000 with step equal to 250, and from 2000 to 10000 with step 1000. The last column of</figDesc><table><row><cell>CELLCYCLE FUN</cell><cell>500</cell><cell>10 ?4</cell><cell>2.0</cell></row><row><cell>DERISI FUN</cell><cell>500</cell><cell>10 ?4</cell><cell>2.0</cell></row><row><cell>EISEN FUN</cell><cell>500</cell><cell>10 ?4</cell><cell>1.7</cell></row><row><cell>EXPR FUN</cell><cell>1000</cell><cell>10 ?4</cell><cell>1.9</cell></row><row><cell>GASCH1 FUN</cell><cell>1000</cell><cell>10 ?4</cell><cell>2.0</cell></row><row><cell>GASCH2 FUN SEQ FUN</cell><cell>500 2000</cell><cell>10 ?4 10 ?4</cell><cell>2.8 2.0</cell></row><row><cell>SPO FUN</cell><cell>250</cell><cell>10 ?4</cell><cell>1.6</cell></row><row><cell>CELLCYCLE GO DERISI GO</cell><cell>1000 500</cell><cell>10 ?4 10 ?4</cell><cell>2.4 2.5</cell></row><row><cell>EISEN GO</cell><cell>500</cell><cell>10 ?4</cell><cell>3.4</cell></row><row><cell>EXPR GO</cell><cell>4000</cell><cell>10 ?5</cell><cell>3.9</cell></row><row><cell>GASCH1 GO</cell><cell>500</cell><cell>10 ?4</cell><cell>2.5</cell></row><row><cell>GASCH2 GO</cell><cell>500</cell><cell>10 ?4</cell><cell>2.8</cell></row><row><cell>SEQ GO SPO GO</cell><cell>9000 500</cell><cell>10 ?5 10 ?4</cell><cell>2.6 3.3</cell></row><row><cell>DIATOMS</cell><cell>2000</cell><cell>10 ?5</cell><cell>2.0</cell></row><row><cell>ENRON IMCLEF07A</cell><cell>1000 1000</cell><cell>10 ?5 10 ?5</cell><cell>3.6 3.4</cell></row><row><cell>IMCLEF07D</cell><cell>1000</cell><cell>10 ?5</cell><cell>2.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">By definition, A ? DA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">All subfigures in Figure 1 correspond to the decision boundaries of f , g, and h in the first of the 10 runs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Links: https://dtai.cs.kuleuven.be/clus/hmcdatasets and http://kt.ijs.si/DragiKocev/PhD/resources<ref type="bibr" target="#b4">5</ref> We also ran the code from<ref type="bibr" target="#b21">[22]</ref>. However, we obtained very different results from the ones reported in the paper. Similar negative results are also reported in<ref type="bibr" target="#b22">[23]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank Francesco Giannini and Marco Gori for useful discussions, and Maria Kiourlappou for her feedback on the broader impact section. Eleonora Giunchiglia is supported by the EPSRC under the grant EP/N509711/1 and by an Oxford-DeepMind Graduate Scholarship. This work was also supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1 and by the AXA Research Fund. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE (EP/P020275/1) and GPU computing support by Scan Computers International Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label prediction of gene function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafer</forename><surname>Barut?uoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="830" to="836" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Should we really use post-hoc tests based on mean-ranks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Benavoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Mangili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilabel classification on tree-and DAG-structured hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label hierarchical classification using a competitive neural network for protein function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bronoski</forename><surname>Helyane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?lio</forename><forename type="middle">C</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nievola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNN</title>
		<meeting>of IJCNN</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification for protein function prediction: A local approach based on neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Carlos Ponce De</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Ferreira De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carvalho</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISDA</title>
		<meeting>of ISDA</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification using local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Carlos Ponce De</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Ferreira De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carvalho</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="56" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reduction strategies for hierarchical multi-label classification in protein function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Carlos Ponce De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Ferreira De Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaochu</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">373</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental algorithms for hierarchical classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Zaniboni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="31" to="54" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Machine Learning and Data Mining for Yeast Functional Genomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Clare</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Wales</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large margin hierarchical classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<editor>Carla E. Brodley</editor>
		<meeting>of ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierchical annotation of medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragi</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzana</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa?o</forename><surname>D?eroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IS</title>
		<meeting>of IS<address><addrLine>Ljubljana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical classification of diatom images using ensembles of predictive clustering trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivica</forename><surname>Dimitrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragi</forename><surname>Kocev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzana</forename><surname>Loskovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saso</forename><surname>Dzeroski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecol. Informatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A hierarchical multi-label classification method based on neural networks for gene function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biotechnology and Biotechnological Equipment</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1613" to="1621" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Enron Corpus: A new dataset for email classification research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML</title>
		<meeting>of ECML</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DeepGO: Predicting protein functions from sequence and interactions using a deep ontology-aware classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxat</forename><surname>Kulmanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mohammad Asif Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoehndorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="660" to="668" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RCV1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DEEPre: Sequence-based enzyme EC number prediction by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramzan</forename><surname>Umarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="760" to="769" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AWX: An integrated approach to hierarchical-multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Masera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Blanzieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML-PKDD</title>
		<meeting>of ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="322" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine learning for discovering missing or wrong protein function annotations -A comparison using updated benchmark datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Kenji</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Lietaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</author>
		<idno>485:1-485:32</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Consistent probabilistic outputs for protein function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Stafford</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A large-scale evaluation of computational protein function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Predrag</forename><surname>Radivojac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="227" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convex calibrated surrogates for hierarchical classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivani</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1852" to="1860" />
			<date type="published" when="2015" />
			<publisher>PMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kernel-based learning of hierarchical multilabel classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ndor</forename><surname>Szedm?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1601" to="1626" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hendrik Blockeel, Dragi Kocev, and Saso Dzeroski. Predicting gene function using hierarchical multi-label decision tree ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leander</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Nascimento</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Alves</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical text classification and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2001 IEEE International Conference on Data Mining</title>
		<meeting>2001 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">True path rule hierarchical ensembles for genome-wide gene function prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Valentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biology Bioinform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="832" to="847" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Decision trees for hierarchical multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Struyf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leander</forename><surname>Schietgat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saso</forename><surname>Dzeroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Blockeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="214" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label classification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonatas</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5225" to="5234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical classification based on label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changdong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5533" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">mlDEEPre: Multi-functional enzyme function prediction with hierarchical multi-label deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
