<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional Generative Adversarial Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Enomoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Sakurada</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Wang</surname></persName>
							<email>weimin@ucl.nuee.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Fukui</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Chubu University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Matsuoka</surname></persName>
							<email>matsuoka.m.ab@m.titech.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Nakamura</surname></persName>
							<email>r.nakamura@aist.go.jp</email>
							<affiliation key="aff3">
								<orgName type="department">Advanced Industrial Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuo</forename><surname>Kawaguchi</surname></persName>
							<email>kawaguti@nagoya-u.jpfhiro@vision.cs.chubu.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Nagoya University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional Generative Adversarial Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a method for cloud removal from visible light RGB satellite images by extending the conditional Generative Adversarial Networks (cGANs) from RGB images to multispectral images. Satellite images have been widely utilized for various purposes, such as natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. However, the obscurity caused by clouds makes it unstable to monitor the situation on the ground with the visible light camera. Images captured by a longer wavelength are introduced to reduce the effects of clouds. Synthetic Aperture Radar (SAR) is such an example that improves visibility even the clouds exist. On the other hand, the spatial resolution decreases as the wavelength increases. Furthermore, the images captured by long wavelengths differs considerably from those captured by visible light in terms of their appearance. Therefore, we propose a network that can remove clouds and generate visible light images from the multispectral images taken as inputs. This is achieved by extending the input channels of cGANs to be compatible with multispectral images. The networks are trained to output images that are close to the ground truth using the images synthesized with clouds over the ground truth as inputs. In the available dataset, the proportion of images of the forest or the sea is very high, which will introduce bias in the training dataset if uniformly sampled from the original dataset. Thus, we utilize the t-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the problem of bias in the training dataset. Finally, we confirm the feasibility of the proposed network on the dataset of four bands images, which include three visible light bands and one near-infrared (NIR) band.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB NIR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McGANs</head><p>Cloud-free RGB Cloud mask Figure 1: McGANs for cloud removal</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Satellite images have been widely utilized in various of fields such as remote sensing, computer vision, environmental science and meteorology. With the help of satellite images, we can observe the situation on the ground for natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. There are many research area dealing with satellite images, e.g., object recognition from the satellite images, change detection for ground usage or disaster situation analysis.</p><p>However, the obscurity caused by the cloud makes it unstable to monitor the situation on the ground with a visible light camera. To be unaffected by the cloud, images captured by longer wavelengths are introduced. Synthetic Aperture Radar (SAR) <ref type="bibr" target="#b5">[6]</ref> is such an example, which improves visibility even in the presence of clouds. On the other hand, the spatial resolution decreases as the wavelength increases. Furthermore, the image captured by a long wavelength differs considerably in appearance from the one captured by visible light. This affects the visibility for observation.</p><p>In this paper, we propose Multispectral conditional Generative Adversarial Networks (McGANs) based on conditional Generative Adversarial NetworkscGANs), for cloud removal from visible light RGB satellite images with multispectral images as inputs. See <ref type="figure">Fig.1</ref> for illustration. Compared with cGANs, the input channels of McGANs are expended for multispectral images. For the input of RGB images obscured by clouds and the registered NIR images, McGANs is trained to output the RGB images that are close to the ground truth. However, it is impractical to capture the cloud-free and the cloud obscured images of the completely same scene at the same time. Hence, we synthesize images with the simulated clouds over the ground truth RGB images to generate the training data. Furthermore, the prediction accuracy is expected to be improved by training the networks to detect the region of cloud simultaneously. Both the synthesized and the ground truth RGB images are color corrected to eliminate the affection of color tone caused by variety of imaging conditions such as weather, lighting and the processing method of the image sensor.</p><p>In the available dataset, the ratio of images of the forest or the sea is very high, which will introduce bias in the training dataset if uniformly sampled from the original dataset. Thus, we utilize the t-Distributed Stochastic Neighbor Embedding (t-SNE) <ref type="bibr" target="#b13">[13]</ref> to reduce the bias problem of the training dataset. Finally, we confirm the feasibility of the proposed networks on the dataset of four bands images, which includes three visible light bands and one near-infrared (NIR) band.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the field of remote sensing, microwave is usually utilized since it is unaffected by the cloud cover. Synthetic Aperture Radar (SAR) is mounted on airplanes and satellites to overcome the shortage of low spatial resolution of the microwave. Nonetheless, the resolution of SAR images is still much lower than that of the images captured by visible light. Besides, it is difficult to understand the SAR images directly. To improve the visibility of SAR images, there also exists the work about coloring these SAR images <ref type="bibr" target="#b5">[6]</ref>.</p><p>In the field of computer vision, many dehazing methods have been proposed for RGB images only <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b1">2]</ref> or for both RGB and NIR images <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">20]</ref>. The pre-knowledge or assumption about the color information of the hazed imaged is necessary in the former method. In the latter, NIR images, which possess higher penetrability through fog than the visible light, are used as the guide to dehaze the RGB images.</p><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">[7]</ref> is the most relevant to our work. GANs is consisted of two types of networks, Generator and Discriminator. Generator is trained to generate images that cannot be discriminated by Discriminator with the ground truth, while Discriminator is trained to discriminate between the generated images by the Generator and the ground truth. The conditional version of GANs was also proposed in <ref type="bibr" target="#b14">[14]</ref>. However, learning by GANs is unstable. To increase the stability, convolutional networks and Batch Normalization are introduced to Deep Convolutional Generative Adversarial Networks (DCGANs) <ref type="bibr" target="#b17">[17]</ref> is proposed.</p><p>Research about image generation based on cGANs and DCGANS has been widely applied for image restoration or the removal of certain objects such as rain and snow <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">21]</ref>. In particular, the method in <ref type="bibr" target="#b10">[10]</ref> can generate general and high-quality images by combing Generator of U-Net <ref type="bibr" target="#b18">[18]</ref> and Discriminator of PatchGAN <ref type="bibr" target="#b12">[12]</ref>. The Generator of U-Net spreads the missing spatial features in the convolution layers of Encoder to each layer of Decoder by introducing the skip connection between layers of Encoder and Decoder. PatchGAN is able to model the high frequencies for sharp details by training the Discriminator on the image patches. Generally, these cGANs-based methods predict the obscured regions of the image with the surrounding unobscured information only from the input RGB images.</p><p>Based on the aforementioned research, we propose the cloud removal networks by taking the advantage of the color information from visible light images and the high penetrability from images captured by longer wave. The proposed networks predict the obscured region from not only the RGB images but also images captured by longer wavelengths that can partly or completely penetrate the cloud. Our final purpose is to implement the networks that can merge SAR images captured by the cloud-penetrating microwave. As the first step, we construct and evaluate the networks for cloud removal with the visible light RGB images and the near-infrared spectrum NIR images in this work (the region of NIR wavelength is the closest to visible light).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Generation for Cloud Removal</head><p>In this work, images captured by the WorldView-2 earth observation satellite are used. Both visible light images and the NIR images possess the resolution of 20, 000 ? 20, 000 with the spatial resolution of 0.5 [m/pixel]. We chose eight comparatively cloudless images, which mainly captured urban areas, for actual learning. In total, 37,000 images with a resolution of 256 ? 256 are extracted for training McGANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Synthesis of cloud-obscured images</head><p>Both cloud obscured images and cloud-free images are indispensable to train the networks for cloud removal, as they form the training and ground truth data respectively. However, the appearance varies greatly as the imaging conditions, such as lighting and status on the ground, changes with time even for the same location. Therefore, we create the dataset for learning by synthesizing the simulated cloud on the cloudless or cloud-free ground truth images. Furthermore, to compensate for the difference in color tone between the cloud synthesized images and the original images, color correction <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b3">4]</ref> is performed on both images. In this work, the clouds are simulated by Perlin noise <ref type="bibr" target="#b16">[16]</ref> firstly. Then the simulated clouds are combined with the RGB images by alpha blending to generate obscured images. <ref type="figure">Fig.2</ref> shows an example of the image synthesis process. The RGB image ( <ref type="figure">Fig.2a</ref>) is overlaid by a Perlin noise simulated cloud ( <ref type="figure">Fig.2b)</ref> with the alpha blending method to synthesize the image <ref type="figure">(Fig.2c</ref>). Then generated image is further processed by color correction <ref type="figure">(Fig. 2d</ref>). To show the necessity of color correction, we take another image ( <ref type="figure">Fig.3</ref>) for comparison. <ref type="figure">Fig.3a</ref> is the original RGB image of a dif-ferent location from that in <ref type="figure">Fig.2</ref>. The color corrected result is shown in <ref type="figure">Fig.3b</ref>. By comparing the two groups of images, we can observe that the variety of color tone is greatly improved with the process of color correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Uniformization of the dataset with t-SNE</head><p>Since most of the earth is covered with seas and forests, the contents of the satellite images used in the work are also mainly of these two types. If we randomly sample the images for training, the learning result is prone to overfitting in certain categories due to the bias of the training data. Hence, we utilized t-SNE to sample the images by categories to avoid this problem.</p><p>First, we extract a feature vector of 4096 dimensions from each image with the AlexNet <ref type="bibr" target="#b11">[11]</ref>. The extracted feature vectors are mapped to the 2D space with t-SNE. Then, we uniformly sample 2000 images from the 2D feature space to create the training dataset.</p><p>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset <ref type="bibr" target="#b2">[3]</ref> and the land use image dataset UC Merced land use dataset <ref type="bibr" target="#b0">[1]</ref> (21 classes and 100 images for each class) are used for training the AlexNet. The processed results of the features from the two datasets after using t-SNE are shown in <ref type="figure" target="#fig_2">Fig.4. Fig.4a</ref> shows the distribution of the training images mapped with features from ImageNet dataset, and <ref type="figure" target="#fig_2">Fig.4b</ref> shows the result with features from UC Merced land use dataset. In the <ref type="figure" target="#fig_2">Fig.4a</ref>, images of the urban areas are clustered in the upper region, forest images are clustered at the right, images of the sea are clustered in the lower region and images of farmlands are clustered at the left. We can see that the images are well clustered by their categories. We also can see a similar result in <ref type="figure" target="#fig_2">Fig. 4b</ref> except that some images from the same category are distributed separately, e.g., images of forests are divided into the left and the lower parts. This is probably caused by the differences between the images used in this work and the  images in UC dataset, in addition to the insufficiency of the images in the UC dataset. Therefore, we adopt the features extracted by the AlexNet from ImageNet for t-SNE.</p><p>The number of images in each cluster is shown in a heat map in <ref type="figure" target="#fig_3">Fig.5</ref>. From <ref type="figure" target="#fig_3">Fig.5</ref> we can see that the images are uniformly distributed except in some regions of the grids. Images are uniformly sampled by the grid to improve the overfitting caused by the bias of in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multispectral conditional Generative Adversarial Networks (McGANs)</head><p>In this paper, we propose Multispectral conditional Generative Adversarial Networks (McGANs), which extends the input of cGANs to multispectral images in order to be capable of merging input visible light images and images of longer wavelengths to remove clouds from the visible light images. The detailed architecture of McGANs are shown in <ref type="figure" target="#fig_4">Fig. 6</ref> and Tab.1.</p><p>We extend the input of the cGANs model proposed in <ref type="bibr" target="#b10">[10]</ref> to four channels RGB-NIR images 1 . Furthermore, the output is also extended to a total number of four channels, including the predicted RGB image after cloud removal and <ref type="bibr" target="#b0">1</ref> By adding images captured using other wavelength, such as far infrared rays and microwaves, the input will be further extended    The objective of a conditional GAN can be expressed as</p><formula xml:id="formula_0">L cGAN (G, D) =E x,y?p data (x,y) [log D(x, y)]+ E x?p data (x),z?pz(z) [log(1 ? D(x, G(x, z)))],<label>(1)</label></formula><p>where Generator G tries to minimize the objective against an adversarial Discriminator D that tries to maximize it. To encourage less blurring, L1 loss can be added to the objective as follows <ref type="bibr" target="#b10">[10]</ref> G * = arg min</p><formula xml:id="formula_1">G max D L cGAN (G, D) + L L1 (G). (2)</formula><p>Let I M be the input multispectral image and I T be the target RGB image with a total of four channels, including RGB and the grayscale mask image of the cloud. The L1 Loss function (denoted as L L1 ) of the Generator is defined in Eq.3. ? c represents the weight of each channel for the loss calculation 2 , and ?(I M ) represents the predicted result from the input image I M from the trained networks.</p><formula xml:id="formula_2">L L1 (G) = 1 4HW 4 c=1 H v=1 W u=1 ? c |I (u,v,c) T ? ?(I M ) (u,v,c) | 1 (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation Results</head><p>To evaluate our proposed method, experimental results are listed and discussed in this Section. From the experimental results, we expect to show that the proposed Mc-GANs are able to improve visibility by cloud removal with RGB and NIR satellite images.</p><p>As explained earlier, the satellite images captured at different times (even though they might be of the same area), vary greatly in their appearance as imaging conditions, such as lighting and the situation on the ground, change. This makes it difficult to acquire the ground truth of the area blocked by the cloud. We use 5,000 groups of images as described in Sec.3 to train the network. Each group includes an image of the area not obscured by the cloud, a mask image of the simulated clouds using Perlin noise, a synthesized image and an NIR image. All images are processed with color correction. The number of minibatch is set to 16 and the number of epochs is 500.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB NIR</head><p>Cloud-free RGB Ground truth Cloud mask To verify the advantage of using multispectral images for cloud removal, we also compare them against the RGB images generated by the networks (NIR-cGANs) with only NIR images as input. NIR images are used as input and images that are not obscured by the cloud are used as ground truth. The same dataset (as in McGANs) is used for training NIR-cGANs. The number of minibatch and epochs is also the same.</p><p>Sample results of the synthesized cloud obscured images are shown in <ref type="figure" target="#fig_6">Fig.7</ref>. The columns represent the synthesized cloud obscured RGB images, NIR images, RGB images predicted by McGANs, the ground truth and the mask images of the clouds predicted by McGANs, from left to right.</p><p>Sample results of real cloud obscured images are shown in <ref type="figure" target="#fig_7">Fig.8, Fig.9</ref> and <ref type="figure">Fig.10</ref>. The columns represents RGB images obscured by the cloud, NIR images, RGB images predicted by McGANs, RGB images predicted by NIR-GANs and the mask images of the clouds predicted by Mc-GANs, from left to right. From <ref type="figure" target="#fig_7">Fig.8</ref>, we can observe that although the images, which are generated only with NIR images, look like visible light RGB images, their colors dif-fer from the ground truth. While the clouds are well removed in the predicted results by McGANs except for the region obscured by the cloud that infrared can not penetrate. Even for these regions in the predicted images, the color appears similarly to the very light color in the input images. This also proves that McGANs dose not predict color from the information only from the NIR images.</p><p>On the other hand, in <ref type="figure">Fig.9</ref>, we can see that the white object is erroneously recognized as the cloud from the output mask image. This indicates that it is difficult to separate the cloud from the white object with only the visible light images and NIR images, when they are overlapped. In addition, as seen in <ref type="figure">Fig.10</ref>, clouds are not removed when they are too thick to be penetrated by NIR. The purpose of this research is to observe the real situation on the ground. Thus, the regions blocked by clouds in the NIR image will not be predicted, which is different from <ref type="bibr" target="#b15">[15]</ref>. When predicting the area blocked by clouds in both visible light and NIR images, it is necessary to model the cloud penetration of NIR based on the visible light images, process the simulated cloud with the penetration model and then synthesize RGB NIR Cloud-free RGB NIR2RGB Cloud mask the modeled cloud on the NIR images. To verify the necessity of the NIR images, we also compare the results generated by our proposed method with that generated from only a RGB image as the input. For thin clouds that can be partly penetrated by the visible light, the results dose not differ much. However, for clouds that can be only penetrated by NIR light, the result with the presence of NIR appears more natural as shown in <ref type="figure">Fig.11</ref>. We can see some line contours of the roads on the ground from the upper left part of the NIR image in <ref type="figure">Fig.11</ref>, while these contours are occluded in the RGB image. This can be considered as the reason why the result generated with both the NIR and RGB images looks more natural that generated with the RGB image. From the above results, we have confirmed that the proposed McGANs can remove clouds and predict the color properly when the cloud is thin enough to be penetrated by the NIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a method to remove thin clouds from satellite images formed using visible light by extending cGANs to multispectral images. The dataset for training networks is constructed by synthesizing simulated clouds with Perlin noise over images without clouds, which makes it possible to generate cloud obscured training images and ground truth of the same area. In addition, to avoid overfitting to certain categories caused by biased datasets, we introduce t-SNE to sample images uniformly in each category. Finally, the experimental results evaluated on the constructed data prove that the clouds in the visible light images can be removed if they are penetrated in NIR images.</p><p>In the future, we will extend McGANs to far infrared (FIR) images and SAR images which captured by longer wavelengths and build the networks that can remove all the clouds in the visible light images. The findings obtained by analyzing the filters of McGANs in this work can also be applied to establish the model of cloud penetration for waves at each wavelength region or to the physical model of SAR. In addition, the simulated clouds with Perlin noise used in this work are somewhat different from real clouds in visible light images. Therefore, statistical analysis of actual cloud images is necessary to improve the reality of the simulated clouds for training data. Furthermore, we aim to improve the prediction accuracy for different areas by increasing the number and variety of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB NIR</head><p>Cloud-free RGB NIR2RGB Cloud mask </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Synthesis of cloud obscured images. a: Original RGB image. b: Simulated cloud using Perlin noise. c: Merged image with the cloud by alpha blending. d: Final result after color correction (a) (b) Example of color correction for real RGB image with cloud. a: Original RGB image obscured by the cloud. b: Color corrected result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization by t-SNE. a:ImageNet<ref type="bibr" target="#b2">[3]</ref>. Images of urban areas are clustered in the upper region, forests images are clustered at the right, images of the sea are clustered in the lower region, ant the farmlands are clustered at the left. b:UC Merced Land Use Dataset<ref type="bibr" target="#b0">[1]</ref>. Some images from the same category are distributed separately, for example images of forests are divided into the left and the lower parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Heat map of image distribution mapped by t-SNE. The colors indicate the number of images in the corresponding 2D feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Network Architecture of Generator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Network ArchitectureDetails of the network structure about McGANs used in this work are shown in Tab.1. The layer of Convolution, Batch Normalization, and ReLU are represented by C, B, R respectively. D indicates that the Dropout is applied. Numbers in parentheses indicate the number, size, stride of the convolution filters sequentially. In addition, Leaky ReLU is used in all ReLU layers of Encoder and Discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Prediction results by McGANs with the synthesized cloud images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Prediction results by McGANs with real cloud images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 11 :</head><label>911</label><figDesc>Failure case due to a white object A prediction result generated from only a RGB image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Network Architecture of McGANs</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell><cell>Discriminator</cell></row><row><cell>CR (64, 3, 1)</cell><cell cols="2">CBRD (512, 4, 2) CBR (64, 4, 2)</cell></row><row><cell cols="3">CBR (128, 4, 2) CBRD (512, 4, 2) CBR (128, 4, 2)</cell></row><row><cell cols="3">CBR (256, 4, 2) CBRD (512, 4, 2) CBR (256, 4, 2)</cell></row><row><cell>CBR (512, 4, 2)</cell><cell>CBR (512, 4, 2)</cell><cell>CBR (512, 4, 2)</cell></row><row><cell>CBR (512, 4, 2)</cell><cell>CBR (256, 4, 2)</cell><cell>C (1, 3, 1)</cell></row><row><cell>CBR (512, 4, 2)</cell><cell>CBR (128, 4, 2)</cell><cell></cell></row><row><cell>CBR (512, 4, 2)</cell><cell>CBR (64, 4, 2)</cell><cell></cell></row><row><cell>CBR (512, 4, 2)</cell><cell>C (4, 3, 1)</cell><cell></cell></row><row><cell cols="3">the grayscale mask image, which is estimated simultane-</cell></row><row><cell cols="3">ously to improve the prediction accuracy. The input RGB-</cell></row><row><cell cols="3">NIR image, the output RGB image and the cloud mask im-</cell></row><row><cell cols="3">age are normalized to [?1, 1] at each channel and then trans-</cell></row><row><cell cols="2">ferred to the network.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">?c is set to 1 in this work.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bag-Of-Visual-Words and Spatial Extensions for Land-Use Classification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-local image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1674" to="1682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Survey of Color Mapping and its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Faridul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chamaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tremeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2014 -State of the Art Reports. The Eurographics Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NEAR-INFRARED GUIDED COLOR IMAGE DEHAZING</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2363" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthetic Aperture Radar (SAR) Utilization for Disaster Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technological Seminar on Environmenal Monitoring</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single Image Haze Removal Using Dark Channel Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Reproduction of Colour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W G</forename><surname>Hunt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image-to-Image Translation with Conditional Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ima-geNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Conditional Generative Adversarial Nets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context Encoders: Feature Learning by Inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="681" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Unsu-Pervised Representation</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Generative</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Networks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Color Image Dehazing using the Near-Infrared</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fredembach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1629" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unified Image Fusion based on Application-Adaptive Importance Measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05957</idno>
		<title level="m">Image Deraining Using a Conditional Generative Adversarial Network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
