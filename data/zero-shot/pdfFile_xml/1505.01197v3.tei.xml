<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Action Recognition with R*CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
							<email>gkioxari@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff2">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Action Recognition with R*CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R * CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R * CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R * CNN is not limited to action recognition. In particular, R * CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider <ref type="figure" target="#fig_0">Figure 1 (a)</ref>. How do we know that the person highlighted with the red box is working on a computer? Could it be that the computer is visible in the image, is it that the person in question has a very specific pose or is it that he is sitting in an office environment? Likewise, how do we know that the person in <ref type="figure" target="#fig_0">Figure 1 (b)</ref> is running? Is it the running-specific pose of her arms and legs or do the scene and the other people nearby also convey the action?</p><p>For the task of action recognition from still images, the pose of the person in question, the identity of the objects surrounding them and the way they interact with those objects and the scene are vital cues. In this work, our objective is to use all available cues to perform activity recognition.</p><p>Formally, we adapt the Region-based Convolutional Network method (RCNN) <ref type="bibr" target="#b9">[11]</ref> to use more than one region when making a prediction. We call our method R * CNN. In R * CNN, we have a primary region that contains the person in question and a secondary region that automatically discovers contextual cues.</p><p>How do we select the secondary region? In other words, how to we decide which region contains information about the action being performed? Inspired by multiple-instance learning (MIL) <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b19">21]</ref> and Latent SVM <ref type="bibr" target="#b7">[9]</ref>, if I is an image and r is a region in I containing the target person, we define the score of action ? as score(?; I, r) = w ? p ? ?(r; I) + max s?R(r;I) w ? s ? ?(s; I), (1) where ?(r; I) is a vector of features extracted from region r in I, while w ? p and w ? s are the primary and secondary weights for action ? respectively. R(r; I) defines the set of candidates for the secondary region. For example, R(r; I) could be the set of regions in the proximity of r, or even the whole set of regions in I. Given scores for each action, we use a softmax to compute the probability that the person in r is performing action ?: P (?|I, r) = exp(score(?; I, r)) ? ?A exp(score(? ; I, r))</p><p>.</p><p>The feature representation ?(?) and the weight vectors w ? p and w ? s in Eq. 1 are learned jointly for all actions <ref type="bibr">Figure 2</ref>. Schematic overview of our approach. Given image I, we select the primary region to be the bounding box containing the person (red box) while region proposals define the set of candidate secondary regions (green boxes). For each action ?, the most informative secondary region is selected (max operation) and its score is added to the primary. The softmax operation transforms scores into probabilities and forms the final prediction.</p><p>? ? A using a CNN trained with stochastic gradient descent (SGD). We build on the Fast RCNN implementation <ref type="bibr" target="#b8">[10]</ref>, which efficiently processes a large number of regions per image. <ref type="figure">Figure 2</ref> shows the architecture of our network. We quantify the performance of R * CNN for action recognition using two datasets: PASCAL VOC Actions <ref type="bibr" target="#b5">[7]</ref> and the MPII Human Pose dataset <ref type="bibr" target="#b0">[2]</ref>. On PASCAL VOC, R * CNN yields 90.2% mean AP, improving the previous state-of-the-art approach [28] by 6 percentage points, according to the leaderboard [1]. We visualize the selected secondary regions in <ref type="figure">Figure 3</ref> and show that indeed the secondary models learn to pick auxiliary cues as desired. On the larger MPII dataset, R * CNN yields 26.7% mean AP, compared to 5.5% mean AP achieved by the best performing approach, as reported by <ref type="bibr" target="#b23">[25]</ref>, which uses holistic <ref type="bibr" target="#b30">[32]</ref> and pose-specific features along with motion cues. In addition to the task of action recognition, we show that R * CNN can successfully be used for fine-grained tasks. We experiment with the task of attribute recognition and achieve state-of-the-art performance on the Berkeley Attributes of People dataset <ref type="bibr" target="#b2">[4]</ref>. Our visualizations in <ref type="figure">Figure 9</ref> show that the secondary regions capture the parts specific to the attribute class being considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Action recognition. There is a variety of work in the field of action recognition in static images. The majority of the approaches use holistic cues, by extracting features on the person bounding box and combining them with contextual cues from the whole image and object models.</p><p>Maji et al. <ref type="bibr" target="#b18">[20]</ref> train action specific poselets and for each instance create a poselet activation vector that is classified using SVMs. They capture contextual cues in two ways: they explicitly detect objects using pre-trained models for the bicycle, motorbike, horse and tvmonitor categories and exploit knowledge of actions of other people in the image. Hoai et al. <ref type="bibr" target="#b14">[16]</ref> use body-part detectors and align them with respect to the parts of a similar instance, thus aligning their feature descriptors. They combine the part based features with object detection scores and train non-linear SVMs. Khosla et al. <ref type="bibr" target="#b32">[34]</ref> densely sample image regions at arbitrary locations and scales with reference to the ground-truth region. They train a random forest classifier to discriminate between different actions. Prest et al. <ref type="bibr" target="#b24">[26]</ref> learn humanobject interactions using only action labels. They localize the action object by finding recurring patterns on images of actions and then capture their relative spatial relations. The aforementioned approaches are based on hand-engineered features such as HOG <ref type="bibr" target="#b3">[5]</ref> and SIFT <ref type="bibr" target="#b17">[19]</ref>.</p><p>CNNs achieve state-of-the-art performance on handwritten digit classification <ref type="bibr" target="#b16">[18]</ref>, and have recently been applied to various tasks in computer vision such as image classification <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b26">28]</ref> and object detection <ref type="bibr" target="#b9">[11]</ref> with impressive results. For the task of action recognition, Oquab et al. <ref type="bibr" target="#b21">[23]</ref> use a CNN on ground-truth boxes for the task of action classification, but observe a small gain in performance compared to previous methods. Hoai <ref type="bibr" target="#b13">[15]</ref> uses a geometrical distribution of regions placed in the image and in the ground-truth box and weights their scores to make a single prediction, using fc7 features from a network trained on the ImageNet-1k dataset <ref type="bibr" target="#b4">[6]</ref>. Gkioxari et al. <ref type="bibr" target="#b10">[12]</ref> train body part detectors (head, torso, legs) on pool5 features in a slidingwindow manner and combine them with the ground-truth box to jointly train a CNN.</p><p>Our work is different than the above mentioned approaches in the following ways. We use bottom up region proposals <ref type="bibr" target="#b28">[30]</ref> as candidates for secondary regions, instead of anchoring regions of specific aspect ratios and at specific locations in the image, and without relying on the reference provided by the ground-truth bounding box. Region proposals have been shown to be effective object candidates allowing for detection of objects irrespective of occlusion and viewpoint. We jointly learn the feature maps and the weights of the scoring models, allowing for action specific representations to emerge. These representations might refer to human-object relations, human-scene relations and human-human relations. This approach is contrary to work that predefines the relations to be captured or that makes use of hand-engineered features, or features from networks trained for different tasks. We allow the classifier to pick the most informative secondary region for the task at hand. As we show in Section 4, the selected secondary region is instance specific and can be an object (e.g., cell phone), a part of the scene (e.g., nearby bicycles), the whole scene, or part of the human body.</p><p>Scene and Context. The scene and its role in vision and perception have been studied for a long time. Biederman et al. <ref type="bibr" target="#b1">[3]</ref> identify five classes of relationships (presence, position, size, support and interposition) between an object and its setting and conduct experiments to measure how well humans identify objects when those relationships are violated. They found that the ability to recognize objects is much weaker and it becomes worse as violations become more severe. More recently, Oliva and Torralba <ref type="bibr" target="#b20">[22]</ref> study the contextual associations of objects with their scene and link various forms of context cues with computer vision.</p><p>Multiple-Instance Learning. Multiple instance learning (MIL) provides a framework for training models when full supervision is not available at train time. Instead of accurate annotations, the data forms bags, with a positive or a negative label <ref type="bibr" target="#b19">[21]</ref>. There is a lot of work on MIL for computer vision tasks. For object detection, Viola et al. <ref type="bibr" target="#b29">[31]</ref> use MIL and boosting to obtain face detectors when ground truth object face locations are not accurately provided at train time. More recently, Song et al. <ref type="bibr" target="#b27">[29]</ref> use MIL to localize objects with binary image-level labels (is the object present in the image or not). For the task of image classification, Oquab et al. <ref type="bibr" target="#b22">[24]</ref> modify the CNN architecture <ref type="bibr" target="#b15">[17]</ref>, which divides the image into equal sized regions and combines their scores via a final max pooling layer to classify the whole image. Fang et al. <ref type="bibr" target="#b6">[8]</ref> follow a similar technique to localize concepts useful for image caption generation.</p><p>In this work, we treat the secondary region for each training example as an unknown latent variable. During training, each time an example is sampled, the forward pass of the CNN infers the current value of this latent variable through a max operation. This is analogous to latent parts locations and component models in DPM <ref type="bibr" target="#b7">[9]</ref>. However, here we perform end-to-end optimization with an online algorithm (SGD), instead of optimizing a Latent SVM. <ref type="figure">Figure 2</ref> shows the architecture of our network. Given an image I, we select the primary region to be the bounding box containing the person (knowledge of this box is given at test time in all action datasets). Bottom up region proposals form the set of candidate secondary regions. For each action ?, the most informative region is selected through the max operation and its score is added to the primary (Eq. 1). The softmax operation transforms scores into estimated posterior probabilities (Eq. 2), which are used to predict action labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">R * CNN</head><p>We build on Fast RCNN (FRCN) <ref type="bibr" target="#b8">[10]</ref>. In FRCN, the input image is upsampled and passed through the convolutional layers. An adaptive max pooling layer takes as input the output of the last convolutional layer and a list of regions of interest (ROIs). It outputs a feature map of fixed size (e.g. 7 ? 7 for the 16-layer CNN by <ref type="bibr" target="#b26">[28]</ref>) specific to each ROI. The ROI-pooled features are subsequently passed through the fully connected layers to make the final prediction. This implementation is efficient, since the computationally intense convolutions are performed at an image-level and are subsequently being reused by the ROI-specific operations.</p><p>The test-time operation of FRCN is similar to SPPnet <ref type="bibr" target="#b12">[14]</ref>. However, the training algorithm is different and enables fine-tuning all network layers, not just those above the final ROI pooling layer, as in <ref type="bibr" target="#b12">[14]</ref>. This property is important for maximum classification accuracy with very deep networks.</p><p>In our implementation, we extend the FRCN pipeline. Each primary region r of an image I predicts a score for each action ? ? A (top stream in <ref type="figure">Figure 2</ref>). At the same time, each region within the set of candidate secondary regions R(r; I) independently makes a prediction. These scores are combined, for each primary region r, by a max operation over r's candidate regions (bottom stream in <ref type="figure">Figure 2)</ref>.</p><p>We define the set of candidate secondary regions R(r; I) as</p><formula xml:id="formula_1">R(r; I) = {s ? S(I) : overlap(s, r) ? [l, u]},<label>(3)</label></formula><p>where S(I) is the set of region proposals for image I. In our experiments, we use Selective Search <ref type="bibr" target="#b28">[30]</ref>. The lower and upper bounds for the overlap, which here is defined as the intersection over union between the boxes, defines the set of the regions that are considered as secondary for each primary region. For example, if l = 0 and u = 1 then R(r; I) = S(I), for each r, meaning that all bottom up proposals are candidates for secondary regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning</head><p>We train R * CNN with stochastic gradient descent (SGD) using backpropagation. We adopt the 16-layer network architecture from <ref type="bibr" target="#b26">[28]</ref>, which has been shown to perform well for image classification and object detection.</p><p>During training, we minimize the log loss of the predictions. If P (? | I, r) is the softmax probability that action ? is performed in region r in image I computed by Eq. 2, then the loss over a batch of training examples</p><formula xml:id="formula_2">B = {I i , r i , l i } M i=1 is given by loss(B) = ? 1 M M i=1 log P (? = l i | I i , r i ),<label>(4)</label></formula><p>where l i is the true label of example r i in image I i .</p><p>Rather than limiting training to the ground-truth person locations, we use all regions that overlap more than 0.5 with a ground-truth box. This condition serves as a form of data augmentation. For every primary region, we randomly select N regions from the set of candidate secondary regions. N is a function of the GPU memory limit (we use a Nvidia K40 GPU) and the batch size.</p><p>We fine-tune our network starting with a model trained on ImageNet-1K for the image classification task. We tie the weights of the fully connected primary and secondary layers (fc6, fc7), but not for the final scoring models. We set the learning rate to 0.0001, the batch size to 30 and consider 2 images per batch. We pick N = 10 and train for 10K iterations. Larger learning rates prevented fine-tuning from converging.</p><p>Due to the architecture of our network, most computation time is spent during the initial convolutions, which happen over the whole image. Computation does not scale much with the number of boxes, contrary to the original implementation of RCNN <ref type="bibr" target="#b9">[11]</ref>. Training takes 1s per iteration, while testing takes 0.4s per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We demonstrate the effectiveness of R * CNN on action recognition from static images on the PASCAL VOC Actions dataset <ref type="bibr" target="#b5">[7]</ref>, the MPII Human Pose dataset <ref type="bibr" target="#b0">[2]</ref> and the Stanford 40 Actions dataset <ref type="bibr" target="#b31">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">PASCAL VOC Action</head><p>The PASCAL VOC Action dataset consists of 10 different actions, Jumping, Phoning, Playing Instrument, Reading, Riding Bike, Riding Horse, Running, Taking Photo, Using Computer, Walking as well as examples of people not performing some of the above action, which are marked as Other. The ground-truth boxes containing the people are provided both at train and test time. During test time, for every example we estimate probabilities for all actions and compute AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Control Experiments</head><p>We experiment with variants of our system to show the effectiveness of R * CNN.</p><p>? RCNN. As a baseline approach we train Fast R-CNN for the task of action classification. This network exploits only the information provided from the primary region, which is defined as the ground-truth region.</p><p>? Random-RCNN. We use the ground-truth box as a primary region and a box randomly selected from the secondary regions. We train a network for this task similar to R * CNN with the max operation replaced by rand</p><p>? Scene-RCNN. We use the ground-truth box as the primary region and the whole image as the secondary. We jointly train a network for this task, similar to R * CNN, where the secondary model learns action specific weights solely from the scene (no max operation is performed in this case)</p><p>? R * CNN (l, u). We experiment with various combinations of values for the only free parameters of our pipeline, namely the bounds (l, u) of the overlaps used when defining the secondary regions R(r; I), where r is the primary region</p><p>? R * CNN (l, u, n S ). In this setting, we use n S &gt; 1 secondary regions instead of one. The secondary regions are selected in a greedy manner. First we select the secondary region s 1 exactly as in R * CNN. The i-th secondary region s i is selected via the max operation from the set R(r; I)?R(s 1 ; I)?...?R(s i?1 ; I), where r is the primary region.</p><p>The Random-and Scene-settings show the value of selecting the most informative region, rather than forcing the secondary region to be the scene or a region selected at random. <ref type="table">Table 1</ref> shows the performance of all the variants on the val set of the PASCAL VOC Actions. Our experiments show that R * CNN performs better across all categories. In particular, Phoning, Reading, Taking Photo perform significantly better than the baseline approach and Scene-RCNN. Riding Bike, Riding Horse and Running show the smallest improvement, probably due to scene bias of the images containing those actions. Another interesting observation is that our approach is not sensitive to the bounds of overlap (l, u). R * CNN is able to perform very well even for the unconstrained setting where all regions are allowed to be picked by the secondary model, (l = 0, u = 1). In our basic R * CNN setting, we use one secondary region. However, one region might not be able to capture all the modes of contextual cues present in the image. Therefore, we extend R * CNN to include n S secondary regions. Our experiments show that for n S = 2 the performance is the same as with R * CNN for the optimal set of parameters of (l = 0.2, u = 0.75).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with published results</head><p>We compare R * CNN to other approaches on the PASCAL VOC Action test set. <ref type="table" target="#tab_2">Table 2</ref> shows the results. Oquab et al. <ref type="bibr" target="#b21">[23]</ref> train an 8-layer network on ground-truth boxes. Gkioxari et al. <ref type="bibr" target="#b10">[12]</ref> use part detectors for head, torso, legs and train a CNN on the part regions and the ground-truth box. Hoai <ref type="bibr" target="#b13">[15]</ref> uses an 8-layer network to extract fc7 features from regions at multiple locations and scales inside the image and and the box and accumulates their scores to get the final prediction. Simonyan and Zisserman <ref type="bibr" target="#b26">[28]</ref> combine a 16-layer and a 19-layer network and train SVMs on fc7 features from the image and the ground-truth box. R * CNN (with (l = 0.2, u = 0.75)) outperforms all other approaches by a substantial margin. R * CNN seems to be performing significantly better for actions which involve small objects and action-specific pose appearance, such as Phoning, Reading, Taking Photo, Walking. <ref type="figure">Figure 3</ref> shows examples from the top predictions for each action on the test set. Each block corresponds to a different action. Red highlights the person to be classified while green the automatically selected secondary region. For actions Jumping, Running and Walking the secondary region is focused either on body parts (e.g. legs, arms) or on more instances surrounding the instance in question (e.g. joggers). For Taking Photo, Phoning, Reading and Playing Instrument the secondary region focuses almost exclusively on the object and its interaction with the arms. For Riding Bike, Riding Horse and Using Computer it focuses on the object, or the presence of similar instances and the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Visualization of secondary regions</head><p>Interestingly, the secondary region seems to be picking different cues depending on the instance in question. For example in the case of Running, the selected region might highlight the scene (e.g. road), parts of the human body (e.g. legs, arms) or a group of people performing the action, as shown in <ref type="figure">Figure 3</ref>. <ref type="figure" target="#fig_1">Figure 4</ref> shows erroneous predictions for each action on the val set (in descending score). Each block corresponds to a different action. The misclassified instance is shown in red and the corresponding secondary region with green. For Riding Bike and Riding Horse, which achieve a very high AP, the mistakes are of very low score. For Jumping, Phoning and Using Computer the mistakes occur due to confusions with instances of similar pose. In addition, for Playing Instrument most of the misclassifications are people performing in concert venues, such as singers. For Tak- ing Photo and Playing Instrument the presence of the object seems to be causing most misclassifications. For Running and Walking they seem to often get confused with each other as well as with standing people (an action which is not present explicitly in the dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">MPII Human Pose Dataset</head><p>The MPII Human Pose dataset contains 400 actions and consists of approximately 40,000 instances and 24,000 images. The images are extracted from videos from YouTube. The training set consists of 15,200 images and 22,900 instances performing 393 actions. The number of positive training examples per category varies drastically <ref type="bibr" target="#b23">[25]</ref>. The amount of training data ranges from 3 to 476 instances, with an average of 60 positives per action. The annotations do not include a ground-truth bounding box explicitly, but provide a point (anywhere in the human body) and a rough scale of the human. This information can be used to extract a rough location of the instance, which is used as input in our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">R * CNN vs. RCNN</head><p>We split the training set into train and val sets. We make sure that frames of the same video belong to the same split to avoid overfitting. This results in 12,500 instances in train and 10,300 instances in val. We train the baseline RCNN network and R * CNN. We pick (l = 0.2, u = 0.5) due to the large number of region proposals generated by <ref type="bibr">[</ref>  <ref type="table">Table 1</ref>. AP on the PASCAL VOC Action 2012 val set. RCNN is the baseline approach, with the ground-truth region being the primary region. Random-RCNN is a network trained with primary the ground-truth region and secondary a random region. Scene-RCNN is a network trained with primary the ground-truth region and secondary the whole image. R * CNN (l, u) is our system where l, u define the lower and upper bounds of the allowed overlap of the secondary region with the ground truth. R * CNN (l, u, nS) is a variant in which nS secondary regions are used, instead of one.  On the val set, RCNN achieves 16.5% mean AP while R * CNN achieves 21.7% mean AP, across all actions. <ref type="figure" target="#fig_2">Figure 5</ref> shows the performance on MPII val for RCNN and R * CNN. On the left, we show a scatter plot of the AP for all actions as a function of their training size. On the right, we show the mean AP across actions belonging to one out of three categories, depending on their training size.</p><p>The performance reported in <ref type="figure" target="#fig_2">Figure 5</ref> is instancespecific. Namely, each instance is evaluated. One could evaluate the performance at the frame-level (as done in <ref type="bibr" target="#b23">[25]</ref>), i.e. classify the frame and not the instance. We can generate frame-level predictions by assigning for each action the maximum score across instances in the frame. That yields 18.2% mean AP for RCNN and 23% mean AP for R * CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison with published results</head><p>In <ref type="bibr" target="#b23">[25]</ref>, various approaches for action recognition are reported on the test set. All the approaches mentioned use motion features, by using frames in the temporal neighborhood of the frame in question. The authors test variants of Dense Trajectories (DT) <ref type="bibr" target="#b30">[32]</ref> which they combine with pose specific features. The best performance on the test set is 5.5% mean AP (frame-level) achieved by the DT combined with a pose specific approach.</p><p>We evaluate R * CNN on the test set 2 and achieve 26.7% <ref type="bibr" target="#b0">2</ref> We sent our results to the authors of <ref type="bibr" target="#b23">[25]</ref> for evaluation since test annotations are not publicly available. mAP for frame-level recognition. Our approach does not use motion, which is a strong cue for action recognition in video, and yet manages to outperform DT by a significant margin. Evaluation on the test set is performed only at the frame-level. <ref type="figure" target="#fig_3">Figure 6</ref> shows the mean AP across actions in a descending order of training size. This figure allows for a direct comparison with the published results, as shown in <ref type="figure">Figure   Figure 3</ref>. Top predictions on the PASCAL VOC Action test set. The instance in question is shown with a red box, while the selected secondary region with a green box. The nature of the secondary regions depends on the action and the image itself. Even within the same action category, the most informative cue can vary. 1(b) in <ref type="bibr" target="#b23">[25]</ref>. <ref type="figure">Figure 7</ref> shows some results on the test set. We highlight the instance in question with red, and the secondary box with green. The boxes for the instances were derived from the point annotations (some point on the person) and the rough scale provided at train and test time. The predicted action label is overlaid in each image.</p><p>Even though R * CNN outperforms DT, there is still need of movement to boost performance for many categories. For example, even though the MPII dataset has a many examples for actions such as Yoga, Cooking or food preparation and Video exercise workout, R * CNN performs badly on those categories (1.1% mean AP). We believe that a hybrid approach which combines image and motion features, similar to <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b11">13]</ref>, would perform even better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Stanford 40 Actions Dataset</head><p>We run R * CNN on the Stanford 40 Actions dataset <ref type="bibr" target="#b31">[33]</ref>. This dataset consists of 9532 images of people performing 40 different actions. The dataset is split in half to comprise the training and test split. Bounding boxes are provided for all people performing actions. R * CNN achieves an average AP of 90.9% on the test set, with performance varying from   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Attribute Classification</head><p>Finally, we show that R * CNN can also be used for the task of attribute classification. On the Berkeley Attributes of People dataset <ref type="bibr" target="#b2">[4]</ref>, which consists of images of people and their attributes, e.g. wears hat, is male etc, we train R * CNN as described above. The only difference is that our loss is no longer a log loss over softmax probabilities, but the cross entropy over independent logistics because attribute prediction is a multi-label task. <ref type="table" target="#tab_4">Table 3</ref> reports the performance in AP of our approach, as well as other competing methods. <ref type="figure">Figure 9</ref> shows results on the test set. From the visualizations, the secondary regions learn to focus on the parts that are specific to the attribute being considered. For example, for the Has Long Sleeves class, the secondary regions focus on the arms and torso of the instance in question, while for Has Hat focus is on the face of the person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduce a simple yet effective approach for action recognition. We adapt RCNN to use more than one region in order to make a prediction, based on the simple observation that contextual cues are significant when deciding what action a person is performing. We call our sys-tem R * CNN. In our setting, both features and models are learnt jointly, allowing for action-specific representations to emerge. R * CNN outperforms all published approaches on two datasets. More interestingly, the auxiliary information selected by R * CNN for prediction captures different contextual modes depending on the instance in question. R * CNN is not limited to action recognition. We show that R * CNN can be used successfully for tasks such as attribute classification. Our visualizations show that the secondary regions capture the region relevant to the attribute considered. Performance varies from 70.5% for texting message to 100% for playing violin. The average AP across all actions achieved by our model is 90.9%. <ref type="figure">Figure 9</ref>. Results on the Berkeley Attributes of People test set. We highlight the person in question with a red box, and the secondary region with a green box. The predicted attribute is overlaid.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of people performing actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Top mistakes on the PASCAL VOC Action val set. The misclassified instance is shown in red, while the selected secondary region in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Performance on MPII val for RCNN (blue ) and R * CNN (brown). Left: AP (%) for all actions as a function of their training size (x-axis). Right: Mean AP (%) for three discrete ranges of training size (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Mean AP (%) on MPII test for R * CNN across actions in descending order of their training size. A direct comparison with published results, as shown inFigure 1(b) in<ref type="bibr" target="#b23">[25]</ref>, can be drawn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>AP (%) of R * CNN on the Stanford 40 dataset per action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>AP on the PASCAL VOC Action 2012 test set. Oquab et al.<ref type="bibr" target="#b21">[23]</ref> train an 8-layer network on ground-truth boxes. Gkioxari et al.<ref type="bibr" target="#b10">[12]</ref> use part detectors for head, torso, legs and train a CNN. Hoai<ref type="bibr" target="#b13">[15]</ref> uses an 8-layer network to extract fc7 features from regions at multiple locations and scales. Simonyan and Zisserman<ref type="bibr" target="#b26">[28]</ref> combine a 16-layer and a 19-layer network and train SVMs on fc7 features from the image and the ground-truth box. R</figDesc><table /><note>* CNN (with (l = 0.2, u = 0.75)) outperforms all other approaches by a significant margin.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>70.5% for texting message to 100% for playing violin.Figure 8 shows the AP performance per action on the test set. Training code and models are publicly available.Figure 7. Predictions on the MPII test set. We highlight the person in question with a red box, and the secondary region with a green box. The predicted action label is overlaid.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pants mAP</cell></row><row><cell>PANDA [35]</cell><cell>5</cell><cell>91.7</cell><cell>82.7</cell><cell>70.0</cell><cell>74.2</cell><cell>49.8</cell><cell>86.0</cell><cell>79.1</cell><cell>81.0</cell><cell>96.4</cell><cell>79.0</cell></row><row><cell>Gkioxari et al. [12]</cell><cell>16</cell><cell>92.9</cell><cell>90.1</cell><cell>77.7</cell><cell>93.6</cell><cell>72.6</cell><cell>93.2</cell><cell>93.9</cell><cell>92.1</cell><cell>98.8</cell><cell>89.5</cell></row><row><cell>RCNN</cell><cell>16</cell><cell>91.8</cell><cell>88.9</cell><cell>81.0</cell><cell>90.4</cell><cell>73.1</cell><cell>90.4</cell><cell>88.6</cell><cell>88.9</cell><cell>97.6</cell><cell>87.8</cell></row><row><cell>R  *  CNN</cell><cell>16</cell><cell>92.8</cell><cell>88.9</cell><cell>82.4</cell><cell>92.2</cell><cell>74.8</cell><cell>91.2</cell><cell>92.9</cell><cell>89.4</cell><cell>97.9</cell><cell>89.2</cell></row></table><note>AP (%) CNN layers Is Male Has Long Hair Has Glasses Has Hat Has T-Shirt Has Long Sleeves Has Shorts Has Jeans Has Long</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>AP on the Berkeley Attributes of People test set. PANDA<ref type="bibr" target="#b33">[35]</ref> uses CNNs trained for each poselet type. Gkioxari et al.<ref type="bibr" target="#b10">[12]</ref> detect parts and train a CNN jointly on the whole and the parts. RCNN is our baseline approach based on FRCN. Both RCNN and R * CNN do not use any additional part annotations at training time.<ref type="bibr" target="#b10">[12]</ref> and R * CNN perform equally well, with the upside that R * CNN does not need use keypoint annotations during training.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source code and models are available at https://github.com/ gkioxari/RstarCNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Intel Visual Computing Center and the ONR SMARTS MURI N000140911051. The GPUs used in this research were generously donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scene perception detecting and judging objects undergoing relational violations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mezzanotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing people: Poselet-based attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://www.image-net.org/challenges/LSVRC/2012/.2" />
		<title level="m">ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC2012)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part based models. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Actions and attributes from wholes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regularized max pooling for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Action recognition from weak alignment of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A framework for multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The role of context in object recognition. Trends in cognitive sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Weakly supervised object recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of interactions between humans and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combining randomization and discrimination for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PANDA: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
