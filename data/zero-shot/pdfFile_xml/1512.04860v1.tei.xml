<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Increasing the Action Gap: New Operators for Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
							<email>bellemare@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
							<email>ostrovski@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
							<email>aguez@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
							<email>philipt@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
							<email>munos@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Increasing the Action Gap: New Operators for Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.</p><p>Value-based reinforcement learning is an attractive solution to planning problems in environments with unknown, unstructured dynamics. In its canonical form, value-based reinforcement learning produces successive refinements of an initial value function through repeated application of a convergent operator. In particular, value iteration <ref type="bibr" target="#b4">(Bellman 1957)</ref> directly computes the value function through the iterated evaluation of Bellman's equation, either exactly or from samples (e.g. Q-Learning, <ref type="bibr" target="#b29">Watkins 1989)</ref>.</p><p>In its simplest form, value iteration begins with an initial value function V 0 and successively computes V k+1 := T V k , where T is the Bellman operator. When the environment dynamics are unknown, V k is typically replaced by Q k , the state-action value function, and T is approximated by an empirical Bellman operator. The fixed point of the Bellman operator, Q * , is the optimal state-action value function or optimal Q-function, from which an optimal policy ? * can be recovered.</p><p>In this paper we argue that the optimal Q-function is inconsistent, in the sense that for any action a which is subop-timal in state x, Bellman's equation for Q * (x, a) describes the value of a nonstationary policy: upon returning to x, this policy selects ? * (x) rather than a. While preserving global consistency appears impractical, we propose a simple modification to the Bellman operator which provides us a with a first-order solution to the inconsistency problem. Accordingly, we call our new operator the consistent Bellman operator.</p><p>We show that the consistent Bellman operator generally devalues suboptimal actions but preserves the set of optimal policies. As a result, the action gap -the value difference between optimal and second best actions -increases. Increasing the action gap is advantageous in the presence of approximation or estimation error <ref type="bibr" target="#b10">(Farahmand 2011)</ref>, and may be crucial for systems operating at a fine time scale such as video games <ref type="bibr" target="#b26">(Togelius et al. 2009;</ref><ref type="bibr" target="#b3">Bellemare et al. 2013</ref>), real-time markets <ref type="bibr" target="#b13">(Jiang and Powell 2015)</ref>, and robotic platforms <ref type="bibr" target="#b19">(Riedmiller et al. 2009;</ref><ref type="bibr" target="#b12">Hoburg and Tedrake 2009;</ref><ref type="bibr" target="#b8">Deisenroth and Rasmussen 2011;</ref><ref type="bibr" target="#b22">Sutton et al. 2011)</ref>. In fact, the idea of devaluating suboptimal actions underpins Baird's advantage learning <ref type="bibr" target="#b2">(Baird 1999)</ref>, designed for continuous time control, and occurs naturally when considering the discretized solution of continuous time and space MDPs (e.g. <ref type="bibr" target="#b16">Munos and Moore 1998;</ref>, whose limit is the Hamilton-Jacobi-Bellman equation <ref type="bibr" target="#b14">(Kushner and Dupuis 2001)</ref>. Our empirical results on the bicycle domain <ref type="bibr" target="#b18">(Randlov and Alstrom 1998)</ref> show a marked increase in performance from using the consistent Bellman operator.</p><p>In the second half of this paper we derive novel sufficient conditions for an operator to preserve optimality. The relative weakness of these new conditions reveal that it is possible to deviate significantly from the Bellman operator without sacrificing optimality: an optimality-preserving operator needs not be contractive, nor even guarantee convergence of the Q-values for suboptimal actions. While numerous alternatives to the Bellman operator have been put forward (e.g. recently <ref type="bibr" target="#b1">Azar et al. 2011;</ref><ref type="bibr" target="#b6">Bertsekas and Yu 2012)</ref>, we believe our work to be the first to propose such a major departure from the canonical fixed-point condition required from an optimality-preserving operator. As proof of the richness of this new operator family we describe a few practical instantiations with unique properties.</p><p>We use our operators to obtain state-of-the-art empirical results on the Arcade Learning Environment <ref type="bibr" target="#b3">(Bellemare et al. 2013)</ref>. We consider the Deep Q-Network (DQN) architecture of <ref type="bibr" target="#b15">Mnih et al. (2015)</ref>, replacing only its learning rule with one of our operators. Remarkably, this one-line change produces agents that significantly outperform the original DQN. Our work, we believe, demonstrates the potential impact of rethinking the core components of value-based reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>We consider a Markov decision process M := (X , A, P, R, ?) where X is the state space, A is the finite action space, P is the transition probability kernel, R is the reward function mapping state-action pairs to a bounded subset of R, and ? ? [0, 1) is the discount factor. We denote by Q := Q X ,A and V := V X the space of bounded real-valued functions over X ? A and X , respectively. For Q ? Q we write V (x) := max a Q(x, a), and follow this convention for related quantities (? forQ, V for Q , etc.) whenever convenient and unambiguous. In the context of a specific (x, a) ? X ? A we further write E P := E x ?P (? | x,a) to mean the expectation with respect to P (? | x, a), with the convention that x always denotes the next state random variable. A deterministic policy ? : X ? A induces a Q-function Q ? ? Q whose Bellman equation is</p><formula xml:id="formula_0">Q ? (x, a) := R(x, a) + ? E P Q ? (x , ?(x )).</formula><p>The state-conditional expected return V ? (x) := Q ? (x, ?(x)) is the expected discounted total reward received from starting in x and following ?.</p><p>The Bellman operator T : Q ? Q is defined pointwise as</p><formula xml:id="formula_1">T Q(x, a) := R(x, a) + ? E P max b?A Q(x , b).</formula><p>(1)</p><p>T is a contraction mapping in supremum norm <ref type="bibr" target="#b5">(Bertsekas and Tsitsiklis 1996)</ref> whose unique fixed point is the optimal Q-function</p><formula xml:id="formula_2">Q * (x, a) = R(x, a) + ? E P max b?A Q * (x , b),</formula><p>which induces the optimal policy ? * :</p><formula xml:id="formula_3">? * (x) := arg max a?A Q * (x, a) ?x ? X .</formula><p>A Q-function Q ? Q induces a greedy policy ?(x) := arg max a Q(x, a), with the property that Q ? = Q if and only if Q = Q * . For x ? X we call ?(x) the greedy action with respect to Q and a = ?(x) a nongreedy action; for ? * these are the usual optimal and suboptimal actions, respectively.</p><p>We emphasize that while we focus on the Bellman operator, our results easily extend to its variations such as SARSA <ref type="bibr" target="#b20">(Rummery and Niranjan 1994)</ref>, policy evaluation <ref type="bibr" target="#b23">(Sutton 1988)</ref>, and fitted Q-iteration <ref type="bibr" target="#b9">(Ernst, Geurts, and Wehenkel 2005)</ref>. In particular, our new operators all have a sample-based form, i.e., an analogue to the Q-Learning rule of <ref type="bibr" target="#b29">Watkins (1989)</ref>. a 1 a 2 p = 1, r = 0 r = 1 "Bad State"</p><p>x 1 x 2 "cake" "no cake" V = 2 (1 + ?) p = 1/2, <ref type="figure">Figure 1</ref>: A two-state MDP illustrating the non-stationary aspect of the Bellman operator. Here, p and r indicate transition probabilities and rewards, respectively. In state x 1 the agent may either eat cake to receive a reward of 1 and transition to x 2 with probability 1 2 , or abstain for no reward. State x 2 is a low-value absorbing state with &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Consistent Bellman Operator</head><p>It is well known (and implicit in our notation) that the optimal policy ? * for M is stationary (i.e., time-independent) and deterministic. In looking for ? * , we may therefore restrict our search to the space ? of stationary deterministic policies. Interestingly, as we now show the Bellman operator on Q is not, in a sense, restricted to ?.</p><p>To begin, consider the two-state MDP depicted in <ref type="figure">Figure  1</ref>. This MDP abstracts a Faustian situation in which an agent repeatedly chooses between an immediately rewarding but ultimately harmful option (a 1 ), or an unrewarding alternative (a 2 ). For concreteness, we imagine the agent as faced with an endless supply of delicious cake (with ? &gt; 0) and call these the "cake" and "no cake" actions.</p><p>Eating cake can cause a transition to x 2 , the "bad state", whose value is independent of the agent's policy:</p><formula xml:id="formula_4">V ? (x 2 ) := ?2(1 + ) 1 ? ?? ? ?.</formula><p>In state x 1 , however, the Q-values depend on the agent's future behaviour. For a policy ? ? ?, the value of a 1 is</p><formula xml:id="formula_5">Q ? (x 1 , a 1 ) = 1 + ? 1 2 V ? (x 1 ) + 1 2 V ? (x 2 ) (2) = 1 + ? 2 V ? (x 1 ) ? (1 + ) = ? 2 V ? (x 1 ) ? .</formula><p>By contrast, the value of a 2 is Q ? (x 1 , a 2 ) = 0 + ?V ? (x 1 ), which is greater than Q ? (x 1 , a 1 ) for all ?. It follows that not eating cake is optimal, and thus V * (x 1 ) = Q * (x 1 , a 2 ) = 0. Furthermore, (2) tells us that the value difference between optimal and second best action, or action gap, is</p><formula xml:id="formula_6">Q * (x 1 , a 2 ) ? Q * (x 1 , a 1 ) = .</formula><p>Notice that Q * (x 1 , a 1 ) = ? does not describe the value of any stationary policy. That is, the policy? with?(x 1 ) = a 1 has value</p><formula xml:id="formula_7">V?(x 1 ) = ? + ? 2 V?(x 1 ) = ? 1 ? ?/2 ,<label>(3)</label></formula><p>and in particular this value is lower than Q * (x 1 , a 1 ). Instead, Q * (x 1 , a 1 ) describes the value of a nonstationary policy which eats cake once, but then subsequently abstains.</p><p>So far we have considered the Q-functions of given stationary policies ?, and argued that these are nonstationary. We now make a similar statement about the Bellman operator: for any Q ? Q, the nongreedy components of Q := T Q do not generally describe the expected return of stationary policies. Hence the Bellman operator is not restricted to ?.</p><p>When the MDP of interest can be solved exactly, this nonstationarity is a non-issue since only the Q-values for optimal actions matter. In the presence of estimation or approximation error, however, small perturbations in the Q-function may result in erroneously identifying the optimal action. Our example illustrates this effect: an estimateQ of Q * which is off by can induce a pessimal greedy policy (i.e.?).</p><p>To address this issue, we may be tempted to define a new Q-function which explicitly incorporates stationarity:</p><formula xml:id="formula_8">Q ? STAT (x, a) := R(x, a) + ? E P max b?A Q ? STAT (x , b), (4) ? (y) := a if y = x, ?(y) otherwise.</formula><p>Under this new definition, the action gap of the optimal policy is 1??/2 &gt; Q * (x 1 , a 2 ) ? Q * (x 1 , a 1 ). Unfortunately, (4) does not visibly yield a useful operator on Q. As a practical approximation we now propose the consistent Bellman operator, which preserves a local form of stationarity:</p><formula xml:id="formula_9">T C Q(x, a) := R(x, a) + (5) ? E P I [x =x ] max b?A Q(x , b) + I [x=x ] Q(x, a) .</formula><p>Effectively, our operator redefines the meaning of Q-values: if from state x ? X an action a is taken and the next state is x = x then a is again taken. In our example, this new Q-value describes the expected return for repeatedly eating cake until a transition to the unpleasant state x 2 .</p><p>Since the optimal policy ? * is stationary, we may intuit that iterated application of this new operator also yields ? * . In fact, below we show that the consistent Bellman operator is both optimality-preserving and, in the presence of direct loops in the corresponding transition graph, gap-increasing: Definition 1. An operator T is optimality-preserving if, for any Q 0 ? Q and x ? X , letting Q k+1 := T Q k ,</p><formula xml:id="formula_10">V (x) := lim k?? max a?A Q k (x, a)</formula><p>exists, is unique,? (x) = V * (x), and for all a ? A,</p><formula xml:id="formula_11">Q * (x, a) &lt; V * (x, a) =? lim sup k?? Q k (x, a) &lt; V * (x).</formula><p>Thus under an optimality-preserving operator at least one optimal action remains optimal, and suboptimal actions remain suboptimal.</p><formula xml:id="formula_12">Definition 2. Let M be an MDP. An operator T for M is gap-increasing if for all Q 0 ? Q, x ? X , a ? A, letting Q k+1 := T Q k and V k (x) := max b Q k (x, b), lim inf k?? V k (x) ? Q k (x, a) ? V * (x) ? Q * (x, a). (6)</formula><p>We are particularly interested in operators which are strictly gap-increasing, in the sense that (6) is a strict inequality for at least one (x, a) pair.</p><p>Our two-state MDP illustrates the first benefit of increasing the action gap: a greater robustness to estimation error. Indeed, under our new operator the optimal Q-value of eating cake becomes</p><formula xml:id="formula_13">Q(x 1 , a 1 ) = ? 2Q (x 1 , a 1 ) ? = ? 1 ? ?/2 ,</formula><p>which is, again, smaller than Q * (x 1 , a 1 ) whenever ? &gt; 0. In the presence of approximation error in the Q-values, we may thus expectQ(x 1 , a 1 ) &lt;Q(x 1 , a 2 ) to occur more frequently than the converse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation Methods</head><p>At first glance, the use of an indicator function in (5) may seem limiting: P (x | x, a) may be zero or close to zero everywhere, or the state may be described by features which preclude a meaningful identity test I <ref type="bibr">[x=x ]</ref> . There is, however, one important family of value functions which have "tabular-like" properties: aggregation schemes (Bertsekas 2011). As we now show, the consistent Bellman operator is well-defined for all aggregation schemes.</p><formula xml:id="formula_14">An aggregation scheme for M is a tuple (Z, A, D) where Z is a set of aggregate states, A is a mapping from X to distributions over Z, and D is a mapping from Z to distri- butions over X . For z ? Z, x ? X let E D := E x?D(? | z) and E A := E z ?A(? | x )</formula><p>, where as before we assign specific roles to x, x ? X and z, z ? Z. We define the aggregation</p><formula xml:id="formula_15">Bellman operator T A : Q Z,A ? Q Z,A as TAQ(z, a) := ED R(x, a) + ? EP EA max b?A Q(z , b) . (7)</formula><p>When Z is a finite subset of X and D corresponds to the identity transition function, i.e. D(x | z) = I <ref type="bibr">[x=z]</ref> , we recover the class of averagers <ref type="bibr" target="#b11">(Gordon 1995;</ref><ref type="bibr">e.g., multilinear</ref> interpolation, illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>) and kernel-based methods <ref type="bibr" target="#b17">(Ormoneit and Sen 2002)</ref>. If A also corresponds to the identity and X is finite, T A reduces to the Bellman operator (1) and we recover the familiar tabular representation (Sutton and Barto 1998).</p><p>Generalizing <ref type="formula">(5)</ref>, we define the consistent Bellman operator T C over Q Z,A :</p><formula xml:id="formula_16">T C Q(z, a) := E D R(x, a) + (8) ? E P E A I [z =z ] max b?A Q(z , b) + I [z=z ] Q(z, a) .</formula><p>Intuitively (see, e.g., Bertsekas 2011), the D and A mappings induce a new MDP, M := (Z, A, P , R , ?) with</p><formula xml:id="formula_17">R (z, a) := E D R(x, a), P (z | z, a) := E D E P E A I [z =z ] .</formula><p>In this light, we see that our original definition of T C and (8) only differ in their interpretation of the transition kernel. Thus the consistent Bellman operator remains relevant in cases where P is a deterministic transition kernel, for example when applying multilinear or barycentric interpolation to continuous space MDPs (e.g. Munos and Moore 1998).</p><formula xml:id="formula_18">{ { ? Figure 2: Multilinear interpolation in two dimensions. The value at x is approximated as V (x) := E z ?A(? | x) V (z ). Here A(z 1 | x) = (1 ? ?)(1 ? ?), A(z 2 | x) = ?(1 ? ?), etc.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q-Value Interpolation</head><p>Aggregation schemes as defined above do not immediately yield a Q-function over X . Indeed, the Q-value at an arbitrary x ? X is defined (in the ordinary Bellman operator sense) as</p><formula xml:id="formula_19">Q(x, a) := R(x, a) + ? E P E A max b?A Q(z , b), (9)</formula><p>which may only be computed from a full or partial model of the MDP, or by inverting D. It is often the case that neither is feasible. One solution is instead to perform Q-value interpolation:</p><formula xml:id="formula_20">Q(x, a) := E z ?A(? | x) Q(z , a),</formula><p>which is reasonable when A(? | x) are interpolation coefficients 1 . This gives the related Bellman operator</p><formula xml:id="formula_21">T QVI Q(z, a) := E D R(x, a) + ? E P max b?A Q(x , b) ,</formula><p>with T QVI Q(z, a) ? T Q(z, a) by convexity of the max operation. From here one may be tempted to define the corresponding consistent operator as</p><formula xml:id="formula_22">T QVI Q(z, a) := E D R(x, a) + ? E P max b?A Q(x , b) ? A(z | x ) Q(z, b) ? Q(z, a) .</formula><p>While T QVI remains a contraction, T QVI Q(z, a) ? T QVI Q(z, a) is not guaranteed, and it is easy to show that T QVI is not optimality-preserving. Instead we define the consistent Q-value interpolation Bellman operator as</p><formula xml:id="formula_23">T CQVI Q := min T QVI Q, T QVI Q .<label>(10)</label></formula><p>As a corollary to Theorem 1 below we will prove that T CQVI is also optimality-preserving and gap-increasing. <ref type="bibr">1</ref> One then typically, but not always, takes D to be the identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on the Bicycle Domain</head><p>We now study the behaviour of our new operators on the bicycle domain <ref type="bibr" target="#b18">(Randlov and Alstrom 1998)</ref>. In this domain, the agent must simultaneously balance a simulated bicycle and drive it to a goal 1km north of its initial position. Each time step consists of a hundredth of a second, with a successful episode typically lasting 50,000 or more steps. The driving aspect of this problem is particularly challenging for value-based methods, since each step contributes little to an eventual success and the "curse of dimensionality" <ref type="bibr" target="#b4">(Bellman 1957</ref>) precludes a fine representation of the state-space. In this setting our consistent operator provides significantly improved performance and stability. We approximated value functions using multilinear interpolation on a uniform 10?? ? ??10 grid over a 6-dimensional feature vector ? := (?,?, ?,?, ?, d). The first four components of ? describe relevant angles and angular velocities, while ? and d are polar coordinates describing the bicycle's position relative to the goal. We approximated Q-functions using Q-value interpolation (T CQVI ) over this grid, since in a typical setting we may not have access to a forward model.</p><p>We are interested here in the quality of the value functions produced by different operators. We thus computed our Q-functions using value iteration, rather than a trajectorybased method such as Q-Learning. More precisely, at each iteration we simultaneously apply our operator to all grid points, with expected next state values estimated from samples. The interested reader may find full experimental details and videos in the appendix. <ref type="bibr">2</ref> While the limiting value functions (? and V * ) coincide on Z ? X (by the optimality-preserving property), they may differ significantly elsewhere. For x ? X we hav?</p><formula xml:id="formula_24">V (x) = max aQ (x, a) = max a E A(? | x)Q (z, a) = V * (x)</formula><p>in general. This is especially relevant in the relatively highdimensional bicycle domain, where a fine discretization of the state space is not practical and most of the trajectories take place "far" from grid points. As an example, consider ?, the relative angle to the goal: each grid cell covers an arc of 2?/10 = ?/5, while a single time step typically changes ? by less than ?/1000. <ref type="figure" target="#fig_0">Figure 3</ref> summarizes our results. Policies derived from our consistent operator can safely balance the bicycle earlier on, and also reach the goal earlier than policies derived from the Bellman operator. Note, in particular, the striking difference in the trajectories followed by the resulting policies. The effect is even more pronounced when using a 8 ? ? ? ? ? 8 grid (results provided in the appendix). Effectively, by decreasing suboptimal Q-values at grid points we produce much better policies within the grid cells. This phenomenon is consistent with the theoretical results of Farahmand (2011) relating the size of action gaps to the quality of derived greedy policies. Thus we find a second benefit to increasing the action gap: it improves policies derived from Q-value interpolation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Family of Convergent Operators</head><p>One may ask whether it is possible to extend the consistent Bellman operator to Q-value approximation schemes which lack a probabilistic interpretation, such as linear approximation <ref type="bibr" target="#b24">(Sutton 1996)</ref>, locally weighted regression <ref type="bibr" target="#b0">(Atkeson 1991)</ref>, neural networks <ref type="bibr" target="#b25">(Tesauro 1995)</ref>, or even informationtheoretic methods . In this section we answer by the affirmative.</p><p>The family of operators which we describe here are applicable to arbitrary Q-value approximation schemes. While these operators are in general no longer contractions, they are gap-increasing, and optimality-preserving when the Qfunction is represented exactly. Theorem 1 is our main result; one corollary is a convergence proof for Baird's advantage learning <ref type="bibr" target="#b2">(Baird 1999)</ref>. Incidentally, our taking the minimum in (10) was in fact no accident, but rather a simple application of this theorem. Theorem 1. Let T be the Bellman operator defined by (1). Let T be an operator with the property that there exists an ? ? [0, 1) such that for all Q ? Q, x ? X , a ? A, and letting V (</p><formula xml:id="formula_25">x) := max b Q(x, b), 1. T Q(x, a) ? T Q(x, a), and 2. T Q(x, a) ? T Q(x, a) ? ? [V (x) ? Q(x, a)].</formula><p>Then T is both optimality-preserving and gap-increasing.</p><p>Thus any operator which satisfies the conditions of Theorem 1 will eventually yield an optimal greedy policy, assuming an exact representation of the Q-function. Condition 2, in particular, states that we may subtract up to (but not in- <ref type="figure">a)</ref> at each iteration. This is exactly the action gap at (x, a), but for Q k , rather than the optimal Q * . For a particular x, this implies we may initially devalue the optimal action a * := ? * (x) in favour of the greedy action. But our theorem shows that a * cannot be undervalued infinitely often, and in fact Q k (x, a * ) must ultimately reach V * (x). 3 The proof of this perhaps surprising result may be found in the appendix.</p><formula xml:id="formula_26">cluding) max b Q k (x, b) ? Q k (x, a) from Q k (x,</formula><p>To the best of our knowledge, Theorem 1 is the first result to show the convergence of iterates of dynamic programming-like operators without resorting to a contraction argument. Indeed, the conditions of Theorem 1 are particularly weak: we do not require T to be a contraction, nor do we assume the existence of a fixed point (in the Qfunction space Q) of T . In fact, the conditions laid out in Theorem 1 characterize the set of optimality-preserving operators on Q, in the following sense:</p><p>Remark 1. There exists a single-state MDP M and an operator T with either</p><formula xml:id="formula_27">1. T Q(x, a) &gt; T Q(x, a) or 2. T Q(x, a) &lt; T Q(x, a) ? [V (x) ? Q(x, a)],</formula><p>and in both cases there exists a Q 0 ? Q for which lim k?? max a (T ) k Q 0 (x, a) = V * (x).</p><p>We note that the above remark does not cover the case where condition <ref type="formula">(2)</ref> is an equality (i.e., ? = 1). We leave as an open problem the existence of a divergent example for ? = 1.</p><p>Corollary 1. The consistent Bellman operator T C (8) and consistent Q-value interpolation Bellman operator T CQVI (10) are optimality-preserving.</p><p>In fact, it is not hard to show that the consistent Bellman operator <ref type="formula">(7)</ref> is a contraction, and thus enjoys even stronger convergence guarantees than those provided by Theorem 1. Informally, whenever Condition 2 of the theorem is strengthened to an inequality, we may also expect our operators to be gap-increasing; this is in fact the case for both of our consistent operators.</p><p>To conclude this section, we describe a few operators which satisfy the conditions of Theorem 1, and are thus optimality-preserving and gap-increasing. Critically, none of these operators are contractions; one of them, the "lazy" operator, also possesses multiple fixed points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baird's Advantage Learning</head><p>The method of advantage learning was proposed by <ref type="bibr" target="#b2">Baird (1999)</ref> as a means of increasing the gap between the optimal and suboptimal actions in the context of residual algorithms applied to continuous time problems. <ref type="bibr">4</ref> The corresponding operator is</p><formula xml:id="formula_28">T Q(x, a) = K ?1 R(x, a) + ? ?t E P V (x ) + (K ? 1)V (x) ,</formula><p>where ? t &gt; 0 is a time constant and K := C? t with C &gt; 0. Taking ? t = 1 and ? := 1 ? K, we define a new operator with the same fixed point but a now-familiar form:</p><formula xml:id="formula_29">T AL Q(x, a) := T Q(x, a) ? ? [V (x) ? Q(x, a)] .</formula><p>Note that, while the two operators are motivated by the same principle and share the same fixed point, they are not isomorphic. We believe our version to be more stable in practice, as it avoids the multiplication by the K ?1 term. Corollary 2. For ? ? [0, 1), the advantage learning operator T AL has a unique limit V AL ? V, and V AL = V * .</p><p>While our consistent Bellman operator originates from different principles, there is in fact a close relationship between it and the advantage learning operator. Indeed, we can rewrite (5) as</p><formula xml:id="formula_30">T C Q(x, a) = T Q(x, a) ? ?P (x | x, a) [V (x) ? Q(x, a)] ,</formula><p>which corresponds to advantage learning with a (x, a)dependent ? parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persistent Advantage Learning</head><p>In domains with a high temporal resolution, it may be advantageous to encourage greedy policies which infrequently switch between actions -to encourage a form of persistence. We define an operator which favours repeated actions:</p><p>TPALQ(x, a) := max TALQ(x, a), R(x, a) + ? EP Q(x , a) .</p><p>Note that the second term of the max can also be written as</p><formula xml:id="formula_31">T Q(x, a) ? ? E P [V (x ) ? Q(x , a)] .</formula><p>As we shall see below, persistent advantage learning achieves excellent performance on Atari 2600 games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Lazy Operator</head><p>As a curiosity, consider the following operator with ? ? [0, 1):</p><formula xml:id="formula_32">T Q(x, a) := ? ? ? ? ? Q(x, a) if Q(x, a) ? T Q(x, a) and T Q(x, a) ? ?V (x) + (1 ? ?)Q(x, a), T Q(x, a) otherwise.</formula><p>This ?-lazy operator only updates Q-values when this would affect the greedy policy. And yet, Theorem 1 applies! Hence T is optimality-preserving and gap-increasing, even though it may possess a multitude of fixed points in Q. Of note, while Theorem 1 does not apply to the 1-lazy operator, the latter is also optimality-preserving; in this case, however, we are only guaranteed that one optimal action remain optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results on Atari 2600</head><p>We evaluated our new operators on the Arcade Learning Environment (ALE; <ref type="bibr" target="#b3">Bellemare et al. 2013</ref>), a reinforcement learning interface to Atari 2600 games. In the ALE, a frame lasts 1/60 th of a second, with actions typically selected every four frames. Intuitively, the ALE setting is related to continuous domains such as the bicycle domain studied above, in the sense that each individual action has little effect on the game.</p><p>For our evaluation, we trained agents based on the Deep Q-Network (DQN) architecture of <ref type="bibr" target="#b15">Mnih et al. (2015)</ref>. DQN acts according to an -greedy policy over a learned neuralnetwork Q-function. DQN uses an experience replay mechanism to train this Q-function, performing gradient descent on the sample squared error ? Q (x, a) 2 , where</p><formula xml:id="formula_33">?Q(x, a) := R(x, a) + ?V (x ) ? Q(x, a),</formula><p>where (x, a, x ) is a previously observed transition. We define the corresponding errors for our operators as</p><formula xml:id="formula_34">? AL Q(x, a) := ?Q(x, a) ? ?[V (x) ? Q(x, a)], ? PAL Q(x, a) := max ? AL Q(x, a), ?Q(x, a) ? ?[V (x ) ? Q(x , a)] ,</formula><p>where we further parametrized the weight given to Q(x , a) in persistent advantage learning (compare with T PAL ).</p><p>Our first experiment used one of the new ALE standard versions, which we call here the Stochastic Minimal setting. This setting includes stochasticity applied to the Atari 2600 controls, no death information, and a per-game minimal action set. Specifically, at each frame (not time step) the environment accepts the agent's action with probability 1 ? p, or rejects it with probability p (here, p = 0.25). If an action is rejected, the previous frame's action is repeated. In our setting the agent selects a new action every four frames: the stochastic controls therefore approximate a form of reaction delay. As evidenced by a lower DQN performance, Stochastic Minimal is more challenging than previous settings.</p><p>We trained each agent for 100 million frames using either regular Bellman updates, advantage learning (A.L.), or persistent advantage learning (P.A.L.). We optimized the ? parameters over 5 training games and tested our algorithms on 55 more games using 10 independent trials each.</p><p>For each game, we performed a paired t-test (99% C.I.) on the post-training evaluation scores obtained by our algorithms and DQN. A.L. and P.A.L. are statistically better than DQN on 37 and 35 out of 60 games, respectively; both perform worse on one (ATLANTIS, JAMES BOND). P.A.L. often achieves higher scores than A.L., and is statistically better on 16 games and worse on 6. These results are especially remarkable given that the only difference between DQN and our operators is a simple modification to the update rule.</p><p>For comparison, we also trained agents using the Original DQN setting <ref type="bibr" target="#b15">(Mnih et al. 2015)</ref>, in particular using a longer 200 million frames of training. <ref type="figure" target="#fig_1">Figure 4</ref> depicts learning curves for two games, ASTERIX and SPACE INVADERS. These curves are representative of our results, rather than exceptional: on most games, advantage learning outperforms Bellman updates, and persistent advantage learning further improves on this result. Across games, the median score improvement over DQN is 8.4% for A.L. and 9.1% for P.A.L., while the average score improvement is respectively 27.0% and 32.5%. Full experimental details are provided in the appendix.</p><p>The learning curve for ASTERIX illustrates the poor performance of DQN on certain games. Recently, <ref type="bibr" target="#b26">van Hasselt, Guez, and Silver (2016)</ref> argued that this poor performance  stems from the instability of the Q-functions learned from Bellman updates, and provided conclusive empirical evidence to this effect. In the spirit of their work, we compared our learned Q-functions on a single trajectory generated by a trained DQN agent playing SPACE INVADERS in the Original DQN setting. For each Q-function and each state x along the trajectory, we computed V (x) as well as the action gap at x.</p><p>The value functions and action gaps resulting from this experiment 5 are depicted in <ref type="figure" target="#fig_2">Figure 5</ref>. As expected, the action gaps are significantly greater for both of our operators, in comparison to the action gaps produced by DQN. Furthermore, the value estimates are themselves lower, and correspond to more realistic estimates of the true value function. In their experiments, van Hasselt et al. observed a similar effect on the value estimates when replacing the Bellman updates with Double Q-Learning updates, one of many solutions recently proposed to mitigate the negative impact of statistical bias in value function estimation <ref type="bibr" target="#b27">(van Hasselt 2010;</ref><ref type="bibr" target="#b1">Azar et al. 2011;</ref><ref type="bibr" target="#b14">Lee, Defourny, and Powell 2013)</ref>. This bias is positive and is a consequence of the max term in the Bellman operator. We hypothesize that the lower value estimates observed in <ref type="figure" target="#fig_2">Figure 5</ref> are also a consequence of bias reduction. Specifically, increased action gaps are consistent with a bias reduction: it is easily shown that the value estimation bias is strongest when Q-values are close to each other. If our hypothesis holds true, the third benefit of increasing the action gap is thus to mitigate the statistical bias of Q-value estimates. 5 Videos: https://youtu.be/wDfUnMY3vF8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open Questions</head><p>Weaker Conditions for Optimality. At the core of our results lies the redefinition of Q-values in order to facilitate approximate value estimation. Theorem 1 and our empirical results indicate that there are many practical operators which do not preserve suboptimal Q-values. Naturally, preserving the optimal value function V is itself unnecessary, as long as the iterates converge to a Q-functionQ for which arg max aQ (x, a) = ? * (x). It may well be that even weaker conditions for optimality exist than those required by Theorem 1. At the present, however, our proof technique does not appear to extend to this case. Statistical Efficiency of New Operators. Advantage learning (as given by our redefinition) may be viewed as a generalization of the consistent Bellman operator when P (? | x, a) is unknown or irrelevant. In this light, we ask: is there a probabilistic interpretation to advantage learning? We further wonder about the statistical efficiency of the consistent Bellman operator: is it ever less efficient than the usual Bellman operator, when considering the probability of misclassifying the optimal action? Both of these answers might shed some light on the differences in performance observed in our experiments. Maximally Efficient Operator. Having revealed the existence of a broad family of optimality-preserving operators, we may now wonder which of these operators, if any, should be preferred to the Bellman operator. Clearly, there are trivial MDPs on which any optimality-preserving operator performs equally well. However, we may ask whether there is, for a given MDP, a "maximally efficient" optimalitypreserving operator; and whether a learning agent can benefit from simultaneously searching for this operator while estimating a value function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concluding Remarks</head><p>We presented in this paper a family of optimality-preserving operators, of which the consistent Bellman operator is a distinguished member. At the center of our pursuits lay the desire to increase the action gap; we showed through experiments that this gap plays a central role in the performance of greedy policies over approximate value functions, and how significantly increased performance could be obtained by a simple modification of the Bellman operator. We believe our work highlights the inadequacy of the classical Q-function at producing reliable policies in practice, calls into question the traditional policy-value relationship in value-based reinforcement learning, and illustrates how revisiting the concept of value itself can be fruitful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Results</head><p>Lemma 1. Let Q ? Q and ? Q be the policy greedy with respect to Q. Let T be an operator with the properties that, for all x ? X , a ? A, 1. T Q(x, a) ? T Q(x, a), and 2. T Q(x, ? Q (x)) = T Q(x, ? Q (x)). Consider the sequence Q k+1 := T Q k with Q 0 ? Q, and let V k (x) := max a Q k (x, a). Then the sequence (V k : k ? N) converges, and furthermore, for all x ? X , lim</p><formula xml:id="formula_35">k?? V k (x) ? V * (x).</formula><p>Proof. By Condition 1, we have that</p><formula xml:id="formula_36">lim sup k?? Q k (x, a) = lim sup k?? (T ) k Q 0 (x, a) ? lim sup k?? T k Q 0 (x, a) = Q * (x, a),</formula><p>since T has a unique fixed point. From this we deduce the second claim. Now, for a given x ? X , let a k := ? k (x) := arg max a Q k (x, a) and P k := P (? | x, a k ). We have</p><formula xml:id="formula_37">V k+1 (x) ? Q k+1 (x, a k ) = T Q k (x, a k ) = T Q k (x, a k ) = T Q k?1 (x, a k ) + ? E P k [V k (x ) ? V k?1 (x )] ? T Q k?1 (x, a k ) + ? E P k [V k (x ) ? V k?1 (x )] = V k (x) + ? E P k [V k (x ) ? V k?1 (x )] ,</formula><p>where in the second line we used Condition 2 of the lemma, and in the third the definition of T applied to Q k . Thus we have</p><formula xml:id="formula_38">V k+1 (x) ? V k (x) ? ? E P k [V k (x ) ? V k?1 (x )] ,</formula><p>and by induction</p><formula xml:id="formula_39">V k+1 (x) ? V k (x) ? ? k E P 1:k [V 1 (x ) ? V 0 (x )] ,<label>(11)</label></formula><p>where P 1:k := P k P k?1 . . . P 1 is the k-step transition kernel at x derived from the nonstationary policy ? k ? k?1 . . . ? 1 .</p><p>Let? (x) := lim sup k?? V k (x). We now show that lim inf k?? V k (x) =? (x) also. First note that Conditions 1 and 2, together with the boundedness of V 0 , ensure that V 1 is also bounded and thus V 1 ? V 0 ? &lt; ?. By definition, for any ? &gt; 0 and n ? N, ?k ? n such that V k (x) &gt;? (x) ? ?.</p><p>Since P 1:k is a nonexpansion in ?-norm, we have</p><formula xml:id="formula_40">V k+1 (x) ? V k (x) ? ?? k V 1 ? V 0 ? ? ?? n V 1 ? V 0 ? =: ? ,</formula><p>and for all t ? N,</p><formula xml:id="formula_41">V k+t (x) ? V k (x) ? ? t?1 i=0 ? i ? ? 1 ? ? , such that inf t?N V k+t (x) ?? (x) ? ? ? 1 ? ? .</formula><p>It follows that for any x ? X and ? &gt; 0, we can choose an n ? N to make small enough such that for all k ? n,</p><formula xml:id="formula_42">V k (x) &gt;? (x) ? ? . Hence lim inf k?? V k (x) =? (x),</formula><p>and thus V k (x) converges.</p><p>Lemma 2. Let T be an operator satisfying the conditions of Lemma 1, and let R ? := max x,a R(x, a). Then for all</p><p>x ? X and all k ? N,</p><formula xml:id="formula_43">|V k (x)| ? 1 1 ? ? 2 V 0 ? + R ? .<label>(12)</label></formula><p>Proof. Following the derivation of Lemma 1, we have</p><formula xml:id="formula_44">V k+1 (x) ? V 0 (x) ? ? k i=1 ? i V 1 ? V 0 ? ? ?1 1 ? ? V 1 ? V 0 ? .</formula><p>By the same derivation, for a 0 := arg max a Q 0 (x, a) we have V 1 (x) ? T Q 0 (x, a 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>But then</head><formula xml:id="formula_45">V 1 (x) ? V 0 (x) ? R(x, a 0 ) + ? E P0 V 0 (x ) ? V 0 (x),</formula><p>from which the lower bound follows. Now let P k be defined as in the proof of Lemma 1, and assume the upper bound of (12) holds up to k ? N. Then</p><formula xml:id="formula_46">V k+1 (x) = max a Q k+1 (x, a) = max a T Q k (x, a) ? max a T Q k (x, a) = max a R(x, a) + ? E P k V k (x ) ? R ? + ? V k ? ? R ? + ? 1 ? ? [2 V 0 ? + R ? ] ? 1 1 ? ? [2 V 0 ? + R ? ] ,</formula><p>and combined with the fact that (12) holds for k = 0 this proves the upper bound. Consider the sequence Q k+1 := T Q k with Q 0 ? Q, and V k (x) := max a Q k (x, a). Then T is optimality-preserving:</p><formula xml:id="formula_47">for all x ? X , (V k (x) : k ? N) converges, lim k?? V k (x) = V * (x), and Q * (x, a) &lt; V * (x) =? lim sup k?? Q k (x, a) &lt; V * (x).</formula><p>Furthermore, T is also gap-increasing:</p><formula xml:id="formula_48">lim inf k?? V k (x) ? Q k (x, a) ? V * (x) ? Q * (x, a).</formula><p>Proof. Note that these conditions imply the conditions of Lemma 1. Thus for all x ? X , (V k (x) : k ? N) converges to the limit? (x) ? V * (x). Now letQ(x, a) := lim sup k Q k (x, a). We hav?</p><formula xml:id="formula_49">Q(x, a) = lim sup k?? T Q k (x, a) ? lim sup k?? T Q k (x, a) = lim sup k?? R(x, a) + ? E P max b?A Q k (x , b) ? R(x, a) + ? E P lim sup k?? max b?A Q k (x , b) (13) = R(x, a) + ? E P max b lim sup k?? Q k (x , b) (14) = TQ(x, a),<label>(15)</label></formula><p>where in (13) we used Jensen's inequality, and <ref type="formula">(14)</ref> follows from the commutativity of max and lim sup. Now</p><formula xml:id="formula_50">Q k+1 (x, a) = T Q k (x, a) ? T Q k (x, a) ? ? [V k (x) ? Q k (x, a)] = R(x, a) + ? E P V k (x ) ? ?V k (x) + ?Q k (x, a).<label>(16)</label></formula><p>Now, by Lemma 1 V k (x) converges to? (x). Furthermore, using Lemma 2 and Lebesgue's dominated convergence theorem, we have</p><formula xml:id="formula_51">lim k?? E P V k (x ) = E P? (x ).<label>(17)</label></formula><p>We now take the lim sup of both sides of (16), which Lemma 2 guarantees exists, and obtai?</p><formula xml:id="formula_52">Q(x, a) ? R(x, a) + ? E P? (x ) ? ?? (x) + ?Q(x, a) = TQ(x, a) ? ?? (x) + ?Q(x, a). ThusQ (x, a) ? 1 1 ? ? TQ(x, a) ? ?? (x) , and V (x) ? 1 1 ? ? max a?A TQ(x, a) ? ?? (x) ? (x) ? max a?A TQ(x, a).</formula><p>Combining the above with (15), we deduce that</p><formula xml:id="formula_53">V (x) = max a?A TQ(x, a) = max a?A R(x, a) + ? E P? (x )</formula><p>and, by uniqueness of the fixed point of the Bellman operator over V, it must be that? = V * . Now suppose that for some x ? X ,? ? A, we have</p><formula xml:id="formula_54">Q * (x,?) &lt; V * (x).</formula><p>By Condition 1</p><formula xml:id="formula_55">Q k (x,?) = T Q k?1 (x,?) ? T Q k?1 (x,?) = T Q * (x,?) ? ? E P? [V * (x ) ? V k?1 (x )] = Q * (x,?) ? ? E P? [V * (x ) ? V k?1 (x )] ,</formula><p>where P? := P (? | x,?). Using <ref type="formula" target="#formula_51">(17)</ref> we take the lim sup on both sides and find that</p><formula xml:id="formula_56">lim sup k?? Q k (x,?) ? Q * (x,?) ? ? E P? V * (x ) ?? (x ) = Q * (x,?) &lt; V * (x).</formula><p>We conclude that</p><formula xml:id="formula_57">Q * (x, a) &lt; V * (x) =? lim sup k?? Q k (x, a) &lt; V * (x).</formula><p>Hence, T is optimality-preserving. To prove that T is gapincreasing, observe that the statement</p><formula xml:id="formula_58">lim inf k?? V k (x) ? Q k (x, a) ? V * (x) ? Q * (x, a)</formula><p>is now equivalent to</p><formula xml:id="formula_59">lim sup k?? Q k (x, a) ? Q * (x, a)<label>(18)</label></formula><p>since lim k V k (x) = V * (x). But we know (18) to be true from Condition 1 (see the proof of Lemma 1).</p><p>Corollary 3. The consistent Bellman operator T C ((5) in the main text) and consistent Q-value interpolation Bellman operator T CQVI ((9) in the main text) are optimality-preserving and gap-increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Details: Bicycle</head><p>We used the bicycle simulator described by <ref type="bibr" target="#b18">Randlov and Alstrom (1998)</ref>  if goal reached (? 2 /4 ? ? 2 ? 1) ? 0.001 otherwise with c := ( 3 4 ? 2 ? 1) ? 0.001 the largest negative reward achievable by the agent. Empirically, we found this reward function easier to work with, while our results remained qualitatively similar for similar reward functions. We further use a discount factor of ? = 0.99.</p><p>We consider two sample-based operators on Q Z,A , the space of Q-functions over representative states. The samplebased Q-value interpolation Bellman operator is defined as</p><formula xml:id="formula_60">T QVI Q(z, a) := R(z, a) + ? 1 k k i=1 max b?A Q(x i , b),</formula><p>with k ? N and x i ? P (? | z, a). The sample-based consistent Q-value interpolation Bellman operator T CQVI is similarly defined by sampling x from P :</p><formula xml:id="formula_61">T QVI Q(z, a) := R(z, a)+ ? k k i=1 max b?A Q(x , b) ? A(z | x ) (Q(z, b) ? Q(z, a))</formula><p>T CQVI Q(z, a) := min T QVI Q(z, a), T QVI Q(z, a) .</p><p>In both cases, we use Q-value interpolation to define a Qfunction over X :</p><p>Q(x, a) := E z?A(? | x) Q(z, a).</p><p>For each operator T , we computed a sequence of Qfunctions Q k ? Q Z,A using an averaging form of value iteration:</p><formula xml:id="formula_62">Q k+1 (z, a) = (1 ? ?)Q k (z, a) + ?T Q k (z, a),</formula><p>applied simultaneously to all z ? Z and a ? A. We chose this averaging version because it led to faster convergence, and lets us take k = 1 in the definition of both operators. From a parameter sweep we found ? = 0.1 to be a suitable step-size. Our multilinear grid was defined over the six state variables. As done elsewhere in the literature, we defined our grid over the following bounded variables:</p><formula xml:id="formula_63">? ? ? 4 9 ?, 4 9 ? , ? ? [?2, 2], ? ? ? ? 15 , ? 15 , ? ? [?0.5, 0.5] , ? ? [??, ?], d ? [10, 1200].</formula><p>Values outside of these ranges were accordingly set to the range's minimum or maximum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bellman operator</head><p>A.L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fraction of Episodes Ending In Fall</head><p>Iterations 8-grid</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10-grid</head><p>Persistent A.L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistent operator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterations</head><p>Fraction of Episodes Ending At Goal <ref type="figure">Figure 6</ref>: Top. Falling and goal-reaching frequency for greedy policies derived from value iteration on a 8 ? ? ? ? ? 8 grid. Bottom. The same, for a 10 ? ? ? ? ? 10 grid.</p><p>For completeness, <ref type="figure">Figure 6</ref> compares the performance of the Bellman and consistent Bellman operators, as well as advantage learning and persistent advantage learning (with ? = 0.1), on 8 ? ? ? ? ? 8 and 10 ? ? ? ? ? 10 grids. Here, the usual Bellman operator is unable to find a solution to the goal, while the consistent Bellman operator successfully does so. The two other operators also achieve superior performance compared to Bellman operator, although appear slightly more unstable in the smaller grid setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Details: ALE</head><p>We omit details of the DQN architecture, which are provided in <ref type="bibr" target="#b15">Mnih et al. (2015)</ref>. A frame is a single emulation step within the ALE, while a time step consists of four consecutive frames which are treated atomically by the agent.</p><p>Our first Atari 2600 experiment (Stochastic Minimal setting) used stochastic controls, which operate as follows: at each frame (not time step), the environment accepts the agent's action with probability 1?p, or rejects it with probability p. If an action is rejected, the previous frame's action is repeated. In our setting, the agent selects a new action every four frames; in this situation, the stochastic controls approximate a form of reaction delay. This particular setting is part of the latest Arcade Learning Environment. For our experiments we use the ALE 0.5 standard value of p = 0.25, and trained agents for 100 million frames.</p><p>Our second Atari 2600 experiment (Original DQN setting) was averaged over three different trials, ran for 200 million frames (instead of 100 million), defined a lost life as a termination signal, and did not use stochastic controls. This matches the experimental setting of <ref type="bibr" target="#b15">Mnih et al. (2015)</ref>. A full table of our results is provided in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Our last experiment took place in the Original DQN setting. We generated a trajectory from a trained DQN agent playing an -greedy policy with = 0.05. The full trajectory (up to the end of the episode) was recorded in this way. Games with a ? were not used by <ref type="bibr" target="#b3">Bellemare et al. (2013)</ref>. See Section 4 of the main text for more details.</p><p>We then queried the value functions of the trained agents, including the DQN used to generate the trajectory, in order to generate <ref type="figure" target="#fig_1">Figure 4</ref> of the main text. For clarity we report action gaps averaged according to a rolling window of length 50. Out of the 60 games for which we report results, 5 are new when compared to the table of results provided by <ref type="bibr" target="#b3">Bellemare et al. (2013)</ref>. These five games are identified with a ? in <ref type="table" target="#tab_0">Table  1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DQN Implementation Details</head><p>Recall that DQN maintains two networks in parallel: a policy network, which is used to select actions and is updated at every time step, and a target network. The target network is used to compute the error term ?Q, and is only updated every 10,000 time steps <ref type="bibr" target="#b15">(Mnih et al. 2015)</ref>. In our experiments we also used this target network to compute the ? AL Q and ? PAL Q, including the added correction term. Our operators performed worse when the correction term was instead computed from the policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Selection</head><p>We used five training games (ASTERIX, BEAM RIDER, PONG, SEAQUEST, SPACE INVADERS) to select the ? parameter for both of our operators. Specifically, we trained agents using our second experimental setup with parameters ? ? {0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0}, evaluated them according to the highest score achieved, and manually selected the ? value which seemed to achieve the best performance. Note that ? = 0.0 corresponds to DQN in both cases. <ref type="figure">Figure  7</ref> depicts the results of this parameter sweep. A.L. <ref type="figure">Figure 7</ref>: Performance of trained agents in function of the ? parameter. Note that ? = 1.0 does not satisfy our theorem's conditions. We attribute the odd performance of Seaquest agents using Persistent Advantage Learning with ? = 0.9 to a statistical issue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Top. Falling and goal-reaching frequency for greedy policies derived from value iteration. Bottom. Sample bicycle trajectories after 100, 200, . . . , 1000 iterations. In this coarse-resolution regime, the Bellman operator initially yields policies which circle the goal forever, while the consistent operator quickly yields successful trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Learning curves for two Atari 2600 games in the Original DQN setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Action gaps (left) and value functions (right) for a single episode of SPACE INVADERS (Original DQN setting). Our operators yield markedly increased action gaps and lower values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 .</head><label>2</label><figDesc>Let T be the Bellman operator ((1) in the main text). Let T be an operator with the property that there exists an ? ? [0, 1) such that for all Q ? Q, x ? X , a ? A, and letting V (x) := max b Q(x, b), 1. T Q(x, a) ? T Q(x, a), and 2. T Q(x, a) ? T Q(x, a) ? ? [V (x) ? Q(x, a)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>with a reward function which encourages driving towards the goal. Recall that Randlov and Alstrom's rereached (4 ? ? 2 ) ? 0.00004 otherwise As noted by Randlov and Alstrom themselves, this reward function is unsuitable for value iteration methods, since it rewards driving away from the goal. Instead we use the following related reward function R(x, a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Game</cell><cell cols="3">Bellman Advantage Learning Persistent A.L.</cell></row><row><cell>ASTERIX</cell><cell>6074.98</cell><cell>12852.08</cell><cell>19564.90</cell></row><row><cell>BEAM RIDER</cell><cell>9316.10</cell><cell>10054.58</cell><cell>13145.34</cell></row><row><cell>PONG</cell><cell>19.80</cell><cell>19.66</cell><cell>19.76</cell></row><row><cell>SEAQUEST</cell><cell>5458.17</cell><cell>8670.50</cell><cell>13230.74</cell></row><row><cell>SPACE INVADERS</cell><cell>2067.19</cell><cell>3460.79</cell><cell>3277.59</cell></row><row><cell>ALIEN</cell><cell>3154.67</cell><cell>4990.91</cell><cell>5699.81</cell></row><row><cell>AMIDAR</cell><cell>969.88</cell><cell>1557.43</cell><cell>1451.65</cell></row><row><cell>ASSAULT</cell><cell>4573.67</cell><cell>3661.51</cell><cell>3304.33</cell></row><row><cell>ASTEROIDS</cell><cell>1827.97</cell><cell>1924.42</cell><cell>1673.52</cell></row><row><cell cols="2">ATLANTIS 636657.62</cell><cell>553591.67</cell><cell>1465250.00</cell></row><row><cell>BANK HEIST</cell><cell>511.00</cell><cell>633.63</cell><cell>874.99</cell></row><row><cell>BATTLE ZONE</cell><cell>28082.91</cell><cell>28789.29</cell><cell>34583.07</cell></row><row><cell>BERZERK</cell><cell>667.61</cell><cell>747.26</cell><cell>1328.25</cell></row><row><cell>BOWLING</cell><cell>74.62</cell><cell>57.41</cell><cell>71.59</cell></row><row><cell>BOXING</cell><cell>88.66</cell><cell>93.94</cell><cell>94.30</cell></row><row><cell>BREAKOUT</cell><cell>378.69</cell><cell>425.32</cell><cell>431.89</cell></row><row><cell>CARNIVAL</cell><cell>5238.14</cell><cell>5111.40</cell><cell>4679.93</cell></row><row><cell>CENTIPEDE</cell><cell>5719.11</cell><cell>4225.18</cell><cell>4539.55</cell></row><row><cell>CHOPPER COMMAND</cell><cell>8195.88</cell><cell>5431.36</cell><cell>5734.93</cell></row><row><cell cols="2">CRAZY CLIMBER 114105.56</cell><cell>123410.71</cell><cell>130002.71</cell></row><row><cell>DEFENDER  ?</cell><cell>16746.68</cell><cell>30643.59</cell><cell>32038.93</cell></row><row><cell>DEMON ATTACK</cell><cell>23212.19</cell><cell>27153.48</cell><cell>70908.17</cell></row><row><cell>DOUBLE DUNK</cell><cell>-6.23</cell><cell>-0.15</cell><cell>-2.51</cell></row><row><cell>ELEVATOR ACTION</cell><cell>26675.00</cell><cell>27088.89</cell><cell>29100.00</cell></row><row><cell>ENDURO</cell><cell>776.14</cell><cell>1252.70</cell><cell>1343.10</cell></row><row><cell>FISHING DERBY</cell><cell>11.65</cell><cell>21.32</cell><cell>28.13</cell></row><row><cell>FREEWAY</cell><cell>31.14</cell><cell>31.72</cell><cell>32.30</cell></row><row><cell>FROSTBITE</cell><cell>1485.42</cell><cell>2305.82</cell><cell>3248.96</cell></row><row><cell>GOPHER</cell><cell>8479.98</cell><cell>11912.68</cell><cell>10611.81</cell></row><row><cell>GRAVITAR</cell><cell>448.74</cell><cell>417.65</cell><cell>446.92</cell></row><row><cell>H.E.R.O.</cell><cell>18490.97</cell><cell>24788.86</cell><cell>24175.79</cell></row><row><cell>ICE HOCKEY</cell><cell>-2.13</cell><cell>-1.24</cell><cell>-0.25</cell></row><row><cell>JAMES BOND</cell><cell>867.84</cell><cell>848.46</cell><cell>772.09</cell></row><row><cell>KANGAROO</cell><cell>9157.98</cell><cell>10809.16</cell><cell>11478.46</cell></row><row><cell>KRULL</cell><cell>8500.48</cell><cell>9548.92</cell><cell>8689.81</cell></row><row><cell>KUNG-FU MASTER</cell><cell>25977.53</cell><cell>32182.99</cell><cell>34650.91</cell></row><row><cell>MONTEZUMA'S REVENGE</cell><cell>0.64</cell><cell>0.42</cell><cell>1.72</cell></row><row><cell>MS. PAC-MAN</cell><cell>3081.29</cell><cell>4065.80</cell><cell>3917.55</cell></row><row><cell>NAME THIS GAME</cell><cell>8585.03</cell><cell>11025.26</cell><cell>10431.33</cell></row><row><cell>PHOENIX  ?</cell><cell>14278.95</cell><cell>22038.27</cell><cell>14495.56</cell></row><row><cell>PITFALL!  ?</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>POOYAN</cell><cell>4736.79</cell><cell>4801.27</cell><cell>5858.84</cell></row><row><cell>PRIVATE EYE</cell><cell>957.83</cell><cell>5276.16</cell><cell>339.15</cell></row><row><cell>Q*BERT</cell><cell>10840.83</cell><cell>14368.03</cell><cell>14254.78</cell></row><row><cell>RIVER RAID</cell><cell>7315.20</cell><cell>10585.12</cell><cell>12813.27</cell></row><row><cell>ROAD RUNNER</cell><cell>38042.07</cell><cell>52351.23</cell><cell>37856.16</cell></row><row><cell>ROBOTANK</cell><cell>61.97</cell><cell>69.31</cell><cell>70.53</cell></row><row><cell cols="2">SKIING -13049.42</cell><cell>-13264.51</cell><cell>-12173.35</cell></row><row><cell>SOLARIS  ?</cell><cell>4638.85</cell><cell>4785.16</cell><cell>3274.70</cell></row><row><cell>STAR GUNNER</cell><cell>55558.27</cell><cell>61353.59</cell><cell>61521.87</cell></row><row><cell>SURROUND</cell><cell>-5.79</cell><cell>-4.15</cell><cell>0.72</cell></row><row><cell>TENNIS</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>TIME PILOT</cell><cell>5788.96</cell><cell>8969.12</cell><cell>8749.26</cell></row><row><cell>TUTANKHAM</cell><cell>200.17</cell><cell>245.22</cell><cell>197.33</cell></row><row><cell>UP AND DOWN</cell><cell>12831.57</cell><cell>13909.74</cell><cell>13542.07</cell></row><row><cell>VENTURE</cell><cell>373.79</cell><cell>198.69</cell><cell>243.75</cell></row><row><cell cols="2">VIDEO PINBALL 611840.72</cell><cell>543504.00</cell><cell>542052.00</cell></row><row><cell>WIZARD OF WOR</cell><cell>2410.47</cell><cell>9541.14</cell><cell>10254.01</cell></row><row><cell>YAR'S REVENGE  ?</cell><cell>21440.45</cell><cell>24240.03</cell><cell>17141.56</cell></row><row><cell>ZAXXON</cell><cell>6416.06</cell><cell>9129.61</cell><cell>8155.60</cell></row><row><cell>Times Best</cell><cell>12</cell><cell>21</cell><cell>31</cell></row></table><note>: Highest performance achieved by each of our operators. For each game, the score of the best operator is highlighted.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Videos: https://youtu.be/0pUFjNuom1A</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">When two or more actions are optimal, we are only guaranteed that one of them will ultimately be correctly valued. The "1-lazy" operator described below exemplifies this possibility.4  Advantage updating, also by Baird, is a popular but different idea where an agent maintains both V and A := Q ? V .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Michael Bowling, Csaba Szepesv?ri, Craig Boutilier, Dale Schuurmans, Marty Zinkevich, Lihong Li, Thomas Degris, and Joseph Modayil for useful discussions, as well as the anonymous reviewers for their excellent feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>This appendix is divided into three sections. In the first section we present the proofs of our theoretical results. In the second we provide experimental details and additional results for the Bicycle domain. In the final section we provide details of our experiments on the Arcade Learning Environment, including results on 60 games.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using locally weighted regression for robot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1991 IEEE International Conference on Robotics and Automation</title>
		<meeting>1991 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="958" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speedy Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reinforcement learning through gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Baird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neuro-Dynamic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Q-learning and enhanced policy iteration in discounted dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="94" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximate policy iteration: A survey and some new methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Control Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="310" to="335" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PILCO: A model-based and data-efficient approach to policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tree-based batch mode reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="503" to="556" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action-gap phenomenon in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farahmand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stable function approximation in dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">System identification of post stall aerodynamics for UAV perching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hoburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AIAA Infotech Aerospace Conference</title>
		<meeting>the AIAA Infotech Aerospace Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal hour ahead bidding in the real time electricity market with battery storage using approximate dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="525" to="543" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Numerical methods for stochastic control problems in continuous time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Defourny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Adaptive Dynamic Programming And Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Biascorrected Q-learning to control max-operator bias in Qlearning</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Humanlevel control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Barycentric interpolators for continuous space &amp; time reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">; R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moore</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 11. Munos</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="291" to="323" />
		</imprint>
	</monogr>
	<note>Variable resolution discretization in optimal control</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ormoneit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="161" to="178" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to drive a bicycle using reinforcement learning and shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Randlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alstrom</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning for robot soccer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="73" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On-line Qlearning using connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pilarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Autonomous Agents and Multiagents Systems</title>
		<meeting>the Tenth International Conference on Autonomous Agents and Multiagents Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization in reinforcement learning: Successful examples using sparse coarse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1038" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal difference learning and TD-Gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karakovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Symposium on Computational Intelligence and Games. to appear</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Double Q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compress and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning From Delayed Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Cambridge, England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Cambridge University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Dissertation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
