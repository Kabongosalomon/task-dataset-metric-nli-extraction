<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Channel Dimensions for Efficient Model Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Channel Dimensions for Efficient Model Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing an efficient model within the limited computational cost is challenging. We argue the accuracy of a lightweight model has been further limited by the design convention: a stage-wise configuration of the channel dimensions, which looks like a piecewise linear function of the network stage. In this paper, we study an effective channel dimension configuration towards better performance than the convention. To this end, we empirically study how to design a single layer properly by analyzing the rank of the output feature. We then investigate the channel configuration of a model by searching network architectures concerning the channel configuration under the computational cost restriction. Based on the investigation, we propose a simple yet effective channel configuration that can be parameterized by the layer index. As a result, our proposed model following the channel parameterization achieves remarkable performance on ImageNet classification and transfer learning tasks including COCO object detection, COCO instance segmentation, and fine-grained classifications. Code and ImageNet pretrained models are available at https: //github.com/clovaai/rexnet. arXiv:2007.00992v3 [cs.CV] 8 Jun 2021 Network Stem / Building blocks' output channel dimensions Top-1 Params Flops MobileNetV2 [47] 32 / 16(</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Designing a lightweight network architecture is crucial for both researchers and practitioners. Popular networks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47]</ref> designed for ImageNet classification share a similar design convention where a low dimensional input channel is expanded by a few channel expansion layers towards surpassing the number of classes. Lightweight models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b50">51]</ref> also follow this configuration but further shrinks some channels for computational efficiency, which leads to the promising tradeoffs between the computational cost and accuracy. In other words, the degree of channel expansion at layers is quite different, where earlier layers have a smaller channel dimension; the penultimate layer that largely expands dimension above the number of classes. This is to realize flopefficiency by narrow channel dimensions at earlier layers; to get model expressiveness with sufficient channel dimension at the final feature (see <ref type="table">Table 1</ref>).</p><p>This channel configuration was firstly introduced by Mo-bileNetV2 <ref type="bibr" target="#b46">[47]</ref> and became the design convention of configuring channel dimensions in lightweight networks, but how to adjust the channel dimensions towards the optimal under the restricted computational cost has not been profoundly studied. As shown in <ref type="table">Table 1</ref>, even network architecture search (NAS)-based models <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> were designed upon the convention or little more exploration within few options near the configuration <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b53">54]</ref> and focused on searching building blocks. Take one step further from the design convention, we hypothesize that the compact models designed by the conventional channel configuration may be limited in the expressive power due to mainly focusing on flop-efficiency; there would exist a more effective configuration over the traditional one.</p><p>In this paper, we investigate an effective channel configuration of a lightweight network with additional accuracy gain. Inspired by the works <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b61">62]</ref>, we conjecture the expressiveness of a layer can be estimated by the matrix rank of the output feature. Technically, we study with the averaged rank computed from the output feature of a bunch of networks that are randomly generated with random sizes to reveal the proper range of expansion ratio at an expansion layer and make a link with a rough design principle. Based on the principle, we move forward to find out an overall channel configuration in a network. Specifically, we search network architectures to identify the channel configuration yielding a better accuracy over the aforementioned convention. It turns out that the best channel configuration is parameterized as a linear function by the block index in a network. This parameterization is similar to the configuration used in the works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>, and we reveal the parameterization is also effective in designing a lightweight model.</p><p>Based on the investigation, we propose a new model upon the searched channel parameterization. It turns out that a simple modification upon MobileNetV2 could show remarkable improvement in performance on ImageNet classification. Only with the new channel configuration, our models outperform the state-of-the-art networks such as Effi- <ref type="table">Table 1</ref>. Channel configurations in lightweight models. We present the output channel dimensions of the stem's 3?3 convolution (e.g., <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32)</ref> and the building blocks with the number of repeated layers (e.g., ?1, ?2, ?3, ?4, ?5) in block index order. All the models after MobileNetV2 have similar channel configurations to that of MobileNetV2, which have repeated channel dimensions for each stage.</p><p>cientNets <ref type="bibr" target="#b50">[51]</ref> whose architectures were found by the compound scaling with TPUs. This stresses the effectiveness of our channel configuration over the convention and may encourage the researchers in the NAS field to adopt our channel configuration into the network search space for further performance boosts. The performance improvement of Im-ageNet classification is well transferred to the object detection and instance segmentation on the COCO dataset <ref type="bibr" target="#b32">[33]</ref> and the various fine-grained classification tasks. This indicates our backbones work as strong feature extractors.</p><p>Our contributions are 1) a study on designing a single layer ( ?3); 2) a network architecture exploration concerning the channel configuration towards a simple yet effective parameterization ( ?4); 3) using our models to achieve remarkable results on ImageNet <ref type="bibr" target="#b45">[46]</ref> outperformed recent lightweight models including NAS-based models ( ?5); 4) revealing the high applicability of our ImageNet-pretrained backbones transferring to several tasks including object detection, instance segmentation and fine-grained classification, which indicate the high expressiveness of our model and the effectiveness of our channel configuration ( ?5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>After appearance of AlexNet <ref type="bibr" target="#b29">[30]</ref>, VGG <ref type="bibr" target="#b47">[48]</ref>, GoogleNet <ref type="bibr" target="#b48">[49]</ref>, and ResNet <ref type="bibr" target="#b15">[16]</ref> which show significant improvements in ImageNet classification, much lighter models such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> have been proposed with lowered computational budgets. Using the new operator depthwise convolution (dwconv) <ref type="bibr" target="#b20">[21]</ref>, several architectures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b35">36]</ref> have been proposed with further efficient architecture designs. Taking advantage of the depthwise convolution could reduce a large amount of learnable parameters, and showed significant FLOPs reduction. Recently, structured network architecture search (NAS) methods have been proposed to yield the lightweight models <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref>, and EfficientNet <ref type="bibr" target="#b50">[51]</ref> which based on compound scaling of width, depth, and resolution, became a de facto state-of-art model. Take one step forward from the existing lightweight models, we focus on finding an effective channel config-uration for an inverted bottleneck module, which is an alternative to searching building blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Designing an Expansion Layer</head><p>In this section, we study how to design a layer properly considering the expressiveness, which is essential to design an entire network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>Estimating the expressiveness. In language modeling, the authors <ref type="bibr" target="#b57">[58]</ref> firstly highlighted that the softmax layer may suffer from turning the logits to the entire class probability due to the rank deficiency. This stems from the low input dimensionality of the final classifier and the vanished nonlinearity at the softmax layer when computing the logprobability. The authors proposed a remedy of enhancing the expressiveness, which improved the model accuracy. This implies that a network can be improved by dealing with the lack of expressiveness at certain layers. Estimating the expressiveness was studied in a model compression work <ref type="bibr" target="#b61">[62]</ref>. The authors compressed a model at layer-level by a low-rank approximation; investigated the amount of compression by computing the singular values of each feature. Inspired by the works, we conjecture that the rank may be closely related to the expressiveness of a network, and studying it may provide an effective layer design guide. Layer designs in practice. ResNet families <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57]</ref> have bottleneck blocks doubling the input channel dimensions (i.e., 64-128-256-512 in order) to make the final dimension (2048) above the number of classes at last. The recent efficient models <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> increase the channel dimensions steadily in inverted bottlenecks; therefore, they commonly involve a large expansion layer at the penultimate layer. The output dimension of the stem is set to 32 which expands the 3-dimensional input. Both of the inverted bottleneck <ref type="bibr" target="#b46">[47]</ref> and bottleneck block <ref type="bibr" target="#b15">[16]</ref> have the convolutional expansion layer with the predefined expansion ratio (mostly 6 or 4). Are these layers designed correctly and just need to design a new model accordingly?  <ref type="figure">Figure 1</ref>. Visualization of the output rank. We measure the rank ratio (i.e., rank/output channel dimension) vs. channel dimension ratio (i.e., input channel dimension/output channel dimension) from diverse architectures averaged over 1,000 random-sized networks with various nonlinear functions: (a) A single 1?1 convolution; (b) A single 3?3 convolution; (c) An inverted bottleneck with a 3?3 convolution; (d) An inverted bottleneck with a 3?3 depthwise convolution <ref type="bibr" target="#b46">[47]</ref>. We fundamentally observe that all the ranks are expanded above the input channel dimensions by the nonlinear functions with different network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Empirical study</head><p>Sketch of the study. We aim to study a design guide of a single expansion layer that expands the input dimension. We measure the rank of the output features from the diverse architectures (over 1,000 random-sized networks) and see the trend as varying the input dimensions towards the output dimensions. The rank is originally bounded to the input dimension, but the subsequent nonlinear function will increase the rank above the input dimension <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b57">58]</ref>. However, a certain network fails to expand the rank close to the output dimension, and the feature will not be fully utilized. We statistically verify the way of avoiding failure when designing the network. The study further uncovers the effect of complicated nonlinear functions such as ELU <ref type="bibr" target="#b7">[8]</ref> or SiLU (Swish-1) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> and where to use them when designing lightweight models.</p><p>Materials. We generate a bunch of networks with the building blocks consists of 1) a single 1?1 or 3?3 convolution layer; 2) an inverted bottleneck block <ref type="bibr" target="#b46">[47]</ref> with a 3?3 convolution or 3x3 depthwise convolution inside. We have the layer output (i.e., feature) f (WX) with W?R dout?din and the input X?R din?N , where f denotes a nonlinear function 1 with the normalization (we use a BN <ref type="bibr" target="#b24">[25]</ref> here). d out is randomly sampled to realize a random-sized network, and d in is proportionally adjusted for each channel dimension ratio (i.e., d in /d out ) in the range [0.1, 1.0]. N denotes the batch size, where we set N &gt;d out &gt;d in . We compute rank ratio (i.e., rank(f (WX))/d out ) for each model and average them. An inverted bottleneck block is similarly handled as a single convolution layer to compute rank 2 .</p><p>Observations. <ref type="figure">Figure 1</ref> shows how the rank changes with respect to the input channel dimension on average. Dimension ratio (d in /d out ) on x-axis denotes the reciprocal of the expansion ratio <ref type="bibr" target="#b46">[47]</ref>. We observe the followings:</p><p>(i) Drastic channel expansion harms the rank. This holds for a single convolution layer and an inverted bottleneck both as shown in <ref type="figure">Figure 1</ref>. The impact gets bigger with 1) an inverted bottleneck (see <ref type="figure">Figure 1c</ref>); 2) a depthwise convolution (see <ref type="figure">Figure 1d</ref> (ii) Nonlinearities expand rank. <ref type="figure">Figure 1</ref> shows averaged rank is expanded above the input channel dimension by nonlinear functions. They expand rank more at smaller dimension ratio, and complicated ones such as ELU, or SiLU do more;</p><p>(iii) Nonlinearities are critical for convolutions. Nonlinearities expand the rank of 1?1 and 3?3 single convolutions more than an inverted bottleneck (see <ref type="figure">Figure 1a</ref> and 1b vs. <ref type="figure">Figure 1c</ref> and 1d).</p><p>What we learn from the observations. We learned the followings: 1) an inverted bottleneck is needed to design with the expansion ratio of 6 or smaller values at the first 1?1 convolution; 2) each inverted bottleneck with a depthwise convolution in a lightweight model needs a higher channel dimension ratio; 3) a complicated nonlinearity such as ELU and SiLU needs to be placed after 1?1 convolutions or 3?3 convolutions (not depthwise convolutions). Based on the knowledge, in the following section, we perform channel dimension searches to find an effective channel configuration for entire channel dimensions. This is to uncover whether the conventional way of configuring channels shown in <ref type="table">Table 1</ref> is optimal or not, albeit the models have worked well with high accuracy.</p><p>Verification of the study. We finally provide an experimental backup to make sure what we have learned contributes to improving accuracy. We train the models consist-  <ref type="table">Table 2</ref>. Factor analysis of the study. We use four different models with similar computational complexity and report the accuracy and rank represented by the nuclear norm of the final feature. We average all the numbers over three models trained on CIFAR-100 <ref type="bibr" target="#b28">[29]</ref>. Each factor successively improves the accuracy and expands the rank without extra computational cost.</p><p>ing of two inverted bottlenecks (IBs) <ref type="bibr" target="#b3">4</ref> to adjust the channel dimension ratio (DR) of IBs and the first 1?1 convolutions in each IB. Starting from the baseline with the low DR 1/20, we successively study through the picked models with 1) increasing DR of the first 1?1 conv to 1/6; 2) increasing DR at every IB from 0.22 to 0.8; 3) replacing the first ReLU6 with SiLU in each IB. <ref type="table">Table 2</ref> shows each factor works well, and the rank and accuracy increase together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Designing with Channel Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem Formulation</head><p>Our goal is to reveal an effective channel configuration of designing a network under the computational demands. We formulate the following problem:</p><formula xml:id="formula_0">max ci,i=1...d Acc(N (c 1 , . . . c d )) s.t. c 1 ? c 2 ? ? ? ? ? c d?1 ? c d , Params(N ) ? P, FLOPs(N ) ? F,<label>(1)</label></formula><p>where Acc denotes the top-1 accuracy of the model; c i denotes output channel of i-th block among d building blocks; P and F denote the target parameter size and FLOPs. We involve the monotonic increasing of c i because this contains the channel configurations shown in <ref type="table">Table 1</ref>; we note that the opposite case, channel dimension consistently decreasing, requires a hard-to-use amount of FLOPs. Here, we are not targeting a hardware-specific model, so we concern with FLOPs rather than inference latency. Notice that many NAS methods <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> search the network N with fixing c i with based on the predefined channel configurations as shown in <ref type="table">Table 1</ref>, but on the other hand we search c i while fixing the network N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Searching with channel parameterization</head><p>We observe a general trend of architectural shapes for diverse computational demands through searches. Alternative to optimizing eq.(1) directly, we represent the channel dimensions at each building block with a piecewise linear function, which can reduce the search space. We parameterize the channel dimensions as c i = af (i) + b, where a and b are to be searched; let f (i) as a piecewise linear function by picking a subset of f (i) up from 1 . . . d. In this fashion, the channel parameterizations contain the conventional MobileNetV2-based channel configurations (i.e., stage-wise channel configurations) shown in <ref type="table">Table 1</ref>. We adopt the CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b28">[29]</ref> as done in NAS methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref> to search the parameterization.</p><p>To control the other variables, we set all the networks that have the fixed channel dimension at the stem 3?3 convolution of 16 followed by a BN <ref type="bibr" target="#b24">[25]</ref> with a ReLU and have the large expansion layer at the penultimate layer. We use the original inverted bottleneck (expansion ratio of 6) <ref type="bibr" target="#b46">[47]</ref> as our building blocks, which is a fundamental block of lightweight NAS methods, so we do not search the building blocks' expansion ratio. The chosen elements are based on the above investigation of single-layer design. Optimization is done alternatively by searching and training a network. We train each model for 30 epochs for faster training <ref type="bibr" target="#b41">[42]</ref> and the early stopping strategy <ref type="bibr" target="#b30">[31]</ref>. Each training is repeated three times for averaging the accuracies to reduce the accuracy fluctuation caused by random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Search Results</head><p>We perform individual searches under different constraints of computational costs to provide reliable and generalizable search results. We assign four search constraints to aim to search across different target model sizes. After each search, we collect top-10%, middle-10% (i.e., the models between top-50% and 60%), and bottom-10% models in terms of the model accuracy from 200 searched models to analyze them. To this end, we first visualize the collected models' channel configuration in <ref type="figure">Figure 2</ref> of each search; we then report the detailed performance statistics of the models and the best/worst-performing models' channel configuration in <ref type="table">Table 3</ref>. <ref type="figure">Figure 2</ref> illustrates the clear trends in which channel configuration is more effective in terms of accuracy. We observe that the linear parameterizations by the block index as colored with red enjoy higher accuracies while maintaining similar computational costs. This parameterization is regularly found throughout searching in diverse environments as shown in the figure. Note that the best models in <ref type="table">Table 3</ref> have the channels configured with linearly increasing that is identical to the linear parameterization. The models in green have highly reduced the input-side channels and therefore, most of the weight parameters are placed at the output-side resulting in the loss of accuracy. In addition, intriguingly, blue represents the models at the middle 10%accuracy, which looks similar to the channel configurations in <ref type="table">Table 1</ref>. Their conventional configurations are designed to attain a flop-efficiency by limiting the channels at earlier layers and giving more channels close to the output. Therefore, we may safely suggest that we need to change the con-  <ref type="table">Table 3</ref>. Detailed searched channel configurations. We provide the detailed individual searched results under the different search constraints. Along with <ref type="figure">Figure 2</ref>, we report the detailed numbers including the averaged accuracy, # of parameters, FLOPs over top-10%, midddle-10%, and bottom-10% accuracy models. We further present each of the best and the worst models' channel configurations.</p><p>vention (blue) towards the new channel configuration (red) that can achieve additional accuracy gains. We have additional search results shown in <ref type="figure" target="#fig_2">Figure 4</ref> when fixing the network depth and searching under the different computational costs; we found identical linear parameterizations. Furthermore, we perform the same experiment on CIFAR-10 with the constraints of computational costs and found the same trends in <ref type="figure">Figure A1</ref>. As mentioned earlier, we believe that the success of the works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> may stem from similar parameterizations. In the case of training the 18-depth models, which have about 30 MFLOPs, it takes 1.5 GPU days for searching and training about 100 models for individual 30 epochs training with 3 runs for training each model. The training cost for the entire search is 30 MFLOPs?30 epochs?3 runs?100 models = 2.7?100 GFLOPs epochs, and this is smaller than training a single ImageNet model ResNet50 for 100 epochs (4.0 GFLOPs?100 epochs). Although the actual training time is less than that of ResNet50, comparing directly with the ImageNet model may be inappropriate. We provide a rough estimate of the amount of computation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Network upgrade</head><p>We rebuild the existing model based on the investigations in practice. From the baseline MobileNetV2 <ref type="bibr" target="#b46">[47]</ref> which introduced the convention of channel configuration, we only reassign each output channel dimension of inverted bottlenecks by following the parameterization. We use the identical setting of the stem (i.e., 3?3 convolution with BN and ReLU6) and the inverted bottleneck with the expansion ratio 6. We use the same large expansion layer at the penultimate layer. To fairly compare with the aforementioned lightweight models MobileNetV3 <ref type="bibr" target="#b19">[20]</ref>, MixNet <ref type="bibr" target="#b51">[52]</ref>, Effi-cientNet <ref type="bibr" target="#b50">[51]</ref>, AtomNas <ref type="bibr" target="#b38">[39]</ref>, FairNAS <ref type="bibr" target="#b5">[6]</ref>, FairDARTS <ref type="bibr" target="#b6">[7]</ref>, DART+ <ref type="bibr" target="#b30">[31]</ref>, we further replace ReLU6 with SiLU <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> and adopt SE <ref type="bibr" target="#b21">[22]</ref> in the inverted bottlenecks.</p><p>Based on the investigation in ?3, we replace ReLU6 only after the first 1?1 convolution in each inverted bottleneck because we observed the layer with a smaller dimension ratio needs to be more addressed; the second depthwise convolution has the channel dimension ratio of 1, so we do not replace ReLU6 here. This can further realize the simplicity of model design and benefit from faster training speeds since ReLU6s remain. Using other nonlinear functions such as ELU shows similar accuracy gains (see <ref type="table" target="#tab_14">Table A2</ref>, but we use SiLU (Swish-1) for a fair comparison with the aforementioned lightweight models which use SiLU.</p><p>Note that only with these simple modifications, our model outperforms NAS-based methods in many experiments ( ?5), which signifies the importance of the channel configuration. We call our model Rank Expansion Networks (ReXNet) as observed the actual rank expansion in ?6. Additionally, we build another model with the linear parameterization upon MobileNetV1 <ref type="bibr" target="#b20">[21]</ref>, which shows a large accuracy improvement (+2.3pp) over the baseline 72.5%. We call this model ReXNet (plain). The detailed model specification of ReXNets is provided in Appendix B.  <ref type="bibr" target="#b46">[47]</ref> 74.7% 92.5% 0.59B 6.9M ShuffleNetV1 (x2) <ref type="bibr" target="#b60">[61]</ref> 73.7% -0.52B -ShuffleNetV2 (x2) <ref type="bibr" target="#b35">[36]</ref> 75.4% -0.60B -NASNet-A <ref type="bibr" target="#b62">[63]</ref> 74.0% 91.7% 0.56B 5.3M AmoebaNet-A <ref type="bibr" target="#b43">[44]</ref> 75.5% 92.0% 0.56B 5.1M PNASNet <ref type="bibr" target="#b33">[34]</ref> 74.2% 91.9% 0.59B 5.1M DARTS <ref type="bibr" target="#b34">[35]</ref> 73.1% 91.0% 0.60B 4.9M FBNet-C <ref type="bibr" target="#b54">[55]</ref> 74.9% -0.38B 5.5M Proxyless-R <ref type="bibr" target="#b2">[3]</ref> 74.6% 92.2% 0.32B 4.1M P-DARTS <ref type="bibr" target="#b3">[4]</ref> 75 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageNet Classification</head><p>Training on ImageNet. We train our model on the Im-ageNet dataset <ref type="bibr" target="#b45">[46]</ref> using the standard data augmentation <ref type="bibr" target="#b48">[49]</ref> with stochastic gradient descent (SGD) and minibatch size of 512 on four GPUs. Learning rate is initially set to 0.5 and is scheduled by cosine learning rate scheduling. Weight decay is set to 1e-5. Additionally, we train our model with RandAug <ref type="bibr" target="#b9">[10]</ref> to compare it fairly with the NAS-based models <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> using additional regularizations (denoted by * in <ref type="table" target="#tab_5">Table 4</ref>) such as Mixup <ref type="bibr" target="#b59">[60]</ref>, AutoAug <ref type="bibr" target="#b8">[9]</ref>, and RandAug <ref type="bibr" target="#b9">[10]</ref>, our model improves all the models including EfficientNet-B0 (with AutoAug), FairNas-A, FairDARTS-C, and SE- <ref type="bibr" target="#b4">5</ref> MobileNetV3 uses Hard Swish <ref type="bibr" target="#b19">[20]</ref>, and MnasNet does not use SiLU.   To this end, we adopt the width multiplier concept <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref> for scaling. Note that all our models are trained with the fixed resolution 224?224 unlike Ef-ficientNets trained with the resolutions from 224?224 to 600?600. We do not use the method such as FixResNet <ref type="bibr" target="#b52">[53]</ref> which performs additional training. <ref type="table" target="#tab_6">Table 5</ref> shows our models adjusted by the multiplier from ?0.9 to ?2.0 with remarkable accuracy increments (see more models in Appendix B.3). We measure CPU and GPU inference speeds to show the efficiency; we average the latencies over 1,000 runs with the batch size 1 on an Intel Xeon CPU E5-2630 and the batch size 64 on a V100 GPU, respectively. <ref type="figure" target="#fig_1">Figure 3</ref> visualizes our models' computational efficiency compared with EfficientNets; as the model size is larger, we observe that our models are much faster than EfficientNets. Notice that ReXNet (?2.0) is about 1.4? and 2.0? faster than EfficientNet-B3 on CPU and GPU, respectively with almost the same accuracy and improves +2.5pp top-1 accuracy on EfficientNet-B1 at a similar speed. This benefit may come from the fixed resolution and network depth for training and inference over all the models which can reduce the memory access time.   <ref type="table">Table 7</ref>. COCO object detection results with Faster RCNN <ref type="bibr" target="#b44">[45]</ref> and FPN <ref type="bibr" target="#b31">[32]</ref>. We report box APs on val2017.  <ref type="table">Table 8</ref>. Transfer learning results on fine-graned datasets. Our ReXNet (?1.0) is more efficient than ResNet50 and EfficientNet-B0, yet transfers well to the various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">COCO object detection</head><p>Training SSDLite. We validate our backbones through object detection on the COCO dataset <ref type="bibr" target="#b32">[33]</ref> in SSDLite <ref type="bibr" target="#b46">[47]</ref> which has lightweight detection heads suitable for seeing the feature extractor's capability. We follow the identical design elements <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref> by building the first head on top of the final expansion layer which has the stride 16 and another head on top of the final layer. We train ReXNets (?0.9, ?1.0, and ?1.3) and EfficientNets-B0, B1, and B2 in SSDLites with the same training settings <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref> including 320?320 image resolution for fair comparison. <ref type="table" target="#tab_9">Table 6</ref> shows ours largely outperform the other backbones with comparable computational costs. ReXNet (?1.0) outperforms EfficientNet-B1 trained with 240?240 by +1.8pp, and ReXNet (?1.3) outperforms EfficientNet-B2 trained with 260?260 by +0.7pp under similar computational costs. Interestingly, ReXNet (?0.9) achieves +1.2pp AP improvement over EfficientNet-B0 with less computa-tional costs. The large AP improvements indicate our channel dimension configuration can help finetuning as well. We provide more detection results in Appendix C.2. Note that ReXNet (?0.9) is faster than EfficientNet-B0 (75ms vs. 77ms), and ReXNet (?1.3) is faster than EfficientNet-B2 (88ms vs. 101ms) on an Intel Xeon CPU E5-2630.</p><p>Training Faster RCNN. We adopt Faster RCNN <ref type="bibr" target="#b44">[45]</ref> to explore the maximal performance of ReXNets. We plug ReXNets (?0.9) and (?2.2), EfficientNet-B0, and ResNet50 into FPN <ref type="bibr" target="#b31">[32]</ref> and train with the image size of 1200?800 following the original setting <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32]</ref> such as freezing all BNs. <ref type="table">Table 7</ref> shows ReXNets' superiority over others; ReXNet (?0.9) and ReXNet (?2.2) improves EfficientNet-B0 and ResNet50 by +0.5pp and +3.9pp APs, respectively with smaller computational costs; ReXNet (?2.2) achieves 41.5 AP only with the standard Faster RCNN framework without any bells and whistles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Fine-grained classifications</head><p>We finetune the ImageNet-pretrained models on the datasets Food-101 <ref type="bibr" target="#b1">[2]</ref>, Stanford Cars <ref type="bibr" target="#b27">[28]</ref>, FGVC Aircraft <ref type="bibr" target="#b37">[38]</ref>, and Oxford Flowers-102 <ref type="bibr" target="#b40">[41]</ref> to verify the transferability. We compare ReXNet (?1.0) with ResNet50 <ref type="bibr" target="#b15">[16]</ref> and EfficientNet-B0 [51] on each dataset. We exhaustively search the hyper-parameters including learning rate and weight decay for the best results as done in the work <ref type="bibr" target="#b26">[27]</ref> for each model. We train all the layers using SGD with the same learning rates without using additional training techniques. Training and evaluation are done with 224?224 image size; we use center-cropped images from the resized images with the shorter side of 256 for evaluation. <ref type="table">Table 8</ref> shows ReXNet (?1.0) outperforms EfficientNet-B0 for all the datasets with large margins and mostly surpasses  <ref type="table">Table 9</ref>. COCO instance segmentation results with Mask RCNN <ref type="bibr" target="#b14">[15]</ref> and FPN <ref type="bibr" target="#b31">[32]</ref>. We report box and mask APs on val2017. ResNet50 which has 5? more parameters. This indicates our backbone can perform as a more generalizable feature extractor than other models even with fewer parameters and may reflect the effectiveness of our channel configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">COCO Instance Segmentation</head><p>We use Mask RCNN <ref type="bibr" target="#b14">[15]</ref> to validate the performance of ReXNets on instance segmentation. We train the models with the identical setting in the Faster RCNN training in <ref type="table">Table 7</ref>. <ref type="table">Table 9</ref> shows our backbones' efficiency; ReXNet (?0.9) outperforms EfficientNet-B0 by +0.6pp mask AP and +0.5pp bbox AP with fewer parameters; ReXNet (?2.2) gains +3.2pp mask AP and +3.5pp bbox AP over ResNet50 with much less computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Fixing network depth and searching models. We further verify the linear channel parameterization by searching for new models under different constraints. We fix the network depth as 18 and 30 and give the constraints with FLOPs of 30M, 50M, and 70M for 18-depth models; 100M, 150M, and 200M for 30-depth models, respectively. Eventually, <ref type="figure" target="#fig_2">Figure 4</ref> shows all the top 10% models have identical shapes that are linear functions but have different slopes due to the different FLOPs. This shows that linear channel configurations outperform the conventional configuration for various computational demands.</p><p>Rank visualization. We study the model expressiveness by analyzing the rank of trained models. This analysis is to see how the linear parameterization affects the rank. We visualize the rank computed from the trained models in two </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we have studied a new approach to designing lightweight models. We have conjectured that the conventional stage-wise channel configurations which have widely used in NAS methods after proposed by Mo-bileNetV2 have limited the model accuracy; if we find a more effective channel configuration, the models would have accuracy gains. We first studied an appropriate way of designing a single building block and a layer targeted to lightweight models; we then proposed a search method for a channel configuration via piece-wise linear functions of block index. The search space contains the conventions, and we found an effective channel configuration which can be parameterized by a linear function of the block index. Based on the parameterization, we have achieved a new model which could outperform the recent lightweight models including NAS-based models on ImageNet dataset. Our ReXNets further showed remarkable finetuning performances on COCO object detection, instance segmentation, and fine-grained classifications. Consequently, we believe our work highlighted a new perspective of architecture search that helps to search with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Search Results</head><p>We provide the additional results of searching an effective channel dimension configuration by the proposed search method described in ?4 to show the consistent trend of the searched channel configurations over different datasets. We perform searches on the CIFAR-10 dataset <ref type="bibr" target="#b28">[29]</ref> with the identical search constraints presented in <ref type="table">Table 3</ref>.</p><p>Following the previous analysis, we collect top-10%, middle-10% (i.e., the models between the top 50% and 60%), and bottom-10% models in terms of the model accuracy from 200 searched models to show the channel configurations with the performance statistics after each search. <ref type="figure">Figure A1</ref> shows that the searched models as colored with red, which look linear functions, have higher accuracy while maintaining the similar computational costs. These similar trends are regularly observed while searching under the various constraints, and we can parameterize the models with a linear function by the block index again. The models in green have highly reduced the input-side channels and many output-side weight parameters resulting in the loss of accuracy. In addition, blue represents the models at the middle-10% accuracy, which looks similar to the conventional channel configurations such as MobileNetV2's <ref type="bibr" target="#b46">[47]</ref>.</p><p>All the searched channel configurations which can be approximated to linear parameterizations by the block index have higher accuracy (red) then blue ones, which are show the identical trends to <ref type="figure">Figure 2</ref>. It is worth noting that all the red lines in <ref type="figure">Figure A1</ref> have higher slopes compared to those in <ref type="figure">Figure 2</ref> in the main paper. This is because CIFAR-100 models have more parameters at the final classifier due to a larger number of classes, so the early layers employ fewer parameters than those of the models trained on the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Upgrade (cont'd)</head><p>In this section, we give further information of ReXNets and introduce our new model rebuilt upon Mo-bileNetV1 <ref type="bibr" target="#b20">[21]</ref> called ReXNet (plain) which does not use skip connections <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47]</ref> at each building block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. ReXNet (cont'd)</head><p>We have described our upgraded model based on Mo-bileNetV2 <ref type="bibr" target="#b46">[47]</ref>, which follows the searched linear parameterization on channel dimensions with some minor modifications in ?4.4. Here, we illustrate the network architecture of our ReXNet (?1.0) in <ref type="figure" target="#fig_5">Figure A2a</ref>. We observe ReXNet (?1.0) has the identical block configuration to that of Mo-bileNetV2 where a single-type building block MB6 3x3, which is the original inverted bottleneck <ref type="bibr" target="#b46">[47]</ref> with the 3?3 depthwise convolution and the expansion ratio 6 is used as the basic building block except for the first inverted bottleneck. Every inverted bottleneck block that expands the channel dimensions (except for the downsampling blocks) has a skip connection where the expanded channel dimensions are padded with zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MB6_3?3</head><p>MB6_3?3 <ref type="table" target="#tab_5">Stem  Conv_3x3   MB1_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3  MB6_3?3   32?112?112  16?112?112  27?56?56  38?56?56  50?28?28  61?28?28  72?14?14  84?14?14  95?14?14  106?14?14  117?14?14  128?14?14  140?7?7  151?7?7  162?7?7  174?7?7  185?7?7</ref> Conv_1x1 <ref type="table" target="#tab_5">Conv_3x3  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  dwconv3?3  conv1?1  32?112?112  96?56?56  114?56?56  192?28?28  240?28?28  288?14?14  336?14?14  384?14?14  432?14?14  480?14?14  528?14?14  576?7?7  624?7?7</ref>   <ref type="table" target="#tab_12">Table A1</ref>, ReXNet (plain) does not achieve the best accuracy, but it is extremely faster than ReXNets on both CPU and GPU even with larger FLOPs.</p><formula xml:id="formula_1">Pooling + FC 1280?7?7 (a) ReXNet (?1.0) dwconv3?3 conv1?1 dwconv3?3 conv1?1 Stem</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Overall models</head><p>In addition to the models introduced in <ref type="table" target="#tab_6">Table 5</ref>, we provide additional models adjusted by different width multipliers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. ReXNet-lite</head><p>We additionally provide faster models based on ReXNet. We make slight changes upon ReXNet: 1) removing SE <ref type="bibr" target="#b21">[22]</ref> from each inverted bottleneck; 2) replacing all SiLUs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> with ReLU6s. We further incorporate a fully connected layer before the classifier. We compare our models with   <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> 77.9 93.9 0.40B 4.8M <ref type="table">Table A3</ref>. Nonlinear functions and ImageNet accuracy.</p><p>EfficientNet-lites 7 which are lighter version of Effciient-Nets <ref type="bibr" target="#b50">[51]</ref> in <ref type="table" target="#tab_14">Table A2</ref>. We measure CPU and GPU inference speeds by averaging the latencies over 1,000 runs with the batch size 1 on an Intel Xeon CPU E5-2630 and the batch size 64 on a V100 GPU, respectively. As shown in the table, ReXNet-lite clearly show better accuracy and faster speed than each of the counterparts in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Empirical Studies</head><p>C.1. Impact of nonlinear functions.</p><p>We have studied how nonlinearity can affect rank in the investigation in S3. We further study the actual impact of them by training the models on ImageNet. We train ReXNet  (?1.0) with ELU, SoftPlus, LeakyReLU, ReLU6, and SiLU (Swish-1) with the identical training setup. As shown in <ref type="table">Table A3</ref>, we obtain the results of top-1 accuracy in the order of SiLU (77.9%), ELU (77.6%), SoftPlus (77.6%), Leaky ReLU (77.4%), and ReLU6 (77.3%), and the trend is similar to the result in the empirical study in ?3.2. The result indicates the quality of different nonlinearities that relates to model expressiveness; SiLU shows the best performance. This may provide a backup for why the recent lightweight models use SiLU (Swish-1) as the nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. COCO object detection (cont'd)</head><p>We further provide more comparisons of ReXNets with EfficientNets <ref type="bibr" target="#b50">[51]</ref> in SSDLite <ref type="bibr" target="#b46">[47]</ref> on object detection. We first replace the EfficientNet backbone used in ?5.2 with a stronger EfficientNet <ref type="bibr" target="#b55">[56]</ref>. We then compare them trained from scratch on the COCO dataset <ref type="bibr" target="#b32">[33]</ref>.</p><p>Comparison with NoisyStudent EfficientNets. We now compare ReXNets with stronger EfficientNets <ref type="bibr" target="#b55">[56]</ref>, where the backbones are trained by a self-training method with extra large-scale data and RandAug <ref type="bibr" target="#b9">[10]</ref>. Our goal is to show ReXNet's architectural capability over the Efficient-Nets without using the extra data when applying the backbones to a downstream task. We borrow the AP scores on val 2017 from the ReXNet+SSDLite models in <ref type="table" target="#tab_9">Table 6</ref> and train EfficientNet-B0, B1, and B2 in SSDLite using the pretrained NoisyStudent+RA EfficientNet models which are publicly released <ref type="bibr" target="#b7">8</ref> . All the AP scores are evaluated by each checkpoint cached at the last iteration. Table A5 shows that ReXNets outperform the counterparts 8 https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet with the comparable computational costs. This indicates ReXNets pretrained on ImageNet are still promising.</p><p>Comparison of the models trained from scratch. We aim to verify the model expressiveness itself without using ImageNet-pretrained backbones. This is because one may wonder using a pretrained models trained with different training setups such as optimizer or regularizers affect the performance of downstream tasks. As shown in the work <ref type="bibr" target="#b13">[14]</ref>, the COCO dataset <ref type="bibr" target="#b32">[33]</ref> is able to be trained from scratch, we verify the model's pure expressiveness by training the models from scratch.</p><p>We individually train ReXNets (?0.9, ?1.0, and ?1.3) and EfficientNet-B0, B1, and B2 in SSDLite without using ImageNet pretrained backbones. All AP scores are evaluated by each checkpoint cached at the last iteration again. <ref type="table" target="#tab_6">Table A5</ref> shows that ReXNets outperform the counterparts by +1.4pp, +0.3pp, and +1.2pp in AP score. With similar computational demands, ReXNets beat the Efficient-Net counterparts by large margins, and surprisingly, the training from scratch makes ReXNet (?1.0) outperforms EfficienetNet-B1 by +0.3pp with much less computational costs. This indicates that our models are more powerful in terms of expressiveness even without the aids of the supervision of ImageNet pretrained models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) 3 ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>ImageNet accuracy vs. FLOPs and latencies. We visualize the numbers of ReXNets (?1.0, ?1.3, ?1.5, and ?2.0) and EfficientNets-B0, B1, B2, and B3 in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>, Acc (%): 71.0?0.1 FLOPs: 150M, Acc (%): 69.8?0.1 FLOPs: 100M, Acc (%): 68.8?0.1 (b) Top-10% models (30-depth) Searched configuration under fixed depth. We similarly plot the searched channel dimensions of top-10% models from the searched candidates with fixed depths. The search constraints (FLOPs) are Red: 70M (left) 200M (right); Blue: 50M (left) and 150M (right); Green: 30M (left) and 100M (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of rank. The models with higher accuracy have higher nuclear norms (left); our ReXNet has larger singular values than MobileNetV2 (right). manners; we show the distribution of accuracy vs. rank (represented by the nuclear norm) from 18-depth models in ?4; we then compare MobileNetV2 with ReXNet (?0.9) by visualizing the cumulative distribution of the singular values that are normalized to [0, 1] computed from the features of the images in the ImageNet validation set. As shown inFigure 5, we observe 1) a higher-accuracy model has a higher rank; 2) ReXNet clearly expands the rank over the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Acc (%): 91.8?0.1 Mid-10% Acc (%): 91.4?0.0 Bot-10% Acc (%): 91.2?0.1 (c) Depth: 30 (# inverted bot.: 9), # Params 1.0M, FLOPs 200M Acc (%): 92.4?0.1 Mid-10% Acc (%): 91.9?0.0 Bot-10% Acc (%): 91.4?0.2 (d) Depth: 42 (# inverted bot.: 13), # Params 3.0M, FLOPs 350M Figure A1. Visualization of searched models' channel dimension on CIFAR-10. Red: top 10%-accuracy models; Blue: middle 10% models; Green: bottom 10% models; we plot the averaged channel configurations with the 1-sigma range and report the averaged top-1 accuracy over each searched candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A2 .</head><label>A2</label><figDesc>Architectures of ReXNet (?1.0) and ReXNet (plain). MB1 and MB6 refer to MobileNetV2 [47]'s inverted bottlenecks with the expansion ratio of 1 and 6, respectively. Each model has almost similar architectural elements compared to the original ones.NetworkTop-1 Top-5 FLOPs Params. CPU GPU train the model by following the identical training setup in ?5.1. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Increase DR of 1?1 conv (1/20?1/6) 99M 52.1% 6655.9 + Increase DR of IB (0.22?0.8) 105M 53.8% 6703.2 + Replace ReLU6 with SiLU 105M 54.6% 6895.9</figDesc><table><row><cell>Network</cell><cell>FLOPs Top-1 Nuc. norm</cell></row><row><cell>Baseline</cell><cell>103M 48.0% 5997.5</cell></row><row><cell>+</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Visualization of the searched models' channel dimensions vs. block index. Red: top-10%; blue: middle-10%; green: bottom-10% accuracy models; we plot the averaged channel configurations with the 1-sigma range over each searched candidate.</figDesc><table><row><cell>40 50 60 70 80 90 Channel Dimension</cell><cell></cell><cell cols="3">Top-10% Acc (%): 63.1?0.1 Mid-10% Acc (%): 62.2?0.0 Bot-10% Acc (%): 61.4?0.2</cell><cell>50 100 150 Channel Dimension</cell><cell cols="2">Top-10% Acc (%): 68.8?0.1 Mid-10% Acc (%): 68.0?0.1 Bot-10% Acc (%): 67.4?0.3</cell><cell>50 100 150 200 250 300 Channel Dimension</cell><cell>Top-10% Acc (%): 71.0?0.1 Mid-10% Acc (%): 70.3?0.0 Bot-10% Acc (%): 69.9?0.2</cell><cell>100 200 300 400 Channel Dimension</cell><cell>Top-10% Acc (%): 73.0?0.1 Mid-10% Acc (%): 72.1?0.1 Bot-10% Acc (%): 71.7?0.1</cell></row><row><cell>30</cell><cell>1</cell><cell>2</cell><cell>3 Block Index</cell><cell>4</cell><cell>5</cell><cell cols="3">1 2 3 4 5 6 7 8 9 Block Index</cell><cell>1 2 3 4 5 6 7 8 9 Block Index</cell><cell>1 2 3 4 5 6 7 8 9 10111213 Block Index</cell></row><row><cell></cell><cell cols="4">(a) 18-depth models</cell><cell></cell><cell cols="2">(b) 30-depth models</cell><cell></cell><cell>(c) 30-depth models</cell><cell>(d) 42-depth models</cell></row><row><cell cols="4">Figure 2. Search constraints</cell><cell></cell><cell>Ranking</cell><cell cols="4">Acc (%) Params (M) FLOPs (M) Best and worst models among searched configurations</cell></row><row><cell cols="5">(a) Models with 5 inverted bot., # Params 0.2M, FLOPs 30M</cell><cell cols="2">Top-10% 63.1?0.1 Mid-10% 62.2?0.0 Bot-10% 61.4?0.2</cell><cell>0.2?0.0 0.2?0.0 0.2?0.0</cell><cell>30?1 30?1 30?1</cell><cell>Best: 34-34-45-55-66 (Acc: 63.4%) Worst: 36-36-36-36-83 (Acc: 61.1%)</cell></row><row><cell cols="5">(b) Models with 9 inverted bot., # Params 0.5M, FLOPs 100M</cell><cell cols="2">Top-10% 68.8?0.1 Mid-10% 68.0?0.1 Bot-10% 67.4?0.3</cell><cell>0.5?0.0 0.5?0.0 0.5?0.0</cell><cell>101?5 100?6 97?8</cell><cell>Best: 24-33-42-50-59-68-77-85-94 (Acc: 68.9%) Worst: 39-39-39-39-39-39-39-87-158 (Acc: 67.6%)</cell></row><row><cell cols="5">(c) Models with 9 inverted bot., # Params 1.0M, FLOPs 200M</cell><cell cols="2">Top-10% 71.0?0.1 Mid-10% 70.3?0.0 Bot-10% 69.9?0.2</cell><cell>1.0?0.0 1.0?0.0 1.0?0.0</cell><cell>210?15 198?18 200?15</cell><cell>Best: 30-45-59-74-88-103-117-132-146 (Acc: 71.1%) Worst: 47-47-70-70-70-70-70-70-364 (Acc: 69.6%)</cell></row><row><cell cols="5">(d) Models with 13 inverted bot., # Params 3.0M, FLOPs 300M</cell><cell cols="2">Top-10% 73.0?0.1 Mid-10% 72.1?0.1 Bot-10% 71.7?0.1</cell><cell>3.0?0.0 3.0?0.0 3.0?0.0</cell><cell>351?5 351?5 351?6</cell><cell>Best: 34-34-34-40-64-88-112-136-160-184-208-232-256 (Acc: 73.2%) Worst: 52-52-52-52-52-52-52-52-52-115-263-263-412 (Acc: 71.6%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>show the</cell></row><row><cell>performance comparison with popular lightweight mod-</cell></row><row><cell>els where all the reported models are trained and evalu-</cell></row><row><cell>ated with 224?224 image size. Comparing with the mod-</cell></row><row><cell>els [20, 50, 52, 51] with SE [22] and SiLU 5 [18, 43], our</cell></row><row><cell>model outperforms the most of the models searched by NAS</cell></row><row><cell>including MobileNetV3-Large, MNasNet-A3, MixNet-M,</cell></row><row><cell>and EfficientNet-B0 (without AutoAug [9]) with at least</cell></row><row><cell>+0.3pp accuracy improvement. Our model outperforms</cell></row><row><cell>MixNet-M and AtomNas-C+ which use mixed-kernel op-</cell></row><row><cell>erations under similar computational costs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>We observe ReXNet outperforms each of the EfficientNet counterparts in practice.</figDesc><table><row><cell>Network</cell><cell cols="3">Top-1 Top-5 FLOPs Params</cell><cell>CPU GPU</cell></row><row><cell cols="3">ReXNet (?0.9) 77.2% 93.5% 0.35B</cell><cell>4.1M</cell><cell>45ms 20ms</cell></row><row><cell>Eff-B0 [51]</cell><cell cols="2">77.3% 93.5% 0.39B</cell><cell>5.3M</cell><cell>47ms 23ms</cell></row><row><cell cols="3">ReXNet (?1.0) 77.9% 93.9% 0.40B</cell><cell>4.8M</cell><cell>47ms 21ms</cell></row><row><cell>Eff-B1 [51]</cell><cell cols="2">79.2% 94.5% 0.70B</cell><cell>7.8M</cell><cell>70ms 37ms</cell></row><row><cell cols="3">ReXNet (?1.3) 79.5% 94.7% 0.66B</cell><cell>7.6M</cell><cell>55ms 28ms</cell></row><row><cell>Eff-B2 [51]</cell><cell>80.3% 95.0%</cell><cell>1.0B</cell><cell>9.2M</cell><cell>77ms 48ms</cell></row><row><cell cols="2">ReXNet (?1.5) 80.3% 95.2%</cell><cell>0.9B</cell><cell>9.7M</cell><cell>59ms 31ms</cell></row><row><cell>Eff-B3 [51]</cell><cell>81.7% 95.6%</cell><cell>1.8B</cell><cell cols="2">12M 100ms 78ms</cell></row><row><cell cols="2">ReXNet (x2.0) 81.6% 95.7%</cell><cell>1.5B</cell><cell>16M</cell><cell>69ms 40ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Scalablity</figDesc><table /><note>of our models. We adjust ReXNet (?1.0) via width multipliers to compare with EfficientNets [51] on the Ima- geNet dataset. We report the overall performances with CPU and GPU latencies in practice.DARTS+ by at least +0.4pp 6 . Strikingly, our model does not require further searches, but it either outperforms or is comparable to NAS-based models. Comparison with Efficientnets. We compare ReXNets with EfficientNets [51] about model scalability with the per- formances.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>COCO object detection results with SSDLite<ref type="bibr" target="#b46">[47]</ref>. We report box AP scores on testdev2017 of our models with SSDLite comparing with lightweight models(FLOPs 1.0B). ? : the model performances are trained by ourselves.</figDesc><table><row><cell>Backbone</cell><cell>Input Size</cell><cell>AP</cell><cell>Bbox AP at IOU AP 50</cell><cell>AP 75</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>EfficientNet-B0 [51] + FPN</cell><cell>1200?800</cell><cell>38.0</cell><cell>60.1</cell><cell>40.4</cell><cell>21.0M</cell><cell>123.0B</cell></row><row><cell>ReXNet (?0.9) + FPN</cell><cell>1200?800</cell><cell>38.0</cell><cell>60.6</cell><cell>40.8</cell><cell>20.1M</cell><cell>123.0B</cell></row><row><cell>ResNet50 [16] + FPN</cell><cell>1200?800</cell><cell>37.6</cell><cell>58.2</cell><cell>40.9</cell><cell>41.8M</cell><cell>202.2B</cell></row><row><cell>ReXNet (?2.2) + FPN</cell><cell>1200?800</cell><cell>41.5</cell><cell>64.0</cell><cell>44.9</cell><cell>33.0M</cell><cell>153.8B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A1</head><label>A1</label><figDesc>shows the ReXNets (?1.1, ?1.2, ?1.4, ?2.2, and ?3.0) and our new model ReXNet (plain) with the corresponding performances.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A2 .</head><label>A2</label><figDesc>Performance of ReXNet-lites. We further report the ImageNet performances of ReXNet-lites. We compare with EfficientNet-lites and report the overall performances with CPU and GPU (FP32 and FP16) latencies in practice.</figDesc><table><row><cell>Nonlinearity</cell><cell cols="4">Top-1 (%) Top-5 (%) FLOPs Params.</cell></row><row><cell>ReLU6 [47]</cell><cell>77.3</cell><cell>93.5</cell><cell>0.40B</cell><cell>4.8M</cell></row><row><cell>Leaky ReLU [37]</cell><cell>77.4</cell><cell>93.6</cell><cell>0.40B</cell><cell>4.8M</cell></row><row><cell>Softplus [12]</cell><cell>77.6</cell><cell>93.8</cell><cell>0.40B</cell><cell>4.8M</cell></row><row><cell>ELU [8]</cell><cell>77.6</cell><cell>93.7</cell><cell>0.40B</cell><cell>4.8M</cell></row><row><cell>SiLU</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A4 .</head><label>A4</label><figDesc>ReXNets vs. Noisy Student EfficientNets on COCO object detection. We compare our ReXNets trained solely on ImageNet with stronger EfficientNets trained by Noisy Student training method<ref type="bibr" target="#b55">[56]</ref> with RandAug<ref type="bibr" target="#b9">[10]</ref>. Note that ReXNets here are equivalent to the models in ?5.2. We report box APs on val2017.</figDesc><table><row><cell>Model</cell><cell>Input Size</cell><cell>AP</cell><cell cols="2">Bbox AP at IOU AP 50 AP 75</cell><cell>Params</cell><cell>FLOPs</cell></row><row><cell>EfficientNet-B0 [51] + SSDLite</cell><cell>320?320</cell><cell>23.6</cell><cell>39.4</cell><cell>23.3</cell><cell>6.2M</cell><cell>0.97B</cell></row><row><cell>ReXNet (?0.9) + SSDLite</cell><cell>320?320</cell><cell>24.6</cell><cell>41.2</cell><cell>24.6</cell><cell>5.0M</cell><cell>0.88B</cell></row><row><cell>EfficientNet-B1 [51] + SSDLite</cell><cell>320?320</cell><cell>25.6</cell><cell>42.2</cell><cell>25.8</cell><cell>8.7M</cell><cell>1.35B</cell></row><row><cell>ReXNet (?1.0) + SSDLite</cell><cell>320?320</cell><cell>25.2</cell><cell>41.9</cell><cell>25.3</cell><cell>5.7M</cell><cell>1.01B</cell></row><row><cell>EfficientNet-B2 [51] + SSDLite</cell><cell>320?320</cell><cell>26.4</cell><cell>43.4</cell><cell>26.6</cell><cell>10.0M</cell><cell>1.55B</cell></row><row><cell>ReXNet (?1.3) + SSDLite</cell><cell>320?320</cell><cell>27.1</cell><cell>44.7</cell><cell>27.4</cell><cell>8.4M</cell><cell>1.60B</cell></row><row><cell>Model</cell><cell>Input Size</cell><cell cols="3">Avg. Precision at IOU AP AP 50 AP 75</cell><cell>Params.</cell><cell>FLOPs</cell></row><row><cell>EfficienetNet-B0 [51] + SSDLite</cell><cell>320x320</cell><cell>23.9</cell><cell>39.6</cell><cell>24.1</cell><cell>6.2M</cell><cell>0.97B</cell></row><row><cell>ReXNet (?0.9) + SSDLite</cell><cell>320x320</cell><cell>25.3</cell><cell>41.4</cell><cell>25.9</cell><cell>5.0M</cell><cell>0.88B</cell></row><row><cell>EfficienetNet-B1 [51] + SSDLite</cell><cell>320x320</cell><cell>25.6</cell><cell>41.9</cell><cell>26.0</cell><cell>8.7M</cell><cell>1.35B</cell></row><row><cell>ReXNet (?1.0) + SSDLite</cell><cell>320x320</cell><cell>25.9</cell><cell>42.6</cell><cell>26.3</cell><cell>5.7M</cell><cell>1.01B</cell></row><row><cell>EfficienetNet-B2 [51] + SSDLite</cell><cell>320x320</cell><cell>26.5</cell><cell>43.3</cell><cell>26.7</cell><cell>10.0M</cell><cell>1.55B</cell></row><row><cell>ReXNet (?1.3) + SSDLite</cell><cell>320x320</cell><cell>27.7</cell><cell>45.1</cell><cell>28.0</cell><cell>8.4M</cell><cell>1.60B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table A5 .</head><label>A5</label><figDesc>ReXNets vs. EfficientNets on COCO object detection.. Note that all the models are trained from scratch with the identical training setup except for the doubled training iterations. We report box APs on val 2017.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use ReLU<ref type="bibr" target="#b39">[40]</ref>, ReLU6<ref type="bibr" target="#b46">[47]</ref>, LeakyReLU<ref type="bibr" target="#b36">[37]</ref>, ELU<ref type="bibr" target="#b7">[8]</ref>, Soft-Plus<ref type="bibr" target="#b11">[12]</ref>, Hard Swish<ref type="bibr" target="#b19">[20]</ref>, and SiLU (Swish-1)<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref>.<ref type="bibr" target="#b1">2</ref> We denote the input and output of an inverted bottleneck as the input of the first 1?1 convolution; the output after the addition operation of the shortcut and the bottleneck, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Actually, many recent lightweight models avoid increasing the channel dimension drastically (i.e., dimension ratio is usually higher than 0.5) at an inverted bottleneck; the models have inverted bottlenecks where the first 1?1 convolution has the expansion ratio 3, 4, or 6.Figure 1atells us the design factors are tenable; the architects may know this empirically, but we believe that we have provided an underlying reason through the study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The models with a single IB cannot be similar in computational complexity with the fixed stem, so the models should contain at least two IBs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">ReXNet (?1.0) has been improved to 78.1% trained by the novel optimizer AdamP<ref type="bibr" target="#b18">[19]</ref> replacing SGD and further improved to 78.4% trained with the new training method ReLabel<ref type="bibr" target="#b58">[59]</ref>, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank NAVER AI Labs members including Sanghyuk Chun, Seong Joon Oh, and Junsuk Choe for fruitful discussions and peer-reviews. We also thank Jung-Woo Ha who suggested the name of the architecture and the NAVER Smart Machine Learning (NSML) <ref type="bibr" target="#b25">[26]</ref> team for support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. ReXNet (plain)</head><p>We now present a new model redesigned based on Mo-bileNetV1 <ref type="bibr" target="#b20">[21]</ref>. We choose MobileNetV1 as another baseline because we intend to show a network architecture without skip connections (so-called a plain network) is able to be redesigned by following the proposed linear parameterization to show performance improvement. We do not change the depth of MobileNetV1. We use the identical configuration at the stem (i.e., 3?3 convolution with BN and ReLU) and the same large expansion layer at the penultimate layer.</p><p>We reassign the output channel dimensions of each 1?1 convolution as we did for ReXNet in ?4.4. Following the investigation of single-layer design, we only replace the Re-LUs with SiLU <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b42">43]</ref> after the expansion layers such as all the 1?1 convolutions. We leave the ReLUs right after each depthwise convolution where the channel dimension ratio is 1. All the other channel dimensions including the stem and the penultimate layer are not changed. Since the network is a plain network, we do not adopt SE <ref type="bibr" target="#b21">[22]</ref>. Our ReXNet (plain) is illustrated in <ref type="figure">Figure A2b</ref>. We provide the ImageNet performance of ReXNet (plain) in <ref type="table">Table A1</ref>. We</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lowrank matrix approximation using point-wise operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farokh</forename><surname>Marvasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="302" to="310" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware. ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fair darts: Eliminating unfair advantages in differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chamnet: Towards efficient network design through platform-aware model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>B?lisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwhan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyuwan</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08217</idno>
		<title level="m">Adamp: Slowing down the slowdown for momentum optimizers on scale-invariant weights</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and? 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the MLaaS platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Do better imagenet models transfer better? In CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2661" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collecting a large-scale dataset of fine-grained cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Fine-Grained Visual Categorization</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kechen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darts+</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06035</idno>
		<title level="m">Improved differentiable architecture search with early stopping</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Darts: Differentiable architecture search. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Atomnas: Finegrained end-to-end neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mixconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A highrank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Re-labeling imagenet: from single to multi-labels, from global to localized labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05022</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. Efficient and accurate approximations of nonlinear convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rexnet</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">74</biblScope>
		</imprint>
	</monogr>
	<note>plain</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Performance of ReXNets. We report the Ima-geNet [46] performances of ReXNets</title>
	</analytic>
	<monogr>
		<title level="m">Table A1</title>
		<imprint/>
	</monogr>
	<note>In addition to Table 5 in the main paper, we provide more models including ReXNet (plain) and ReXNets (?1.1, ?1.2, ?1.4, ?2.2, ?3.0). All the models are trained and evaluated with the resolution 224?224</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
