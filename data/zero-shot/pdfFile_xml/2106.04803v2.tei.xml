<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoAtNet: Marrying Convolution and Attention for All Data Sizes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<email>zihangd@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<email>hanxiaol@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
							<email>tanmingxing@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CoAtNet: Marrying Convolution and Attention for All Data Sizes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced "coat" nets), a family of hybrid models built from two key insights:</p><p>(1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since the breakthrough of AlexNet <ref type="bibr" target="#b0">[1]</ref>, Convolutional Neural Networks (ConvNets) have been the dominating model architecture for computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Meanwhile, with the success of self-attention models like Transformers <ref type="bibr" target="#b5">[6]</ref> in natural language processing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, many previous works have attempted to bring in the power of attention into computer vision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. More recently, Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> has shown that with almost 1 only vanilla Transformer layers, one could obtain reasonable performance on ImageNet-1K <ref type="bibr" target="#b13">[14]</ref> alone. More importantly, when pre-trained on large-scale weakly labeled JFT-300M dataset <ref type="bibr" target="#b14">[15]</ref>, ViT achieves comparable results to state-of-the-art (SOTA) ConvNets, indicating that Transformer models potentially have higher capacity at scale than ConvNets.</p><p>While ViT has shown impressive results with enormous JFT 300M training images, its performance still falls behind ConvNets in the low data regime. For example, without extra JFT-300M pre-training, the ImageNet accuracy of ViT is still significantly lower than ConvNets with comparable model size <ref type="bibr" target="#b4">[5]</ref> (see <ref type="table" target="#tab_0">Table 13</ref>). Subsequent works use special regularization and stronger data augmentation to improve the vanilla ViT <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, yet none of these ViT variants could outperform the SOTA convolution-only models on ImageNet classification given the same amount of data and computation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. This suggests that vanilla Transformer layers may lack certain desirable inductive biases possessed by ConvNets, and thus require significant amount of data and computational resource to compensate. Not surprisingly, many recent works have been trying to incorporate the inductive biases of ConvNets into Transformer models, by imposing local receptive fields for attention layers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> or augmenting the attention and FFN layers with implicit or explicit convolutional operations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. However, these approaches are either ad-hoc or focused on injecting a particular property, lacking a systematic understanding of the respective roles of convolution and attention when combined.</p><p>In this work, we systematically study the problem of hybridizing convolution and attention from two fundamental aspects in machine learning -generalization and model capacity. Our study shows that convolutional layers tend to have better generalization with faster converging speed thanks to their strong prior of inductive bias, while attention layers have higher model capacity that can benefit from larger datasets. Combining convolutional and attention layers can achieve better generalization and capacity; however, a key challenge here is how to effectively combine them to achieve better trade-offs between accuracy and efficiency. In this paper, we investigate two key insights: First, we observe that the commonly used depthwise convolution can be effectively merged into attention layers with simple relative attention; Second, simply stacking convolutional and attention layers, in a proper way, could be surprisingly effective to achieve better generalization and capacity. Based on these insights, we propose a simple yet effective network architecture named CoAtNet, which enjoys the strengths from both ConvNets and Transformers.</p><p>Our CoAtNet achieves SOTA performances under comparable resource constraints across different data sizes. Specifically, under the low-data regime, CoAtNet inherits the great generalization property of ConvNets thanks to the favorable inductive biases. Moreover, given abundant data, CoAtNet not only enjoys the superior scalability of Transformer models, but also achieves faster convergence and thus improved efficiency. When only ImageNet-1K is used for training, CoAtNet achieves 86.0% top-1 accuracy, matching the prior art NFNet <ref type="bibr" target="#b19">[20]</ref> under similar computation resource and training conditions. Further, when pre-trained on ImageNet-21K with about 10M images, CoAtNet reaches 88.56% top-1 accuracy when finetuned on ImageNet-1K, matching the ViT-Huge pre-trained on JFT-300M, a 23? larger dataset. Finally, when JFT-3B is used for pre-training, CoAtNet exhibits better efficiency compared to ViT, and pushes the ImageNet-1K top-1 accuracy to 90.88% while using 1.5x less computation of the prior art set by ViT-G/14 <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In the section, we focus on the question of how to "optimally" combine the convolution and transformer. Roughly speaking, we decompose the question into two parts:</p><p>1. How to combine the convolution and self-attention within one basic computational block?</p><p>2. How to vertically stack different types of computational blocks together to form a complete network?</p><p>The rationale of the decomposition will become clearer as we gradually reveal our design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Merging Convolution and Self-Attention</head><p>For convolution, we mainly focus on the MBConv block <ref type="bibr" target="#b26">[27]</ref> which employs depthwise convolution <ref type="bibr" target="#b27">[28]</ref> to capture the spatial interaction. A key reason of this choice is that both the FFN module in Transformer and MBConv employ the design of "inverted bottleneck", which first expands the channel size of the input by 4x and later project the the 4x-wide hidden state back to the original channel size to enable residual connection.</p><p>Besides the similarity of inverted bottleneck, we also notice that both depthwise convolution and self-attention can be expressed as a per-dimension weighted sum of values in a pre-defined receptive field. Specifically, convolution relies on a fixed kernel to gather information from a local receptive field</p><formula xml:id="formula_0">y i = j?L(i) w i?j x j (depthwise convolution),<label>(1)</label></formula><p>where x i , y i ? R D are the input and output at position i respectively, and L(i) denotes a local neighborhood of i, e.g., a 3x3 grid centered at i in image processing.</p><p>In comparison, self-attention allows the receptive field to be the entire spatial locations and computes the weights based on the re-normalized pairwise similarity between the pair (x i , x j ): 2</p><formula xml:id="formula_1">y i = j?G exp x i x j k?G exp x i x k Ai,j x j (self-attention),<label>(2)</label></formula><p>where G indicates the global spatial space. Before getting into the question of how to best combine them, it is worthwhile to compare their relative strengths and weaknesses, which helps to figure out the good properties we hope to retain.</p><p>? First of all, the depthwise convolution kernel w i?j is an input-independent parameter of static value, while the attention weight A i,j dynamically depends on the representation of the input. Hence, it is much easier for the self-attention to capture complicated relational interactions between different spatial positions, a property that we desire most when processing high-level concepts. However, the flexibility comes with a risk of easier overfitting, especially when data is limited. ? Secondly, notice that given any position pair (i, j), the corresponding convolution weight w i?j only cares about the relative shift between them, i.e. i ? j, rather than the specific values of i or j. This property is often referred to translation equivalence, which has been found to improve generalization under datasets of limited size <ref type="bibr" target="#b28">[29]</ref>. Due to the usage of absolution positional embeddings, standard Transformer (ViT) lacks this property. This partially explains why ConvNets are usually better than Transformers when the dataset is not enormously large. ? Finally, the size of the receptive field is one of the most crucial differences between self-attention and convolution. Generally speaking, a larger receptive field provides more contextual information, which could lead to higher model capacity. Hence, the global receptive field has been a key motivation to employ self-attention in vision. However, a large receptive field requires significantly more computation. In the case of global attention, the complexity is quadratic w.r.t. spatial size, which has been a fundamental trade-off in applying self-attention models. (2), a straightforward idea that could achieve this is simply to sum a global static convolution kernel with the adaptive attention matrix, either after or before the Softmax normalization, i.e.,</p><formula xml:id="formula_2">y post i = j?G exp x i x j k?G exp x i x k + w i?j x j or y pre i = j?G exp x i x j + w i?j k?G exp x i x k + w i?k x j . (3)</formula><p>Interestingly, while the idea seems overly simplified, the pre-normalization version y pre corresponds to a particular variant of relative self-attention <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. In this case, the attention weight A i,j is decided jointly by the w i?j of translation equivariance and the input-adaptive x i x j , which can enjoy both effects depending on their relative magnitudes. Importantly, note that in order to enable the global convolution kernel without blowing up the number of parameters, we have reloaded the notation of w i?j as a scalar (i.e., w ? R O(|G|) ) rather than a vector in Eqn. <ref type="bibr" target="#b0">(1)</ref>. Another advantage of the scalar formulation of w is that retrieving w i?j for all (i, j) is clearly subsumed by computing the pairwise dot-product attention, hence resulting in minimum additional cost (see Appendix A.1). Given the benefits, we will use the Transformer block with the pre-normalization relative attention variant in Eqn. (3) as the key component of the proposed CoAtNet model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vertical Layout Design</head><p>After figuring out a neat way to combine convolution and attention, we next consider how to utilize it to stack an entire network.</p><p>As we have discuss above, the global context has a quadratic complexity w.r.t. the spatial size. Hence, if we directly apply the relative attention in Eqn.</p><p>(3) to the raw image input, the computation will be excessively slow due to the large number of pixels in any image of common sizes. Hence, to construct a network that is feasible in practice, we have mainly three options:</p><p>(A) Perform some down-sampling to reduce the spatial size and employ the global relative attention after the feature map reaches manageable level. (B) Enforce local attention, which restricts the global receptive field G in attention to a local field L just like in convolution <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>. (C) Replace the quadratic Softmax attention with certain linear attention variant which only has a linear complexity w.r.t. the spatial size <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>We briefly experimented with option (C) without getting a reasonably good result. For option (B), we found that implementing local attention involves many non-trivial shape formatting operations that requires intensive memory access. On our accelerator of choice (TPU), such operation turns out to be extremely slow <ref type="bibr" target="#b33">[34]</ref>, which not only defeats the original purpose of speeding up global attention, but also hurts the model capacity. Hence, as some recent work has studied this variant <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, we will focus on option (A) and compare our results with theirs in our empirical study (Section 4).</p><p>For option (A), the down-sampling can be achieved by either (1) a convolution stem with aggressive stride (e.g., stride 16x16) as in ViT or (2) a multi-stage network with gradual pooling as in ConvNets.</p><p>With these choices, we derive a search space of 5 variants and compare them in controlled experiments.</p><p>? When the ViT Stem is used, we directly stack L Transformer blocks with relative attention, which we denote as VIT REL . ? When the multi-stage layout is used, we mimic ConvNets to construct a network of 5 stages (S0, S1, S2, S3 &amp; S4), with spatial resolution gradually decreased from S0 to S4. At the beginning of each stage, we always reduce the spatial size by 2x and increase the number of channels (see Appendix A.1 for the detailed down-sampling implementation). The first stage S0 is a simple 2-layer convolutional Stem and S1 always employs MBConv blocks with squeeze-excitation (SE), as the spatial size is too large for global attention. Starting from S2 through S4, we consider either the MBConv or the Transformer block, with a constraint that convolution stages must appear before Transformer stages. The constraint is based on the prior that convolution is better at processing local patterns that are more common in early stages. This leads to 4 variants with increasingly more Transformer stages, C-C-C-C, C-C-C-T, C-C-T-T and C-T-T-T, where C and T denote Convolution and Transformer respectively.</p><p>To systematically study the design choices, we consider two fundamental aspects generalization capability and model capacity: For generalization, we are interested in the gap between the training loss and the evaluation accuracy. If two models have the same training loss, then the model with higher evaluation accuracy has better generalization capability, since it can generalize better to unseen evaluation dataset. Generalization capability is particularly important to data efficiency when training data size is limited. For model capacity, we measure the ability to fit large training datasets. When training data is abundant and overfitting is not an issue, the model with higher capacity will achieve better final performance after reasonable training steps. Note that, since simply increasing the model size can lead to higher model capacity, to perform a meaningful comparison, we make sure the model sizes of the 5 variants are comparable.</p><p>To compare the generalization and model capacity, we train different variants of hybrid models on ImageNet-1K (1.3M) and JFT (&gt;300M) dataset for 300 and 3 epochs respectively, both without any regularization or augmentation. The training loss and evaluation accuracy on both datasets are summarized in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>? From the ImageNet-1K results, a key observation is that, in terms of generalization capability (i.e., gap between train and evaluation metrics), we have  Particularly, VIT REL is significantly worse than variants by a large margin, which we conjecture is related to the lack of proper low-level information processing in its aggressive down-sampling Stem. Among the multi-stage variants, the overall trend is that the more convolution stages the model has, the smaller the generalization gap is. ? As for model capacity, from the JFT comparison, both the train and evaluation metrics at the end of the training suggest the following ranking:</p><formula xml:id="formula_3">C-C-C-C ? C-C-C-T ? C-C-T-T &gt; C-T-T-T VIT REL .</formula><formula xml:id="formula_4">C-C-T-T ? C-T-T-T &gt; VIT REL &gt; C-C-C-T &gt; C-C-C-C.</formula><p>Importantly, this suggests that simply having more Transformer blocks does NOT necessarily mean higher capacity for visual processing. On one hand, while initially worse, VIT REL ultimately catch up with the two variants with more MBConv stages, indicating the capacity advantage of Transformer blocks. On the other hand, both C-C-T-T and C-T-T-T clearly outperforming VIT REL suggest that the ViT stem with an aggressive stride may have lost too much information and hence limit the model capacity. More interestingly, the fact that C-C-T-T ? C-T-T-T indicates the for processing low-level information, static local operations like convolution could be as capable as adaptive global attention mechanism, while saving computation and memory usage substantially.</p><p>Finally, to decide between C-C-T-T and C-T-T-T, we conduct another transferability test 3 -we finetune the two JFT pre-trained models above on ImageNet-1K for 30 epochs and compare their transfer performances. From <ref type="table" target="#tab_2">Table 2</ref>, it turns out that C-C-T-T achieves a clearly better transfer accuracy than C-T-T-T, despite the same pre-training performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Convolutional network building blocks. Convolutional Networks (ConvNets) have been the dominating neural architectures for many computer vision tasks. Traditionally, regular convolutions, such as ResNet blocks <ref type="bibr" target="#b2">[3]</ref>, are popular in large-scale ConvNets; in contrast, depthwise convolutions <ref type="bibr" target="#b27">[28]</ref> are popular in mobile platforms due to its lower computational cost and smaller parameter size <ref type="bibr" target="#b26">[27]</ref>.</p><p>Recent works show that an improved inverted residual bottlenecks (MBConv <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>), which is built upon depthwise convolutions, can achieve both high accuracy and better efficiency <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. As discussed in Section 2, due to the strong connection between MBConv and Transformer blocks , this paper mostly employs MBConv as convolution building blocks.</p><p>Self-attention and Transformers. With the key ingredients of self-attention, Transformers have been widely adopted for neural language processing and speech understanding. As an early work, stand-alone self-attention network <ref type="bibr" target="#b33">[34]</ref> shows self-attention alone can work well for different vision tasks, though with some practical difficulties. Recently, ViT <ref type="bibr" target="#b12">[13]</ref> applies a vanilla Transformer to ImageNet classification, and achieves impressive results after pre-training on a large-scale JFT dataset. However, ViT still largely lags behind state-of-the-art ConvNets when training data is limited. Following that, many recent works have been focused on improving vision Transformers for data efficiency and model efficiency. For a more comprehensive review of vision Transformers, we refer readers to the dedicated surveys <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Relative attention. Under the general name of relative attention, there have been various variants in literature <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31]</ref>. Generally speaking, we can separate them into two categories: (a) the input-dependent version where the extra relative attention score is a function of the input states f (x i , x j , i ? j), and (b) the input-independent version f (i ? j). The variant in CoAtNet belongs to the input-independent version, and is similar to the one used in T5 <ref type="bibr" target="#b30">[31]</ref>, but unlike T5, we neither share the relative attention parameters across layers nor use the bucketing mechanism. As a benefit of the input independence, obtaining f (i ? j) for all (i, j) pairs is computationally much cheaper than the input-dependent version on TPU. In addition, at inference time, this only needs to be computed once and cached for future use. A recent work <ref type="bibr" target="#b21">[22]</ref> also utilizes such an input-independent parameterization, but it restricts the receptive field to a local window.</p><p>Combining convolution and self-attention. The idea of combining convolution and self-attention for vision recognition is not new. A common approach is to augment the ConvNet backbone with explicit self-attention or non-local modules <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, or to replace certain convolution layers with standard self-attention <ref type="bibr" target="#b10">[11]</ref> or a more flexible mix of linear attention and convolution <ref type="bibr" target="#b40">[41]</ref>. While self-attention usually improves the accuracy, they often come with extra computational cost and hence are often regarded as an add-on to the ConvNets, similar to squeeze-and-excitation <ref type="bibr" target="#b41">[42]</ref> module. In comparison, after the success of ViT and ResNet-ViT <ref type="bibr" target="#b12">[13]</ref>, another popular line of research starts with a Transformer backbone and tries to incorporate explicit convolution or some desirable properties of convolution into the Transformer backbone <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>While our work also belongs to this category, we show that our relative attention instantiation is a natural mixture of depthwise convolution and content-based attention with minimum additional cost. More importantly, starting from the perspectives of generalization and model capacity, we take a systematic approach to the vertical layout design and show how and why different network stages prefer different types of layers. Therefore, compared to models that simply use an off-the-shelf ConvNet as the stem layer, such as ResNet-ViT <ref type="bibr" target="#b12">[13]</ref>, CoAtNet also scales the Convolution stage (S2) when the overall size increases. On the other hand, compared to models employing local attention <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref>, CoAtNet consistently uses full attention for S3 &amp; S4 to ensure the model capacity, as S3 occupies the majority of the computation and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we compare CoAtNet with previous results under comparable settings. For completeness, all the hyper-parameters not mentioned here are included in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>CoAtNet model family. To compare with existing models of different sizes, we also design a family of CoAtNet models as summarized in <ref type="table">Table 3</ref>. Overall, we always double the number of channels from S1 to S4, while ensuring the width of the Stem S0 to be smaller or equal to that of S1. Also, for simplicity, when increasing the depth of the network, we only scale the number of blocks in S2 and S3.</p><p>Evaluation Protocol. Our experiments focus on image classification. To evaluate the performance of the model across different data sizes, we utilize three datasets of increasingly larger sizes, namely ImageNet-1K (1.28M images), ImageNet-21K (12.7M images) and JFT (300M images). Following previous works, we first pre-train our models on each of the three datasets at resolution 224 for 300, 90 and 14 epochs respectively. Then, we finetune the pre-trained models on ImageNet-1K at the desired <ref type="table">Table 3</ref>: L denotes the number of blocks and D denotes the hidden dimension (#channels). For all Conv and MBConv blocks, we always use the kernel size 3. For all Transformer blocks, we set the size of each attention head to 32, following <ref type="bibr" target="#b21">[22]</ref>. The expansion rate for the inverted bottleneck is always 4 and the expansion (shrink) rate for the SE is always 0.25.</p><formula xml:id="formula_5">Stages Size CoAtNet-0 CoAtNet-1 CoAtNet-2 CoAtNet-3 CoAtNet-4 S0-Conv 1 /2 L=2 D=64 L=2 D=64 L=2 D=128 L=2 D=192 L=2 D=192 S1-MbConv 1 /4 L=2 D=96 L=2 D=96 L=2 D=128 L=2 D=192 L=2 D=192 S2-MBConv 1 /8 L=3 D=192 L=6 D=192 L=6 D=256 L=6 D=384 L=12 D=384 S3-TFMRel 1 /16 L=5 D=384 L=14 D=384 L=14 D=512 L=14 D=768 L=28 D=768 S4-TFMRel 1 /32 L=2 D=768 L=2 D=768 L=2 D=1024 L=2 D=1536 L=2 D=1536</formula><p>resolutions for 30 epochs and obtain the corresponding evaluation accuracy. One exception is the ImageNet-1K performance at resolution 224, which can be directly obtained at the end of pre-training. Note that similar to other models utilizing Transformer blocks, directly evaluating models pre-trained on ImageNet-1K at a larger resolution without finetuning usually leads to performance drop. Hence, finetuning is always employed whenever input resolution changes.</p><p>Data Augmentation &amp; Regularization. In this work, we only consider two widely used data augmentations, namely RandAugment <ref type="bibr" target="#b44">[45]</ref> and MixUp <ref type="bibr" target="#b45">[46]</ref>, and three common techniques, including stochastic depth <ref type="bibr" target="#b46">[47]</ref>, label smoothing <ref type="bibr" target="#b47">[48]</ref> and weight decay <ref type="bibr" target="#b48">[49]</ref>, to regularize the model. Intuitively, the specific hyper-parameters of the augmentation and regularization methods depend on model size and data scale, where strong regularization is usually applied for larger models and smaller dataset.</p><p>Under the general principle, a complication under the current paradigm is how to adjust the regularization for pre-training and finetuning as data size can change. Specifically, we have an interesting observation that if a certain type of augmentation is entirely disabled during pre-training, simply turning it on during fine-tuning would most likely harm the performance rather than improving. We conjecture this could be related to data distribution shift. As a result, for certain runs of the proposed model, we deliberately apply RandAugment and stochastic depth of a small degree when pre-training on the two larger datasets, ImageNet21-K and JFT. Although such regularization can harm the pre-training metrics, this allows more versatile regularization and augmentation during finetuning, leading to improved down-stream performances.    ImageNet-1K The experiment results with only the ImageNet-1K dataset are shown in <ref type="table" target="#tab_3">Table 4</ref>. Under similar conditions, the proposed CoAtNet models not only outperform ViT variants, but also match the best convolution-only architectures, i.e., EfficientNet-V2 and NFNets. Additionally, we also visualize the all results at resolution 224x224 in <ref type="figure" target="#fig_2">Fig. 2</ref>. As we can see, CoAtNet scales much better than previous model with attention modules. ImageNet-21K As we can see from <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure" target="#fig_4">Fig. 3</ref> Moreover, as we further push the training resource towards the level used by ViT-G/14 and utilize the same JFT-3B dataset of an even larger size <ref type="bibr" target="#b25">[26]</ref>, with over 4x less computation, CoAtNet-6 is able to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this section, we will ablate our design choices for CoAtNet.</p><p>Firstly, we study the importance of the relative attention from combining convolution and attention into a single computation unit. Specifically, we compare two models, one with the relative attention and the other without, under both the ImageNet-1K alone and ImageNet-21K transfer setting. As we can see from <ref type="table" target="#tab_6">Table 6</ref>, when only the ImageNet-1K is used, relative attention clearly outperforms the standard attention, indicating a better generalization. In addition, under the ImageNet-21K transfer setting, the relative attention variant achieves a substantially better transfer accuracy, despite their very close pre-training performances. This suggests the main advantage of relative attention in visual processing is not in higher capacity but in better generalization.  Secondly, as S2 with MBConv blocks and S3 with relative Transformer blocks occupy most of the computation of the CoAtNet, a question to ask is how to split the computation between S2 (MBConv) and S3 (Transformer) to achieve a good performance. In practice, it boils down to deciding the number of blocks to have in each stage, which we will refer to as "layout" design. For this purpose, we compare a few different layouts that we experimented with in <ref type="table" target="#tab_7">Table 7</ref>. ? If we keep the total number of blocks in S2 and S3 fixed and vary the number in each stage, we observe that V0 is a sweet spot between V1 and V2. Basically, having more Transformer blocks in S3 generally leads to better performance until the number of MBConv blocks in S2 is too small to generalize well. ? To further evaluate whether the sweet spot also holds in the transfer setting, where a higher capacity is often regarded more important, we further compare V0 and V1 under the ImageNet-21K transferring to ImageNet-1K setup. Interestingly, despite that V1 and V0 have the same performance during ImageNet-21K pre-training, the transfer accuracy of V1 clearly falls behind V0. Again, this suggests the importance of convolution in achieving good transferability and generalization.</p><p>Lastly, we study two choices of model details, namely the dimension of each attention (default to 32) head as well as the type of normalization (default to BatchNorm) used in MBConv blocks. From <ref type="table" target="#tab_8">Table 8</ref>, we can see increasing head size from 32 to 64 can slightly hurt performance, though it actually improves the TPU speed by a significant amount. In practice, this will be a quality-speed trade-off one can make. On the other hand, BatchNorm and LayerNorm have almost the same performance, while BatchNorm is 10 -20% faster on TPU depending on the per-core batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we systematically study the properties of convolutions and Transformers, which leads to a principled way to combine them into a new family of models named CoAtNet. Extensive experiments show that CoAtNet enjoys both good generalization like ConvNets and superior model capacity like Transformers, achieving state-of-the-art performances under different data sizes and computation budgets.</p><p>Note that this paper currently focuses on ImageNet classification for model development. However, we believe our approach is applicable to broader applications like object detection and semantic segmentation. We will leave them for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Details</head><p>First of all, the overview of CoAtNet is illustrated in <ref type="figure">Fig. 4</ref>.</p><p>S2: Repeat L 2 times ( 28 x 28 ) S1: Repeat L 1 times ( 56 x 56 ) Pre-Activation To promote homogeneity in the model architecture, we consistently use preactivation structure <ref type="bibr" target="#b49">[50]</ref> for both the MBConv and the Transformer block, i.e.,</p><p>x ? x + Module (Norm(x)) ,</p><p>where Module denotes the MBConv, Self-Attention or FFN module, while Norm corresponds to BatchNorm for MBConv and LayerNorm for Self-Attention and FFN. We have experimented with using LayerNorm in the MBConv block, which achieves the same performance while being significantly slower on our accelerator (TPU). In general, we recommend whichever is faster on your device. Following the same spirit, Gaussian Error Linear Units (GELUs) <ref type="bibr" target="#b50">[51]</ref> is used as the activation function in both the MBConv blocks and Transformer blocks.</p><p>Down-Sampling For the first block inside each stage from S1 to S4, down-sampling is performed independently for the residual branch and the identity branch. Specifically, for the Transformer block, the standard max pooling of stride 2 is directly applied to the input states of both branches of the self-attention module, similar to Funnel Transformer <ref type="bibr" target="#b51">[52]</ref>. Also, a channel projection is applied to the identity branch to enlarge the hidden size. Hence, the down-sampling self-attention module can be expressed as x ? Proj(Pool(x)) + Attention (Pool(Norm(x))) .</p><p>As for the MBConv block, the down-sampling in the residual branch is instead achieved by using a stride-2 convolution to the normalized inputs, i.e.,</p><p>x ? Proj(Pool(x)) + Conv (DepthConv (Conv (Norm(x), stride = 2)))) .</p><p>This is different from the standard MBConv where the down-sampling is done by applying stride-2 depthwise convolution to the inverted bottleneck hidden states. We later found using stride-2 depthwise convolution is helpful but slower when model is small but not so much when model scales, as shown in <ref type="table" target="#tab_10">Table 9</ref>. Hence, if not mentioned otherwise, numbers reported in the main text uses the down-sampling implementation in Eqn. <ref type="bibr" target="#b4">(5)</ref>. In practice, this could be yet another quality-speed trade-off one can tweak for smaller models. Classification head Instead of adding an additional &lt;cls&gt; token as in ViT to perform classification, we apply global average pooling to the last-stage output to get the representation for simplicity. The hyper-parameters used for the main experiments presented in Section 4 are summarized in <ref type="table" target="#tab_0">Table  10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyper-Parameters</head><p>The model size of CoAtNet-5 used in the JFT experiment is summarized in <ref type="table" target="#tab_0">Table 11</ref>. Different from the standard CoAtNet models in <ref type="table">Table 3</ref>, we set the size of each attention head to 64 rather than 32 for CoAtNet-5, as this achieves a better speed-performance trade-off as discussed in Section 4.3. For CoAtNet-6 and CoAtNet-7, to reduce the memory consumption, we move 2 /3 of the MBConv blocks of S2 into S3 and double its hidden dimension. While this modification does not change the complexity in terms of FLOPs, this will reduce the activation related memory usage of these MBConv blocks by half, which enables us to build a larger model. With this adjustment, the S3 becomes a stage of mixed block types and hidden dimensions. In addition, we increase the attention head size to 128 further to boost the speed-performance trade-off. The specific sizes are summarized in <ref type="table" target="#tab_0">Table 12</ref>. Basically, CoAtNet-6 and CoAtNet-7 share the same depth but differ in width.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparison for model generalization and capacity under different data size. For fair comparison, all models have similar parameter size and computational cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy-to-FLOPs scaling curve under ImageNet-1K only setting at 224x224.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(ViT-H/14 JFT Pre-train) 88.56</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy-to-Params scaling curve under ImageNet-21K ? ImageNet-1K setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Desirable properties found in convolution or self-attention.</figDesc><table><row><cell>Properties</cell><cell>Convolution Self-Attention</cell></row><row><cell>Translation Equivariance</cell><cell></cell></row><row><cell>Input-adaptive Weighting</cell><cell></cell></row><row><cell>Global Receptive Field</cell><cell></cell></row><row><cell cols="2">Given the comparison above, an ideal model should be able to combine the 3 desirable properties in</cell></row><row><cell cols="2">Table 1. With the similar form of depthwise convolution in Eqn. (1) and self-attention in Eqn.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Transferability test results.Taking generalization, model capacity, transferability and efficiency into consideration, we adapt the C-C-T-T multi-stage layout for CoAtNet. More model details are included in Appendix A.1.</figDesc><table><row><cell>Metric</cell><cell cols="2">C-C-T-T C-T-T-T</cell></row><row><cell>Pre-training Precision@1 (JFT)</cell><cell>34.40</cell><cell>34.36</cell></row><row><cell>Transfer Accuracy 224x224</cell><cell>82.39</cell><cell>81.78</cell></row><row><cell>Transfer Accuracy 384x384</cell><cell>84.23</cell><cell>84.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Model performance on ImageNet. 1K only denotes training on ImageNet-1K only; 21K+1K denotes pre-training on ImageNet-21K and finetuning on ImageNet-1K; PT-RA denotes applying RandAugment during 21K pre-training, and E150 means 150 epochs of 21K pre-training, which is longer than the standard 90 epochs. More results are in Appendix A.3.</figDesc><table><row><cell cols="2">Models</cell><cell cols="5">Eval Size #Params #FLOPs ImageNet Top-1 Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1K only</cell><cell>21K+1K</cell></row><row><cell></cell><cell>EfficientNet-B7</cell><cell>600 2</cell><cell>66M</cell><cell>37B</cell><cell>84.7</cell><cell>-</cell></row><row><cell>Conv Only</cell><cell>EfficientNetV2-L</cell><cell>480 2</cell><cell>121M</cell><cell>53B</cell><cell>85.7</cell><cell>86.8</cell></row><row><cell></cell><cell>NFNet-F3</cell><cell>416 2</cell><cell>255M</cell><cell>114.8B</cell><cell>85.7</cell><cell>-</cell></row><row><cell></cell><cell>NFNet-F5</cell><cell>544 2</cell><cell>377M</cell><cell>289.8B</cell><cell>86.0</cell><cell>-</cell></row><row><cell></cell><cell>DeiT-B</cell><cell>384 2</cell><cell>86M</cell><cell>55.4B</cell><cell>83.1</cell><cell>-</cell></row><row><cell>ViT-Stem TFM</cell><cell>ViT-L/16 CaiT-S-36</cell><cell>384 2 384 2</cell><cell>304M 68M</cell><cell>190.7B 48.0B</cell><cell>-85.0</cell><cell>85.3 -</cell></row><row><cell></cell><cell>DeepViT-L</cell><cell>224 2</cell><cell>55M</cell><cell>12.5B</cell><cell>83.1</cell><cell>-</cell></row><row><cell></cell><cell>Swin-B</cell><cell>384 2</cell><cell>88M</cell><cell>47.0B</cell><cell>84.2</cell><cell>86.0</cell></row><row><cell>Multi-stage TFM</cell><cell>Swin-L</cell><cell>384 2</cell><cell>197M</cell><cell>103.9B</cell><cell>-</cell><cell>86.4</cell></row><row><cell></cell><cell>BotNet-T7</cell><cell>384 2</cell><cell>75.1M</cell><cell>45.8B</cell><cell>84.7</cell><cell>-</cell></row><row><cell></cell><cell>LambdaResNet-420</cell><cell>320 2</cell><cell>-</cell><cell>-</cell><cell>84.8</cell><cell>-</cell></row><row><cell>Conv+TFM</cell><cell>T2T-ViT-24</cell><cell>224 2</cell><cell>64.1M</cell><cell>15.0B</cell><cell>82.6</cell><cell>-</cell></row><row><cell></cell><cell>CvT-21</cell><cell>384 2</cell><cell>32M</cell><cell>24.9B</cell><cell>83.3</cell><cell>-</cell></row><row><cell></cell><cell>CvT-W24</cell><cell>384 2</cell><cell>277M</cell><cell>193.2B</cell><cell>-</cell><cell>87.7</cell></row><row><cell></cell><cell>CoAtNet-0</cell><cell>224 2</cell><cell>25M</cell><cell>4.2B</cell><cell>81.6</cell><cell>-</cell></row><row><cell></cell><cell>CoAtNet-1</cell><cell>224 2</cell><cell>42M</cell><cell>8.4B</cell><cell>83.3</cell><cell>-</cell></row><row><cell></cell><cell>CoAtNet-2</cell><cell>224 2</cell><cell>75M</cell><cell>15.7B</cell><cell>84.1</cell><cell>87.1</cell></row><row><cell></cell><cell>CoAtNet-3</cell><cell>224 2</cell><cell>168M</cell><cell>34.7B</cell><cell>84.5</cell><cell>87.6</cell></row><row><cell></cell><cell>CoAtNet-0</cell><cell>384 2</cell><cell>25M</cell><cell>13.4B</cell><cell>83.9</cell><cell>-</cell></row><row><cell></cell><cell>CoAtNet-1</cell><cell>384 2</cell><cell>42M</cell><cell>27.4B</cell><cell>85.1</cell><cell>-</cell></row><row><cell>Conv+TFM</cell><cell>CoAtNet-2</cell><cell>384 2</cell><cell>75M</cell><cell>49.8B</cell><cell>85.7</cell><cell>87.1</cell></row><row><cell>(ours)</cell><cell>CoAtNet-3</cell><cell>384 2</cell><cell>168M</cell><cell>107.4B</cell><cell>85.8</cell><cell>87.6</cell></row><row><cell></cell><cell>CoAtNet-4</cell><cell>384 2</cell><cell>275M</cell><cell>189.5B</cell><cell>-</cell><cell>87.9</cell></row><row><cell></cell><cell>+ PT-RA</cell><cell>384 2</cell><cell>275M</cell><cell>189.5B</cell><cell>-</cell><cell>88.3</cell></row><row><cell></cell><cell>+ PT-RA-E150</cell><cell>384 2</cell><cell>275M</cell><cell>189.5B</cell><cell>-</cell><cell>88.4</cell></row><row><cell></cell><cell>CoAtNet-2</cell><cell>512 2</cell><cell>75M</cell><cell>96.7B</cell><cell>85.9</cell><cell>87.3</cell></row><row><cell></cell><cell>CoAtNet-3</cell><cell>512 2</cell><cell>168M</cell><cell>203.1B</cell><cell>86.0</cell><cell>87.9</cell></row><row><cell></cell><cell>CoAtNet-4</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>-</cell><cell>88.1</cell></row><row><cell></cell><cell>+ PT-RA</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>-</cell><cell>88.4</cell></row><row><cell></cell><cell>+ PT-RA-E150</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>-</cell><cell>88.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, when ImageNet-21K is used for pretraining, the advantage of CoAtNet becomes more obvious, substantially outperforming all previous models. Notably, the best CoAtNet variant achieves a top-1 accuracy of 88.56%, matching the ViT-H/14 performance of 88.55%, which requires pre-training the 2.3x larger ViT model on a 23x larger proprietary weakly labeled dataset (JFT) for 2.2x more steps. This marks a dramatic improvement in both data efficiency and computation efficiency.JFT Finally, inTable 5, we further evaluate CoAtNet under the large-scale data regime with JFT-300M and JFT-3B. Encouragingly, our CoAtNet-4 can almost match the best previous performance with JFT-300M set by NFNet-F4+, while being 2x more efficient in terms of both TPU training</figDesc><table /><note>time and parameter count. When we scale up the model to consume similar training resource as NFNet-F4+, CoAtNet-5 reaches 89.77% on top-1 accuracy, outperforming previous results under comparable settings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance Comparison on large-scale JFT dataset. TPUv3-core-days denotes the pretraining time, Top-1 Accuracy denotes the finetuned accuracy on ImageNet. Note that the last 3 rows use a larger dataset JFT-3B<ref type="bibr" target="#b25">[26]</ref> for pre-training, while others use JFT-300M<ref type="bibr" target="#b14">[15]</ref>. See Appendix A.2 for the size details of CoAtNet-5/6/7. ? : Down-sampling in the MBConv block is achieved by stride-2 Depthwise Convolution. : ViT-G/14 computation consumption is read fromFig. 1of the paper<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell>Models</cell><cell cols="5">Eval Size #Params #FLOPs TPUv3-core-days Top-1 Accuracy</cell></row><row><cell>ResNet + ViT-L/16</cell><cell>384 2</cell><cell>330M</cell><cell>-</cell><cell>-</cell><cell>87.12</cell></row><row><cell>ViT-L/16</cell><cell>512 2</cell><cell>307M</cell><cell>364B</cell><cell>0.68K</cell><cell>87.76</cell></row><row><cell>ViT-H/14</cell><cell>518 2</cell><cell>632M</cell><cell>1021B</cell><cell>2.5K</cell><cell>88.55</cell></row><row><cell>NFNet-F4+</cell><cell>512 2</cell><cell>527M</cell><cell>367B</cell><cell>1.86K</cell><cell>89.2</cell></row><row><cell>CoAtNet-3  ?</cell><cell>384 2</cell><cell>168M</cell><cell>114B</cell><cell>0.58K</cell><cell>88.52</cell></row><row><cell>CoAtNet-3  ?</cell><cell>512 2</cell><cell>168M</cell><cell>214B</cell><cell>0.58K</cell><cell>88.81</cell></row><row><cell>CoAtNet-4</cell><cell>512 2</cell><cell>275M</cell><cell>361B</cell><cell>0.95K</cell><cell>89.11</cell></row><row><cell>CoAtNet-5</cell><cell>512 2</cell><cell>688M</cell><cell>812B</cell><cell>1.82K</cell><cell>89.77</cell></row><row><cell>ViT-G/14</cell><cell>518 2</cell><cell>1.84B</cell><cell>5160B</cell><cell>&gt;30K</cell><cell>90.45</cell></row><row><cell>CoAtNet-6</cell><cell>512 2</cell><cell>1.47B</cell><cell>1521B</cell><cell>6.6K</cell><cell>90.45</cell></row><row><cell>CoAtNet-7</cell><cell>512 2</cell><cell>2.44B</cell><cell>2586B</cell><cell>20.1K</cell><cell>90.88</cell></row><row><cell cols="6">match the performance of ViT-G/14 of 90.45%, and with 1.5x less computation, CoAtNet-7 achieves</cell></row><row><cell cols="5">89.77% on top-1 accuracy 90.88%, achieving the new state-of-the-art performance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation on relative attention.</figDesc><table><row><cell>Setting</cell><cell>Metric</cell><cell cols="2">With Rel-Attn Without Rel-Attn</cell></row><row><cell>ImageNet-1K</cell><cell>Accuracy (224 2 ) Accuracy (384 2 )</cell><cell>84.1 85.7</cell><cell>83.8 85.3</cell></row><row><cell>ImageNet-21K</cell><cell>Pre-train Precision@1 (224 2 )</cell><cell>53.0</cell><cell>52.8</cell></row><row><cell>? ImageNet-1K</cell><cell>Finetune Accuracy (384 2 )</cell><cell>87.9</cell><cell>87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Ablation on architecture layout.</figDesc><table><row><cell>Setting</cell><cell>Models</cell><cell>Layout</cell><cell>Top-1 Accuracy</cell></row><row><cell></cell><cell cols="2">V0: CoAtNet-2 [2, 2, 6, 14, 2]</cell><cell>84.1</cell></row><row><cell>ImageNet-1K</cell><cell>V1: S2 ? S3</cell><cell>[2, 2, 2, 18, 2]</cell><cell>83.4</cell></row><row><cell></cell><cell>V2: S2 ? S3</cell><cell>[2, 2, 8, 12, 2]</cell><cell>84.0</cell></row><row><cell>ImageNet-21K</cell><cell cols="2">V0: CoAtNet-3 [2, 2, 6, 14, 2]</cell><cell>53.0 ? 87.6</cell></row><row><cell>? ImageNet-1K</cell><cell>V1: S2 ? S3</cell><cell>[2, 2, 2, 18, 2]</cell><cell>53.0 ? 87.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation on head size and normalization type.</figDesc><table><row><cell>Setting</cell><cell>Models</cell><cell cols="2">Image Size Top-1 Accuracy</cell></row><row><cell></cell><cell>CoAtNet-2</cell><cell>224 2</cell><cell>84.1</cell></row><row><cell>ImageNet-1K</cell><cell>Head size: 32 ? 64</cell><cell>224 2</cell><cell>83.9</cell></row><row><cell></cell><cell>Norm type: BN ? LN</cell><cell>224 2</cell><cell>84.1</cell></row><row><cell>ImageNet-21K</cell><cell>CoAtNet-3</cell><cell>384 2</cell><cell>87.9</cell></row><row><cell>? ImageNet-1K</cell><cell>Norm type: BN ? LN</cell><cell>384 2</cell><cell>87.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>2D Relative Attention To implement the pre-norm version of relative attention in Eqn. 3 for 2D images of size [H?W ], for each head, we create a trainable parameter P of size [(2H?1)?(2W ?1)], as the maximum distance is 2H ? 1 and 2W ? 1 respectively. Then, for two spatial locations (i, j) and (i , j ), the corresponding relative bias is P i?i +H,j?j +W under 1-based indexing. For implementation, we need to index H 2 W 2 elements from the [(2H ? 1) ? (2W ? 1)] matrix. On TPU, we utilize two einsums, along the height and width axis respectively, to index the relative bias with complexity O(HW (H + W )), which is strictly subsumed by the O(H 2 W 2 D) attention complexity. On GPUs, the indexing can be done more efficiently with gather, which only requires memory access. Note that, at inference time, indexing the H 2 W 2 elements from the [(2H ? 1) ? (2W ? 1)] matrix can be pre-computed and cached to further increase the throughput.</figDesc><table><row><cell>224 x 224</cell><cell>Input</cell><cell>(S=2)</cell><cell>Conv 3x3</cell><cell>Conv 3x3</cell><cell>+</cell><cell>Conv 1x1</cell><cell>(E=4)</cell><cell>DConv 3x3</cell><cell>Conv 1x1</cell><cell>+</cell><cell>Conv 1x1</cell><cell>(E=4)</cell><cell>DConv 3x3</cell><cell>Conv 1x1</cell><cell>+</cell><cell>Rel-Attention</cell><cell>+</cell><cell>FFN (E=4)</cell><cell>+</cell><cell>Rel-Attention</cell><cell>+</cell><cell>FFN (E=4)</cell><cell>+</cell><cell>Global Pool</cell><cell>FC</cell><cell>Output</cell></row><row><cell></cell><cell></cell><cell cols="3">S0: Stem stage ( 112 x 112 )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">S3: Repeat L 3 times ( 14 x 14 )</cell><cell cols="4">S4: Repeat L 4 times ( 7 x 7 )</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="13">Figure 4: Overview of the proposed CoAtNet.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>When finetuned on a larger resolution, we simply use bi-linear interpolation to increase the size [(2H ? 1) ? (2W ? 1)] to the desired size [(2H ? 1) ? (2W ? 1)] for any H &gt; H and W &gt; W .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>The effect of performing down-sampling in first Conv v.s. the Depthwise Conv.</figDesc><table><row><cell>Models</cell><cell cols="4">Eval Size #Params #FLOPs ImageNet Top-1 Accuracy</cell></row><row><cell>CoAtNet-0</cell><cell>224 2</cell><cell>25M</cell><cell>4.2B</cell><cell>81.6</cell></row><row><cell>Strided DConv</cell><cell>224 2</cell><cell>25M</cell><cell>4.6B</cell><cell>82.0</cell></row><row><cell>CoAtNet-1</cell><cell>224 2</cell><cell>42M</cell><cell>8.4B</cell><cell>83.3</cell></row><row><cell>Strided DConv</cell><cell>224 2</cell><cell>42M</cell><cell>8.8B</cell><cell>83.5</cell></row><row><cell>CoAtNet-2</cell><cell>224 2</cell><cell>75M</cell><cell>15.7B</cell><cell>84.1</cell></row><row><cell>Strided DConv</cell><cell>224 2</cell><cell>75M</cell><cell>16.6B</cell><cell>84.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameters used in the main experiments. The slash sign " / " is used to separate the different hyper-parameters used for various CoAtNet model sizes. : For finetuning the slightly larger CoAtNet-3, RandAugment of 2, 20 is used. ? : RandAugment of 2, 5 is applied to the PT-RA variants of CoAtNet-4 inTable 14.</figDesc><table><row><cell></cell><cell cols="2">ImageNet-1K</cell><cell cols="2">ImageNet-21K</cell><cell>JFT</cell><cell></cell></row><row><cell>Hyper-parameter</cell><cell cols="4">Pre-Training Finetuning Pre-Training Finetuning</cell><cell>Pre-Training</cell><cell>Finetuning</cell></row><row><cell></cell><cell cols="2">(CoAtNet-0/1/2/3)</cell><cell cols="2">(CoAtNet-2/3/4)</cell><cell cols="2">(CoAtNet-3/4/5)</cell></row><row><cell>Stochastic depth rate</cell><cell cols="2">0.2 / 0.3 / 0.5 / 0.7</cell><cell cols="2">0.3 / 0.5 / 0.7</cell><cell cols="2">0.0 / 0.1 / 0.0 0.1 / 0.3 / 0.2</cell></row><row><cell>Center crop</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell><cell>True</cell><cell>False</cell></row><row><cell>RandAugment</cell><cell>2, 15</cell><cell>2, 15</cell><cell cols="2">None / None / 2, 5  ?</cell><cell>2, 5</cell><cell>2, 5</cell></row><row><cell>Mixup alpha</cell><cell>0.8</cell><cell>0.8</cell><cell>None</cell><cell>None</cell><cell>None</cell><cell>None</cell></row><row><cell>Loss type</cell><cell>Softmax</cell><cell>Softmax</cell><cell>Sigmoid</cell><cell>Softmax</cell><cell>Sigmoid</cell><cell>Softmax</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.0001</cell><cell>0.1</cell><cell>0.0001</cell><cell>0.1</cell></row><row><cell>Train epochs</cell><cell>300</cell><cell>30</cell><cell>90</cell><cell>30</cell><cell>14</cell><cell>30</cell></row><row><cell>Train batch size</cell><cell>4096</cell><cell>512</cell><cell>4096</cell><cell>1024</cell><cell>4096</cell><cell>512</cell></row><row><cell>Optimizer type</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>Peak learning rate</cell><cell>1e-3</cell><cell>5e-5</cell><cell>1e-3</cell><cell>5e-5</cell><cell>1e-3 / 5e-4 / 5e-4</cell><cell>5e-5</cell></row><row><cell>Min learning rate</cell><cell>1e-5</cell><cell>5e-5</cell><cell>1e-5</cell><cell>5e-5</cell><cell>1e-5</cell><cell>5e-5</cell></row><row><cell>Warm-up</cell><cell>10K steps</cell><cell>None</cell><cell>5 epochs</cell><cell>None</cell><cell>20K steps</cell><cell>None</cell></row><row><cell>LR decay schedule</cell><cell>Cosine</cell><cell>None</cell><cell>Linear</cell><cell>None</cell><cell>Linear</cell><cell>None</cell></row><row><cell>Weight decay rate</cell><cell>0.05</cell><cell>1e-8</cell><cell>0.01</cell><cell>1e-8</cell><cell>0.01</cell><cell>1e-8</cell></row><row><cell>Gradient clip</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>EMA decay rate</cell><cell>None</cell><cell>0.9999</cell><cell>None</cell><cell>0.9999</cell><cell>None</cell><cell>0.9999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>CoAtNet-5 model sizes.</figDesc><table><row><cell>Stages</cell><cell>Size</cell><cell cols="2">CoAtNet-5</cell></row><row><cell>S0-Conv</cell><cell>1 /2</cell><cell>L=2</cell><cell>D=192</cell></row><row><cell>S1-MbConv</cell><cell>1 /4</cell><cell>L=2</cell><cell>D=256</cell></row><row><cell>S2-MBConv</cell><cell>1 /8</cell><cell cols="2">L=12 D=512</cell></row><row><cell>S3-TFM Rel</cell><cell cols="3">1 /16 L=28 D=1280</cell></row><row><cell>S4-TFM Rel</cell><cell cols="2">1 /32 L=2</cell><cell>D=2048</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Model sizes for the scaled models.</figDesc><table><row><cell>Stages</cell><cell>Size</cell><cell cols="2">CoAtNet-6</cell><cell cols="2">CoAtNet-7</cell></row><row><cell>S0-Conv</cell><cell cols="2">1 /2 L=2</cell><cell>D=192</cell><cell>L=2</cell><cell>D=192</cell></row><row><cell>S1-MbConv</cell><cell cols="2">1 /4 L=2</cell><cell>D=192</cell><cell>L=2</cell><cell>D=256</cell></row><row><cell cols="3">S2-MBConv 1 /8 L=4</cell><cell>D=384</cell><cell>L=4</cell><cell>D=512</cell></row><row><cell cols="2">S3-MBConv 1 /16 S3-TFM Rel</cell><cell cols="4">L=8 L=42 D=1536 L=42 D=2048 D=768 L=8 D=1024</cell></row><row><cell>S4-TFM Rel</cell><cell cols="2">1 /32 L=2</cell><cell cols="2">D=2048 L=2</cell><cell>D=3072</cell></row><row><cell>A.3 Complete Comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Complete comparison under the ImageNet-21K pre-training + ImageNet-1K finetuning set up. "PT-RA" denotes applying RandAugment during 21K pre-training and "E150" means 150 epochs of pre-training, which is longer than the standard 90 epochs.</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="4">Eval Size #Params #FLOPs Top-1 Accuracy</cell></row><row><cell></cell><cell>ENetV2-S</cell><cell>384 2</cell><cell>24M</cell><cell>8.8B</cell><cell>85.0</cell></row><row><cell>Conv Only</cell><cell>ENetV2-M</cell><cell>480 2</cell><cell>55M</cell><cell>24B</cell><cell>86.1</cell></row><row><cell></cell><cell>ENetV2-L</cell><cell>480 2</cell><cell>121M</cell><cell>53B</cell><cell>86.8</cell></row><row><cell>ViT-Stem TFM Only</cell><cell>ViT-B/16 ViT-L/16</cell><cell>384 2 384 2</cell><cell>87M 304M</cell><cell>55.4B 190.7B</cell><cell>84.6 85.3</cell></row><row><cell></cell><cell>HaloNet-H4</cell><cell>384 2</cell><cell>85M</cell><cell>-</cell><cell>85.6</cell></row><row><cell>Multi-Stage TFM Only</cell><cell>HaloNet-H4</cell><cell>512 2</cell><cell>85M</cell><cell>-</cell><cell>85.8</cell></row><row><cell></cell><cell>Swin-B</cell><cell>384 2</cell><cell>88M</cell><cell>47.0B</cell><cell>86.0</cell></row><row><cell></cell><cell>Swin-L</cell><cell>384 2</cell><cell>197M</cell><cell>103.9B</cell><cell>86.4</cell></row><row><cell></cell><cell>HaloNet-Conv-H4</cell><cell>384 2</cell><cell>87M</cell><cell>-</cell><cell>85.5</cell></row><row><cell>Multi-Stage Conv+TFM</cell><cell>HaloNet-Conv-H4</cell><cell>512 2</cell><cell>87M</cell><cell>-</cell><cell>85.8</cell></row><row><cell></cell><cell>CvT-13</cell><cell>384 2</cell><cell>20M</cell><cell>16B</cell><cell>83.3</cell></row><row><cell></cell><cell>CvT-21</cell><cell>384 2</cell><cell>32M</cell><cell>25B</cell><cell>84.9</cell></row><row><cell></cell><cell>CvT-W24</cell><cell>384 2</cell><cell>277M</cell><cell>193.2B</cell><cell>87.7</cell></row><row><cell></cell><cell>CoAtNet-2</cell><cell>384 2</cell><cell>75M</cell><cell>49.8B</cell><cell>87.1</cell></row><row><cell></cell><cell>CoAtNet-3</cell><cell>384 2</cell><cell>168M</cell><cell>107.4B</cell><cell>87.6</cell></row><row><cell></cell><cell>CoAtNet-4</cell><cell>384 2</cell><cell>275M</cell><cell>189.5B</cell><cell>87.9</cell></row><row><cell>Proposed</cell><cell>+ PT-RA</cell><cell>384 2</cell><cell>275M</cell><cell>189.5B</cell><cell>88.3</cell></row><row><cell>Multi-Stage Conv+TFM</cell><cell>+ PT-RA-E150</cell><cell>384 2</cell><cell>275M</cell><cell>189.5B</cell><cell>88.4</cell></row><row><cell></cell><cell>CoAtNet-2</cell><cell>512 2</cell><cell>75M</cell><cell>96.7B</cell><cell>87.3</cell></row><row><cell></cell><cell>CoAtNet-3</cell><cell>512 2</cell><cell>168M</cell><cell>203.1B</cell><cell>87.9</cell></row><row><cell></cell><cell>CoAtNet-4</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>88.1</cell></row><row><cell></cell><cell>+ PT-RA</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>88.4</cell></row><row><cell></cell><cell>+ PT-RA-E150</cell><cell>512 2</cell><cell>275M</cell><cell>360.9B</cell><cell>88.56</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">To simplify the presentation, we deliberately omit the multi-head query, key and value projections for now. In the actual implementation, we always use the multi-head projections.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Rigorously speaking, this test examines not only the transferability but also the generalization.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<title level="m">Rethinking model scaling for convolutional neural networks. ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3531" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnetv2</surname></persName>
		</author>
		<title level="m">Smaller models and faster training. ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">High-performance largescale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling local self-attention for parameter efficient visual backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04560</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis section 6.2</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A data and compute efficient design for limited-resources deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirgahney</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09691</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Music transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Transformer dissection: A unified understanding of transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11775</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03236</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Table 13: Complete comparison under the ImageNet-1K only setting</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
