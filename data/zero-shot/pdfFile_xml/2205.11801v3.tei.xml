<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SepIt: Approaching a Single Channel Speech Separation Bound</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Lutati</surname></persName>
							<email>shahar761@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Meta AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<email>wolf@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SepIt: Approaching a Single Channel Speech Separation Bound</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: speech separation</term>
					<term>single channel</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an upper bound for the Single Channel Speech Separation task, which is based on an assumption regarding the nature of short segments of speech. Using the bound, we are able to show that while the recent methods have made great progress for a few speakers, there is room for improvement for five and ten speakers. We then introduce a Deep neural network, SepIt, that iteratively improves the different speakers' estimation. At test time, SpeIt has a varying number of iterations per test sample, based on a mutual information criterion that arises from our analysis. In an extensive set of experiments, SepIt outperforms the state of the art neural networks for 2, 3, 5, and 10 speakers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Single Channel Speech Separation (SCSS) problem is a specific setting of the general Blind Source Separation (BSS) problem. By employing deep neural networks, great improvement has been achieved in the last few years in separating two and three speakers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, leading to the very recent state of the art <ref type="bibr" target="#b2">[3]</ref>. Somewhat disappointingly, in terms of model sizes versus improvement obtained, the model size has more than tripled (26M <ref type="bibr" target="#b2">[3]</ref> vs 7.5M <ref type="bibr" target="#b1">[2]</ref>), while the improvement as measured when training on same dataset without dynamic mixing is only 0.1dB. One may, therefore, wonder whether additional improvement in performance is possible. This drives the need for a theoretical upper bound for SCSS.</p><p>SCSS has unique characteristics in comparison to other BSS problems. The speech signal is not stationary, unless short segments are considered. Jensen et al. <ref type="bibr" target="#b3">[4]</ref> have shown empirically that when the length of the speech segment is longer than 20[ms], the distribution is closer to the Laplace distribution than to the normal distribution. As a result, the known bounds for BSS <ref type="bibr" target="#b4">[5]</ref> do not hold. In this work, we derive such a bound by employing the assumption of a Laplacian distribution. Treating the speech mixture as a random process, we derive a bound that expresses the maximum achievable Signal to Distortion Ratio (SDR) by any neural network. We then present a deep learning method called SepIt, which uses the bound during its training.</p><p>In an extensive set of experiments, we validate the assumption made in our analysis, as well as the bound itself. We then show that the SepIt model outperforms the state of the art in separating two and three speakers for the WSJ <ref type="bibr" target="#b5">[6]</ref> benchmark and five and ten speakers for the LibriMix <ref type="bibr" target="#b6">[7]</ref> benchmark.</p><p>Related Work SCSS using deep learning techniques has been explored intensively in the last years. Erdogan et al. <ref type="bibr" target="#b7">[8]</ref> introduced a phase-sensitive loss function trained using a LSTM neural network. Hershey et al. <ref type="bibr" target="#b8">[9]</ref> developed the WSJ-2mix benchmark and proposed a neural separation network with a clustering-based embedding. The TasNet architecture <ref type="bibr" target="#b9">[10]</ref> employs a time domain encoder-decoder. Subsequently, Luo et al. <ref type="bibr" target="#b10">[11]</ref> introduced the ConvTasNet architecture based on a convo-lutional neural network with masking. ConvTasNet was further improved with the dual-path recurrent neural network (DPRNN) architecture by <ref type="bibr" target="#b11">[12]</ref>. Zeghidour et al. <ref type="bibr" target="#b12">[13]</ref> introduced Wavesplit, which uses clustering on speaker representation to separate the mixture. Nachmani et al. <ref type="bibr" target="#b1">[2]</ref> proposed the VSUNS model, which is based on the DPRNN model, but removes the masking sub-network. The SepFormer architecture is a transformer-based architecture that captures both short-and long-term dependencies <ref type="bibr" target="#b2">[3]</ref>. Yao et al. <ref type="bibr" target="#b13">[14]</ref> introduced a coarse-to-fine framework called the Stepwise-Refining Speech Separation Network (SRSSN). Hu et al. <ref type="bibr" target="#b14">[15]</ref> presented the Fully Recurrent Convolutional Neural Network (FRCNN) architecture, which uses lateral connections to fuse information processed at various timescales.</p><p>Independently, upper bounds have been developed for the BSS problem. Sahlin et al. <ref type="bibr" target="#b15">[16]</ref> derived an asymptotic Cramer Rao bound for the MIMO case, which assumes a full-rank channel matrix. Doron et al. <ref type="bibr" target="#b4">[5]</ref> presented a Cramer Rao bound for Gaussian sources. Kautsky et al. <ref type="bibr" target="#b16">[17]</ref> perform an analysis for both the Gaussian and non-Gaussian sources, presenting a Cramer Rao induced bound. However, the analysis only addresses linear operations over the mixture signal. As far as we can ascertain, no upper bound for non-Gaussian sources exists for non-linear methods such as Deep Neural Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SCSS Upper bound</head><p>We start by formulating the required lemmas and stating the assumption we employ. </p><p>Lemma 2.2 (Joint Estimation Lemma). For N i.i.d random variables with parameter ?, the joint Fisher Information is</p><formula xml:id="formula_1">J(?joint) = N ? J(?)<label>(2)</label></formula><p>Jensen et al. <ref type="bibr" target="#b3">[4]</ref> demonstrated empirically that speech, divided into short segments, neglecting quiet segments, is well modeled as a stationary process. Furthermore, short segments of a single speaker follow a Laplace random variable. Assumption 2.3. Short speech segments can be captured by a Laplace random variable with zero mean and scale parameter b.</p><formula xml:id="formula_2">f (voice = x) = 1 /2b ?1 e ?| x b | (3)</formula><p>Mixture Model Let the different speakers' signals in the mixture compose a matrix V = [v0, v1, ..., vC?1] T , where vi ? R L is a single speaker signal in the time domain, with length L. To maintain the stationary requirements we use windowing of size w as in Assumption 2.3. The mixture m = i ai ? vi is modeled as a linear combination of the columns of V , where ai ? R are the mixture coefficients. Definition 2.4 (Signal to Distortion Ratio (SDR)). Let the error be defined as = v0 ?v0. The Signal to Distortion Ratio (SDR) is,</p><formula xml:id="formula_3">SDR = 10log10 V ar(v0) V ar( )<label>(4)</label></formula><p>Denote a single segment of size w from m as mr, and a single segment of vi as vi,r, where 0 ? r ? L w . Without loss of generality, assume that the goal is to separate v0 from the mixture m. Denote byv0 the estimation of v0, using deep neural network, D.</p><p>Lemma 2.5. Given a single segment v0,r,mr and deep neural network, D, the following inequality holds,</p><formula xml:id="formula_4">L w ? I(mr, v0,r) ? J(v0)<label>(5)</label></formula><p>Proof. Considering the Markov chain v0,r ?? mr D ??v0,r and using the Data Processing Theorem,</p><formula xml:id="formula_5">I(mr,v0,r) &lt; I(mr, v0,r)<label>(6)</label></formula><p>For neural network D that estimates jointly all of the segments we have from Lemma 2.2,</p><formula xml:id="formula_6">J(v0) = L w ? J(v0,r)<label>(7)</label></formula><p>Recall thatv0 is the estimator of v0 from m. Referring v0 as ? in Lemma 2.1 gives,</p><formula xml:id="formula_7">I(mr, v0,r) ? J(v0,r)<label>(8)</label></formula><p>Plugging Eq.8, and Eq.7 gives,</p><formula xml:id="formula_8">L w ? I(mr, v0,r) ? J(v0)<label>(9)</label></formula><p>Based on this lemma, we obtain the following bound.</p><p>Theorem 2.6 (SDR upper bound). Let mr, v0,r, L, w be defined as previously stated. For any neural network, the SDR upper bound that separates a mixture of C speakers is given by:</p><formula xml:id="formula_9">SDR(v0,v0) ? 10log10 L w ? V ar(v0) ? I(mr, v0,r)<label>(10)</label></formula><p>Proof. The Cramer Rao Lower Bound <ref type="bibr" target="#b18">[19]</ref> states that for any unbiased estimator there is a lower bound on the estimator square error i.e. for estimator ofv0,r</p><formula xml:id="formula_10">V ar(v0,r) ? 1 J(v0,r)<label>(11)</label></formula><p>Applying Eq. 11 to Definition. 2.4 gives an upper bound for estimating the single segment:</p><formula xml:id="formula_11">(12) SDR(v0,r,v0,r) ? 10log10(V ar(v0,r)J(v0,r))</formula><p>Using the upper bound for the Fisher information Lemma.2.1 yields:</p><p>SDR(v0,r,v0,r) ? 10log10(V ar(v0,r)I(mr,v0,r)) <ref type="bibr" target="#b12">(13)</ref> The data processing inequality states the the mutual information is reduced after each processing, thus we obtain the following: SDR(v0,r,v0,r) ? 10log10(V ar(v0,r)I(mr, v0,r)) (14)</p><p>Using Lemma.2.2 gives the upper bound:</p><formula xml:id="formula_12">SDR(v0,v0) ? 10log10 L w ? V ar(v0)I(mr, v0,r) (15)</formula><p>Theorem 2.6 states that the upper bound for separation algorithm, is a function of: (i) the number of speakers C, (ii) the number of segments that jointly estimated L w , (iii) the mutual information I(mr, v0,r).</p><p>Specifically, the bound is linear with L, the length of the sequence. Furthermore, it is inversely proportional to the segment length w. There is a trade-off between w and the lowest frequencies in each segment, thus, decreasing w also decreases V ar(v0,r).</p><p>A clear limitation of the bound is that it holds only for a network D that jointly process i.i.d stationary segments. This is not the case if a neural network processes the entire signal without segmentation. However, current architectures, including the attention architecture, tend to be myopic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our method, denoted as as SepIt, consists of sequential processing of the estimated signals, where each iteration contains a replica of the basic model. Consider again the mixture signal, m, consisting of C speakers and of length L. First, a backbone network, B, is used for initial separation estimation. The backbone can change between different data sets, and it does not learn during the training phase of the SepIt model. Expanding the notation in Sec. 2, denote the estimation of the i-th speaker in the j-th iteration asv j i . Collectively, we denote the list of obtained speakers asv</p><formula xml:id="formula_13">j = [v j 0 ,v j 1 , . . . ,v j C?1 ]. The output of the backbone, B, isv 0 .v 0 = B(m)<label>(16)</label></formula><p>Wherev ? R C?L is all speaker estimation. Both the current estimation,v j?1 , and the mixture, m, pass through an encoder, E, of a 1-D convolution with N channels, kernel size K and stride K 2 , followed by a ReLU activation function:</p><formula xml:id="formula_14">(17) m = E(m) (18) v j?1 = E(v j?1 )</formula><p>Where? ? R N ?L ,v j?1 ? R C?N ?L are the latent space for the mixture and current speech estimations, respectively, and The latent estimationv j?1 is treated as a prior on the latent masking. A deep neural network, Pm, is used to evaluate a latent space mask. Pm consists of 3 ResBlocks, each a 1-D version of the Residual block from ResNet, each with N channels, kernel size of 3, and ReLU activation function. The latent mask is multiplied element-wise by the mixture latent representation. To subtract residuals from other sources, a 1 ? 1 Convolution layer Pc with N ? C channels is then introduced. The procedure is:</p><formula xml:id="formula_15">j = Pc(Pm(v j?1 ) ??) ,<label>(19)</label></formula><p>where? j ? R C?N ?L is the latent space residual, and ? is the element-wise product operator. The latent residual? is passed to a decoder layer, D. Later, a skip-connection is introduced, such that the j-th iteration estimation is the summation of the decoded and the previous estimation,</p><formula xml:id="formula_16">y j =? j?1 + D(? j )<label>(20)</label></formula><p>A single SepIt block is depicted in <ref type="figure" target="#fig_1">Fig 1.</ref> The training algorithm is depicted in Alg. 1. The algorithm starts with the initialization of the threshold u to 1. The zero-th iteration is set using the backbone model as in line 3. Next, the algorithm evaluates the SepIt neural network over the previous iteration, and takes a gradient descent step over the loss function. The threshold u is updated by the difference between I(m,v j ) and I(m,v j?1 ). The algorithm proceeds to apply the SepIt neural network until the maximum number of iterations is reached or the threshold is lower or equal to zero. This stopping criterion follows the derivation of Thm.2.6, which indicates that the mutual information between the speaker estimationv j and the mixture m can indicates the maximal achievable SDR. Thus, a per-sample stopping criterion is derived when I(m,v j ) stops increasing relative to previous iteration I(m,v j?1 ). This stopping criterion can be computed also on the test-set samples since it does not require the ground truth.</p><p>Loss Function Following <ref type="bibr" target="#b19">[20]</ref>, a Scale Invariant Signal to Distortion Ratio (SI-SDR) is used as the loss function, which is a similar loss to SDR, but demonstrated better convergence rate. The overall loss function is given by:</p><formula xml:id="formula_17">L(v,v) = 10log10 ||?|| 2 ||?|| 2<label>(21)</label></formula><p>wherev is the estimated source,? = &lt;v,v&gt;v ||v|| 2 , and? =v ??.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Validating Assumption 2.3 This assumption is extensively validated by <ref type="bibr" target="#b3">[4]</ref>, yet we re-evaluated it again over the LibriMix training set. The LibriMix dataset containing over 100 hours of speech is investigated. Each speaker signal is split into different non-overlapping segments, each 20[ms] long. <ref type="figure">Fig. 2</ref> compares the empirical PDF with the best fitted Laplace distribution and normal distribution. Evidently, the Laplacian distribution provides a much better fit, The Kullback-Leibler that presented in the figure supports this conclusion too. l ? SDR(v j , v)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Take gradient descent step on ?l 9:</p><p>j ? j + 1 <ref type="bibr">10:</ref> u ? I(m,v j ) ? I(m,v j?1 ) 11: end while 12: returnv j <ref type="figure">Figure 2</ref>: Distribution comparison of empirical PDF, best fitted normal and Laplace distribution. X axis -the signal sample amplitude, Y-axis the PDF of the sample amplitude. The Laplace distribution is the best fit to the empirical PDF LSTM-based. Each backbone is used in the range for which it is currently the state of the art. We use dynamic mixing augmentation, introduced in <ref type="bibr" target="#b12">[13]</ref>, which consist of creating new mixtures in run time, noted as DM. For all experiments, 5 iterations are conducted, where SC implies that the stopping criterion per sample is activated. <ref type="figure" target="#fig_4">Fig. 4</ref> depicts the behavior of the criterion on a typical sample from the test set.</p><p>Transformer Backbone As seen from the upper bound, the current state-of-the-art results are close to the bound. Thus, only a small improvement is expected when experimenting above it with SepIt. The results using a current state-of-the-art Transformer network <ref type="bibr" target="#b2">[3]</ref> as B over the Wall Street Journal Mix 2/3 (WSJ0-2/3 Mix) are summarized in Tab. 2.</p><p>First, we observe that the current state of the art is approach-   LSTM Backbone In LSTM-based architectures the stateof-the-art network for a large number of speakers was provided by <ref type="bibr" target="#b21">[22]</ref>. We apply the SepIt network over the Libri5Mix and Libri10Mix datasets. The results are summarized in Tab. 3. As expected, here the improvement over the current state-of-the-art architecture is more prominent than in the Transformer case, where for 5 speakers, SepIt improves results by 1.0dB, and for 10 speakers by 0.5dB. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>In this work, a general upper bound for the Single Channel Speech Separation (SCSS) problem is derived. The upper bound is obtained using the Cramer-Rao bound, with fundamental modeling of the speech signal. We show that the gap between the current state-of-the-art network for 2 and 3 speakers and the obtained bound is relatively small, 0.8[dB] and 1.4[dB] respectively. This may indicate that future research should focus more on reducing the model size or the required training set and less on improving overall accuracy. The gap for 5 and 10 speakers is larger, and there is still room for substantial improvement in separation accuracy. Using the upper bound, a new neural network named SepIt is introduced. SepIt takes a backbone network estimation, and iteratively improves the estimation, until a stopping criterion based on the upper bound is met. SepIt is shown to outperform current state-of-the-art networks for 2, 3, 5 and 10 speakers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 2 . 1 (</head><label>21</label><figDesc>Fisher Information Upper Bound). From [18] the Fisher Information J(?) is upper bounded by the Mutual Information I(X, ?): J(?) ? I(X, ?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The SepIt block architecture. L = L K is the strided latent dimension. The encoder of m shares its weights with the encoder ofv j?1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 3 : u ? 1 4 : l ? 0 5 :</head><label>1345</label><figDesc>Separation results For all experiments, the Adam [21] optimizer is used with a learning rate of 5 ? 10 ?4 and a decay factor of 0.95 every epoch. The window size w = 20[ms] and speech length L = 4[s]. Other hyperparameters are summarized in Tab. 1. The experiments are divided into two categories based on their Backbone architecture: (i) Transformer -based and (ii) The SepIt algorithm. Input: m -mixture signal, C -number of speakers, B -Backbone network, MAXITER -maximum number of iterations Output:v j -estimated separated speakers. 1:v 0 ? B(m) 2: j ? 0 while u ? 0 and j ? M AXIT ER do 6:v j ? SepItj(v j?1 , m) 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The upper bound for single channel source separation (blue curve). The upper bound decreases when the number of speakers C increases (Theorem2.6). Furthermore, the score of SDR improvement for different separation algorithms is presented. SepIt, denoted by a pink triangle, produces state-of-the-art results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>SDR improvement for 5 speakers and the Mutual information between the mixture and the estimated speaker. training stops when mutual information is no longer increases, evidently the SI-SDR improvement degrades too.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters for the different number of speakers.</figDesc><table><row><cell cols="2">No. speakers N</cell><cell cols="2">K Params[M]</cell></row><row><cell>2</cell><cell cols="2">256 4</cell><cell>4.6</cell></row><row><cell>3</cell><cell cols="2">256 4</cell><cell>7.2</cell></row><row><cell>5</cell><cell cols="2">128 4</cell><cell>1.95</cell></row><row><cell>10</cell><cell cols="2">128 4</cell><cell>2.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The performance obtained by our method, as well as the Transformer backbone network. See text for details.ing the upper bound. For 2 and 3 speakers, the gap between the upper bound and current results is 0.8dB and 1.4dB, respectively. Second, as expected, our SepIt model is able to improve only slightly the result with 2 speakers, with an improvement of 0.1dB over the current state of the art, while for 3 speakers, SepIt improves the result by 0.3dB. We note that for 2/3 speakers only few examples had early stopping with the stopping criterion (SC), which resulted in similar results.</figDesc><table><row><cell></cell><cell cols="2">SI-SDRi [dB](?)</cell></row><row><cell>Method</cell><cell cols="2">WSJ0-2Mix WSJ0-3Mix</cell></row><row><cell>Upper Bound</cell><cell>23.1</cell><cell>21.2</cell></row><row><cell>(i) SepFormer</cell><cell>20.4</cell><cell>17.6</cell></row><row><cell>(ii) SepFormer + DM</cell><cell>22.3</cell><cell>19.8</cell></row><row><cell>(iii) SepIt + SepFormer + DM</cell><cell>22.4</cell><cell>20.1</cell></row><row><cell>(iv) SepIt + SepFormer + DM + SC</cell><cell>22.4</cell><cell>20.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The performance obtained by our method, as well as the LSTM backbone network. See text for details.</figDesc><table><row><cell>SI-SDRi [dB] (?)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A study of the distribution of time-domain speech samples and discrete fourier coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Batina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cram?r-rao-induced bound for blind separation of stationary parametric gaussian sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yeredor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tichavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="417" to="420" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Csr-i (wsj0) complete ldc93s6a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Librimix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tasnet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
	</analytic>
	<monogr>
		<title level="m">speech, and language processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06379</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stepwise-refining speech separation network via fine-grained encoding in high-order latent domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech separation using an asynchronous fully recurrent convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Lemercier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The asymptotic cramer-rao lower bound for blind signal separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lindgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 8th Workshop on Statistical Signal and Array Processing</title>
		<meeting>8th Workshop on Statistical Signal and Array Processing</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="328" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cram?r-rao bounds for complex-valued independent component extraction: Determined and piecewise determined mixing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kautsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Koldovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tichavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zarzoso</surname></persName>
		</author>
		<idno type="DOI">10.1109/TSP.2020.3022827</idno>
		<ptr target="http://dx.doi.org/10.1109/TSP.2020.3022827" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="5230" to="5243" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mutual information, fisher information, and population coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Nadal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1731" to="57" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Information and the Accuracy Attainable in the Estimation of Statistical Parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-0919-5_16</idno>
		<ptr target="https://doi.org/10.1007/978-1-4612-0919-516" />
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="235" to="247" />
			<pubPlace>New York, NY; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sdrhalf-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Many-speakers single channel speech separation with optimal permutation training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dovrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
