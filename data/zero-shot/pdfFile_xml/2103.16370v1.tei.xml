<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distribution Alignment: A Unified Framework for Long-tail Visual Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Shanghai Institute of Microsystem and Information Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shanghai Engineering Research Center of Intelligent Vision and Imaging</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Distribution Alignment: A Unified Framework for Long-tail Visual Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the recent success of deep neural networks, it remains challenging to effectively model the long-tail class distribution in visual recognition tasks. To address this problem, we first investigate the performance bottleneck of the two-stage learning framework via ablative study. Motivated by our discovery, we propose a unified distribution alignment strategy for long-tail visual recognition. Specifically, we develop an adaptive calibration function that enables us to adjust the classification scores for each data point. We then introduce a generalized re-weight method in the two-stage learning to balance the class prior, which provides a flexible and unified solution to diverse scenarios in visual recognition tasks. We validate our method by extensive experiments on four tasks, including image classification, semantic segmentation, object detection, and instance segmentation. Our approach achieves the state-of-the-art results across all four recognition tasks with a simple and unified framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While deep convolutional networks have achieved great successes in many vision tasks, it usually requires a large number of training examples for each visual category. More importantly, prior research mostly focuses on learning from a balanced dataset <ref type="bibr" target="#b22">[23]</ref>, where different object classes are approximately evenly distributed. However, for large-scale vision recognition tasks, partially due to the non-uniform distribution of natural object classes and varying annotation costs, we typically learn from datasets with a long-tail class label distribution. In such scenarios, the number of training instances per class varies significantly, from as few as one example for tail classes to hundreds or thousands for head classes <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">56,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b39">40]</ref>. The intrinsic long-tail property of our visual data introduces a multitude of challenges for recognition in the wild <ref type="bibr" target="#b0">[1]</ref>, as a deep network model has to simultaneously cope with imbalanced annotations among the head and mediumsized classes, and few-shot learning in the tail classes. A naively learned model would be largely dominated by those few head classes while its performance is much degraded for many other tail classes.</p><p>Early works on re-balancing data distribution focus on learning one-stage models, which achieve limited successes due to lack of principled design in their strategies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref>. More recent efforts aim to improve the long-tail prediction by decoupling the representation learning and classifier head learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b23">24]</ref>. However, such a two-stage strategy typically relies on heuristic design to adjust the decision boundary of the initially learned classifier head, which often requires tedious hyper-parameter tuning in practice. This severely limits its capacity to resolve the mismatch between imbalanced training data distribution and balanced evaluation metrics.</p><p>In this work, we first perform an ablative analysis on the two-stage learning strategy to shed light on its performance bottleneck. Specifically, our study estimates an 'ideal' clas-sification accuracy using a balanced dataset to retrain the classifier head while keeping the first-stage representation fixed. Interestingly, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, we find a substantial gap between this ideal performance and the baseline network, which indicates that the first-stage learning with unbalanced data provides a good representation, but there is a large room for improvement in the second stage due to the biased decision boundary (See Sec. 3.1 for details).</p><p>Based on those findings, we propose a simple and yet effective two-stage learning scheme for long-tail visual recognition problems. Our approach focuses on improving the second-stage training of the classifier after learning a feature representation in a standard manner. To this end, we develop a unified distribution alignment strategy to calibrate the classifier output via matching it to a reference distribution of classes that favors the balanced prediction. Such an alignment strategy enables us to exploit the class prior and data input in a principled manner for learning class decision boundary, which eliminates the needs for tedious hyper-parameter tuning and can be easily applied to various visual recognition tasks.</p><p>Specifically, we develop a light-weight distribution alignment module for calibrating classification scores, which consists of two main components. In the first component, we introduce an adaptive calibration function that equips the class scores with an input-dependent, learnable magnitude and margin. This allows us to achieve a flexible and confidenceaware distribution alignment for each data point. Our second component explicitly incorporates a balanced class prior by employing a generalized re-weight design for the reference class distribution, which provides a unified strategy to cope with diverse scenarios of label imbalance in different visual recognition tasks.</p><p>We extensively validate our model on four typical visual recognition tasks, including image classification on three benchmarks (ImageNet-LT <ref type="bibr" target="#b27">[28]</ref>, iNaturalist <ref type="bibr" target="#b39">[40]</ref> and Places365-LT <ref type="bibr" target="#b27">[28]</ref>), semantic segmentation on ADE20k dataset <ref type="bibr" target="#b53">[54]</ref>, object detection and instance segmentation on LVIS dataset <ref type="bibr" target="#b14">[15]</ref>. The empirical results and ablative study show our method consistently outperforms the state-of-theart approaches on all the benchmarks. To summarize, the main contributions of our works are three-folds:</p><p>? We conduct an empirical study to investigate the performance bottleneck of long-tail recognition and reveal a critical gap caused by biased decision boundary.</p><p>? We develop a simple and effective distribution alignment strategy with a generalized re-weight method, which can be easily optimized for various long-tail recognition tasks without whistles and bells.</p><p>? Our models outperform previous work with a large margin and achieve state-of-the-art performance on longtail image classification, semantic segmentation, object detection, and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>One-stage Imbalance Learning To alleviate the adverse effect of the long-tail class distribution in visual recognition, prior work have extensively studied the one-stage methods, which either leverage the re-balancing ideas or explore knowledge transfer from head categories. The basic idea of resample-based methods is to over-sample the minority categories <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> or to under-sample the frequent categories in the training process <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>. Class-aware sampling <ref type="bibr" target="#b36">[37]</ref> proposes to choose samples of each category with equal probabilities, which is widely used in vision tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b11">12]</ref>. Repeat factor sampling <ref type="bibr" target="#b28">[29]</ref> is a smoothed sampling method conducting repeated sampling for tail categories, which demonstrates its efficacy in instance segmentation <ref type="bibr" target="#b14">[15]</ref>. In addition, <ref type="bibr" target="#b40">[41]</ref> proposes to increase the sampling rate for categories with low performance after each training epoch and balances the feature learning for under-privileged categories. An alternative strategy is to re-weight the loss function in training. Class-level methods typically re-weight the standard loss with category-specific coefficients correlated with the sample distributions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref>. Sample-level methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref> try to introduce a more finegrained control of loss for imbalanced learning. Other work aim to enhance the representation or classifier head of tail categories by transferring knowledge from the head classes <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref>. Nevertheless, these methods require designing a task specific network module or structure, which is usually non-trivial to be generalized to different vision tasks.</p><p>Two-stage Imbalance Learning More recent efforts aims to improve the long-tail prediction by decoupling the learning of representation and classifier head. Decouple <ref type="bibr" target="#b19">[20]</ref> proposes an instance-balanced sampling scheme, which generates more generalizable representations and achieves strong performance after properly re-balancing the classifier heads. The similar idea is adopted in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b23">24]</ref>, which develop effective strategies for long-tail object detection tasks. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref> improve the two-stage ideas by introducing a post-process to adjust the prediction score. However, such a two-stage strategy typically relies on heuristic design in order to adjust the decision boundary of initially learned classifiers and requires tedious hyper-parameter tuning in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Recognition Tasks</head><p>Visual recognition community has witnessed significant progress with deep convolutional networks in recent years. In this study, we focus on four types of visual tasks, including image classification, object detection, semantic and instance segmentation, which have been actively studied in a large amount of prior work. For object detection, we consider the typical deep network architecture used in the R-CNN series method <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35]</ref>, which detects objects based on the region proposals. For instance segmentation, we take the Mask R-CNN <ref type="bibr" target="#b16">[17]</ref> as our example, which extends the Faster R-CNN <ref type="bibr" target="#b34">[35]</ref> by adding a branch for predicting the object masks in parallel with the existing branch for bounding box recognition. For the pixel-wise task, semantic segmentation, we use the FCN-based methods <ref type="bibr" target="#b35">[36]</ref> and the widely-adopted encoder-decoder structures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Despite those specific choices, we note that our strategy can be easily extended to other types of deep network methods for those visual recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Our goal is to address the problem of large-scale long-tail visual recognition, which typically has a large number of classes and severe class imbalance in its training data. To this end, we adopt a two-stage learning framework that first learns a feature representation and a classifier head from the unbalanced data, followed by a calibration stage that adjusts the classification scores. Inspired by our ablative study on existing two-stage methods, we propose a principled calibration method that aligns the model prediction with a reference class distribution favoring the balanced evaluation metrics. Our distribution alignment strategy is simple and yet effective, enabling us to tackle different types of large-scale long-tail visual recognition tasks in a unified framework.</p><p>Below we start with a brief introduction to the long-tail classification and an empirical study of two-stage methods in Sec.3.1. We then describe our proposed distribution alignment strategy in Sec.3.2. Finally, we present a comparison with previous methods from the distribution match perspective in Sec.3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setting and Empirical Study</head><p>We now introduce the problem setting of long-tail classification and review the two-stage learning framework for deep networks. Subsequently, we perform an empirical ablative study on a large-scale image classification task, which motivates our proposed approach.</p><p>Problem Definition The task of long-tail recognition aims to learn a classification model from a training dataset with long-tail class distribution. Formally, we denote the input as I, and the target label space as C = {c 1 , ? ? ? , c K }, where K is the number of classes. The classification model M defines a mapping from the input to the label space: y = M(I; ?), where y ? C and ? are its parameters. Our goal is to learn the model parameter from an imbalanced training dataset D tr so that M achieves optimal performance on an evaluation dataset D eval with respect to certain balanced metrics (e.g., mean accuracy).</p><p>In the two-stage framework, we typically consider a deep network model M with two main components: a feature extractor network f (?) and a classifier head h(?). The feature extractor f first extracts an input representation x, which is then fed into the classifier head h to compute class prediction scores z as follows:</p><formula xml:id="formula_0">x = f (I, ? f ) ? R d , z = h(x, ? h ) ? R K<label>(1)</label></formula><p>where ? f and ? h are the parameter of f (?) and h(?), respectively. Here z = {z 1 , ? ? ? , z K } indicate the class prediction scores for K classes and the model predicts the class label by taking y = arg max (z).</p><p>In this work, we instantiate the classifier head h as a linear classifier or a cosine similarity classifier as follows:</p><formula xml:id="formula_1">Linear : z j = w j x<label>(2)</label></formula><p>Cosine Similarity :</p><formula xml:id="formula_2">z j = s ? w j x ||w j ||||x||<label>(3)</label></formula><p>where w j ? R d is the parameter of j-th class and the s is a scale factor as in <ref type="bibr" target="#b32">[33]</ref>. We note that the above formulation can be instantiated for multiple visual recognition tasks by changing the input I: e.g., an image for image classification, an image with a pixel location for semantic segmentation, or an image with a bounding box for object detection.</p><p>Empirical Analysis on Performance Bound The twostage learning method tackles the long-tail classification by decoupling the representation and the classifier head learning <ref type="bibr" target="#b19">[20]</ref>. Specifically, it first learns the feature extractor f and classifier head h jointly, and then with the representation fixed, re-learns the classifier head with a class balancing strategy. While such design achieves certain success, an interesting question to ask is which model component(s) impose a bottleneck on its balanced performance. In the following, we attempt to address the question by exploiting the full set of the ImageNet dataset. Particularly, we follow the decoupling idea to conduct a series of ablative studies on two model components under an 'ideal' balanced setting.</p><p>We first investigate whether the feature representation learned on the imbalanced dataset is restrictive for the balanced performance. To this end, we start from learning the feature extractor on the imbalanced ImageNet-LT training set with several re-balancing strategies (e.g. instance-balanced, class-balanced, or square-root sampling). We then keep the representation fixed and re-train the classifier head with the ideal balanced ImageNet train set (excluding ImageNet-LT val set). Our results are shown in the left panel of <ref type="figure" target="#fig_1">Fig. 2</ref>, which indicate that the first stage produces a strong feature representation that can potentially lead to large performance gain and the instance-based sampling achieves better overall results (cf. <ref type="bibr" target="#b19">[20]</ref>).</p><p>Moreover, we conduct an empirical study on the effectiveness of the recent decoupling method (e.g. cRT <ref type="bibr" target="#b19">[20]</ref>) compared with the above 'ideal' classifier head learning. The right panel of <ref type="figure" target="#fig_1">Fig. 2</ref> shows that there remains a large performance gap between the existing methods and the upper-bound. Those empirical results indicate that the biased decision boundary in the feature space seems to be the performance bottleneck of the existing long-tail methods. Consequently, a better strategy to address this problem would further improve the two-stage learning for the longtail classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Distribution Alignment</head><p>To tackle the aforementioned issue, we now introduce a unified distribution alignment strategy to calibrate the classifier output via matching it to a reference distribution of classes that favors the balanced prediction. In this work, we adopt a two-stage learning scheme for all visual recognition tasks, which consists of a joint learning stage and a distribution calibration stage as follows. 1) Joint Learning Stage. The feature extractor f (?) and original classifier head (denoted as h o (?) for clarity) are jointly learned on imbalanced D tr with instance-balanced strategy in the first stage, where the original h o (?) is severely biased due to the imbalanced data distribution.</p><p>2) Distribution Calibration Stage. For the second stage, the parameters of f (?) are frozen and we only focus on the classifier head to adjust the decision boundary. To this end, we introduce an adaptive calibration function (in Sec. 3.2.1) and a distribution alignment strategy with generalized reweighting (in Sec. 3.2.2) to calibrate the class scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Adaptive Calibration Function</head><p>To learn the classifier head h(?) in the second stage, we propose an adaptive calibration strategy that fuses the original classifier head h o (?) (parameters of h o (?) are frozen) and a learned class prior in an input-dependent manner. As shown below, unlike previous work (e.g. cRT <ref type="bibr" target="#b19">[20]</ref>), our design does not require a re-training of the classifier head from scratch and has much fewer free parameters. This enables us to reduce the adverse impact from the limited training data of the tail categories. Moreover, we introduce a flexible fusion mechanism capable of controlling the magnitude of calibration based on input features. Specifically, denote the class scores from</p><formula xml:id="formula_3">h o (?) as z o = [z o 1 , ? ? ? , z o K ]</formula><p>, we first introduce a class-specific linear transform to adjust the score as follows:</p><formula xml:id="formula_4">s j = ? j ? z o j + ? j , ?j ? C<label>(4)</label></formula><p>where ? j and ? j are the calibration parameters for each class, which will be learned from data. As mentioned above, we then define a confidence score function ?(x) to adaptively combine the original and the transformed class scores:</p><formula xml:id="formula_5">z j = ?(x) ? s j + (1 ? ?(x)) ? z o j (5) = (1 + ?(x)? j ) ? z o j + ?(x) ? ? j<label>(6)</label></formula><p>where the confidence score has a form of g(v x), which is implemented as a linear layer followed by a non-linear activation function (e.g., sigmoid function) for all input x. The confidence ?(x) controls how much calibration is needed for a specific input x. Given the calibrated class scores, we finally define a prediction distribution for our model with the Softmax function:</p><formula xml:id="formula_6">p m (y = j|x) = exp(? j ) C k=1 exp(? k ) .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Alignment with Generalized Re-weighting</head><p>Given a train dataset D tr = {(x i , y i )} N i=1 , we introduce a calibration strategy based on distribution alignment between our model prediction p m (?) and a reference distribution of classes that favors the balanced prediction.</p><p>Formally, denote the reference distribution as p r (y|x), we aim to minimize the expected KL-divergence between </p><formula xml:id="formula_7">L = E Dtr [KL(p r (y|x)||p m (y|x))] (8) ? ? 1 N N i=1 ? ? y?C p r (y|x i ) log(p m (y|x i )) ? ? + C (9)</formula><p>where the expectation is approximated by an empirical average on D tr and C is a constant.</p><p>In this work, we adopt a re-weighting approach <ref type="bibr" target="#b8">[9]</ref> and introduce a generalized re-weight strategy for the alignment in order to exploit the class prior. Formally, we represent the reference distribution as a weighted empirical distribution on the training set,</p><formula xml:id="formula_8">p r (y = c|x i ) = w c ? ? c (y i ), ?c ? C<label>(10)</label></formula><p>where w c is the class weight, and ? c (y i ) is the Kronecker delta function(equals 1 if y i = c, otherwise equals 0). We then define the reference weight based on the empirical class frequencies r = [r 1 , ? ? ? , r K ] on the training set:</p><formula xml:id="formula_9">w c = (1/r c ) ? K k=1 (1/r k ) ? , ?c ? C<label>(11)</label></formula><p>where ? is a scale hyper-parameter to provide more flexibility in encoding class prior. Note that our scheme reduces to the instance-balance re-weight method with ? = 0, and to the class-balanced re-weight method with ? = 1. We illustrate the curve of re-weight coefficients based on ImageNet-LT dataset in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Connection with Recent Work</head><p>Below we discuss the connections between our proposed distribution alignment strategy and recent two-stage methods. Detailed comparison is reported in Tab. 2. Notably, Logit Adjustment <ref type="bibr" target="#b29">[30]</ref> and Deconfound <ref type="bibr" target="#b38">[39]</ref> introduce a hand-craft</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Align Method margin to adjust the distribution while keep the magnitude as 1.0, and incorporate the class prior directly in r i or w i without re-training. LWS <ref type="bibr" target="#b19">[20]</ref> and ? -normalized <ref type="bibr" target="#b19">[20]</ref> try to achieve a similar goal by learning a magnitude scale and discarding the margin adjustment. All these methods can be considered as the special cases of our DisAlign approach, which provides a unified and simple form to model the distribution mismatch in a learnable way. Moreover, the resample based strategy is not easy to be applied for the instance-level (object detection/instance segmentation) or pixel-level (semantic segmentation) tasks, our generalized re-weight provides an alternative solution to incorporate the class prior in a simple and effective manner. Experimental results in Sec. 4 also demonstrate the strength of our method compared with the aforementioned works.</p><formula xml:id="formula_10">Type Balance Magnitude Margin Joint - - - - LWS[20] L CB-RS ? j 0 ? -Normalized[20] H CB-RS 1/||w j || ? 0 Logit Adjust[30] H - 1.0 ?? log(r j ) Deconfound * [39] H - 1.0 ??d(x, e)w j e DisAlign L G-RW 1 + ?(x)? j ?(x)? j DisAlign * L G-RW 1 + ?(x)? j ?(x)? j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct a series of experiments to validate the effectiveness of our method. Below we present our experimental analysis and ablation study on the image     Accuracy on Val(%) <ref type="figure">Figure 5</ref>: Effects of the different generalized re-weight scale. Performance is reported on ImageNet-LT val split.</p><p>baseline and the learnable margin also achieves competitive results at 49.9%, which demonstrate the effectiveness of individual modules in our design. 3) Generalized Re-weight Scale We also investigate the influence of the generalized re-weight scale on the validation set of ImageNet-LT and plot the accuracy-scale curve in <ref type="figure">Fig. 5</ref>. It is evident that adjusting generalized reweight is able to achieve significant performance improvement. Moreover, we find the setting of ? &gt; 1 is able to outperform the class-balanced re-weight (? = 1), which indicates that the generalized re-weight is more effective in coping with long-tail distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Semgnetaion on ADE20k Dataset</head><p>To further validate our method, we apply DisAlign strategy to segmentation networks and report our performance on the semantic segmentation benchmark, ADE20k <ref type="bibr" target="#b53">[54]</ref>.</p><p>Dataset and Implementation Details Follow a similar protocol as in image classification, we divide the 150 categories into 3 subsets according to the percentage of pixels in every category over the entire dataset. Specifically, we define three disjoint subsets as follows: head classes (each with more than 1.0% of total pixels), body classes (each with  <ref type="table">Table 7</ref>: Results on LVIS v0.5 dataset with different backbones and different architectures. The results are reported based on the Detectron2 <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">55]</ref> framework. We refer the reader to the supplementary material for the detailed comparison with the state of art. a percentage ranging from 0.1% to 1% of total pixels) and tail classes (each with less than 0.1% of total pixels). <ref type="bibr" target="#b1">2</ref> Quantitative Results We evaluate our method using two widely-adopted segmentation models (FCN <ref type="bibr" target="#b35">[36]</ref> and DeepLabV3 + <ref type="bibr" target="#b6">[7]</ref>) based on different backbone networks, ranging from ResNet-50, ResNet-101 to the latest ResNeSt-101, and report the performance in Tab. 6. Our method achieves 2.0 and 2.3 improvement in mIoU using FCN-8s with ResNet-50 and ResNet-101, respectively. The performance on the body and tail are improved significantly. Moreover, our method outperforms the baseline with large margin at 5.7 in mean accuracy with ResNet-101 backbone. Even with a stronger backbone: ResNeSt-101 <ref type="bibr" target="#b49">[50]</ref>, our method also achieves 0.7 mIoU and 2.8 improvement in mean accu-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object Detection and Instance Segmentation</head><p>Experimental Configuration We conduct experiments on LVIS <ref type="bibr" target="#b14">[15]</ref> dataset. For evaluation, we use a COCO-style average precision (AP) metric that averages over categories and different box/mask IoU threshold <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results and Ablation Study</head><p>We first compare our method with recent work and report quantitative results in Tab. 8. We find our DisAlign with cosine classifier head achieves 25.6 in AP bbox , and 26.3 in AP mask when applied to the Mask R-CNN+FPN with the ImageNet pretrained ResNet-50 backbone. Moreover, our strategy can be further improved to achieve 27.6 in AP bbox and 27.9 in AP mask based on the COCO pre-trained model. In both cases, our method is able to maintain the performance of the frequent (also called head) categories, and gain significant improvement on common (also called body) and rare (also called tail) categories. We also report performance with more power detection framework (e.g.Cascade R-CNN) and stronger backbones (e.g. ResNet-50/101, and ResNeXt-101) in Tab. 7 and Tab. 9. It is worth noting that even with the stronger backbones or frameworks, the performance gain of our DisAlign over the baseline is still significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a unified two-stage learning strategy for the large-scale long-tail visual recognition tasks. To tackle the biased label prediction, we develop a confidence-aware distribution alignment method to calibrate initial classification predictions. In particular, we design a generalized re-weight scheme to leverage the category prior for the alignment process. Extensive experiments show that our method outperforms previous works with a large margin on a variety of visual recognition tasks(image classification, semantic segmentation, and object detection/segmentation). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments of Image Classification</head><p>In this section, we first introduce the dataset and evaluation metrics for image classification task in Sec.A.1. Then the training configuration will be detailed in Sec.A.2, followed by results on three benchmarks in Sec.A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Dataset and Evaluation Metrics</head><p>Datasets To demonstrate our methods, we conduct experiments on three large-scale long-tailed datasets, including Places-LT <ref type="bibr" target="#b27">[28]</ref>, ImageNet-LT <ref type="bibr" target="#b27">[28]</ref>, and iNaturalist 2018 <ref type="bibr" target="#b39">[40]</ref>. Places-LT and ImageNet-LT are artificially generated by sampling a subset from their balanced versions (Places-365 <ref type="bibr" target="#b27">[28]</ref> and ImageNet-2012 <ref type="bibr" target="#b9">[10]</ref>) following the Parento distribution. iNaturalist 2018 is a real-world, naturally longtailed dataset, consisting of samples from 8,142 species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics We report the class-balanced average</head><p>Top-1 accuracy on the corresponding validation/test set, and also calculate the accuracy of three disjoint subsets, 'Many', 'Medium' and 'Few', which are defined according to the amount of training data per class <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training Configuration</head><p>Configuration Detail Following <ref type="bibr" target="#b19">[20]</ref>, we use PyTorch <ref type="bibr" target="#b31">[32]</ref> framework for all experiments. For ImageNet-LT, we report performance with ResNet-{50,101,152} and ResNeXt-{50,101,152} and mainly use ResNet-50 for ablation study. For iNaturalist 2018, performance is reported with ResNet-{50,101,152}. For Places-LT, ResNet-152 is used as backbone and we pre-train it on the full ImageNet-2012 dataset.</p><p>We use the SGD optimizer with momentum 0.9, batch size 256, cosine learning rate schedule gradually decaying from 0.1 to 0, and image resolution 224?224. For the joint learning stage, the backbone network and original classifier head are jointly trained with 90 epochs for ImageNet-LT, and 90/200 epochs for iNaturalist-2018. For the Places-LT dataset, the models are trained with 30 epochs with the all layers frozen expect the last ResNet block in the first stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of Our Method</head><p>In the second distribution alignment stage, we restart the learning rate and train it for 10/30 epochs as <ref type="bibr" target="#b19">[20]</ref> while keeping the backbone network and original classifier head fixed(10 epochs for ImageNet-LT and Places-LT, 30 epochs for iNaturalist-2018). For all three datasets, we set the generalized re-weight scale ? = 1.2 for dot-product classifier head, ? = 1.5 for cosine normalized classifier head. The ? and ? are initialized with 1.0 and 0.0, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Detailed Experimental Results</head><p>ImageNet-LT. We present the detailed quantitative results for ImageNet-LT in <ref type="table" target="#tab_0">Table 10</ref>.</p><p>iNaturalist and Places-LT. To further demonstrate our method, we conduct experiments on two extra large-scale long-tail benchmarks and report the performance in <ref type="table" target="#tab_0">Table  13</ref> and <ref type="table" target="#tab_0">Table 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Ablation Study</head><p>Influence of Model Components We report an ablation study of the two main components of our method with ResNeXt-50 in Tab. 11, which shows that both adaptive calibration and generalized re-weighting(G-RW) contribute to the performance improvement of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the Calibration</head><p>We plot the learned magnitude and margin according to the class sizes below. They share a similar trend, in which the tail/body classes have larger value than head. Thus our calibration alleviates the bias in the original prediction by boosting the tail scores.</p><p>Confidence Score We study confidence-based calibration in the table below, which shows that the input-aware calibration outperforms the input-agnostic counterpart and the baselines using only magnitude or margin. We also observe that the example whose biased prediction probability is low on its ground-truth class tends to be improved with higher confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments of Semantic Segmentation</head><p>Similar to image classification, the large-scale semantic segmentation task still suffers from the long-tail data distribution. To further validate the effectiveness of our method, we also apply DisAlign on large-scale semantic segmentation benchmark: ADE-20k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Dataset and Evaluation</head><p>Dataset. ADE20K dataset is a scene parsing benchmark, which contains 150 stuff/object categories. The dataset includes 20K/2K/3K images for training, validation, and testing. Compared with the image classification <ref type="bibr" target="#b27">[28]</ref>, the imbalance of ADE20K is more serve than the image classification, which has an imbalance ratio of 788(Max/Min). Follow the similar protocol in image classification, we divide the 150     <ref type="table" target="#tab_0">Table 12</ref>: Ablation of the Confidence Score. We extend the Tab.5(main paper) to analyze the influence of confidence score.</p><p>categories into 3 groups according to the ratio of pixel number over the whole dataset. Specifically, three disjoint subsets are: head classes(classes each with a ratio over 1.0%), body classes(classes each with a ratio ranging from 0.1% to 1%) and tail classes(classes under a ratio of 0.1%), the complete list of the split is reported in Tab. <ref type="bibr" target="#b15">16</ref>.</p><p>Evaluation. For the evaluation metric, we use the mean intersection of union(mIoU) and mean pixel accuracy(mAcc). We also report the mIoU and mAcc of each group(head, body and tail) for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Training Configuration</head><p>We implement our method based on MMSegmentation toolkit <ref type="bibr" target="#b30">[31]</ref>. In the joint learning training phase, we set the learning rate to 0.01 initially, which gradually decreases to 0 by following the 'poly' strategy as <ref type="bibr" target="#b48">[49]</ref>. The images are cropped to 512 ? 512 and augmented with randomly scaling(from 0.5 to 2.0) and flipping. ResNet-50, ResNet-101 and ResNeSt-101 <ref type="bibr" target="#b49">[50]</ref> are used as the backbone. For the evaluation metric, we use the mean intersection of union(mIoU)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Quantitative Results</head><p>We evaluate our method with two state-of-the-art segmentation models(FCN <ref type="bibr" target="#b35">[36]</ref> and DeepLabV3+ <ref type="bibr" target="#b6">[7]</ref>)based on different backbone networks, ranging from ResNet-50, ResNet-101 to the latest ResNeSt-101, and report the performance in Tab.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on LVIS Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Dataset and Evaluation Protocol</head><p>Dataset. LVIS v0.5 <ref type="bibr" target="#b14">[15]</ref> dataset is a benchmark dataset for research on large vocabulary object detection and instance segmentation, which contains 56K images over 1230 categories for training, 5K images for validation. This chal-lenging dataset is an appropriate benchmark to study the large-scale long-tail problem, where the categories can be binned into three types similar with ImageNet-LT: rare(1-10 training images), common(11-100 training images), and frequent(&gt; 100 training images).</p><p>Evaluation Protocol. We evaluate our method on LVIS for object detection and instance segmentation. For evaluation, we use a COCO-style average precision(AP) metric that averages over categories and different box/mask intersection over union(IoU) threshold <ref type="bibr" target="#b26">[27]</ref>. All standard LVIS evaluation metrics including AP, AP r , AP c , AP f for box bounding boxes and segmentation masks. Subscripts 'r', 'c', and 'f' refer to rare, common and frequent category subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Training Configuration</head><p>Experimental Details. We train our models for object detection and instance segmentation based on Detecron2 <ref type="bibr" target="#b47">[48]</ref>, which is implemented in PyTorch. Unless specified, we use the ResNet backbone(pre-trained on ImageNet) with FPN <ref type="bibr" target="#b24">[25]</ref>. Following the training procedure in <ref type="bibr" target="#b14">[15]</ref>, we resize the images so that the shorter side is 800 pixels. All baseline experiments are conducted on 8 GPUs with 2 images per GPU for 90K iterations, with a learning rate of 0.02 which is decreased by 10 at the 60K and 80K iteration. We use SGD with a weight decay of 0.0001 and momentum of 0.9. Scale jitter is applied for all experiments in default same with <ref type="bibr" target="#b14">[15]</ref>.</p><p>For the DisAlign, we freeze all network parameters and learn the magnitude and margin for extra 9K iterations with a learning rate of 0.02. Generalized re-weight is only used for fore-ground categories. Generalized re-weight scale ? is set to 0.8 for all experiments.    <ref type="table" target="#tab_0">Table 16</ref>: Splits of ADE-20K: The ratio of each category is reported according to <ref type="bibr" target="#b53">[54]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Quantitative Results</head><p>We report the detailed results in <ref type="table">Table.</ref>17 and Tab.18.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*Figure 1 :</head><label>1</label><figDesc>This work was done when Songyang Zhang was a research intern at Megvii Technology. This work was supported by Shanghai NSF Grant (No. 18ZR1425100), National Key R&amp;D Program of China (No. 2017YFA0700800), and Beijing Academy of Artificial Intelligence (BAAI). Code is available: https://github.com/Megvii-BaseDetection/DisAlign Per-class performance of the two-stage learning baseline and our empirical classification bound on ImageNet-LT val split. Two methods share the same representation while our bound setting retrains the classifier head with the balanced full dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Empirical analysis of the performance bottleneck. Left: Baseline vs. ideal performance for representations learned with different sampling strategy. Right: Comparison of prior arts and ideal performance for the classifier head calibration. Cls-Bound: ideal performance bound. IB: instance-balanced sampling. CB: class-balanced sampling. SR: square-root sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The category frequency r i is plotted in gray color and the right axis denotes the re-weight coefficient at different scale ? on ImageNet-LT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Analysis of the Calibration. We use model trained on ImageNet-LT with ResNeXt-50 for analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Framework</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results on ImageNet-LT. * denotes the model uses cosine classifier. R-50 and X-50 means the ResNet-50 and ResNeXt-50, respectively.</figDesc><table><row><cell>Method</cell><cell>Align Type</cell><cell></cell><cell cols="2">Top-1 Accuracy@R-50</cell><cell></cell><cell></cell><cell cols="2">Top-1 Accuracy@X-50</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="8">Average Many Medium Few Average Many Medium Few</cell></row><row><cell>Baseline[20]</cell><cell>-</cell><cell>41.6</cell><cell>64.0</cell><cell>33.8</cell><cell>5.8</cell><cell>44.4</cell><cell>65.9</cell><cell>37.5</cell><cell>7.7</cell></row><row><cell>Baseline  *</cell><cell>-</cell><cell>48.4</cell><cell>68.4</cell><cell>41.7</cell><cell>15.2</cell><cell>49.2</cell><cell>68.9</cell><cell>42.8</cell><cell>15.6</cell></row><row><cell>NCM[20]</cell><cell></cell><cell>44.3</cell><cell>53.1</cell><cell>42.3</cell><cell>26.5</cell><cell>47.3</cell><cell>56.6</cell><cell>45.3</cell><cell>28.1</cell></row><row><cell>? -Norm[20] Logit Adjust(post)[30]</cell><cell>Hand-Craft</cell><cell>46.7 50.4</cell><cell>56.6 -</cell><cell>44.2 -</cell><cell>27.4 -</cell><cell>49.4 -</cell><cell>59.1 -</cell><cell>46.9 -</cell><cell>30.7 -</cell></row><row><cell>Deconfound  *  [39]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.8</cell><cell>62.7</cell><cell>48.8</cell><cell>31.6</cell></row><row><cell>cRT[20]</cell><cell></cell><cell>47.3</cell><cell>58.8</cell><cell>44.0</cell><cell>26.1</cell><cell>49.6</cell><cell>61.8</cell><cell>46.2</cell><cell>27.4</cell></row><row><cell>cRT  *  [20]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.7</cell><cell>60.4</cell><cell>46.8</cell><cell>29.3</cell></row><row><cell>LWS[20]</cell><cell>Learnable</cell><cell>47.7</cell><cell>57.1</cell><cell>45.2</cell><cell>29.3</cell><cell>49.9</cell><cell>60.2</cell><cell>47.2</cell><cell>30.3</cell></row><row><cell>DisAlign</cell><cell></cell><cell>51.3</cell><cell>59.9</cell><cell>49.9</cell><cell>31.8</cell><cell>52.6</cell><cell>61.5</cell><cell>50.7</cell><cell>33.1</cell></row><row><cell>DisAlign  *</cell><cell></cell><cell>52.9</cell><cell>61.3</cell><cell>52.2</cell><cell>31.4</cell><cell>53.4</cell><cell>62.7</cell><cell>52.1</cell><cell>31.4</cell></row></table><note>p r (y|x) and the model prediction p m (y|x) as follows:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with related methods. * denotes cosine classifier, L: learnable, H:hand-craft, CB-RS: classbalanced resampling, G-RW: generalized re-weight, r j : class frequency for the j-th class, ?: hypper-parameter, e: mean feature of training data, d(?): cosine distance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Average accuracy on iNaturalist-2018. * denotes the cosine classifier.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">ResNet-152</cell><cell></cell></row><row><cell></cell><cell cols="4">Average Many Medium Few</cell></row><row><cell>Focal Loss[28]</cell><cell>34.6</cell><cell>41.1</cell><cell>34.8</cell><cell>22.4</cell></row><row><cell cols="2">Range Loss[28] 35.1</cell><cell>41.1</cell><cell>35.4</cell><cell>23.2</cell></row><row><cell>OLTR[28]</cell><cell>35.9</cell><cell>44.7</cell><cell>37.0</cell><cell>25.3</cell></row><row><cell cols="2">Feature Aug[8] 36.4</cell><cell>42.8</cell><cell>37.5</cell><cell>22.7</cell></row><row><cell>Baseline</cell><cell>30.2</cell><cell>45.7</cell><cell>27.3</cell><cell>8.2</cell></row><row><cell>NCM</cell><cell>36.4</cell><cell>40.4</cell><cell>37.1</cell><cell>27.3</cell></row><row><cell>cRT</cell><cell>36.7</cell><cell>42.0</cell><cell>37.6</cell><cell>24.9</cell></row><row><cell>LWS</cell><cell>37.6</cell><cell>40.6</cell><cell>39.1</cell><cell>28.6</cell></row><row><cell>? -norm</cell><cell>37.9</cell><cell>37.8</cell><cell>40.7</cell><cell>31.8</cell></row><row><cell>DisAlign</cell><cell>39.3</cell><cell>40.4</cell><cell>42.4</cell><cell>30.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">GR MT MG Average Many Medium Few</cell></row><row><cell>41.6</cell><cell>64.0</cell><cell>33.8</cell><cell>5.8</cell></row><row><cell>50.1</cell><cell>60.4</cell><cell>48.0</cell><cell>28.8</cell></row><row><cell>49.9</cell><cell>63.9</cell><cell>46.9</cell><cell>21.2</cell></row><row><cell>51.3</cell><cell>59.9</cell><cell>49.9</cell><cell>31.8</cell></row></table><note>Results on Place365-LT with ResNet-152.classification task in Sec. 4.1, followed by our results on semantic segmentation task in Sec. 4.2. In addition, we further evaluate our methods on object detection and instance segmentation tasks in Sec. 4.3.4.1. Image Classification Experimental Details To demonstrate our methods, we conduct experiments on three large-scale long-tail datasets, including ImageNet-LT [28], iNaturalist 2018 [40], and Places-LT [28]. We follow the experimental setting and implementation of [20] 1 . For the ImageNet-LT dataset, we report performance with ResNet/ResNeXt-{50,101,152} as backbone, and mainly use ResNet-50 for ablation study. For iNaturalist 2018 and Places-LT, our comparisons are performed under the settings of ResNet-{50,101,152}. Comparison with previous methods 1) ImageNet-LT. We present the quantitative results for ImageNet-LT in Tab. 10. Our approach achieves 52.9% in per-class average accuracy based on ResNet-50 backbone and 53.4% based on ResNeXt-50, which outperform the state-of-the-art methods by a significant margin of 2.5% and 1.6%, respectively. 2) iNaturalist. In Tab. 13, our method DisAlign with cosine1 Detailed configuration and results are provided in the supplementary materials.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of DisAlign. GR means the generalized reweight strategy. MT means the learnable magnitude parameter (1+?(x)?) and MG is the learnable margin parameter ?(x)?. DisAlign 40.1(+2.0) 65.0(+0.4) 42.8(+2.8) 31.3(+1.7) 51.4(+5.1) 78.6(+0.0) 56.1(+6.8) 40.6(+5.2) DisAlign 43.7(+2.3) 67.4(+0.4) 46.1(+2.8) 35.7(+2.5) 55.9(+5.7) 80.6(+0.0) 59.7(+6.8) 46.4(+6.3)</figDesc><table><row><cell></cell><cell>56</cell><cell>Baseline</cell><cell>cRT</cell><cell>LWS</cell><cell>DisAlign</cell><cell>DisAlign</cell></row><row><cell>Average Accuracy(%)</cell><cell>44 46 48 50 52 54</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>42</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>R-50</cell><cell cols="4">X-50 R-101 X-101 R-152 X-152</cell></row></table><note>* Figure 4: Performance of DisAlign with different back- bone on ImageNet-LT. Detailed results will be reported in supplementary material.classifier achieves 69.5% per-class average accuracy using ResNet-50 backbone and 90 epochs of training, surpassing the prior art LDAM by a large margin at 1.5%. It also shows that our performance can be further improved with larger backbone and/or more training epochs. 3) Places-LT. In Tab. 14, we show the experimental results under the same setting as [20] on Places-LT. Our method achieves 39.3% per-class average accuracy based on ResNet-152, with a no- table performance gain at 1.4% over the prior methods. We also report the detailed performance of these three datasets with ResNet-{50,101,152} in the supplementary materials. Ablation Study 1) Different Backbone: We validate our method on different types of backbone networks, ranging from ResNet-{50,101,152} to ResNeXt-{50, 101, 152}, re- ported in Fig. 4. Our method achieves 54.9% with ResNet- 152, and 55.0% with ResNeXt-152.method. Tab. 5 summarizes the results of our ablation ex- periments, in which we compare our full model with several partial model settings. From the table, we find the learnable magnitude has a significant improvement compared with</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance of semantic segmentation on ADE-20K: All baseline models are trained with an image size of 512?512 and 160K iteration in total.</figDesc><table><row><cell></cell><cell>B is backbone network(R-50, R-101, S-101 denote ResNet-50, ResNet-101 and</cell></row><row><cell cols="2">ResNeSt-101, respectively).</cell></row><row><cell>53</cell><cell></cell></row><row><cell>52</cell><cell></cell></row><row><cell>51</cell><cell></cell></row><row><cell>50</cell><cell></cell></row><row><cell>49</cell><cell></cell></row><row><cell>48</cell><cell>Reweight Scale 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison with the-state-of-art on LVIS with Mask-R-CNN-FPN(ResNet-50 backbone). All results are evaluated on the LVIS v0.5 validation set with the score threshold at 0.0001. ( * denotes cosine classifier for bbox classification.)</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell></cell><cell cols="2">BBox AP</cell><cell></cell><cell></cell><cell cols="2">Mask AP</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">AP bbox AP r bbox</cell><cell>AP c bbox</cell><cell>AP f bbox</cell><cell cols="2">AP mask AP r mask</cell><cell>AP c mask</cell><cell>AP f mask</cell></row><row><cell>ResNet-50</cell><cell>Baseline  *  DisAlign  *</cell><cell>26.5 30.5</cell><cell>8.7 17.9</cell><cell>25.0 30.1</cell><cell>36.0 36.5</cell><cell>23.5 27.0</cell><cell>8.1 15.7</cell><cell>22.4 27.0</cell><cell>31.5 31.9</cell></row><row><cell></cell><cell>De-confound[39]</cell><cell>25.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.5</cell><cell>5.2</cell><cell>22.7</cell><cell>32.3</cell></row><row><cell>ResNet-101</cell><cell>De-confound TDE[39]</cell><cell>30.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>27.1</cell><cell>16.0</cell><cell>26.9</cell><cell>32.1</cell></row><row><cell></cell><cell>Baseline  *</cell><cell>28.9</cell><cell>11.8</cell><cell>27.7</cell><cell>37.8</cell><cell>25.6</cell><cell>10.5</cell><cell>24.9</cell><cell>33.0</cell></row><row><cell></cell><cell>DisAlign  *</cell><cell>32.7</cell><cell>20.5</cell><cell>32.8</cell><cell>38.1</cell><cell>28.9</cell><cell>18.0</cell><cell>29.3</cell><cell>33.3</cell></row><row><cell>ResNeXt-101</cell><cell>Baseline  *  DisAlign  *</cell><cell>30.7 33.7</cell><cell>14.2 21.4</cell><cell>29.3 33.1</cell><cell>39.6 39.7</cell><cell>27.3 29.7</cell><cell>13.0 18.4</cell><cell>26.4 29.7</cell><cell>34.6 34.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Results on LVIS v1.0 dataset with Cascade R-CNN. * denotes cosine classifier head.racy, where the tail categories have a performance gain of 1.2 in mIoU and 3.7 in mean accuracy. We further validate our method using DeepLabV3 + , which is a more powerful semantic segmentation model. Our DisAlign improves the performance of DeepLabV3 + by a margin of 0.5 based on ResNeSt-101, which achieves the new state-of-the-art (47.8 in mIoU) on the ADE20k dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>[ 55 ]</head><label>55</label><figDesc>Benjin Zhu*, Feng Wang*, Jianfeng Wang, Siwei Yang, Jianhu Chen, and Zeming Li. cvpods: All-in-one toolbox for computer vision research, 2020. 7 [56] Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan. Capturing long-tail distributions of object subcategories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), 2014. 1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Top-1 Accuracy on ImageNet-LT test set. All models use the feature extractor and original classifier head trained with 90 epoch in joint learning stage, * denotes the model uses cosine classifier head.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Influence of Model Components. Backbone is ResNeXt-50, * means cosine classifier.</figDesc><table><row><cell>Generalized Re-weighting</cell><cell></cell></row><row><cell>Magnitude(w/o Confidence)</cell><cell></cell></row><row><cell>Magnitude</cell><cell></cell></row><row><cell>Margin(w/o Confidence)</cell><cell></cell></row><row><cell>Margin</cell><cell></cell></row><row><cell>Average Accuracy</cell><cell>41.6 49.9 50.1 49.6 49.9 51.0 51.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13</head><label>13</label><figDesc>* denotes the model uses cosine classifier head.</figDesc><table><row><cell></cell><cell cols="5">: Top-1 Accuracy on iNaturalist 2018 with different backbones(ResNet-{50,101,152}) and different training epochs(90</cell></row><row><cell cols="2">&amp; 200),  Backbone Method</cell><cell></cell><cell cols="2">Top-1 Accuracy</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Average Many Medium Few</cell></row><row><cell>R-50</cell><cell>Baseline DisAlign</cell><cell>29.2 37.8</cell><cell>45.3 39.3</cell><cell>25.5 40.7</cell><cell>8.0 28.5</cell></row><row><cell>R-101</cell><cell>Baseline DisAlign</cell><cell>30.2 38.5</cell><cell>46.1 39.1</cell><cell>26.9 42.0</cell><cell>8.4 29.1</cell></row><row><cell>R-152</cell><cell>Baseline DisAlign</cell><cell>30.2 39.3</cell><cell>45.7 40.4</cell><cell>27.3 42.4</cell><cell>8.2 30.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table><row><cell>Top-1 Accuracy on Places-LT with different</cell></row><row><cell>backbones(ResNet-{50,101,152}).</cell></row><row><cell>and mean pixel accuracy(mAcc). All models are trained</cell></row><row><cell>with 160k iterations with a batch size of 32 based on 8 V100</cell></row><row><cell>GPUs. In the DisAlign stage, we follow a similar protocol as</cell></row><row><cell>stage-1 and only training the model with 8k iterations. We</cell></row><row><cell>set ? = 0.3 for all experiments.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 15 :</head><label>15</label><figDesc>Results on ADE-20K: All baseline models are trained with a image size of 512x512 and 160K iteration in total. Aug denotes multi-scale is used for inference.</figDesc><table><row><cell>Category</cell><cell>Ratio</cell><cell cols="2">Group Category</cell><cell>Ratio</cell><cell cols="2">Group Category</cell><cell>Ratio</cell><cell cols="2">Group Category</cell><cell>Ratio</cell><cell cols="2">Group Category</cell><cell>Ratio</cell><cell>Group</cell></row><row><cell>'wall'</cell><cell cols="2">0.1576, Head</cell><cell>'armchair'</cell><cell cols="2">0.0044, Body</cell><cell>'river'</cell><cell cols="2">0.0015, Body</cell><cell>'airplane'</cell><cell cols="2">0.0007, Tail</cell><cell>'food'</cell><cell>0.0005, Tail</cell></row><row><cell>'building'</cell><cell cols="2">0.1072, Head</cell><cell>'seat'</cell><cell cols="2">0.0044, Body</cell><cell>'bridge'</cell><cell cols="2">0.0015, Body</cell><cell>'dirt track'</cell><cell cols="2">0.0007, Tail</cell><cell>'step'</cell><cell>0.0004, Tail</cell></row><row><cell>'sky'</cell><cell cols="2">0.0878, Head</cell><cell>'fence'</cell><cell cols="2">0.0033, Body</cell><cell>'bookcase'</cell><cell cols="2">0.0014, Body</cell><cell>'apparel'</cell><cell cols="2">0.0007, Tail</cell><cell>'tank'</cell><cell>0.0004, Tail</cell></row><row><cell>'floor'</cell><cell cols="2">0.0621, Head</cell><cell>'desk'</cell><cell cols="2">0.0031, Body</cell><cell>'blind'</cell><cell cols="2">0.0014, Body</cell><cell>'pole'</cell><cell cols="2">0.0006, Tail</cell><cell>'trade name'</cell><cell>0.0004, Tail</cell></row><row><cell>'tree'</cell><cell>0.048,</cell><cell>Head</cell><cell>'rock'</cell><cell>0.003,</cell><cell>Body</cell><cell>'coffee table'</cell><cell cols="2">0.0014, Body</cell><cell>'land'</cell><cell cols="2">0.0006, Tail</cell><cell>'microwave'</cell><cell>0.0004, Tail</cell></row><row><cell>'ceiling'</cell><cell>0.045,</cell><cell>Head</cell><cell>'wardrobe'</cell><cell cols="2">0.0027, Body</cell><cell>'toilet'</cell><cell cols="2">0.0014, Body</cell><cell>'bannister'</cell><cell cols="2">0.0006, Tail</cell><cell>'pot'</cell><cell>0.0004, Tail</cell></row><row><cell>'road'</cell><cell cols="2">0.0398, Head</cell><cell>'lamp'</cell><cell cols="2">0.0026, Body</cell><cell>'flower'</cell><cell cols="2">0.0014, Body</cell><cell>'escalator'</cell><cell cols="2">0.0006, Tail</cell><cell>'animal'</cell><cell>0.0004, Tail</cell></row><row><cell>'bed'</cell><cell cols="2">0.0231, Head</cell><cell>'bathtub'</cell><cell cols="2">0.0024, Body</cell><cell>'book'</cell><cell cols="2">0.0013, Body</cell><cell>'ottoman'</cell><cell cols="2">0.0006, Tail</cell><cell>'bicycle'</cell><cell>0.0004, Tail</cell></row><row><cell cols="3">'windowpane' 0.0198, Head</cell><cell>'railing'</cell><cell cols="2">0.0024, Body</cell><cell>'hill'</cell><cell cols="2">0.0013, Body</cell><cell>'bottle'</cell><cell cols="2">0.0006, Tail</cell><cell>'lake'</cell><cell>0.0004, Tail</cell></row><row><cell>'grass'</cell><cell cols="2">0.0183, Head</cell><cell>'cushion'</cell><cell cols="2">0.0023, Body</cell><cell>'bench'</cell><cell cols="2">0.0013, Body</cell><cell>'buffet'</cell><cell cols="2">0.0006, Tail</cell><cell>'dishwasher'</cell><cell>0.0004, Tail</cell></row><row><cell>'cabinet'</cell><cell cols="2">0.0181, Head</cell><cell>'base'</cell><cell cols="2">0.0023, Body</cell><cell>'countertop'</cell><cell cols="2">0.0012, Body</cell><cell>'poster'</cell><cell cols="2">0.0006, Tail</cell><cell>'screen'</cell><cell>0.0004, Tail</cell></row><row><cell>'sidewalk'</cell><cell cols="2">0.0166, Head</cell><cell>'box'</cell><cell cols="2">0.0022, Body</cell><cell>'stove'</cell><cell cols="2">0.0012, Body</cell><cell>'stage'</cell><cell cols="2">0.0006, Tail</cell><cell>'blanket'</cell><cell>0.0004, Tail</cell></row><row><cell>'person'</cell><cell>0.016,</cell><cell>Head</cell><cell>'column'</cell><cell cols="2">0.0022, Body</cell><cell>'palm'</cell><cell cols="2">0.0012, Body</cell><cell>'van'</cell><cell cols="2">0.0006, Tail</cell><cell>'sculpture'</cell><cell>0.0004, Tail</cell></row><row><cell>'earth'</cell><cell cols="2">0.0151, Head</cell><cell>'signboard'</cell><cell>0.002,</cell><cell>Body</cell><cell>'kitchen island'</cell><cell cols="2">0.0012, Body</cell><cell>'ship'</cell><cell cols="2">0.0006, Tail</cell><cell>'hood'</cell><cell>0.0004, Tail</cell></row><row><cell>'door'</cell><cell cols="2">0.0118, Head</cell><cell cols="3">'chest of drawers' 0.0019, Body</cell><cell>'computer'</cell><cell cols="2">0.0011, Body</cell><cell>'fountain'</cell><cell cols="2">0.0005, Tail</cell><cell>'sconce'</cell><cell>0.0003, Tail</cell></row><row><cell>'table'</cell><cell>0.011,</cell><cell>Head</cell><cell>'counter'</cell><cell cols="2">0.0019, Body</cell><cell>'swivel chair'</cell><cell>0.001,</cell><cell>Tail</cell><cell>'conveyer belt'</cell><cell cols="2">0.0005, Tail</cell><cell>'vase'</cell><cell>0.0003, Tail</cell></row><row><cell>'mountain'</cell><cell cols="2">0.0109, Head</cell><cell>'sand'</cell><cell cols="2">0.0018, Body</cell><cell>'boat'</cell><cell cols="2">0.0009, Tail</cell><cell>'canopy'</cell><cell cols="2">0.0005, Tail</cell><cell>'traffic light'</cell><cell>0.0003, Tail</cell></row><row><cell>'plant'</cell><cell cols="2">0.0104, Head</cell><cell>'sink'</cell><cell cols="2">0.0018, Body</cell><cell>'bar'</cell><cell cols="2">0.0009, Tail</cell><cell>'washer'</cell><cell cols="2">0.0005, Tail</cell><cell>'tray'</cell><cell>0.0003, Tail</cell></row><row><cell>'curtain'</cell><cell cols="2">0.0104, Head</cell><cell>'skyscraper'</cell><cell cols="2">0.0018, Body</cell><cell>'arcade machine'</cell><cell cols="2">0.0009, Tail</cell><cell>'plaything'</cell><cell cols="2">0.0005, Tail</cell><cell>'ashcan'</cell><cell>0.0003, Tail</cell></row><row><cell>'chair'</cell><cell cols="2">0.0103, Head</cell><cell>'fireplace'</cell><cell cols="2">0.0018, Body</cell><cell>'hovel'</cell><cell cols="2">0.0009, Tail</cell><cell cols="3">'swimming pool' 0.0005, Tail</cell><cell>'fan'</cell><cell>0.0003, Tail</cell></row><row><cell>'car'</cell><cell cols="2">0.0098, Body</cell><cell>'refrigerator'</cell><cell cols="2">0.0018, Body</cell><cell>'bus'</cell><cell cols="2">0.0009, Tail</cell><cell>'stool'</cell><cell cols="2">0.0005, Tail</cell><cell>'pier'</cell><cell>0.0003, Tail</cell></row><row><cell>'water'</cell><cell cols="2">0.0074, Body</cell><cell>'grandstand'</cell><cell cols="2">0.0018, Body</cell><cell>'towel'</cell><cell cols="2">0.0008, Tail</cell><cell>'barrel'</cell><cell cols="2">0.0005, Tail</cell><cell>'crt screen'</cell><cell>0.0003, Tail</cell></row><row><cell>'painting'</cell><cell cols="2">0.0067, Body</cell><cell>'path'</cell><cell cols="2">0.0018, Body</cell><cell>'light'</cell><cell cols="2">0.0008, Tail</cell><cell>'basket'</cell><cell cols="2">0.0005, Tail</cell><cell>'plate'</cell><cell>0.0003, Tail</cell></row><row><cell>'sofa'</cell><cell cols="2">0.0065, Body</cell><cell>'stairs'</cell><cell cols="2">0.0017, Body</cell><cell>'truck'</cell><cell cols="2">0.0008, Tail</cell><cell>'waterfall'</cell><cell cols="2">0.0005, Tail</cell><cell>'monitor'</cell><cell>0.0003, Tail</cell></row><row><cell>'shelf'</cell><cell cols="2">0.0061, Body</cell><cell>'runway'</cell><cell cols="2">0.0017, Body</cell><cell>'tower'</cell><cell cols="2">0.0008, Tail</cell><cell>'tent'</cell><cell cols="2">0.0005, Tail</cell><cell>'bulletin board' 0.0003, Tail</cell></row><row><cell>'house'</cell><cell>0.006,</cell><cell>Body</cell><cell>'case'</cell><cell cols="2">0.0017, Body</cell><cell>'chandelier'</cell><cell cols="2">0.0008, Tail</cell><cell>'bag'</cell><cell cols="2">0.0005, Tail</cell><cell>'shower'</cell><cell>0.0003, Tail</cell></row><row><cell>'sea'</cell><cell cols="2">0.0053, Body</cell><cell>'pool table'</cell><cell cols="2">0.0017, Body</cell><cell>'awning'</cell><cell cols="2">0.0007, Tail</cell><cell>'minibike'</cell><cell cols="2">0.0005, Tail</cell><cell>'radiator'</cell><cell>0.0003, Tail</cell></row><row><cell>'mirror'</cell><cell cols="2">0.0052, Body</cell><cell>'pillow'</cell><cell cols="2">0.0017, Body</cell><cell>'streetlight'</cell><cell cols="2">0.0007, Tail</cell><cell>'cradle'</cell><cell cols="2">0.0005, Tail</cell><cell>'glass'</cell><cell>0.0002, Tail</cell></row><row><cell>'rug'</cell><cell cols="2">0.0046, Body</cell><cell>'screen door'</cell><cell cols="2">0.0015, Body</cell><cell>'booth'</cell><cell cols="2">0.0007, Tail</cell><cell>'oven'</cell><cell cols="2">0.0005, Tail</cell><cell>'clock'</cell><cell>0.0002, Tail</cell></row><row><cell>'field'</cell><cell>0.004</cell><cell>Body</cell><cell>'stairway'</cell><cell cols="2">0.0015 Body</cell><cell cols="3">'television receiver' 0.0007 Tail</cell><cell>'ball'</cell><cell cols="2">0.0005 Tail</cell><cell>'flag'</cell><cell>0.0002 Tail</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 17 :</head><label>17</label><figDesc>Results on LVIS v0.5 dataset with Mask R-CNN. * denotes the model use cosine classifier head.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell></cell><cell cols="2">BBox AP</cell><cell></cell><cell></cell><cell cols="2">Mask AP</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">AP bbox AP r bbox</cell><cell>AP c bbox</cell><cell>AP f bbox</cell><cell cols="2">AP mask AP r mask</cell><cell>AP c mask</cell><cell>AP f mask</cell></row><row><cell></cell><cell>Baseline</cell><cell>25.2</cell><cell>3.7</cell><cell>24.3</cell><cell>34.8</cell><cell>23.0</cell><cell>3.5</cell><cell>23.0</cell><cell>30.8</cell></row><row><cell>ResNet-50</cell><cell>DisAlgin</cell><cell>28.7</cell><cell>9.0</cell><cell>30.2</cell><cell>34.6</cell><cell>26.1</cell><cell>8.4</cell><cell>28.1</cell><cell>30.7</cell></row><row><cell></cell><cell>Baseline  *</cell><cell>28.8</cell><cell>15.4</cell><cell>28.2</cell><cell>34.9</cell><cell>26.2</cell><cell>13.6</cell><cell>26.3</cell><cell>31.1</cell></row><row><cell></cell><cell>DisAlgin  *</cell><cell>32.2</cell><cell>21.6</cell><cell>33.3</cell><cell>35.2</cell><cell>29.4</cell><cell>19.4</cell><cell>30.9</cell><cell>31.4</cell></row><row><cell></cell><cell>Baseline</cell><cell>26.1</cell><cell>3.4</cell><cell>25.4</cell><cell>35.9</cell><cell>24.0</cell><cell>3.3</cell><cell>24.2</cell><cell>32.0</cell></row><row><cell>ResNet-101</cell><cell>DisAlgin</cell><cell>29.7</cell><cell>8.1</cell><cell>31.7</cell><cell>35.8</cell><cell>27.3</cell><cell>7.8</cell><cell>29.7</cell><cell>32.0</cell></row><row><cell></cell><cell>Baseline  *</cell><cell>30.4</cell><cell>15.5</cell><cell>30.3</cell><cell>36.5</cell><cell>28.1</cell><cell>13.9</cell><cell>29.2</cell><cell>32.4</cell></row><row><cell></cell><cell>DisAlgin  *</cell><cell>33.7</cell><cell>22.1</cell><cell>34.9</cell><cell>36.9</cell><cell>30.9</cell><cell>19.0</cell><cell>33.2</cell><cell>32.8</cell></row><row><cell></cell><cell>Baseline</cell><cell>28.4</cell><cell>4.6</cell><cell>28.6</cell><cell>37.5</cell><cell>26.1</cell><cell>4.6</cell><cell>27.2</cell><cell>33.4</cell></row><row><cell>ResNeXt-101</cell><cell>DisAlgin</cell><cell>31.3</cell><cell>9.5</cell><cell>33.2</cell><cell>37.7</cell><cell>28.7</cell><cell>9.0</cell><cell>31.1</cell><cell>33.6</cell></row><row><cell></cell><cell>Baseline  *</cell><cell>32.6</cell><cell>18.5</cell><cell>32.8</cell><cell>37.9</cell><cell>29.8</cell><cell>16.9</cell><cell>30.9</cell><cell>33.7</cell></row><row><cell></cell><cell>DisAlgin  *</cell><cell>34.7</cell><cell>24.6</cell><cell>35.3</cell><cell>38.1</cell><cell>31.8</cell><cell>22.0</cell><cell>33.2</cell><cell>33.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 18 :</head><label>18</label><figDesc>Results on LVIS v0.5 dataset with Cascade R-CNN. * denotes the model use cosine classifier head.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The complete list of the split is reported in supplementary material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sharing representations for long tail computer vision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on International Conference on Multimodal Interaction</title>
		<meeting>the ACM on International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>TPAMI)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision(ECCV)</title>
		<meeting>the European Conference on Computer Vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision(ECCV)</title>
		<meeting>the European Conference on Computer Vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Learning from Imbalanced Datasets</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Solution for large-scale hierarchical object detection datasets with incomplete annotation and data imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ti</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision(ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision(ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Borderlinesmote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Computing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision(ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision(ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations(ICLR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Striking the right balance with uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
			<publisher>TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>2017. 13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision(ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision(ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision(ECCV)</title>
		<meeting>the European Conference on Computer Vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision(ECCV)</title>
		<meeting>the European Conference on Computer Vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Long-tail learning via logit adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<idno>2020. 12</idno>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
	</analytic>
	<monogr>
		<title level="j">Open MMLab. Mmsegmentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2007" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision(ECCV)</title>
		<meeting>the European Conference on Computer Vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Longtailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Data augmentation for object detection via progressive and selective instance-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The devil is in classification: A simple framework for long-tail instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision(ECCV)</title>
		<meeting>the European Conference on Computer Vision(ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems(NeurIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-mimic learning for small-scale pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<idno>2020. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Solving long-tailed recognition with deep realistic taxonomic classi?? er</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision(ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unequal-training for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<idno>2017. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
