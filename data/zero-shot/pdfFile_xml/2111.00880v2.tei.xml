<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarks for Corruption Invariant Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Chen</surname></persName>
							<email>ming_hui.chen@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Southern</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Benchmarks for Corruption Invariant Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When deploying person re-identification (ReID) model in safety-critical applications, it is pivotal to understanding the robustness of the model against a diverse array of image corruptions. However, current evaluations of person ReID only consider the performance on clean datasets and ignore images in various corrupted scenarios. In this work, we comprehensively establish five ReID benchmarks for learning corruption invariant representation. In the field of ReID, we are the first to conduct an exhaustive study on corruption invariant learning in single-and cross-modality datasets, including Market-1501, CUHK03, MSMT17, RegDB, SYSU-MM01. After reproducing and examining the robustness performance of 21 recent ReID methods, we have some observations: 1) transformer-based models are more robust towards corrupted images, compared with CNN-based models, 2) increasing the probability of random erasing (a commonly used augmentation method) hurts model corruption robustness, 3) cross-dataset generalization improves with corruption robustness increases. By analyzing the above observations, we propose a strong baseline on both single-and cross-modality ReID datasets which achieves improved robustness against diverse corruptions. Our codes are available on https://github.com/MinghuiChen43/CIL-ReID.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (ReID) is regarded as a fine-grained instance retrieval problem. Unlike image classification tasks, the goal of person ReID is to match and rank pedestrian images across multiple non-overlapping cameras <ref type="bibr" target="#b49">[50]</ref>. Due to its vast applications for intelligent security and video surveillance, person ReID has become a hot topic in computer vision. However, there are still many problems in deploying current person ReID models to the real world. Unlike object classification and detection, ReID is a instance-level recognition and ranking problem that relies on extracting robust and detailed information. Unfortunately, current neural networks are easily confused by various forms of corruptions such as noise, blurring and snow <ref type="bibr" target="#b15">[16]</ref>. Therefore, learning invariant representation towards corrupted images in this task is challenging and merits extra investigations.</p><p>To comprehensively study the corruption invariant learning of models in various scenarios, we make the first attempt to establish the corruption invariant ReID benchmarks on both single-and cross-modality datasets, including Market-1501 <ref type="bibr" target="#b52">[53]</ref>, CUHK-03 <ref type="bibr" target="#b22">[23]</ref>, MSMT17 <ref type="bibr" target="#b45">[46]</ref>, RegDB <ref type="bibr" target="#b29">[30]</ref>, and SYSU-MM01 <ref type="bibr" target="#b46">[47]</ref>. The statistics of these datasets are shown in Tab. 1. We re-construct these five datasets by applying <ref type="bibr" target="#b19">20</ref>   The performance of the corrupted test set (corrupted query and gallery) is shown on the right. The overall mAP is significantly lower, and recent methods (e.g. LUPerson) are even less robust than PCB (proposed in 2017). Refer to the appendix for corresponding literature of above methods and performance on other datasets. each corruption type contains five levels of severity. The benchmark datasets are constructed based on agnostic corruption types that are not encountered in model training. Since corruption types in the real world are numerous and unpredictable, it will be more practical to learn a corruption generalized model without additional training and adaptation. To measure the corruption robustness comprehensively, we present robustness performance in three evaluation settings: 1) corrupted query, 2) corrupted gallery, and 3) corrupted query and gallery.</p><p>Based on the corruption invariant learning benchmark, we reproduce 21 advanced ReID methods in recent years and conduct large-scale evaluations on the corruption robustness of various state-of-theart CNN-based and transformer-based models. While the performance of ReID methods on the clean test set has shown an upward trend in recent years (see <ref type="figure" target="#fig_1">Fig. 1</ref>), the performance on the corrupted test set is significantly lower, with plenty of room for improvements. Specifically, we have some main findings based on robustness evaluation: 1) transformer-based models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> excel in corrupted test set comparing with CNN-based models. This demonstrates that the transformer-based models are capable of mining rich structured patterns, which is especially important when dealing with corrupted data. 2) In contrary to the clean test set, increasing the probability of the random erasing <ref type="bibr" target="#b56">[57]</ref>, a frequently used augmentation technique, impairs model performance on the corrupted test set. We argue that random erasing method hinders models from mining rich discriminative information from corrupted images. 3) Interestingly, the cross-dataset generalization tends to improve with the corruption robustness. This clearly refutes that robustness towards synthetic corruption do not help with robustness on naturally occurring distribution shifts <ref type="bibr" target="#b38">[39]</ref>.</p><p>From the above investigations, we introduce a strong baseline on the corruption invariant learning benchmarks in person ReID. In summary, our contributions are as follows:</p><p>? We propose benchmarks for corruption invariant Person ReID, including both single-and cross-modality datasets Market-1501, CUHK-03, MSMT17, RegDB, and SYSU-MM01. ? We reproduce 21 advanced ReID methods and have some interesting findings on learning corruption invariant representation in terms of network architectures and data augmentation. ? We are the first to reveal that cross-dataset generalization tends to increases with corruption robustness. This intriguing finding demonstrates the practical utility of learning a corruption invariant model towards real-world distribution shift, which has been overlooked by previous research on corruption robustness. ? We establish a strong baseline for corruption invariant person ReID, improving random erasing, BNNeck and identity loss.  <ref type="bibr" target="#b49">[50]</ref>. A feature of the open-world settings is that pedestrian data might be heterogeneous data, including infrared images <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b46">47]</ref>, cross-resolution images <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref> and even text descriptions <ref type="bibr" target="#b21">[22]</ref>. Corrupted images can be regarded as heterogeneous data for clean images, and our corruption-invariant ReID benchmarks as an open-world ReID setting defined in <ref type="bibr" target="#b49">[50]</ref>. Current person ReID system contains three main components: feature representation learning, deep metric learning and ranking optimization <ref type="bibr" target="#b49">[50]</ref>. The goal of feature representation learning is to efficiently extract discriminative features. This is accomplished through the use of attention mechanisms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>, the capture of multi-scale features <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48]</ref>, and the mining of local features <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51]</ref>. In ReID, deep metric learning entails developing a reasonable loss function <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref> and devising an appropriate sampling strategy <ref type="bibr" target="#b24">[25]</ref>. The purpose of rank optimization is to improve retrieval performance during the inference stage. The most frequently used strategy is to optimize the ranking list by leveraging gallery-to-gallery similarity <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Corruption Robustness</head><p>The human visual system is not easily fooled by a wide range of image corruptions, such as noise, blurring and pixelation or their combination. In contrast, current deep neural networks suffer from severe performance degradation towards corrupted images <ref type="bibr" target="#b15">[16]</ref>. Study on corruption robustness has a long history in computer vision <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b5">6]</ref> and has recently received more attention due to the release of corruption benchmarks for image recognition, such as CIFAR-10-C, CIFAR-100-C and ImageNet-C <ref type="bibr" target="#b15">[16]</ref>. Since then, similar benchmarks on common corruption have also been proposed in the field of object detection <ref type="bibr" target="#b28">[29]</ref>, semantic segmentation <ref type="bibr" target="#b20">[21]</ref> and pose estimation <ref type="bibr" target="#b42">[43]</ref>. These benchmarks reveal that the generalization ability of advanced models under corrupted input still needs to be further improved <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>. For improving the corruption robustness, various data augmentation techniques have been proposed recently. For example, AugMix <ref type="bibr" target="#b16">[17]</ref> utilizes a formulation to mix multiple augmented images and obtains significant improvement on ImageNet-C. Rusak et al. <ref type="bibr" target="#b34">[35]</ref> design a data augmentation algorithm based on adversarial framework for defending against common corruptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corruption Invariant ReID Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Metrics</head><p>To evaluate the performance of a ReID system, mAP (mean average precision) <ref type="bibr" target="#b52">[53]</ref> and CMC-k (cumulative matching characteristics, a.k.a, Rank-k matching accuracy) <ref type="bibr" target="#b43">[44]</ref> are two widely used measurements. Besides common used metrics mAP and CMC-k, we also present mINP (mean inverse negative penalty) to evaluate the ability to retrieve the hardest correct match <ref type="bibr" target="#b49">[50]</ref>. For a robust Re-ID system, the correct matches should have low rank values. The mINP is represented by</p><formula xml:id="formula_0">mINP = 1 n i (1 ? NP i ) = 1 n i (1 ? R hard i ? |G i | R hard i ) = 1 n i |G i | R hard i ,<label>(1)</label></formula><p>where R hard i indicates the rank position of the hardest match, |G i | represents the total number of correct matches for query i, and NP represents negative penalty. The INP (the highest, the better) is inverse of NP, which is a computationally efficient metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Benchmark Datasets</head><p>Our robust person ReID benchmarks are composed of five datasets: Market-1501, CUHK-03, MSMT17, RegDB, and SYSU-MM01. The detailed information and statics of these datasets are shown in Tab. 1. We employ 15 image corruptions from ImageNet-C dataset and 4 image corruptions from Extra ImageNet-C <ref type="bibr" target="#b15">[16]</ref>. In addition, we introduce a rain corruption type, which is a common In contrast to object classification <ref type="bibr" target="#b15">[16]</ref> and detection <ref type="bibr" target="#b28">[29]</ref>, the ReID task is an image pair matching problem with a query and gallery as a test set. To assess the robustness on a broad scale, we present three evaluation settings: both the query and gallery are corrupted, the query is corrupted alone, and the gallery is corrupted alone. For the cross-modality datasets RegDB and SYSU-MM01, only RGB images from the gallery are corrupted. Additionally, considering that the size of the ReID test set has a great impact on performance indicators, we do not directly add all corruption types and all severity levels to the test set. We randomly select one corruption type and one severity level from each image in the test set to create the query or gallery. We repeat the preceding evaluation ten times with the same query and gallery size as the clean test set (three times for large scale datasets MSMT17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation Models</head><p>Our standard backbone is the widely used ResNet50 <ref type="bibr" target="#b11">[12]</ref>. We follow a standard training pipeline, which includes initialization with an ImageNet pre-trained model and modification of the dimension of the fully connected layer to N <ref type="bibr" target="#b25">[26]</ref>. N is the number of identities in the training set. In the early training epochs, we adopt a learning warmup strategy. Additionally, the transformer-based models <ref type="bibr" target="#b40">[41]</ref> (e.g. ViT <ref type="bibr" target="#b6">[7]</ref>, DeiT <ref type="bibr" target="#b39">[40]</ref>) for object ReID are based on TransReID proposed by He et al. <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Strong Baseline</head><p>Based on our findings from corruption robustness evaluation, we design a new robust baseline for person ReID, which achieves competitive performance on both single-and cross-modality ReID tasks. Our baseline contains the following key components.</p><p>Local-based augmentation. Random erasing <ref type="bibr" target="#b56">[57]</ref> is an augmentation method that randomly selects a rectangle region in an image and erases its pixel with a random value (see <ref type="figure" target="#fig_2">Fig. 2</ref>). If no special statement is present, we utilize random cropping, horizontal flipping, and 0.5-probabilistic random erasing as default data augmentations. Random erasing yields consistent improvement on various person ReID datasets, but we find that the performance on corrupted datasets decreases with increasing erasing probability (see <ref type="figure" target="#fig_4">Fig. 4</ref> right part). Additionally, we observe that an augmentation method called RandomPatch <ref type="bibr" target="#b57">[58]</ref> also degrades the corruption robustness. RandomPatch works by first creating a patch pool of randomly extracted image patches and then pasting a random patch from the patch pool onto an input image at a random position. We believe that these two augmentation methods, which heavily occlude images and introduce additional perturbation information, will impair the models' ability to mine salient local information, which is critical for retrieving corrupted images. To compensate for the loss of discriminative information caused by strong erasing, we propose a data augmentation technique called soft random erasing, in which the erased area is not completely replaced with random pixels but retains a proportion of the original pixels, as shown in <ref type="figure" target="#fig_2">Fig.  2</ref> (a). To alleviate the strong perturbation introduced by RandomPatch, we propose a mixing-based augmentation technique called self patch mixing (SelfPatch). As illustrated in <ref type="figure" target="#fig_2">Fig. 2 (b)</ref>, SelfPatch works by randomly cutting a block from the original image and then remixing it with another random position.</p><p>Consistent identity loss. The classical identity (ID) loss <ref type="bibr" target="#b54">[55]</ref> is computed by the cross-entropy</p><formula xml:id="formula_1">L id = ? 1 n n i=1 log(p(y i |x i )).<label>(2)</label></formula><p>Given an input image x i with label y i , the predicted probability of x i being recognized as class y i is encoded with a softmax function, represented by p(y i |x i ). The identity loss is then computed by the cross-entropy, where n represents the number of training samples within each batch <ref type="bibr" target="#b49">[50]</ref>. The previous ID loss only calculates the loss of a single augmented sample per image. To enforce model response smoother for different augmented variants, we utilize the Jensen-Shannon divergence among the posterior distribution of the original sample x orig and its augmented variants <ref type="bibr" target="#b16">[17]</ref>. That is, for</p><formula xml:id="formula_2">p orig =p(y | x orig ), p aug1 =p(y | x aug1 ), p aug2 =p(y|x aug2 ).</formula><p>The self identity loss can be computed by first obtaining M = (p orig + p aug1 + p aug2 )/3 and then computing</p><formula xml:id="formula_3">L cid (p orig ; p aug1 ; p aug2 ) = 1 3 KL[p orig M ] + KL[p aug1 M ] + KL[p aug2 M ] .<label>(3)</label></formula><p>The gain of training with consistent ID loss is obvious when combining with global data augmentation (e.g. AugMix). Inference before BNNeck. BNNeck <ref type="bibr" target="#b25">[26]</ref> is a batch normalization (BN) <ref type="bibr" target="#b19">[20]</ref> layer after features for triplet loss and before classifier fully-connected layers in ReID tasks. The motivation of BNNeck is to make features gaussianly distribute near the surface of the hypersphere and make the ID loss easier to converge <ref type="bibr" target="#b25">[26]</ref>. However, we discover that the corruption robustness of models will decrease when using features after BNNeck (see appendix). One reasonable explanation for this is that the BN layer will memorize the statistical information of the train set, while the statistical information of the corrupted test set and the clean train set are quite different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmarking SOTA Methods</head><p>In this part, we evaluate the corruption robustness of 21 ReID methods, including AGW <ref type="bibr" target="#b49">[50]</ref>, BoT <ref type="bibr" target="#b25">[26]</ref>, ABD-Net <ref type="bibr" target="#b2">[3]</ref>, OS-Net <ref type="bibr" target="#b57">[58]</ref>, DG-Net <ref type="bibr" target="#b54">[55]</ref>, MHN <ref type="bibr" target="#b1">[2]</ref>, BDB <ref type="bibr" target="#b4">[5]</ref>, TransReID <ref type="bibr" target="#b13">[14]</ref>, LGPR <ref type="bibr" target="#b9">[10]</ref>, F-LGPR <ref type="bibr" target="#b10">[11]</ref>, TDB <ref type="bibr" target="#b32">[33]</ref>, LUPerson <ref type="bibr" target="#b7">[8]</ref>, LightMBN <ref type="bibr" target="#b18">[19]</ref>, PLR-OSNet <ref type="bibr" target="#b47">[48]</ref>, CaceNet <ref type="bibr" target="#b50">[51]</ref>, PCB <ref type="bibr" target="#b37">[38]</ref>, Pyramid <ref type="bibr" target="#b51">[52]</ref>, AlignedReID++ <ref type="bibr" target="#b26">[27]</ref>, RRID <ref type="bibr" target="#b31">[32]</ref>, VPM <ref type="bibr" target="#b36">[37]</ref>, and MGN <ref type="bibr" target="#b41">[42]</ref> (see our appendix for specific parameter settings of models). <ref type="figure" target="#fig_3">Fig. 3</ref> illustrates the Rank-1, mAP, and mINP performance indicators of 21 ReID methods in recent years for both the clean test set and the corrupted dataset (the corrupted query and gallery). The bubble size indicates the relative level of mINP indicator (see the appendix for more details). In general, existing methods perform poorly on the corrupted test set, and there is vast room for improvement. In <ref type="figure" target="#fig_3">Fig. 3</ref>, there is no obvious trade-off or positive correlation between the model performance on the clean test set and the corrupted test set.</p><p>TransReID <ref type="bibr" target="#b13">[14]</ref> significantly outperforms other methods in terms of indicators (most notably the mINP) of corrupted test sets. It is worth noting that the mINP index measures the ability to retrieve difficult samples, which makes it an appropriate indicator of the ReID model corruption robustness. From <ref type="figure" target="#fig_1">Fig. 1 and 3</ref>, we can observe that part-level based ReID methods perform well on clean and corrupted test sets. This demonstrates that learning local features is still critical for the corrupted images, and it can also make the model more robust to corruption variation. On the corrupted test set, the performance of the vanilla PCB <ref type="bibr" target="#b37">[38]</ref> is still competitive, even surpassing some methods that perform excellently on the clean dataset.</p><p>Some of the above reproduced ReID methods were proposed to learn a noise-robust model. These sample noises include heavy occlusion (e.g. VPM <ref type="bibr" target="#b36">[37]</ref>), inaccurate bounding boxes caused by sampling errors (e.g. Pyramid <ref type="bibr" target="#b51">[52]</ref>), illumination variation (e.g. BDB <ref type="bibr" target="#b4">[5]</ref>), style changing (e.g. DG-Net <ref type="bibr" target="#b54">[55]</ref>), and adversarial perturbations (e.g. F-LGPR <ref type="bibr" target="#b10">[11]</ref>). But unfortunately, the corruption robustness of the above methods is not particularly strong. Therefore, we argue that the corruption invariant ReID is complementary to the previous research on noise-robust ReID and merits special investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Connection with Generalizable Person ReID</head><p>In the previous corruption robustness research, it is less clear how a robust model generalizes across different datasets. For image classification, Taori et al. <ref type="bibr" target="#b38">[39]</ref> found that current robustness measures for synthetic distribution shift are at most weakly predictive for robustness on the natural distribution shifts presently available. However, we found that corruption robustness measures are predictive for robustness on the natural distribution shifts on person ReID. As illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>, extensive experiments with various methods and data enhancements reveal that the ability to generalize across datasets increases as corruption robustness increases. The cross-dataset generalization ability refers to the performance of the model trained on the Market-1501 dataset and tested on another dataset (e.g. CUHK-03 and MSMT17). The histogram represents the performance on the clean Market-1501, and the mAP is the value on the left y-axis. The blue and green lines respectively reflect the trend of the corruption robustness and the cross-dataset generalization, and the mAP is the value on the right y-axis. As illustrated in the left panel of <ref type="figure" target="#fig_4">Fig. 4</ref>, the cross-dataset generalization exhibits a consistent upward trend with the corruption robustness (correlation coefficient ? = 0.97). However, there is no obvious correlation between cross-dataset generalization and clean sample performance (correlation coefficient ? = 0.22).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarking Network Architecture</head><p>To further analyze the corruption robustness of the TransReID method, we compare CNN-based models and transformer-based models in this part. The number of parameters (Params) and multi-adds (MACs) of evaluated models are presented. To begin, the performance of TransReID illustrated in <ref type="figure" target="#fig_1">Fig.  1</ref> is identical to that of Trans-Vit-base in <ref type="figure" target="#fig_3">Fig. 3</ref>. Although it outperforms other models, it requires more memory and computation time. Additionally, we present the robustness performance in two settings, one in which only the query is corrupted and one in which only the gallery is corrupted. In general, a corrupted query makes models more difficult to sort simple samples correctly (Rank-1 is low), whereas a corrupted gallery makes models more difficult to retrieve difficult samples (mINP is low). When memory and computational overhead are considered, we discover that the ViT architecture is still superior in terms of corruption robustness. On the corrupted test set (corrupted query and gallery setting), ViT-S and DeiT-S outperform all CNN-based models except for ResNeXt-101-ibn <ref type="bibr" target="#b48">[49]</ref>. Additionally, we discover that when ResNet-50 is combined with an IBN <ref type="bibr" target="#b30">[31]</ref> module, the corruption robustness of ResNet-50 is significantly improved. This is consistent with their findings <ref type="bibr" target="#b30">[31]</ref> that instance normalization (IN) learns features that are invariant to changes in appearance, such as colors and styles. In summary, incorporating attention modules and judicious use of IN into network architecture can significantly improve the corruption robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Benchmarking Data Augmentation</head><p>Data augmentation is vital to improve the corruption robustness. From Tab. 3, we find that the AugMix is significantly more effective in boosting robustness than other augmentation methods, which is consistent with previous research <ref type="bibr" target="#b16">[17]</ref>. As depicted in the right part of <ref type="figure" target="#fig_4">Fig. 4</ref>, corruption robust decreases with increasing erasing probability. Besides, soft random erasing are more effective for improving corruption robustness and less sensitive to the tuning of erasing probability compared  with random erasing. In Tab. 3, we have a similar observation that soft random benefits more for improving corruption robustness. Meanwhile, the SelfPatch augmentation method outperforms the RandomPatch augmentation method on clean and corrupted test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">A Strong Baseline on Corruption Invariant ReID</head><p>On the basis of the foregoing investigation, we propose three general and simple techniques for enhancing corruption robustness (detailed ablations see the appendix). The first is the consistent ID loss that enforces a smoother network response <ref type="bibr" target="#b16">[17]</ref>. The second technique is inference with features before BNNeck, in case the feature is too domain-specific. The third one is the proposed local-based augmentation techniques, soft random erasing and self patch mixing. Our baseline is CIL (Consistent identity loss, Inference before BNNeck and Local-based augmentation). In single-modality datasets (see Tab. 4), our proposed baseline CIL achieves competitive performance on the clean test set and outstanding results on three corrupted situations. We also evaluate the corruption robustness of the  CIL baseline using a two-stream architecture on the cross-modality visible-infrared ReID task. As seen by the results in Tab. 5, our baseline CIL considerably improves corruption robustness while compromising little performance on clean test sets.</p><p>We conduct ablation experiments on the components of our proposed baseline, as shown in Tab. 6. The standard ResNet-50 we use here is built on the AGW baseline, which deletes the non-local block and adds the loss function used by the SBS baseline. It can be seen from <ref type="table" target="#tab_6">Table 6</ref> that our <ref type="figure">Figure 5</ref>: Ablation study on different corruption types (corrupted query and gallery), including 20 types of algorithmically generated corruptions from noise, blur, weather, and digital categories. suggested pre-BNNeck inference and local data augmentation approaches can increase the corruption robustness, and the consistency ID loss can effectively maintain the corruption robustness while boosting the performance on clean samples. In addition, we also perform ablation experiments of different corruption types on our CIL baseline to see the impacts of each individual corruption, as shown in <ref type="figure">Fig. 5</ref>. Experimental results are also averaged after ten evaluations. We can see that the model is more vulnerable to corruption types such as saturate, contrast, and fog that produce greater color interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present detailed, large-scale robustness evaluations of 21 advanced ReID methods. Based on the study, we have some interesting findings and build a strong baseline on robust person ReID in the hope of extracting practical lessons for the broader community. First of all, we demonstrated the transformer's potential on ReID. Even when images are corrupted, they can still extract rich structured patterns. Moreover, given the limitations of the existing commonly used data augmentation, we design two new simple but effective data augmentation methods for mining more robust local features. Additionally, we discover that cross-dataset generalization increases with corruption robustness in ReID, which was overlooked by previous research on corruption robustness and may serve as an inspiration for generalizable person ReID.</p><p>Limitations and broader impact. Compared with other baselines, the performance of our proposed baseline on clean samples is slightly degraded. Additionally, we cannot establish a clear relationship between corruption robustness and performance on clean images in ReID tasks. As for positive impact, we demonstrate through extensive experiments that synthetic corruption robustness contributes to performance on naturally occurring distribution shifts. Hence our synthetic corrupted datasets can serve as useful proxies and have the potential to mitigate ethical concerns associated with the collecting of vast volumes of pedestrian data.</p><p>However, when an object ReID system is used to identify pedestrians and vehicles in a surveillance system, it may infringe people's privacy. Because ReID system typically (not all) depends on unauthorized surveillance data, which means that not all human subjects were known they were being recorded. As a result, governments and officials must take considerable steps to develop stringent regulations and legislation governing the use of ReID technology. Otherwise, malicious agents may be able to monitor pedestrians or vehicles without their consent using multiple closedcircuit television cameras <ref type="bibr" target="#b8">[9]</ref>. Additionally, researchers should avoid using datasets that raise ethical concerns. For instance, the DukeMTMC dataset <ref type="bibr" target="#b33">[34]</ref> should no longer be used after it was shut down for violating data collection restrictions. Also, our benchmarks have excluded evaluation on DukeMTMC. Meanwhile, it is worth mentioning that the demographic composition of datasets does not accurately reflect the general population. Current data-driven deep learning systems only learn what is taught to them. Accuracy and fairness are jeopardized if they are not taught with diverse datasets. Each of us expects the ReID system to perform equally well across different individuals or populations. Therefore, the research community and developers need to be thoughtful about what data they use for training. This is essential for developing artificial intelligence systems which can help to make the world more fair <ref type="bibr" target="#b27">[28]</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualization</head><p>In <ref type="figure">Fig. 6</ref>, we present more augmented examples under different data augmentation methods.</p><p>In <ref type="figure">Fig. 7</ref>, we present more comparison examples of activation maps under different data augmentation training methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>types of corruption that commonly occur in the real world. Meanwhile, * Equal Contribution ? Corresponding Author 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. arXiv:2111.00880v2 [cs.CV] 25 Apr 2022 (a) Clean dataset (b) Corrupted dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Large scale evaluation on ReID methods in recent years. (a) The performance of the clean test set (Market-1501) is shown on the left, demonstrating the increasing trend in mAP for various methods over the last few years. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) Local-based data augmentation (b) Visualization of activation map Visualization of our augmented examples and activation maps. (a) Left are four different data augmentation methods. As can be seen, random erasing and random patch mixing introduce severe occlusion compared with soft random erasing and self patch mixing. (b) Right are activation maps of models trained with different augmentations. From left to right of each are input images, activation maps from the (1) standard model, a model trained with (2) AugMix, (3) random erasing, (4) soft random erasing, (5) random patch mixing and (6) self patch mixing. Models trained with our proposed augmentations (soft random erasing and random patch mixing) capture more discriminative parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Performance evaluations of the person ReID methods in recent years. The x-axis, y-axis, and bubble size indicate Rank-1, mAP, and mINP, respectively. (a) Evaluations on clean Market-1501 test dataset. (b) Evaluations on Market-1501-C (corrupted query and gallery). In general, performance on the clean test set is not positively correlated with performance on the corrupted test set, and there is considerable room for improvement on corruption robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Cross-dataset generalization (Market-1501 to MSMT17) improves as corruption robustness increases. In contrast, the cross-dataset generalization has little correlation with performance on the clean samples. The histogram represents performance on the clean Market-1501 test set, the blue line depicts performance on the Market-1501-C (corrupted query and gallery), and the green line depicts performance when transferring directly to the MSMT17 dataset. On the left are the results of various methods, while on the right are the results of the same model trained with different augmentations. For different augmentations, the green histograms represent the random (left half) and soft random erasing (right half), respectively. The value on the x-axis represents the probability of erasure. See our appendix for more experiment results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Visualization of different augmented examples. Visualization of activation maps. Each septet contains, from left to right, original image, activation maps from the standard model, a model trained with AugMix, random erasing, soft random erasing, random patch mixing and self patch mixing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Existing person ReID techniques fall into two main categories: closed-world and open-world settings.With the performance saturation under closed-world setting, the research focus on person Re-ID has recently shifted to the open-world setting, facing more challenging issues</figDesc><table><row><cell>2 Related Work</cell></row><row><cell>2.1 Person ReID</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of our benchmarking datasets for single-and cross-modality person ReID.</figDesc><table><row><cell>Single-modality datasets</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of transformer-based models and CNN-based models. All the models are trained with 256 ? 128 inputs. Trans-denotes the TransReID method. Compared to the original ViT, the TransReID includes SIE and JPM modules. While ViT-based models generally outperform the CNN-based models in terms of corruption robustness, the advantage of the ViT over CNN-based models with an attention mechanism is not obvious.<ref type="bibr" target="#b54">55</ref>.42 83.40 92.70 1.10 19.33 43.62 23.85 43.84 51.91 2.55 43.87 82.86 ViT-B 11.03 85.61 64.08 87.11 94.60 1.82 25.84 51.31 31.41 51.54 59.44 4.40 52.00 87.60 Trans-ViT-S 11.33 53.72 57.01 84.46 93.74 1.02 18.23 42.57 23.65 43.19 50.71 2.48 43.29 83.47 Trans-DeiT-B 19.55 92.70 67.16 88.54 95.07 1.70 24.71 51.67 32.39 50.79 57.12 3.84 50.03 87.65 Trans-ViT-B 19.55 92.70 69.31 88.97 95.10 1.93 27.10 52.77 34.52 52.30 57.96 4.18 52.19 88.60</figDesc><table><row><cell>Network</cell><cell cols="2">MACs Params</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell><cell>Corrupted Query</cell><cell>Corrupted Gallery</cell></row><row><cell></cell><cell>(G)</cell><cell>(M)</cell><cell cols="4">mINP mAP R-1 mINP mAP R-1 mINP mAP R-1 mINP mAP R-1</cell></row><row><cell>ResNet-50</cell><cell>4.06</cell><cell cols="5">23.51 59.30 85.06 93.38 0.21 8.50 27.30 14.34 26.42 30.52 0.39 27.00 77.15</cell></row><row><cell>ResNet-50-ibn</cell><cell>4.06</cell><cell cols="5">23.51 67.79 86.55 94.36 0.33 13.90 37.75 20.45 36.77 43.39 0.85 35.78 83.34</cell></row><row><cell>ResNeSt-50</cell><cell>4.68</cell><cell cols="5">25.44 65.49 87.97 95.28 0.26 9.82 30.57 18.26 31.71 37.65 0.51 31.79 82.93</cell></row><row><cell>ResNet-101-ibn</cell><cell>6.49</cell><cell cols="5">42.50 65.27 87.90 95.22 0.37 13.96 37.35 21.99 37.35 43.42 0.90 36.61 84.09</cell></row><row><cell cols="2">SE-ResNet-101-ibn 6.50</cell><cell cols="5">47.25 67.75 89.08 95.49 0.69 16.99 43.14 27.87 45.28 51.48 2.60 47.39 88.32</cell></row><row><cell>ResNeXt-101-ibn</cell><cell>6.51</cell><cell cols="5">42.13 67.81 89.05 95.04 1.34 21.65 48.25 31.38 50.49 56.59 3.78 50.30 88.48</cell></row><row><cell>DeiT-S</cell><cell>2.78</cell><cell cols="5">22.31 56.36 83.90 93.23 1.07 19.30 44.38 23.41 43.30 51.49 2.57 43.98 83.73</cell></row><row><cell>ViT-S</cell><cell>6.16</cell><cell cols="5">47.81 53.34 82.42 92.79 1.02 18.06 42.47 22.76 43.94 53.39 2.68 43.64 82.83</cell></row><row><cell>ViT-S + SIE</cell><cell>6.16</cell><cell cols="5">47.81 55.99 83.70 93.35 0.82 16.88 40.26 22.54 42.34 50.76 2.29 43.07 82.96</cell></row><row><cell>ViT-S + JPM</cell><cell>6.94</cell><cell>53.72</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons of various data augmentations. The upper part compares the global augmentation methods (Random affine transformation, AutoAugment and AugMix). The bottom part compares the local-based augmentation methods when combined with the AugMix. REA stands for random erasing, S-REA stands for soft random erasing, R-PATCH stands for random patch mixing, and S-PATCH stands for self patch mixing augmentation.Standard 45.70 77.76 91.69 96.44 97.83 0.43 14.31 37.31 53.36 59.99 16.79 34.45 42.99 53.06 0.90 33.54 77.74 90.28 R-Affine 59.34 85.69 93.88 98.16 98.93 0.27 8.01 27.46 39.70 45.20 16.28 30.69 36.34 45.04 0.83 33.02 81.29 92.12 AutoAug 46.39 80.18 92.34 97.60 98.57 0.37 10.55 30.40 42.71 48.41 14.39 31.87 40.23 49.24 1.12 35.22 80.64 91.86 AugMix 45.92 77.16 91.03 96.88 98.13 1.05 22.47 48.06 65.07 71.27 21.79 43.46 53.93 65.58 1.95 41.32 80.25 92.21 + REA 57.10 83.40 93.08 97.83 98.43 1.49 24.32 49.80 66.82 72.68 26.86 48.08 57.33 68.92 3.30 47.46 84.71 94.45 + S-REA 57.34 83.48 92.99 97.57 98.43 2.19 26.66 52.60 69.64 75.52 28.75 50.33 56.69 71.49 4.44 49.54 85.27 94.58 + R-PATCH 47.37 77.78 90.97 96.53 98.10 1.00 22.05 47.49 64.35 70.42 21.97 43.20 53.60 64.84 1.74 41.19 80.85 92.45 + S-PATCH 54.30 81.86 92.55 97.48 98.49 1.17 22.72 47.99 64.84 70.73 25.19 45.78 55.02 66.49 2.45 44.60 83.18 93.83</figDesc><table><row><cell>Augmentation</cell><cell cols="2">Clean Eval.</cell><cell></cell><cell cols="3">Corrupted Eval.</cell><cell cols="2">Corrupted Query</cell><cell></cell><cell cols="2">Corrupted Gallery</cell></row><row><cell></cell><cell>mINP mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10 mINP mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>R-10 mINP mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>mINP mAP</cell><cell>R-1</cell><cell>R-5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Corruption invariant person ReID benchmarks on single-modality datasets. SBS<ref type="bibr" target="#b12">[13]</ref> represents a stronger baseline on top of BoT. In single-modality datasets, our proposed baseline CIL achieves competitive performance on the clean test set and remarkable results on three corrupted scenarios.BoT  59.30 85.06 93.38 97.71 0.20 8.42 27.05 40.28 14.56 26.89 31.92 40.24 0.39 26.82 76.78 89.57 AGW 64.03 86.51 94.00 98.01 0.35 12.13 31.90 46.54 19.44 31.75 35.25 44.09 0.67 33.38 80.45 91.90 SBS 60.03 88.33 95.90 98.49 0.29 11.54 34.13 47.28 18.47 35.33 42.06 51.21 0.53 32.65 83.11 92.87 CIL 57.90 84.04 93.38 97.95 1.76 28.03 55.57 72.34 29.99 52.53 62.29 73.34 3.45 48.95 85.52 94.76 MSMT17 BoT 9.91 48.34 73.53 85.29 0.07 5.28 20.20 31.11 2.75 15.78 25.92 35.50 0.09 16.10 59.06 76.48 AGW 12.38 51.84 75.21 86.30 0.08 6.53 22.77 34.08 3.82 18.42 28.06 37.33 0.15 18.08 61.45 78.43 SBS 10.26 56.62 82.02 90.39 0.05 7.89 28.77 40.00 3.23 22.71 36.68 46.53 0.12 21.16 70.65 83.95 CIL 12.45 52.40 76.10 87.19 0.32 15.33 39.79 54.83 5.84 29.08 45.51 58.27 0.50 27.99 68.31 82.87 CUHK03 AGW 49.97 62.25 64.64 81.50 0.46 3.45 5.90 11.59 12.69 17.20 16.26 26.29 2.89 19.40 33.43 53.85 CIL 53.87 65.16 67.29 83.79 4.25 16.33 22.96 39.89 26.61 34.62 34.03 50.44 9.07 31.81 46.81 69.66</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell><cell>Corrupted Query</cell><cell>Corrupted Gallery</cell></row><row><cell></cell><cell></cell><cell cols="4">mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5</cell></row><row><cell>Market-1501</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>17 47.65 47.50 74.68 14.73 29.99 34.42 62.26 59.74 62.97 54.17 83.50 35.39 40.98 33.80 61.61 CIL 38.15 47.64 45.41 73.95 22.48 35.92 36.95 65.54 57.41 60.45 50.98 81.34 43.11 48.65 40.73 71.44 RegDB AGW 54.10 68.82 75.78 85.24 32.88 43.09 45.44 55.26 52.40 68.15 75.29 83.74 6.00 41.37 67.54 81.23 CIL 55.68 69.75 74.96 84.71 38.66 49.76 52.25 65.83 55.50 69.21 74.95 86.12 11.94 47.90 67.17 83.25</figDesc><table><row><cell></cell><cell></cell><cell>Mode A</cell><cell></cell><cell>Mode B</cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell></row><row><cell></cell><cell cols="5">mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5</cell></row><row><cell>SYSU-MM01</cell><cell>AGW 36.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Corruption invariant person ReID benchmarks on cross-modality datasets. For SYSU- MM01 dataset, Mode A and Mode B mean all-search (including indoor and outdoor cameras) and indoor-search experimental settings, respectively. For RegDB dataset, Mode A and Mode B represent visible-to-thermal and thermal-to-visible experimental settings, respectively. Note that we only corrupt RGB (visible) images in the corruption evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on CIL components, including pre-BNNeck inference, local-based augmentation (soft random erasing and self patch mixing) and consistent ID loss.85.52 94.48 97.95 0.55 15.26 39.87 55.84 21.67 38.36 45.00 55.29 1.27 38.48 83.50 93.48 + infer. before BNNeck 47.39 79.58 91.66 97.15 0.89 18.16 42.41 58.98 20.53 42.94 53.35 65.29 2.14 42.65 82.23 93.27 + soft random erasing 59.57 84.74 93.26 98.07 1.37 25.98 53.89 70.92 29.44 50.96 60.11 71.89 2.67 46.82 85.68 94.86 + self patch mixing 55.96 82.93 93.05 97.51 1.78 27.59 57.37 71.81 30.33 53.69 64.08 75.81 3.23 48.08 84.70 94.59 + consistent ID loss 57.90 84.04 93.38 97.95 1.76 28.03 55.57 72.34 29.99 52.53 62.29 73.34 3.45 48.95 85.52 94.76</figDesc><table><row><cell>Component</cell><cell cols="2">Clean Eval.</cell><cell></cell><cell cols="2">Corrupted Eval.</cell><cell></cell><cell cols="2">Corrupted Query</cell><cell></cell><cell cols="2">Corrupted Gallery</cell></row><row><cell></cell><cell>mINP mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>mINP mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>mINP mAP</cell><cell>R-1</cell><cell>R-5</cell><cell>mINP mAP</cell><cell>R-1</cell><cell>R-5</cell></row><row><cell>Standard ResNet-50</cell><cell>60.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Details of the investigated methods.</figDesc><table><row><cell>Method</cell><cell>abbreviation</cell><cell>backbone evaluated datasets</cell><cell>source code</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Robustness evaluation under different corruption severity levels.</figDesc><table><row><cell>Severity</cell><cell>mINP</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell cols="4">level-0 57.90 (0.00) 84.04 (0.00) 93.38 (0.00)</cell></row><row><cell cols="4">1evel-1 15.74 (0.44) 57.05 (0.30) 80.81 (0.40)</cell></row><row><cell cols="4">1evel-2 5.21 (0.22) 39.97 (0.40) 69.09 (0.76)</cell></row><row><cell cols="4">1evel-3 2.84 (0.17) 30.33 (0.21) 59.41 (0.77)</cell></row><row><cell cols="4">1evel-4 0.86 (0.06) 17.22 (0.33) 43.40 (0.64)</cell></row><row><cell cols="4">1evel-5 0.35 (0.05) 9.60 (0.23) 30.10 (0.71)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Robustness evaluation under different corruption types.</figDesc><table><row><cell></cell><cell>type</cell><cell>mINP</cell><cell>mAP</cell><cell>Rank-1</cell></row><row><cell></cell><cell>Gaussian</cell><cell cols="3">4.34 (0.07) 31.40 (0.42) 57.65 (0.72)</cell></row><row><cell>Noise</cell><cell>shot</cell><cell cols="3">4.35 (0.12) 31.15 (0.21) 57.62 (0.35)</cell></row><row><cell></cell><cell>impulse</cell><cell cols="3">6.20 (0.13) 37.30 (0.36) 64.10 (0.31)</cell></row><row><cell></cell><cell>speckle</cell><cell cols="3">14.30 (0.11) 48.87 (0.22) 73.64 (1.03)</cell></row><row><cell></cell><cell>defocus</cell><cell cols="3">15.06 (0.35) 49.57 (0.41) 73.32 (0.96)</cell></row><row><cell></cell><cell>glass</cell><cell cols="3">26.02 (0.48) 60.85 (0.22) 80.56 (0.17)</cell></row><row><cell>Blur</cell><cell>motion</cell><cell cols="3">19.42 (0.15) 55.87 (0.19) 77.45 (0.18)</cell></row><row><cell></cell><cell>zoom</cell><cell cols="3">38.06 (0.27) 71.79 (0.01) 87.52 (0.36)</cell></row><row><cell></cell><cell>Gaussian</cell><cell cols="3">15.48 (0.55) 52.20 (0.24) 75.97 (0.33)</cell></row><row><cell></cell><cell>snow</cell><cell cols="3">11.94 (0.29) 44.98 (0.16) 68.66 (0.58)</cell></row><row><cell></cell><cell>frost</cell><cell cols="3">3.86 (0.10) 29.36 (0.23) 56.71 (0.36)</cell></row><row><cell>Weather</cell><cell>fog</cell><cell cols="3">3.10 (0.05) 24.78 (0.04) 48.97 (0.67)</cell></row><row><cell></cell><cell>brightness</cell><cell cols="3">23.34 (0.28) 61.18 (0.04) 82.14 (0.43)</cell></row><row><cell></cell><cell>spatter</cell><cell cols="3">24.25 (0.56) 62.12 (0.51) 82.73 (0.38)</cell></row><row><cell></cell><cell>rain</cell><cell cols="3">15.54 (0.15) 50.20 (0.40) 73.47 (0.49)</cell></row><row><cell></cell><cell>contrast</cell><cell cols="3">0.70 (0.06) 19.29 (0.06) 45.10 (0.57)</cell></row><row><cell></cell><cell>elastic</cell><cell cols="3">16.30 (0.09) 47.95 (0.08) 70.70 (0.21)</cell></row><row><cell>Digital</cell><cell>pixel</cell><cell cols="3">30.06 (0.03) 66.00 (0.27) 83.19 (0.13)</cell></row><row><cell></cell><cell cols="4">JPEG compression 15.72 (0.39) 50.36 (0.24) 73.81 (0.70)</cell></row><row><cell></cell><cell>saturate</cell><cell cols="3">0.48 (0.02) 18.43 (0.15) 55.58 (0.37)</cell></row><row><cell cols="2">B More Experimental Results</cell><cell></cell><cell></cell></row></table><note>B.1 Robustness under different corruption settings Tab. 8 shows the robustness under different corruption severity levels. Tab. 9 shows the robustness under different corruption types.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Corruption robustness evaluations of the SOTA methods on datasets Market-1501, CUHK03 (detected) and MSMT17.(Note that Market standards for dataset Market-1501) 88.10 95.00 98.20 0.30 10.80 33.40 45.80 19.10 33.40 38.70 46.80 1.00 36.00 84.10 93.70 BoT 51.00 83.90 94.30 97.80 0.10 6.60 26.20 37.90 12.90 29.10 36.70 44.70 0.30 21.70 74.30 86.30 ABD-Net 64.72 87.94 94.98 98.37 0.26 9.81 29.65 42.49 17.81 31.10 36.02 44.15 0.60 31.32 81.38 91.89 OS-Net 56.78 85.67 94.69 98.22 0.23 10.37 30.96 44.06 15.82 31.10 36.97 45.06 0.45 29.29 79.81 90.74 DG-Net 61.60 86.09 94.77 97.98 0.35 9.96 31.75 44.92 16.93 29.76 35.93 43.64 1.04 34.59 83.30 93.05 MHN 55.27 85.33 94.50 97.92 0.38 10.69 33.29 47.28 16.83 33.08 39.87 48.39 1.07 33.50 82.29 92.29 BDB 61.78 85.47 94.63 97.80 0.32 10.95 33.79 47.84 18.93 31.58 38.42 47.48 0.70 31.82 81.40 92.19 TransReID 69.29 88.93 95.07 98.46 1.98 27.38 53.19 68.95 34.39 52.40 58.27 68.97 4.47 52.59 88.50 96.05 LGPR 58.71 86.09 94.51 98.34 0.24 8.26 27.72 38.56 15.24 29.52 35.82 42.65 0.69 32.51 82.93 92.45 F-LGPR 65.48 88.22 95.37 98.31 0.23 9.08 29.35 40.86 17.97 31.21 36.16 43.47 0.78 33.98 83.76 93.00 TDB 56.41 85.77 94.30 98.01 0.20 8.90 28.56 41.14 14.94 29.53 34.71 43.10 0.33 25.91 79.14 90.30 LUPerson 68.71 90.32 96.32 98.81 0.29 10.37 32.22 44.53 19.68 34.16 40.09 47.20 0.79 34.35 85.68 93.63 LightMBN 73.29 91.54 96.53 98.84 0.50 14.84 38.68 52.30 27.09 41.81 46.33 54.56 1.41 41.38 87.98 95.31 PLR-OS 66.42 88.93 95.19 98.40 0.48 14.23 37.56 52.17 23.30 38.75 43.49 52.68 1.25 38.98 85.15 93.94 PCB 41.97 82.19 94.15 97.74 0.41 12.72 34.93 49.88 12.66 32.35 39.72 49.30 0.85 33.03 80.89 91.55 Pyramid 61.61 87.50 94.86 98.31 0.36 12.75 35.72 50.39 19.66 35.48 41.09 49.56 0.95 34.70 81.90 92.78 Aligned++ 47.31 79.10 91.83 96.97 0.32 10.95 31.00 45.55 14.70 29.43 35.15 44.95 0.67 30.10 75.86 89.74 RRID 67.14 88.43 95.19 98.10 0.46 13.45 36.57 51.90 22.18 36.26 41.29 49.58 1.09 37.44 84.00 93.58 VPM 50.09 81.43 93.79 98.19 0.31 10.15 31.17 45.23 15.48 31.75 39.23 49.11 0.71 31.06 79.42 91.15 MGN 60.86 86.51 93.88 0.00 0.29 9.72 29.56 42.92 17.08 30.00 34.25 42.49 0.69 30.83 79.36 91.21 CUHK03 Pyramid 61.41 73.14 79.54 93.16 1.10 8.03 10.42 17.93 20.18 26.36 28.96 44.82 4.21 26.14 30.66 45.39 MGN 51.18 62.73 69.14 88.93 0.46 4.20 5.44 10.86 10.90 14.39 15.42 28.44 1.77 15.55 18.92 32.43 Aligned++ 47.32 59.76 62.07 79.93 0.56 4.87 7.99 15.87 12.12 16.34 15.35 24.88 2.19 17.78 31.21 51.76 RRID 55.81 67.63 74.99 91.51 1.00 7.30 9.66 17.35 18.25 23.88 26.44 42.55 3.98 24.35 28.89 44.96 MHN 56.52 66.77 72.21 86.43 0.46 3.97 8.27 15.42 14.20 17.79 18.78 28.16 2.91 18.40 36.65 56.84 CaceNet 65.22 75.13 77.64 90.43 2.09 10.62 17.04 28.93 25.50 31.59 30.99 43.17 6.43 30.78 51.49 71.69 PLR-OS 62.72 74.67 78.14 91.14 0.89 6.49 10.99 20.22 19.61 25.96 25.91 37.41 5.03 26.37 45.41 65.92 MSMT17 OS-Net 4.05 40.05 71.86 82.65 0.08 7.86 28.51 40.19 1.71 18.77 35.40 45.20 0.14 16.99 59.90 74.69 BoT 9.91 48.34 73.53 85.29 0.07 5.28 20.20 31.11 2.75 15.78 25.92 35.50 0.09 16.10 59.06 76.48 AGW 12.38 51.84 75.21 86.30 0.08 6.53 22.77 34.08 3.82 18.42 28.06 37.33 0.15 18.08 61.45 78.43 B.2 Detailed Results of the SOTA methods In Tab. 10 and Tab. 11, we present the results of corruption robustness evaluations of the SOTA methods on datasets Market-1501, CUHK03 (detected), MSMT17, SYSU-MM01 and RegDB respectively.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell><cell>Corrupted Query</cell><cell>Corrupted Gallery</cell></row><row><cell></cell><cell></cell><cell cols="4">mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5</cell></row><row><cell></cell><cell>AGW</cell><cell>65.40</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Market</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Corruption robustness evaluations of methods on cross-modality datasets SYSU-MM01 and RegDB. AGW 36.17 47.65 47.50 74.68 14.73 29.99 34.42 62.26 59.74 62.97 54.17 83.50 35.39 40.98 33.80 61.61 DGTL 42.55 55.82 57.78 83.00 18.43 35.59 41.36 68.46 65.40 69.40 62.99 87.70 41.15 47.90 42.22 69.98 HCTL 42.14 57.65 61.94 85.85 13.85 33.15 42.38 68.87 63.69 68.41 63.70 85.66 33.29 40.61 37.26 59.54 RegDB AGW 54.10 68.82 75.78 85.24 32.88 43.09 45.44 55.26 52.40 68.15 75.29 83.74 6.00 41.37 67.54 81.23 DGTL 61.15 75.63 85.00 91.21 37.02 48.35 53.50 62.02 55.90 73.39 82.67 89.37 6.21 46.29 75.20 87.54 HCTL 68.15 82.46 90.63 94.66 43.77 55.83 62.09 67.78 66.50 81.05 89.17 94.27 6.80 53.79 83.63 92.82</figDesc><table><row><cell></cell><cell></cell><cell>Mode A</cell><cell></cell><cell>Mode B</cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell><cell>Clean Eval.</cell><cell>Corrupted Eval.</cell></row><row><cell></cell><cell></cell><cell cols="4">mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5 mINP mAP R-1 R-5</cell></row><row><cell>SYSU-MM01</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixed high-order attention network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8350" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salienceguided cascaded suppression network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canmiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch dropblock network for person re-identification and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3690" to="3700" />
		</imprint>
	</monogr>
	<note>Siyu Zhu, and Ping Tan</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study and comparison of human and deep learning recognition performance under visual distortions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCCN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengpan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2012.03753</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An effective data augmentation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Zeng</surname></persName>
		</author>
		<idno>abs/2101.08533</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bin Weng, and Feng Ye. A person re-identification data augmentation method with adversarial defense effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/2101.08783</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fastreid: A pytorch toolbox for general instance re-identification. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Transreid: Transformer-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<idno>abs/2102.04378</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Augmix: A simple data processing method to improve robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lightweight multi-branch network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunbo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<idno>abs/2101.10774</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8825" to="8835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5187" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-scale learning for low-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3765" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient PSD constrained asymmetric metric learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3685" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bag of tricks and a strong baseline for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzhi</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1487" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Alignedreid++: Dynamically matching local information for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nalini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rog?rio</forename><surname>Ratha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<title level="m">Diversity in faces. CoRR, abs</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Benchmarking robustness in object detection: Autonomous driving when winter is coming. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person recognition system based on a combination of body images from visible light and thermal cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyung</forename><forename type="middle">Gil</forename><surname>Dat Tien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ki-Wan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang Ryoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">605</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11839" to="11847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Top-db-net: Top dropblock for activation enhancement in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Quispe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?lio</forename><surname>Pedrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2980" to="2987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9914</biblScope>
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple way to make neural networks robust against diverse image corruptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgenia</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><forename type="middle">S</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Bitterwolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition-tangent distance and tangent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="239" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and A strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (4)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Measuring robustness to natural distribution shifts in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">When human pose estimation meets robustness: Adversarial algorithms and benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/2105.06152</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Shape and appearance context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Resource aware person re-identification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequn</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8042" to="8051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Person transfer GAN to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rgb-infrared crossmodality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5390" to="5399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning diverse features with part-level resolution for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12307</biblScope>
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno>abs/2001.04193</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Devil&apos;s in the details: Aligning visual clues for conditional embedding in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fufu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05250</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8514" to="8522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoyan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3346" to="3355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A discriminatively learned CNN embedding for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno>13:1-13:20</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multim. Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Re-ranking person re-identification with kreciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3701" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
