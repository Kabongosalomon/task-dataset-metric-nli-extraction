<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MicronNet: A Highly Compact Deep Convolutional Neural Network Architecture for Real-time Embedded Traffic Sign Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
							<email>a28wong@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Waterloo Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Javad</forename><surname>Shafiee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Waterloo Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>St</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jules</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Waterloo Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">DarwinAI Corp</orgName>
								<address>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MicronNet: A Highly Compact Deep Convolutional Neural Network Architecture for Real-time Embedded Traffic Sign Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traffic sign recognition is a very important computer vision task for a number of real-world applications such as intelligent transportation surveillance and analysis. While deep neural networks have been demonstrated in recent years to provide state-of-the-art performance traffic sign recognition, a key challenge for enabling the widespread deployment of deep neural networks for embedded traffic sign recognition is the high computational and memory requirements of such networks. As a consequence, there are significant benefits in investigating compact deep neural network architectures for traffic sign recognition that are better suited for embedded devices. In this paper, we introduce MicronNet, a highly compact deep convolutional neural network for real-time embedded traffic sign recognition designed based on macroarchitecture design principles (e.g., spectral macroarchitecture augmentation, parameter precision optimization, etc.) as well as numerical microarchitecture optimization strategies. The resulting overall architecture of MicronNet is thus designed with as few parameters and computations as possible while maintaining recognition performance, leading to optimized information density of the proposed network. The resulting MicronNet possesses a model size of just ?1MB and ?510,000 parameters (?27x fewer parameters than state-of-theart) while still achieving a human performance level top-1 accuracy of 98.9% on the German traffic sign recognition benchmark. Furthermore, MicronNet requires just ?10 million multiply-accumulate operations to perform inference, and has a time-to-compute of just 32.19 ms on a Cortex-A53 high efficiency processor. These experimental results show that highly compact, optimized deep neural network architectures can be designed for real-time traffic sign recognition that are well-suited for embedded scenarios.</p><p>Preprint. Work in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1804.00497v3 [cs.CV] 3 Oct 2018</head><p>Robustness against image degradation. To study the robustness of the proposed Micron-Net network against different levels of image degradation, all 12,630 images in the GTSRB test dataset were contaminated by Gaussian noise at three different degradation levels (i.e., ?= 2.5%, 5%, and 7.5% of the dynamic range). <ref type="table">Table 5</ref> shows the top-1 accuracy of the proposed MicronNet network across the different degradation levels. It can be observed that the proposed MicronNet network is reasonably robustness to image degradation, still achieving a top-1 accuracy of 92.3% at the highest tested degradation level (?=7.5%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traffic sign recognition can be considered an important computer vision task for a number of realworld applications such as intelligent transportation surveillance and analysis (see <ref type="figure">Figure 1</ref>). The arrival of modern breakthroughs in deep learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref> has resulted in significant state-of-the-art results for traffic sign recognition, with much of the research focused on designing deep convolutional neural networks for improved accuracy <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Despite the fact that such traffic sign recognition networks have shown state-of-the-art object detection accuracies beyond what can be achieved by previous state-of-the-art methods, a key challenge for enabling the widespread deployment of deep neural networks for embedded traffic sign recognition is the high computational and memory requirements of such networks. For example, the committee <ref type="figure">Figure 1</ref>: The goal of the traffic signal recognition problem is to identify which type of traffic sign is in the scene. For context, some example images of traffic signs from the German traffic sign recognition benchmark <ref type="bibr" target="#b19">[20]</ref> are shown.</p><p>of deep convolutional neural networks proposed by Ciresan et al. <ref type="bibr" target="#b5">[6]</ref> consists of ?38.5 million parameters while the ensemble of deep convolutional neural networks trained via hinge loss as proposed by Jin et al. <ref type="bibr" target="#b11">[12]</ref> consists of ?23.2 million parameters. More recently, the state-ofthe-art deep convolutional network with spatial transformers proposed by Arcos-Garcia et al. <ref type="bibr" target="#b2">[3]</ref>, while having fewer parameters than the aforementioned approaches, still consisted of over ?14 million parameters. At a significantly smaller sizes than the aforementioned configurations, the macroarchitectures proposed by Ciresan et al. <ref type="bibr" target="#b5">[6]</ref> still consist of ?1.5 million parameters. As such, the design of more compact and efficient deep neural network architectures for traffic sign recognition is highly desired for embedded applications.</p><p>Recently, there has been an increasing focus in exploring small deep neural network architectures that are more suitable for embedded devices <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16]</ref>. For example, in the work by Iandola et al. <ref type="bibr" target="#b10">[11]</ref>, three key design strategies were leveraged to create compact macroarchitectures: 1) filter quantity reduction, 2) input channel reduction, and 3) late downsampling in the network. As a result of such design strategies, a compact SqueezeNet macroarchitecture was introduced that comprised of Fire modules that was ?50X smaller than AlexNet with comparable accuracy on ImageNet <ref type="bibr" target="#b6">[7]</ref> for 1000 classes. In the work by Howard et al. <ref type="bibr" target="#b9">[10]</ref>, they leveraged depth-wise separable convolutions to reduce the number of parameters, as well as two global hyperparameters based on network width and resolution for finding the tradeoff between latency and accuracy. Sandler et al. <ref type="bibr" target="#b16">[17]</ref> expanded upon this by introducing an inverted residual structure that enabled further reductions in the number of parameters while maintaining high performance. Aghdam et al. <ref type="bibr" target="#b0">[1]</ref> presented techniques for optimizing the efficiency of deep neural network architectures for the specific purpose of traffic sign recognition. Based on the practical principles they discussed for building small deep neural network architectures for traffic sign recognition, the authors were able to create a high-performance deep neural network consisting of just ?1.74 million parameters, while still achieving great accuracy.</p><p>In this study, we introduce MicronNet, a highly compact deep convolutional neural network designed specifically for real-time embedded traffic sign recognition. In MicronNet's highly optimized network architecture, the underlying microarchitecture of each convolutional layer in the network (with microarchitecture here referring to the number and size of convolutional filters) is numerically optimized to have as few parameters and computations as possible while maintaining recognition performance, hence resulting in an optimized information density for the underlying network. Furthermore, the macroarchitecture of the proposed MicronNet network is designed via macroarchitecture design strategies (e.g., spectral macroarchitecture augmentation, parameter precision optimization, etc.) <ref type="figure">Figure 2</ref>: Integrated microarchitecture-level and macroarchitecture-level design principles and optimization strategies leveraged for designing MicronNet for high efficiency while maintaining strong accuracy.</p><p>that encourage improved computational efficiency and efficacy in embedded environments. As such, the main contribution of this work is the investigation and exploration of integrating design principles and optimization strategies at both the microarchitecture level and the macroarchitecture level to design deep neural networks with optimized information densities that satisfy real-time embedded requirements while achieving strong accuracy, thus enabling real-time embedded traffic sign recognition. This paper is organized as follows. Section 2 describes the highly optimized network architecture and design considerations underlying the proposed MicronNet network. Section 3 presents experimental results that evaluate the efficacy of the proposed MicronNet network for real-time embedded traffic sign recognition, along with a discussion on some key observations about the network. Finally, conclusions are drawn in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network architecture of MicronNet</head><p>Leveraging macroarchitecture design principles such as those from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b0">1]</ref> and a numerical microarchitecture optimization strategy inspired by <ref type="bibr" target="#b22">[23]</ref>, the overall network architecture of the proposed MicronNet network for real-time embedded traffic sign recognition is inspired by the network macroarchitecture described in <ref type="bibr" target="#b0">[1]</ref> and takes the following microarchitecture-level and macroarchitecture-level design considerations and optimization strategies into account to greatly improve the efficiency of the resulting deep convolutional neural network while maintaining strong accuracy (see <ref type="figure">Figure 2</ref>):</p><p>? Optimizing microarchitectures of each convolutional layer via numerical optimization for reduced number of parameters ? Incorporating spectral augmentations to produce a spectral-spatial macroarchitecture that further reduces number of parameters and computational complexity while maintaining strong accuracy ? Optimizing parameter precision for reduced model size while maintaining strong accuracy <ref type="table" target="#tab_0">Table 1</ref> shows the overall architecture of the proposed MicronNet network architecture. The proposed MicronNet network architecture is a 16-bit floating-point deep convolutional neural network composed of four convolutional layers, followed by two fully-connected layers and a softmax layer. A combination of 1?1 point-wise convolutional layer with 5?5 and 3?3 spatial convolutional layers form a spectral-spatial macroarchitecture for reducing complexity while maintaining accuracy. Furthermore, rectified linear unit (ReLU) activation functions are leveraged within the proposed MicronNet network architecture for low computational complexity and better suitability for real-time embedded applications. Each of the design considerations in the design of MicronNet is discussed in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Numerical microarchitecture optimization</head><p>The first design consideration in obtaining an ideal network architecture for real-time embedded traffic sign recognition in this study is to optimize the network microarchitecture of the proposed MicronNet network. One of the key challenges to identifying the ideal microarchitecture for each of the individual convolutional layers in the deep neural network is to achieve a fine balance between modeling performance and model size as well as computations involved. While a number of existing techniques have focused on uniform microarchitecture design <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>, the strategy employed here is instead focused on numerical microarchitecture optimizations that operates at a more fine-grain level than other techniques, as it was found by the authors to yield a better balance between modeling performance and model size as well as reduced computations.</p><p>Taking that mentality into account here, the key design parameters of the microarchitectures of each convolutional layer are the number of convolutional filters that form the microarchitecture, and their associated sizes. Therefore, here we optimize the number of convolutional filters and their associated sizes in each convolutional layer via numerical optimization. More specifically, the key objective leveraged here is to minimize the number of parameters that compose each convolutional layer in the network architecture while maintaining the overall accuracy of the network.</p><p>One quantifiable assessment of the relative amount of accuracy a given deep neural network captures with respect to a fundamental building block (in this case, a parameter) that ties well with this key objective is the information density <ref type="bibr" target="#b3">[4]</ref> of a deep neural network. By taking into account both model size and network performance by means of a single metric, information density provides a good representation of the network's ability to utilize its full modeling capacity. Therefore, a deep neural network with a good balance of being smaller with fewer parameters yet still maintaining strong performance would be characterized by a higher information density, and hence higher information density indicates better network efficiency and is thus our desired outcome. In this study, the numerical microarchitecture optimization strategy is framed as a constrained optimization problem, where the set of optimization parameters F is set as the number of convolutional filters and their associated sizes in each convolutional layer for a given network N , and the goal is to numerically determine the optimal F that minimizes the total number of network parameters (denoted here as p(N ; F )) for a given F , with the validation accuracy a v (N ) constrained to being greater than or equal to an accuracy lower-bound of l (set to 98.5% in this study based on the performance of <ref type="bibr" target="#b5">[6]</ref>):</p><formula xml:id="formula_0">F = min F p(N ; F ) subject to a v (N ) ? l.<label>(1)</label></formula><p>An approximate solution to the above constrained optimization problem posed in Eq. 1 can be obtained using an iterative optimization approach. The key advantage with leveraging such a numerical microarchitecture optimization strategy is that each layer has its own unique information density limits and thus the degree of microarchitecture optimization that can be achieved for each layer can differ substantially. Therefore, a numerical microarchitecture optimization strategy allows significantly greater flexibility in obtaining the ideal microarchitectures for each convolutional layer with the optimal information densities without being constrained by the need for uniform fine-tuning. As a result, the proposed MicronNet network architecture possesses highly optimized microarchitectures that is optimized for real-time embedded scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spectral macroarchitecture augmentation</head><p>The second design consideration in obtaining an ideal network architecture for real-time embedded traffic sign recognition is to incorporate additional layers to the macroarchitecture of the MicronNet network that enable further reductions in computational complexity to be made while maintaining strong accuracy. Taking inspiration from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> where 1?1 convolutional layers are leveraged to reduce the number of parameters in the network while preserving modeling performance, we augment the proposed MicronNet network an additional 1?1 convolutional layer placed at a strategic location where it would have the most impact on reducing computational complexity while having a positive impact on modeling performance. More specifically, we take inspiration from work on spectralspatial macroarchitectures such as that proposed in <ref type="bibr" target="#b25">[26]</ref>, which are designed to learn spectral features prior to learning spatial features in an end-to-end macroarchitecture, and incorporated an additional 1?1 convolutional layer at the beginning of proposed MicronNet network architecture. This 1?1 convolutional layer sits before a 5 ? 5 convolutional layer and acts as a pointwise feature transform layer where new features are built through computing linear combinations of the input spectral channels. Therefore, from a theoretical perspective, one can view this 1?1 convolutional layer as a spectral feature learning layer that learns the optimal spectral mixing projection between the input color channels in an image to produce a single-channel spectral feature map that feeds into subsequent convolutional layers (see <ref type="figure" target="#fig_0">Figure 3</ref>), resulting in a spectral-spatial network macroarchitecture. The key advantage of this additional 1?1 convolutional layer compared to the strategy used by deep neural networks such as that proposed by <ref type="bibr" target="#b0">[1]</ref>, which converts color input images into grayscale images using a pre-defined conversion scheme, is that it provides a much greater level of flexibility in learning a more discriminative spectral projection into a single feature channel than that can be achieved with a fixed grayscale conversion scheme.</p><p>Based on empirical experiments, the augmentation of this additional pointwise convolutional layer to form a spectral-spatial network architecture enables us to greatly reduce the number of filters needed in the 5?5 convolutional layer to obtain strong modeling accuracy. In addition to reducing the number of parameters in the proposed MicronNet network, the reduction in the number of convolutional filters in the 5?5 convolutional layer is very important as the convolutional filters are used to convolve inputs at the original image resolution, and as such reducing the number of convolutional filters result in a significant reduction in the number of computations that need to be performed. It is important to note that this augmentation is performed on the proposed network architecture prior to the numerical microarchitecture optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameter Precision Optimization and Activation Function Selection</head><p>The third design consideration in obtaining an ideal network architecture for real-time embedded traffic sign recognition is to optimize the precision of the parameters used in the proposed MicronNet network. For embedded applications, the computational requirements and memory requirements are typically quite strict and as such an effective strategy to address these requirements is to reduce the data precision of parameters in a deep neural network. In particular, embedded processors often support accelerated mixed precision operations, and as a result leveraging such parameter precision considerations into the design of the deep neural network can result in noticeable improvements in computational time as well as memory storage for embedded scenarios. For the MicronNet network architecture, all parameters are characterized with half precision floating-point data representations after training to enable further model size reductions while still achieving strong performance. Alongside the use of fixed-point parameter precision for embedded applications, the utilization of halfprecision floating-point parameter precision for deep neural networks has seen widespread adoption for embedded applications and hardware-accelerated in a wide range of embedded processors, including the Nvidia Tegra family of embedded processors as well as widely-used ARM embedded processors such as the Cortex-A53 high efficiency processor tested in this study. In additional, we also produced a variant of the proposed MicronNet network architecture with 16-bit fixed-point data representation for comparison purposes.</p><p>Finally, to reduce the computational complexity of the proposed MicronNet network architecture, the rectified linear unit (ReLU) function is used as the activation function in the deep neural network since it is more suitable for real-time embedded applications when compared to other activation functions such as the scaled hyperbolic tangent function <ref type="bibr" target="#b13">[14]</ref> and the parametric rectifier linear unit (PReLU) function <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Here, we will discuss the training policy for learning the proposed MicronNet network. The proposed MicronNet network was trained for 60,000 iterations in the Caffe framework with a training batch size of 50. Stochastic gradient descent with momentum and exponential decay was utilized as the training policy with the base learning rate set to 0.007, the momentum set to 0.9, the learning rate decay step size set to 1000, and the learning rate decay rate set to 0.9996. A l 2 weight decay with rate 0.00001 was also used on the filters and matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results and Discussion</head><p>To study the efficacy of the proposed MicronNet for real-time embedded traffic sign recognition, we evaluate the following:</p><p>? Top-1 accuracy on the German traffic sign recognition benchmark (GTSRB) <ref type="bibr" target="#b19">[20]</ref> ? Resource usage (model size, number of parameters, number of multiply-accumulate (MAC) operations, time-to-compute on a 1.2GHz Cortex-A53 high efficiency processor) ? Information density <ref type="bibr" target="#b3">[4]</ref> and NetScore <ref type="bibr" target="#b21">[22]</ref> ? Robustness against image degradation For evaluation purposes, the following state-of-the-art traffic sign recognition networks were also compared:</p><p>? STDNN <ref type="bibr" target="#b2">[3]</ref>, deep convolutional neural network with spatial transformers,</p><p>? HLSGD <ref type="bibr" target="#b11">[12]</ref>: hinge loss trained deep convolutional neural network,</p><p>? MCDNN <ref type="bibr" target="#b5">[6]</ref>: multi-column deep convolutional neural network,</p><p>? CDNN <ref type="bibr" target="#b5">[6]</ref>: Ciresan deep convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The German traffic sign recognition benchmark (GTSRB) <ref type="bibr" target="#b19">[20]</ref> used for evaluation purposes in this paper consists of color images of traffic signs (one traffic sign per image, with a total of 43 types of traffic signs) with image sizes varying from 15?15 to 250?250 pixels. There are a total of 39,209 color images in the training set and a total of 12,630 images in the test set. To balance the number of samples in different classes as well as improve the generality of the resulting network, a number of different data augmentation techniques were leveraged including: i) rotation, ii) shifting, iii) sharpening, iv) Gaussian blur, v) motion blur, vi) HSV augmentation, and vii) mirroring. As standard for evaluating performance using GTSRB, all images are cropped and all images are resized to 48?48 pixels <ref type="bibr" target="#b5">[6]</ref>. To evaluate the accuracy of the network, the top-1 accuracy was computed on the GTSRB test set.  <ref type="bibr" target="#b19">[20]</ref>. The results of several state-of-the-art traffic sign recognition are provided, along with the average human performance, for comparison purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Number of Top-1 accuracy Name parameters (GTSRB) Human <ref type="bibr" target="#b20">[21]</ref> -98.8% STDNN <ref type="bibr" target="#b2">[3]</ref> 14M 99.7% HLSGD <ref type="bibr" target="#b11">[12]</ref> 23.2M 99.6% MCDNN <ref type="bibr" target="#b5">[6]</ref> 38.5M 99.5% CDNN ? <ref type="bibr" target="#b5">[6]</ref> 1.54M 98.5% MicronNet (fp16) 0.51M 98.9% MicronNet (fixed16) 0.51M 98.0% ? average reported top-1 accuracy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information density and NetScore</head><p>The model efficiency of the proposed MicronNet network and the state-of-the-art traffic sign recognition networks also being compared were assessed by means of its information density <ref type="bibr" target="#b3">[4]</ref> and NetScore <ref type="bibr" target="#b21">[22]</ref> as well to obtain a better understanding of the amount of relative performance a given deep neural network captured with respect to a fundamental building block. More specifically, the information density (D) of a deep neural network N is defined as the performance of the deep neural network (denoted by a(N )) divided by the number of parameters needed for representing it (denoted by p(N )),</p><formula xml:id="formula_1">D(N ) = a(N ) p(N )<label>(2)</label></formula><p>By taking into account both model size and network performance by means of a single metric, information density (expressed as percent of top-1 accuracy per parameter in this study) provides a good representation of the network's ability to utilize its full modeling capacity, with higher information density indicating better network efficiency.</p><p>One aspect that information capacity does not account for is the computational cost for performing inference with a given deep neural network, which is important for real-time embedded applications. Therefore, the NetScore <ref type="bibr" target="#b21">[22]</ref> metric was also leveraged in this study for assessing the performance of a deep neural network N for practical usage. The NetScore metric (denoted here as ?) can be defined as:</p><formula xml:id="formula_2">?(N ) = 20 log a(N ) ? p(N ) ? m(N ) ? (3)</formula><p>where a(N ) is the accuracy of the network, p(N ) is the number of parameters in the network, m(N ) is the number of multiply-accumulate (MAC) operations performed during network inference, and ?, ?, ? are coefficients that control the influence of accuracy, architectural complexity, and computational complexity of the network on ?. We set ? = 2, ? = 0.5, and ? = 0.5 as proposed in <ref type="bibr" target="#b21">[22]</ref>. <ref type="table">Table 3</ref>: Information density of MicronNet on German traffic sign recognition benchmark (GT-SRB) <ref type="bibr" target="#b19">[20]</ref>. The results of several state-of-the-art traffic sign recognition networks are provided for comparison purposes. Higher is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Information capacity Name (% per MParams) STDNN <ref type="bibr" target="#b2">[3]</ref> 7.1 HLSGD <ref type="bibr" target="#b11">[12]</ref> 4.3 MCDNN <ref type="bibr" target="#b5">[6]</ref> 2.6 CDNN <ref type="bibr" target="#b5">[6]</ref> 64 MicronNet (fp16) 194 MicronNet (fixed16) 192</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Top-1 accuracy. <ref type="table" target="#tab_1">Table 2</ref> shows the number of parameters and the top-1 accuracy of the proposed MicronNet network (both in half-precision floating-point data representation and 16-bit fixed-point data representation) on the GTSRB test dataset, along with the number of parameters and top-1 accuracies for state-of-the-art traffic sign recognition networks. A number of interesting observations can be made. First, the resulting MicronNet possesses just ?510,000 parameters, which is ?27.5x fewer than the state-of-the-art STDNN network <ref type="bibr" target="#b2">[3]</ref>. Even when compared to the smallest state-ofthe-art traffic sign recognition network compared in this paper (i.e., the CDNN network <ref type="bibr" target="#b5">[6]</ref>, which MicronNet outperforms), the proposed MicronNet network still has ?3x fewer parameters. The significantly smaller number of parameters in the proposed MicronNet network compared to all of the evaluated state-of-the-art traffic sign recognition networks illustrates its efficacy for greatly reducing the computational and memory requirements, making the use of MicronNet very well suited for real-time embedded traffic sign recognition purposes. Second, it can be observed that the resulting MicronNet was still able to achieve a top-1 accuracy of 98.9% on the GTSRB test dataset, which is just ?0.8% lower than that achieved using the state-of-the-art STDNN network, and ?0.4% higher than that achieved by the smallest tested network outside of the proposed MicronNet (i.e., CDNN <ref type="bibr" target="#b5">[6]</ref>). Third, it can be observed that the top-1 accuracy of the proposed MicronNet network was equivalent to the average human performance reported in <ref type="bibr" target="#b20">[21]</ref>. The top-1 accuracy results exhibited by MicronNet illustrates the efficacy of this proposed network for providing strong embedded traffic sign recognition capabilities despite its significantly smaller size compared to other state-of-the-art networks. In addition, it can be observed that the variant of the proposed MicronNet with 16-bit fixed-point data representation, while achieving lower top-1 accuracy than the proposed MicronNet with half-precision data representation, still managed to achieve a top-1 accuracy of 98.0% on the GTSRB test dataset.</p><p>To study where the proposed MicronNet encounters difficulties, we examine some of the traffic images from the GTSRB test dataset that has been misclassified by the proposed MicronNet (see <ref type="figure">Fig. 4</ref>). It can be observed that in the example misclassified traffic images, the sign is either heavily motion blurred (left), partially occluded (middle), or exhibit poor illumination (right). The identification of such misclassifications can provide good insight into the weaknesses of a network, as one potential mechanism for improving the robustness to such scenarios may be to extend the data augmentation <ref type="figure">Figure 4</ref>: Examples of traffic images from the GTSRB test dataset that has been misclassified by the proposed MicronNet. It can be seen that in the example misclassified traffic images, the sign is either heavily motion blurred (left), partially occluded (middle), or exhibit poor illumination (right).</p><p>policy to include more synthetic examples at different forms of occlusions as well as different illumination levels.</p><p>Information density and NetScore. <ref type="table">Table 3</ref> shows the information density of the proposed MicronNet network on the GTSRB test dataset, along with the information density for state-of-the-art traffic sign recognition networks. It can be observed that the information density of the resulting MicronNet is significantly higher than all of the other tested traffic sign recognition networks, by as much as ?75x higher in the case of MCDNN <ref type="bibr" target="#b5">[6]</ref>. The high information density of the proposed MicronNet network, for both half-precision floating-point and 16-bit fixed-point data representations, when compared to the other evaluated state-of-the-art traffic sign recognition networks further illustrate the network efficiency of the proposed network. Finally, the NetScore of the proposed MicronNet network was computed to be 102.52, which is quite high and further reinforces the strong balance between accuracy, architectural complexity, and computational cost of the proposed network.</p><p>Resource usage. <ref type="table" target="#tab_2">Table 4</ref> shows the resource usage of the proposed MicronNet network, which is very important for evaluating its efficacy for real-time embedded applications given that both memory and computational resources are very limited in such cases. A number of interesting observations can be made. First, it can be observed that the proposed MicronNet network is just ?1.05MB in size, which can be contributed to the fact that not only is the number of parameters being very low compared to existing state-of-the-art networks but also a result of the fact that the parameters are represented with half-precision float-point values. Second, it can be observed that the proposed MicronNet network requires just ?10.5 million multiply-accumulate (MAC) operations to perform inference, which indicates that the proposed MicronNet network has low computational requirements for performance network inference. To better evaluate the computational requirements of the proposed MicronNet network in a real-world embedded scenario, the network was evaluated on a 1.2GHz Cortex-A53 high efficiency processor in a Broadcom BCM2837B0 SoC. It was found that the time-to-compute was just 32.19 ms on the tested high efficiency processor in half-precision floating-point (fp16) mode with power consumption of ?3W, making it very well-suited for real-time embedded traffic sign recognition. These experimental results clearly demonstrate that very small yet accurate deep neural network architectures can be designed for real-time traffic sign recognition that are well-suited for embedded scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, a highly compact deep convolutional neural network called MicronNet is introduced for real-time embedded traffic sign recognition. By designing a highly optimized network architecture where each layer's microarchitecture is optimized to have as few parameters as possible, along with macroarchitecture augmentation and parameter precision optimization, the resulting MicronNet network achieves a good balance between accuracy and model size as well as inference speed. The resulting MicronNet possess a model size of just ?1MB and ?510,000 parameters (?27x fewer parameters than state-of-the-art), requires just ?10 million multiply-accumulate operations to perform inference (with a time-to-compute of 32.19 ms on a Cortex-A53 high efficiency processor), while still achieving a top-1 accuracy of 98.9% on the German traffic sign recognition benchmark, thus achieving human-level performance. These experimental results show that very small yet accurate deep neural network architectures can be designed for real-time traffic sign recognition that are well-suited for embedded scenarios.</p><p>Future work involves exploring extensions upon MicronNet across a larger range of traffic datasets to improve generalizability in different scenarios. Furthermore, it is also worth exploring and investigating this integrated microarchitecture-level and macroarchitecture-level design principles and optimization strategies on deep neural network architectures for different tasks outside of traffic sign recognition, and the fundamental tradeoffs between microarchitecture-level and macroarchitecturelevel design principles and optimization strategies on such deep neural network architectures and mechanisms to optimize for such tradeoffs to improve generalizability of such an approach. Furthermore, model stability studies that also involve assessing the performance of this approach in the case of less training data given smaller model sizes would be quite interesting to explore as future work. Finally, model performance studies with a wider variety of embedded processors at different floating-point and fixed-point precision levels would be interesting to explore as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Spectral macroarchitecture augmentation: a 1?1 point-wise convolutional layer (a) sits before a 5 ? 5 convolutional layer (b) to form a spectral-spatial macroarchitecture where spectral features are first extracted through computing linear combinations of the input spectral channels in the point-wise convolutional layer, before spatial features are extracted in subsequent convolutional layers (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The optimized network architecture underlying MicronNet</figDesc><table><row><cell cols="3">Type / Stride / Pad Filter Shape Input Size</cell></row><row><cell>Conv / s1 / p0</cell><cell>1 ? 1 ? 1</cell><cell>48 ? 48</cell></row><row><cell>Conv / s1 / p0</cell><cell>5 ? 5 ? 29</cell><cell>48 ? 48</cell></row><row><cell>Pool / s2 / p0</cell><cell>3 ? 3</cell><cell>maxpool</cell></row><row><cell>Conv / s1 / p0</cell><cell>3 ? 3 ? 59</cell><cell>22 ? 22</cell></row><row><cell>Pool / s2 / p0</cell><cell>3 ? 3</cell><cell>maxpool</cell></row><row><cell>Conv / s1 / p0</cell><cell>3 ? 3 ? 74</cell><cell>10 ? 10</cell></row><row><cell>Pool / s2 / p0</cell><cell>3 ? 3</cell><cell>maxpool</cell></row><row><cell>FC / s1</cell><cell>1 ? 300</cell><cell>1 ? 1184</cell></row><row><cell>FC / s1</cell><cell>1 ? 300</cell><cell>1 ? 300</cell></row><row><cell>Softmax / s1</cell><cell>Classifier</cell><cell>1 ? 43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Top-1 accuracy results and number of parameters of MicronNet on German traffic sign recognition benchmark (GTSRB)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Resource usage of MicronNet. The time-to-compute was computed on a 1.2GHz Cortex-A53 high efficiency processor.</figDesc><table><row><cell>Model</cell><cell cols="2">Total number Total number</cell><cell>Time To</cell><cell>Power</cell></row><row><cell>Size</cell><cell>of Parameters</cell><cell>of MACs</cell><cell cols="2">Compute (fp16) Consumption (W)</cell></row><row><cell>1.05MB</cell><cell>0.51M</cell><cell>10.5M</cell><cell>32.19 ms</cell><cell>3W</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Robustness of MicronNet against different levels of image degradation.</figDesc><table><row><cell cols="5">Degradation level ?=0% ?=2.5% ?=5% ?=7.5%</cell></row><row><cell>Top-1 accuracy</cell><cell>98.9%</cell><cell>98.5%</cell><cell>96.5%</cell><cell>92.3%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors thank the Natural Sciences and Engineering Research Council of Canada, Canada Research Chairs Program, and DarwinAI, as well as Nvidia for hardware support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A practical approach for detection and classification of traffic signs using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aghdam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A practical and highly optimized convolutional neural network for classifying traffic signs in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aghdam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural network for traffic sign recognition systems: An analysis of spatial transformers and stochastic optimisation methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arcos-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alvarez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Soria-Morillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International joint conference on neural networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-column deep neural network for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Seyyed Hossein Hasanpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adeli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06205</idno>
		<title level="m">Towards principled design of deep convolutional networks: Introducing simpnet</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and&lt; 0.5 mb model size</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with hinge loss trained convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">YOLO: Real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<ptr target="https://pjreddie.com/darknet/yolo/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong Zhu Andrey Zhmoginov Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Squishednets: Squishing squeezenet further for edge device scenarios via deep evolutionary synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Chwyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07459</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International joint conference on neural networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Netscore: Towards universal metrics for large-scale performance analysis of deep neural networks for practical usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05512</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tiny ssd: A tiny single-shot detection deep convolutional neural network for real-time embedded object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chwyl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Squeezedet: Unified, small, low power fully convolutional neural networks for real-time object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01051</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Traffic sign detection based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spectral-spatial residual network for hyperspectral image classification: A 3-d deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
