<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking and Improving Relative Position Encoding for Vision Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking and Improving Relative Position Encoding for Vision Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relative position encoding (RPE) is important for transformer to capture sequence ordering of input tokens. General efficacy has been proven in natural language processing. However, in computer vision, its efficacy is not well studied and even remains controversial, e.g., whether relative position encoding can work equally well as absolute position? In order to clarify this, we first review existing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedicated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position embeddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be easily plugged into transformer blocks. Experiments demonstrate that solely due to the proposed encoding methods, DeiT [22]  and DETR [1] obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tuning any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield interesting findings, some of which run counter to previous understanding. Code and models are open-sourced at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformer recently has drawn great attention in computer vision because of its competitive performance and superior capability in capturing long-range dependencies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25]</ref>. The core of transformer is self-attention <ref type="bibr" target="#b23">[23]</ref>, which is capable of modeling the relationship of tokens in a sequence. Self-attention, however, has an inherent deficiency -it cannot capture the ordering of input tokens. Therefore, incorporating explicit representations of position information is especially important for transformer, since the model is otherwise entirely invariant to sequence ordering, which is undesirable for modeling structured data. * Equal contributions. Work performed when Kan and Minghao were interns of MSRA. ? Corresponding author: houwen.peng@microsoft.com There are mainly two classes of methods to encode positional representations for transformer. One is absolute, while the other is relative. Absolute methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">23]</ref> encode the absolute positions of input tokens from 1 to maximum sequence length. That is, each position has an individual encoding vector. The encoding vector is then combined with the input token to expose positional information to the model. On the other hand, relative position methods <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b2">3]</ref> encode the relative distance between input elements and learn the pairwise relations of tokens. Relative position encoding (RPE) is commonly calculated via a look-up table with learnable parameters interacting with queries and keys in self-attention modules <ref type="bibr" target="#b18">[18]</ref>. Such scheme allows the modules to capture very long dependencies between tokens. Relative position encoding has been verified to be effective in natural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b15">16]</ref>. However, in computer vision, the efficacy is still unclear. There are few recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">19]</ref> shedding light on it, but obtaining controversial conclusions in vision transformers. For example, Dosovitskiy et al. <ref type="bibr" target="#b5">[6]</ref> observed that the relative position encoding does not bring any gain comparing to the absolute one (please refer to Tab. 8 in <ref type="bibr" target="#b5">[6]</ref>). On the contrary, Srinivas et al. <ref type="bibr" target="#b19">[19]</ref> found that relative position encoding can induce an apparent gain, being superior to the absolute one (please refer to Tab. 4 in <ref type="bibr" target="#b19">[19]</ref>). Moreover, the mostly recent work <ref type="bibr" target="#b1">[2]</ref> claims that the relative positional encoding cannot work equally well as the absolute ones (please refer to Tab. 5 in <ref type="bibr" target="#b1">[2]</ref>). These works draw different conclusions on the effectiveness of relative position encoding in models, that motivates us to rethink and improve the usage of relative positional encoding in vision transformer.</p><p>On the other hand, the original relative position encoding is proposed for language modeling, where the input data is 1D word sequences <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">18]</ref>. But for vision tasks, the inputs are usually 2D images or video sequences, where the pixels are highly spatially structured. It is unclear that: whether the naive extension from 1D to 2D is suitable for vision models; whether the directional information is important in vision tasks?</p><p>In this paper, we first review existing relative position encoding methods, and then propose new methods dedicated to 2D images. We make the following contributions.</p><p>? We analyze several key factors in relative position encoding, including the relative direction, the importance of context, the interactions between queries, keys, values and relative position embeddings, and computational cost. The analysis presents a comprehensive understanding of relative position encoding, and provides empirical guidelines for new method design.</p><p>? We introduce an efficient implementation of relative encoding, which reduces the computational cost from the original O(n 2 d) to O(nkd), where k n. Such implementation is suitable for high-resolution input images, such as object detection and semantic segmentation, where the token number might be very large.</p><p>? We propose four new relative position encoding methods, called image RPE (iRPE), dedicated to vision transformers, considering both efficiency and generalizability. The methods are simple and can be easily plugged into self-attention layers. Experiments show that, without adjusting any hyperparameters and settings, the proposed methods can improve DeiT-S <ref type="bibr" target="#b22">[22]</ref> and DETR-ResNet50 [1] by 1.5% (top-1 Acc) and 1.3% (mAP) over their original models on Ima-geNet <ref type="bibr" target="#b3">[4]</ref> and COCO <ref type="bibr" target="#b11">[12]</ref>, respectively.</p><p>? We answer previous controversial questions. We empirically demonstrate that relative position encoding can replace the absolute encoding for image classification task. Meanwhile, the absolute encoding is necessary for object detection, where the pixel position is important for object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Self-Attention</head><p>Self-attention plays a fundamental role in transformer. It maps a query and a set of key-value pairs to an output. More specifically, for an input sequence, e.g., the embeddings of words or image patches, x = (x 1 , . . . , x n ) of n elements where x i ? R dx , self-attention computes an output sequence z = (z 1 , . . . , z n ) where z i ? R dz . Each output element z i is computed as a weighted sum of input elements:</p><formula xml:id="formula_0">z i = n j=1 ? ij (x j W V ).<label>(1)</label></formula><p>Each weight coefficient ? ij is computed using a softmax:</p><formula xml:id="formula_1">? ij = exp(e ij ) n k=1 exp(e ik ) ,<label>(2)</label></formula><p>where e ij is calculated using a scaled dot-product attention:</p><formula xml:id="formula_2">e ij = (x i W Q )(x j W K ) T ? d z .<label>(3)</label></formula><p>Here, the projections W Q , W K , W V ? R dx?dz are parameter matrices, which are unique per layer. Rather than computing the self-attention once, Multihead self-attention (MHSA) <ref type="bibr" target="#b23">[23]</ref> runs the self-attention multiple times in parallel, i.e., employing h attention heads. The attention head outputs are simply concatenated and linearly transformed into the expected dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Position Encoding</head><p>Absolute Position Encoding. Since transformer contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we need to inject some information about the position of the tokens. The original self-attention considers the absolute position <ref type="bibr" target="#b23">[23]</ref>, and add the absolute positional encodings p = (p 1 , . . . , p n ) to the input token embedding x as</p><formula xml:id="formula_3">x i = x i + p i ,<label>(4)</label></formula><p>where the positional encoding p i ,</p><formula xml:id="formula_4">x i ? R d x .</formula><p>There are several choices of absolute positional encodings, such as the fixed encodings by sine and cosine functions with different frequencies and the learnable encodings through training parameters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>Relative Position Encoding. Besides the absolute position of each input element, recent works also consider the pairwise relationships between elements, i.e., relative position <ref type="bibr" target="#b18">[18]</ref>. Relative relation is presumably important for tasks where the relative ordering or distance of the elements matters. This type of methods encode the relative position between the input elements x i and</p><formula xml:id="formula_5">x j into vectors p V ij , p Q ij , p K ij ? R dz , where d z = d x .</formula><p>The encoding vectors are embedded into the self-attention module, which re-formulates Eq. (1) and Eq. (3) as</p><formula xml:id="formula_6">z i = n j=1 ? ij (x j W V + p V ij ),<label>(5)</label></formula><formula xml:id="formula_7">e ij = (x i W Q + p Q ij )(x j W K + p K ij ) T ? d z .<label>(6)</label></formula><p>In this fashion, the pairwise positional relation is learned during transformer training. Such relative position encoding can be either shared across attention heads or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we first review previous relative position encoding methods and analyze their differences. Then, we propose four new methods dedicated to vision transformer, and their efficient implementation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Previous Relative Position Encoding Methods</head><p>Shaw's RPE. Shaw et al. <ref type="bibr" target="#b18">[18]</ref> propose a relative position encoding for self-attention. The input tokens are modeled as a directed and fully-connected graph. Each edge between two arbitrary positions i and j is presented by a learnable vector p ij ? R dz , namely relative position encoding. Besides, the authors deemed that precise relative position information is not useful beyond a certain distance, so introduced a clip function to reduce the number of parameters. The encoding is formulated as</p><formula xml:id="formula_8">z i = n j=1 ? ij (x j W V + p V clip(i?j,k) ),<label>(7)</label></formula><formula xml:id="formula_9">e ij = (x i W Q )(x j W K + p K clip(i?j,k) ) T ? d z ,<label>(8)</label></formula><p>clip(x, k) = max(?k, min(k, x)),</p><p>where p V and p K are the trainable weights of relative position encoding on values and keys, respectively.</p><formula xml:id="formula_11">p V = (p V ?k , ..., p V k ) and p K = (p K ?k , ..., p K k ) where p V i , p K i ? R dz . The scalar k is the maximum relative distance.</formula><p>RPE in Transformer-XL. Dai et al. <ref type="bibr" target="#b2">[3]</ref> introduce additional bias terms for queries, and uses the sinusoid formulation for relative position encoding, which is formulated as</p><formula xml:id="formula_12">e ij = (x i W Q + u)(x j W K ) T + (x i W Q + v)(s i?j W R ) T ? d z ,<label>(10)</label></formula><p>where u, v ? R dz are two learnable vectors.</p><p>The sinusoid encoding vector s provides the prior of relative position <ref type="bibr" target="#b23">[23]</ref>. W R ? R dz?dz is a trainable matrix, projecting s i?j into a location-based key vector.</p><p>Huang's RPE. Huang et al. <ref type="bibr" target="#b10">[11]</ref> propose a new method considering the interactions of queries, keys and relative positions simultaneously. The equation is given as follows</p><formula xml:id="formula_13">e ij = (x i W Q + p ij )(x j W K + p ij ) T ? p ij p ij T ? d z ,<label>(11)</label></formula><p>where p ij ? R dz is the relative position encoding shared by queries and keys. RPE in SASA. The above three methods are all designed for 1D word sequence in language modeling. Ramachandran et al. <ref type="bibr" target="#b17">[17]</ref> propose an encoding method for 2D images. The idea is simple. It divides the 2D relative encoding into horizontal and vertical directions, such that each direction can by modeled by a 1D encoding. The method formulation is given as follows</p><formula xml:id="formula_14">e ij = (x i W Q )(x j W K + concat(p K ?x , p K ?? )) T ? d z ,<label>(12)</label></formula><p>where ?x =x i ?x j and ?? =? i ?? j denote the relative position offsets on x-axis and y-axis of the image coordinate respectively, p K ?x and p K ?? are learnable vectors with length 1 2 d z , the concat operation concatenates the two encodings to form a final relative encoding with length of d z . In other words, the same offsets on x-axis or y-axis share the same relative position encoding, so this method is able to reduce the number of learnable parameters and computational cost. However, the encoding is only applied on keys.</p><p>In our experiments, we observe that the RPE imposed on keys, queries and values simultaneously is the most effective one, as presented in Tab. 4 and Tab. 5.</p><p>RPE in Axial-Deeplab. Wang et al. <ref type="bibr" target="#b24">[24]</ref> introduce a position-sensitive method that adds qkv-dependent positional bias into self-attention. The position sensitivity is applied on axial attention that propagates information along height-axis and width-axis sequentially. However, when the relative distance is larger than a threshold, the encoding is set to zero. We observe that long-range relative position information is useful, as analysed in Tab. 6. The positionsensitivity might be competitive when imposed on the standard self-attention. If equipped with the proposed piecewise function, it can be further improved and become more efficient for modeling long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Proposed Relative Position Encoding Methods</head><p>We design our image RPE (iRPE) methods to analyze several factors which are not well studied in prior works (see the analysis in Sec. 4.2). First, to study whether the encoding can be independent of the input embeddings, we introduce two relative position modes: bias and contextual. We present a piecewise function to map relative positions to encodings, being different from the conventional clip function. After that, to study the importance of directivity, we design two undirected and two directed methods. Finally we provide an efficient implementation for our methods.</p><p>Bias Mode and Contextual Mode. Previous relative position encoding methods all depend on input embeddings. It brings a question, i.e., whether the encoding can be independent of the input? We introduce bias mode and contextual mode of relative position encoding to study the question. The former one is independent of input embeddings, while the latter one considers the interaction with queries, keys or values. More specifically, we introduce a unified formulation as</p><formula xml:id="formula_15">e ij = (x i W Q )(x j W K ) T +b ij ? d z ,<label>(13)</label></formula><p>where b ij ? R is the 2D relative position encoding, defining the bias or contextual mode. For bias mode,</p><formula xml:id="formula_16">b ij = r ij ,<label>(14)</label></formula><p>where r ij ? R is a learnable scalar and represents the relative position weight between the position i and j. For con-</p><formula xml:id="formula_17">textual mode, b ij = (x i W Q )r ij T ,<label>(15)</label></formula><p>where r ij ? R dz is a trainable vector, interacted with the query embedding. There are multiple variants for b ij in contextual mode. For example, the relative position encoding operated on both queries and keys can be presented as</p><formula xml:id="formula_18">b ij = (x i W Q )(r K ij ) T + (x j W K )(r Q ij ) T ,<label>(16)</label></formula><p>where r K ij , r Q ij ? R dz are both learnable vectors. Besides, contextual mode can also be applied on value embeddings,</p><formula xml:id="formula_19">z i = n j=1 ? ij (x j W V + r V ij ),<label>(17)</label></formula><p>where r V ij ? R dz . The relative position weights r Q ij , r K ij and r V ij can be constructed in the same way. For a unified representation, we use r ij to denote them in bias mode and contextual mode in the following discussion. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the illustration of self-attention modules with 2D relative position encoding on keys in the propsoed two modes.</p><p>A Piecewise Index Function. Before describing the 2D relative position weight r ij , we first introduce a manyto-one function, mapping a relative distance into an integer in finite set, then r ij can be indexed by the integer and share encondings among different relation positions. Such index function can largely reduce computation costs and the number of parameters for long sequence (e.g., high resolution images). Although the clip function h(x) = max(??, min(?, x)) used in <ref type="bibr" target="#b18">[18]</ref> also reduces the cost, the positions whose relative distance is larger than ? are assigned to the same encoding. This method inevitably drops out the contextual information of long-range relative positions. Inspired by <ref type="bibr" target="#b15">[16]</ref>, we introduce a piecewise function g(x) : R ? {y ? Z| ? ? ? y ? ?} for indexing relative distances to corresponding encodings. The function is based on a hypothesis that the closer neighbors are more important than the further ones, and distributes the attention by the relative distance. It is presented as</p><formula xml:id="formula_20">g(x) = [x], |x| ? ? sign(x) ? min(?, [? + ln (|x|/?) ln (?/?) (? ? ?)]), |x| &gt; ?<label>(18)</label></formula><p>where [?] is a round operation, sign() determines the sign of a number, i.e., returning 1 for positive input, -1 for negative, and 0 for otherwise. ? determines the piecewise point, ? controls the output in the range of [??, ?], and ? adjusts the curvature of the logarithmic part.</p><p>We compare the piecewise function g(x) with the clip function h(x) = min(??, max(?, x)), i.e. Eq. (9). In <ref type="figure" target="#fig_2">Fig. 2</ref>, the clip function h(x) distributes uniform attention and leaves out long distance positions, but the piecewise function g(x) distributes different levels of attention by relative distance. We suppose that the potential information in long-range position should be preserved, especially for high resolution images or the tasks requiring long-range feature dependencies, so g(x) is selected to construct our mapping method for r ij .</p><p>2D Relative Position Calculation. In order to calculate relative position on 2D image plane and define the relative weight r ij , we propose two undirected mapping methods, namely Euclidean and Quantization, as well as two directed mapping methods, namely Cross and Product.</p><p>Euclidean method. On image plane, the relative position (x i ?x j ,? i ?? j ) is a 2D coordinate. We compute Euclidean distance between two positions, and maps the distance into the corresponding encoding. The method is undirected and formulated as </p><formula xml:id="formula_21">r ij = p I(i,j) ,<label>(19)</label></formula><formula xml:id="formula_22">I(i, j) = g( (x i ?x j ) 2 + (? i ?? j ) 2 ),<label>(20)</label></formula><p>where p I(i,j) is either a learnable scalar in bias mode or a vector in contextual mode. We regard p I(i,j) as a bucket, which stores the relative position weight. The number of buckets is 2? + 1, as defined in Eq. <ref type="bibr" target="#b18">(18)</ref>. Quantization method. In the above Euclidean method, the closer two neighbors with different relative distances may be mapped into the same index, e.g. the 2D relative positions (1, 0) and (1, 1) are both mapped into the index 1. We suppose that the close neighbors should be separated. Therefore, we quantize Euclidean distance, i.e., different real number is mapped into different integer. We revise I(i, j) in Eq. <ref type="formula" target="#formula_0">(19)</ref> as</p><formula xml:id="formula_23">I(i, j) = g(quant( (x i ?x j ) 2 + (? i ?? j ) 2 ) ). (21)</formula><p>The operation quant(?) maps a set of real numbers {0, 1, 1.41, 2, 2.24, ...} into a set of integers {0, 1, 2, 3, 4, ...}. This method is also undirected.</p><p>Cross method. Positional direction of pixels is also important for images, we thereby propose directed mapping methods. This method is called Cross method, which computes encoding on horizontal and vertical directions separately, then summarizes them. The method is given as follows,</p><formula xml:id="formula_24">r ij = px Ix(i,j) + p? I?(i,j) ,<label>(22)</label></formula><formula xml:id="formula_25">Ix(i, j) = g(x i ?x j ),<label>(23)</label></formula><formula xml:id="formula_26">I?(i, j) = g(? i ?? j ),<label>(24)</label></formula><p>where px I(i,j) and p? I(i,j) are both learnable scalars in bias mode, or a learnable vectors in contextual mode. Similar to the encoding in SASA <ref type="bibr" target="#b17">[17]</ref>, the same offsets on x-axis or y-axis share the same encoding, but the main difference is that we use a piecewise function to distribute attention by relative distance. The number of buckets is 2 ? (2? + 1). Product method. The Cross method encodes different relative positions into the same embedding if the distance on one direction is identical, either horizontal or vertical.</p><p>Besides, the addition operation in Eq. (22) brings extra computational cost. To improve efficiency and involve more directional information, we design Product method which is formulated as below</p><formula xml:id="formula_27">r ij = p Ix(i,j),I?(i,j) .<label>(25)</label></formula><p>The right side of the equation is a trainable scalar in bias mode, or a trainable vector in contextual mode. Ix(i, j) and I?(i, j) are defined in Eq. (23) and Eq. <ref type="formula" target="#formula_1">(24)</ref>, and the combination of them is a 2D index for p. The number of buckets is (2? + 1) 2 . An Efficient Implementation. For the above proposed methods in contextual mode, there is a common term</p><formula xml:id="formula_28">(x i W)p I(i,j)</formula><p>T when putting Eq. <ref type="formula" target="#formula_0">(19)</ref>, Eq. <ref type="formula" target="#formula_1">(22)</ref> or Eq. (25) into Eq. <ref type="bibr" target="#b14">(15)</ref>. Let y ij denote the common term as follows,</p><formula xml:id="formula_29">y ij = (x i W)p I(i,j) T .<label>(26)</label></formula><p>It takes time complexity O(n 2 d) to compute all y ij , where n and d are the length of the input sequence and the number of feature channels, respectively. Due to the many-to-one property of I(i, j), the set size k of I(i, j) is usually less than n in vision transformer. Therefore, we provide an efficient implementation as follows,</p><formula xml:id="formula_30">z i,t = (x i W)p t T , t ? {I(i, j)|i, j ? [0, n)},<label>(27)</label></formula><formula xml:id="formula_31">y ij = z i,I(i,j) .<label>(28)</label></formula><p>It first takes time complexity O(nkd) to pre-compute all z i,t by Eq. <ref type="formula" target="#formula_1">(27)</ref>, then assigns z i,t to all y ij by the mapping t = I(i, j) by Eq. (28). The assignment operation takes time complexity O(n 2 ), whose cost is much smaller than that of the pre-computation procedure. Thus, the computational cost of relative position encoding reduces from the original O(n 2 d) to O(nkd).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first provide some analysis by comparing different position embeddings, followed by experiments on the effects of key factors in relative position encoding. Then, we compare the proposed methods with the state-of-the-art methods on image classification and object detection tasks. Finally, we visualize the relative position encoding and explain why it works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We choose the recent vision transformer model DeiT <ref type="bibr" target="#b22">[22]</ref> as the baseline for most experiments. The relative position encoding is added into all self-attention layers. If not specified, the relative position encoding is only added on keys. We set ?:?:? = 1:2:8 for the piecewise function g(x), and adjust the number of buckets by changing ?. An  extra bucket is used to store the relative position encodings of the classification token.</p><p>For fair comparison, we adopt the same training settings as DeiT <ref type="bibr" target="#b22">[22]</ref>: AdamW <ref type="bibr" target="#b13">[14]</ref> optimizer with weight decay 0.05, initial learning rate 1x10 ?3 and minimal learning 1x10 ?5 with cosine scheduler, 5 epochs warmup, batch size of 1024, 0.1 label smoothing <ref type="bibr" target="#b20">[20]</ref>, and stochastic depth with survival rate probability of 0.9. For training, the images are split into 14x14 non-overlapping patches. Data augmentation methods <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b27">27]</ref> are also consistent with DeiT <ref type="bibr" target="#b22">[22]</ref>. All models are trained from scratch for 300 epochs with 8 NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis on Relative Position Encoding</head><p>Directed v.s. Undirected. As shown in Tab. 1, directed methods (Cross and Product), in general, perform better than undirected ones (Euclidean and Quantization) in vision transformer. This phenomenon illustrates that the directivity is important for vision transformers, because image pixels are highly structured and semantically correlative.</p><p>Bias v.s. Contextual. Tab. 1 shows that the contextual mode achieves superior performance to that of bias mode, regardless of which method uses. The underlying reason might be that contextual mode changes the encoding with the input feature while bias mode keeps static.</p><p>Shared v.s. Unshared. Self-attention contains multiple heads. The relative position encoding can be either shared or unshared across different heads. We show the effects of these two schemes in bias and contextual modes in Tab. 2, respectively. For bias mode, the accuracy drops significantly when sharing encoding across the heads. By contrast, in contextual mode, the performance gap between two schemes is negligible. Both of them achieve an average top-1 accuracy of 80.9%. We conjecture that different head   Piecewise v.s. Clip. We compare the efficacy of the piecewise function g(x) defined in Eq. (18) and the clip function h(x) defined in Eq. (9) in Tab. 3. There is a very small, even negligible, performance gap between these two functions in image classification task. However, in object detection task, we found that clip function is worse than the piecewise one as illustrated in Tab. 6 (#5 v.s. #6). The underlying reason is that the two functions are very similar when the sequence length is short. The piecewise function is effective especially when the sequence size is much larger than the number of buckets. Object detection uses a much higher resolution input compared to classification, leading to a much longer input sequence. We therefore conjecture that when the input sequence is long, the piecewise function should be used since it is able to distribute different attentions to the positions with relative large distance, while the clip function assigns the same encoding when the relative distance is larger than ?.</p><p>Number of buckets. The number of buckets largely affects model parameters, computational complexities and performance. In order to find a balance, we explore the influence of varying the number of buckets for the contextual Product method. <ref type="figure">Fig. 3</ref> shows the change of top-1 accuracy along with the number of buckets. The accuracy increase   <ref type="table">Table 4</ref>: Component-wise analysis on ImageNet <ref type="bibr" target="#b3">[4]</ref>. We add contextual product shared-head relative position encodings into DeiT-S <ref type="bibr" target="#b22">[22]</ref>. The number of buckets is 50. Abs Pos. represents the absolute position encoding. p Q ij , p K ij and p V ij present relative position encodings on queries, keys and values. from 79.9 to 80.9 before 50 buckets. After that, there is no significant improvement. It shows that the number of buckets 50 is a good balance between the computational cost and the accuracy for 14 ? 14 feature map in DeiT-S <ref type="bibr" target="#b22">[22]</ref>.</p><p>Component-wise analysis. We perform a componentwise analysis to study the effects of different position en-  codings for vision transformer models. We select DeiT-S model <ref type="bibr" target="#b22">[22]</ref> as the baseline, and only change the position encoding methods. The learnable absolute position encoding is used in the original model. The relative position encodings are computed by contextual Product method with 50 buckets. The conclusions we got from Tab. 4 are as follows: 1) Removing absolute position encoding from original DeiT-S will cause that the Top-1 accuracy drops from 79.9 to 77.6 (#1 v.s. #2). 2) The models with only relative position encoding surpass the one with only absolute position encoding (#3-5 v.s. #1). It shows that relative position encoding works well as the absolute one. 3) When equipped with relative position encoding, the absolute one does not bring any gains (#3-5 v.s. #8-10). We suppose that the local information is more important than the global one in classification task. 4) The relative position encoding on queries or keys brings more gain than that on values (#3,4 v.s. #5). 5) The combination of the encodings on queries, keys and values brings further improvements (#6,7,11,12 v.s. others).</p><p>Complexity Analysis. We evaluate the computational cost of our proposed methods with respect to different input resolutions. The baseline model is DeiT-S <ref type="bibr" target="#b22">[22]</ref> with only absolute position encoding. We adopt contextual product shared-head relative position encoding to the baseline with 50 buckets. <ref type="figure">Fig. 4</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison on Image Classification</head><p>We compare our proposed methods with the state-ofthe-art methods on image classification tasks. We select DeiT <ref type="bibr" target="#b22">[22]</ref> as the baseline. We adopt contextual Product shared-head method with a buckets number of 50. As shown in Tab. 5, our method brings improvement on all three DeiT models. In particular, we improve the DeiT-Ti/DeiT-S/DeiT-B models by 1.5%/1.0%/0.6% respectively, through adding relative position encoding only on keys. We show that the models could be further improved by adding the proposed relative position on both queries and values. When compared with other methods, ours achieve superior performance with less parameters and MACs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison on Object Detection</head><p>To verify the generality of our method, we further evaluate it on COCO 2017 detection dataset <ref type="bibr" target="#b11">[12]</ref>. We use the transformer-based detection model DETR <ref type="bibr" target="#b0">[1]</ref> as our baseline. We follow the same training and testing settings (including hyperparameters) as DETR <ref type="bibr" target="#b0">[1]</ref>, except injecting relative position encoding into all self-attention modules in the encoder. As shown in Tab. 6 (#1,6 and #8,9), our method consistently improve the performance of DETR by 1.3mAP and 1.7mAP under 150 and 300 training epochs.</p><p>In addition, we conduct ablation studies analyzing that the effects of position encoding on object detection task. Comparing #1, #2 and #4 in Tab. 6, we give the conclusion that position encoding is crucial for DETR. We also show that absolute position embedding is better than relative position embedding in DETR, which is contrast to the observation in classification. We conjecture that DETR needs the prior of absolute position encoding to locate objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization</head><p>We explore the underlying reason of relative position encoding in this subsection. We visualize the extra weights b ij (defined in Eq. (13)) added into the attention by relative position encoding for different positions in each block. From <ref type="figure" target="#fig_5">Fig. 5</ref>, we could see that relative position encoding makes the current patch focus more on its neighboring patches in block 0. However, when it turns to higher block, this phenomenon disappears. We conjecture this is because after passing through multiple layers, the model has already captured enough local information. The shallow layers in transformer are also global attentions, which pay attention to the whole image (consisting of small patches). It is different from CNN models in which shallow layers only capture local information. In theory, without RPEs (or other additional operations such as local windows). transformer does not explicitly capture locality. RPEs inject Conv-like inductive bias (including locality) into transformer, improving the model capability of capturing local patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Transformer. Transformer was originally introduced by Vaswani et al. <ref type="bibr" target="#b23">[23]</ref> for natural language processing, and recently extended to computer vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b0">1]</ref>. In this work we study vision transformers in image classification and object detection tasks, and select DeiT <ref type="bibr" target="#b22">[22]</ref> and DETR <ref type="bibr" target="#b0">[1]</ref> as our baseline models. In ViT <ref type="bibr" target="#b5">[6]</ref> and DeiT <ref type="bibr" target="#b22">[22]</ref> models, an image is split into multiple fixed-size patches. The embedded features of patches are added with absolute position encoding to fed in a standard transformer encoder. An extra trainable classification token is added into the sequence for classification. <ref type="figure" target="#fig_1">In DETR [1]</ref>, a CNN backbone is used for feature extraction first. It outputs a feature map downsampled 32?. Then it is flatten and fed to a transformer. The transformer outputs a certain number of bounding boxes. A learnable or sinusoid absolute position encoding is added in both transformer encoder and decoder.</p><p>Relative Position Encoding. Relative position encoding is proposed firstly by Shaw et al. <ref type="bibr" target="#b18">[18]</ref>, where relative position encodings are added into keys and values. Dai et al. <ref type="bibr" target="#b2">[3]</ref> proposed relative position encoding with the prior of the sinusoid matrix and more learnable parameters. Huang et al. <ref type="bibr" target="#b10">[11]</ref> proposed several 1D encoding variants. The effectiveness of relative position encoding has been verified in natural language processing. There are also some works utilizing relative position encoding on 2D visual tasks. Ramachandran et al. <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b19">19]</ref> proposed 2D relative position encoding that computes and concatenates separate encodings of each dimension. Chu et al. <ref type="bibr" target="#b1">[2]</ref> proposed position encoding generator, inserted between encoders. However, the efficacy of relative position encoding in visual transformer is still unclear, which is discussed and addressed in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Remarks</head><p>In this paper, we review existing relative position encoding methods, and propose four methods dedicated to visual transformers. The abundant experiments show that our methods bring a clear improvement on both classification and detection tasks with negligible extra complexity. Our method could be easily plugged into the self-attention modules in vision models. In addition, we give comparison of different methods and analysis on relative position encoding with following conclusions. 1) Relative position encoding can be shared among different heads for parametersaving. It is able to achieve comparable performance with the non-shared one in contextual mode. 2) Relative position encoding can replace absolute one in image classification task. However, absolute position encoding is necessary for object detection task, which needs to predict locations of objects. 3) Relative position encoding should consider the positional directivity, which is important to structured 2D images. 4) Relative position encoding forces the shallow layers in transformers to pay more attention to local patches.</p><p>In future work, we plan to extend our method to other attention-based models and scenarios, such as highresolution input tasks like semantic segmentation <ref type="bibr" target="#b30">[30]</ref>, and non-pixel input tasks like point cloud classification <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rethinking and Improving Relative Position Encoding for Vision Transformer --Supplementary Material --</head><p>This supplementary material presents additional details of Section 3.2, 4.2, 4.3 and 4.4. Besides, two extra experiments are added to demonstrate the effectiveness and generality of the proposed iRPE. We also provide comparisons on the inference time.</p><p>? Visualization of 2D relative position. To provide an intuitive understanding, we visualize the proposed 2D relative position in Section 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Visualization of 2D Relative Position</head><p>We visualize the proposed four relative position methods, i.e., Euclidean, Quantization, Cross and Product, and present their difference. In DeiT <ref type="bibr" target="#b22">[22]</ref>, an image is split into 14 ? 14 non-overlapping patches, so the number of tokens is 14 ? 14 (except for the classification token). Therefore, in theory, each token has 14 ? 14 relative positions. For visualization, we select the top-left position (0, 0) and the center position <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b6">7)</ref> as the reference positions (presented by a red star in the following figures), and then compute the relative offsets ?x = x i ? x j and ?y = y i ? y j between the reference position and the remaining 14 ? 14 ? 1 posi-  <ref type="formula" target="#formula_1">(25)</ref>, where i is the reference position and j is one of the 14 ? 14 positions. Notice that r ij is either a learnable scalar in bias mode or a vector in contextual mode. Multiple r ij may share an identical bucket, which is presented by the same color in <ref type="figure" target="#fig_6">Fig. 6 -9</ref>. Different bucket is presented by different color.</p><p>Euclidean method. <ref type="figure" target="#fig_6">Fig. 6</ref> shows Euclidean method. It is an undirected method, since the relative position encodings only depend on relative Euclidean distance. For example, in <ref type="figure" target="#fig_6">Fig. 6b</ref> since the relative positions (?1, 0) and (1, 0) have the same relative Euclidean distance of 1, they are mapped into the same bucket (the grids with orange color).</p><p>Quantization method. <ref type="figure" target="#fig_7">Fig. 7</ref> presents Quantization method, another undirected method. It is an improved version of Euclidean method, and addresses the problem that the close two neighbors with different relative distances might be mapped into the same bucket (e.g., he relative position (1, 0) and (1, 1) are both mapped into the same bucket in Euclidean method). Besides, the number of buckets in Quantization method is larger than that in Euclidean method. The reason is that Quantization method quantize Euclidean distance from a set of real numbers {0, 1, 1.41, 2, 2.24, ...} to a set of integers {0, 1, 2, 3, 4, ...}, increasing the number of buckets for adjacent positions.</p><p>Cross method. <ref type="figure" target="#fig_8">Fig. 8</ref> shows Cross method. It is a directed method, in which the relative position encoding depends on relative distances and relative directions simultaneously. It computes the encodings on horizontal and verti-  cal directions separately, then summarizes them. The same offsets along x-axis (or y-axis) direction share the same horizontal (or vertical) encoding. For example, the two relative positions (?1, 0) and (1, 0) share the same encoding on horizontal in <ref type="figure" target="#fig_8">Fig. 8c</ref>, but not on vertical in <ref type="figure" target="#fig_8">Fig. 8d</ref>. Product method. <ref type="figure" target="#fig_9">Fig. 9</ref> shows Product method, which is also a directed method. Unlike Cross method, Product method does not share the same encoding even if the offsets are the same along x-axis or y-axis direction. For example, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Weight Initialization</head><p>The relative position weight r ij in Eq. <ref type="bibr" target="#b13">(14)</ref> and Eq. <ref type="formula" target="#formula_0">(15)</ref> is initialized with zero. We found that there is no difference between zero and normal-distribution initialization. Besides, we do not impose weight decay on the weight of relative position encodings, because its effects on the final performance is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Computation Complexity</head><p>As shown in Tab. 2 (in the main manuscript), the computational costs MACs of shared and unshared relative position encodings across attention heads are the same. Here, we provide the detailed explanation. Let h, n, d, k denote the number of heads, the length of a sequence, the number of channels and the number of buckets, respectively. For bias mode, in Eq. <ref type="bibr" target="#b12">(13)</ref>, the broadcast addition on the dot-product attention (x i W Q )(x j W K ) T with the shape of h ? n ? n and the encoding b ij with the shape of n ? n in shared scheme or h ? n ? n in unshared scheme takes the computational cost of O(hn 2 ). For contextual mode, in Eq. <ref type="formula" target="#formula_1">(27)</ref>, the broadcast multiplication on the input embedding x i W with the shape h ? n ? d and the relative position weight p with the shape of d ? k in shared scheme or h ? d ? k in unshared scheme takes the computational cost of O(hndk). Due to the broadcast operations, the computational cost of shared and unshared schemes is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Injecting Previous RPE Methods into DeiT</head><p>In the Tab. 5, in order to compare with previous 1D relative position encoding methods, we utilize our Prod- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Training and Test Settings of DETR</head><p>We follow the same training protocol and hyperparameter configurations as the original DETR <ref type="bibr" target="#b0">[1]</ref>. The backbone model of DETR <ref type="bibr" target="#b0">[1]</ref> is ResNet-50 <ref type="bibr" target="#b9">[10]</ref>, pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>, and the BatchNorm layers are frozen during training. All transformer blocks are initialized with Xavier initialization <ref type="bibr" target="#b7">[8]</ref>. The image is cropped such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333. When training, random horizontal flipping and random cropping are utilized. The initial learning rates of transformer and backbone are 10 ?4 and 10 ?5 , respectively. Learning rates are divided by 10 in the last 50 epochs in 150 epochs schedule, and the last 100 epochs in 300 epochs schedule. The optimizer is AdamW <ref type="bibr" target="#b13">[14]</ref> with weight decay of 10 ?4 and a mini-batch size of 16. The number of queries is 100. We train the models for 150 epochs and 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">The Effectiveness on Other Vision Transformers</head><p>We further verify the effectiveness of the proposed iRPE on the recent Swin transformer <ref type="bibr" target="#b12">[13]</ref>. Specifically, the original Swin-T model without RPE obtains a top-1 accuracy of 80.5% (Tab. 4 in Swin transformer <ref type="bibr" target="#b12">[13]</ref>), while using RPE bias mode gets +0.8% improvements. Our contextual RPE on QKV can further improve Swin-T to 81.9% on Im-ageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Transfer Learning on Fine-grained Datasets</head><p>We finetune the pretrained models on Stanford Cars and CUB200 2011 datasets using the resolution 224x224 and 300 epochs. DeiT-B <ref type="bibr" target="#b22">[22]</ref> with iRPE on keys obtains a top-1 accuracy of 93.4% and 84.9% on the two datasets respectively, outperforming the original DeiT-B (92.1% and 83.4%) by 1.3% and 1.5% points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Inference Performance</head><p>The inference runtime and memory cost are reported in <ref type="figure" target="#fig_1">Fig. 10</ref>, tested on Nvidia V100 GPU with a batch size of 128. We can see that our iRPE on keys is more effective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of self-attention modules with 2D relative position encoding on keys. The blue parts are newly added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>function y = g(x) the clipping function y = h(x) The comparison between the piecewise function g(x) and the clip function h(x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Method Is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Ablation for the number of buckets in contextual product model with shared relative position encodings on ImageNet [The extra computational cost of relative position encoding with different implementation in different resolutions. The baseline model is DeiT-S [22]. The number of buckets is 50. MACs means multiply-accumulate opera-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of relative position encoding (RPE) in contextual product method. We show the extra weights added to the attention by relative position encoding for different position. (a), (b) display the extra weights on attention for 5 ? 5 reference patches uniformly sampled from 14 ? 14 patches in block 0 and 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of Euclidean method. The red star presents the reference position. Different color means different bucket. The relative positions with the same color share the same encoding. tions. Let (?x, ?y) denote a 2D relative position. We plot the map of the relative encoding r ij , defined in Eq. (19), Eq. (21), Eq. (22) and Eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of Quantization method. The red star presents the reference position. Different color means different bucket. The relative positions with the same color share the same encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of Cross method. The red star presents the reference position. Different color means different bucket. The relative positions with the same color share the same encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of Product method. The red star presents the reference position. Different color means different bucket. The relative positions with the same color share the same encoding.inFig. 9b, the two relative positions (?1, 0) and (1, 0) have independent encodings. Moreover, it is more efficient than Cross method, since there is no extra addition operation in Eq.<ref type="bibr" target="#b22">(22)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>The extra cost brought by RPEs. The reference model is DeiT-S [22] without RPE, taking 1,096 images/s and 8,930 Mb memory. uct method (defined in Sec. 3.2 in the main manuscript) to adapt 1D encoding methods for 2D images. We replace the piecewise function g(x) with the clip function h(x), which is matched with previous methods. The encoding weight is shared across attention heads. DeiT-S(Shaw's), DeiT-S(Trans.-XL's), DeiT-S(Huang's) are DeiT-S [22] models with Shaw's relative position encoding<ref type="bibr" target="#b18">[18]</ref>, relative position encoding in Transformer-XL<ref type="bibr" target="#b2">[3]</ref> and Huang's relative position encoding<ref type="bibr" target="#b10">[11]</ref>, respectively. Besides, the 2D relative position encoding in SASA<ref type="bibr" target="#b17">[17]</ref> is equipped on DeiT-S<ref type="bibr" target="#b22">[22]</ref> directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation of shared and unshared relative position encoding across attention heads. The experiments are conducted over DeiT-S<ref type="bibr" target="#b22">[22]</ref> on ImageNet<ref type="bibr" target="#b3">[4]</ref> with 50 buckets. The models are trained and evaluated by three times.</figDesc><table><row><cell>Function</cell><cell>Mode</cell><cell cols="2">Top-1 Acc(%) Top-5 Acc(%)</cell></row><row><cell>clip</cell><cell>bias contextual</cell><cell>80.1 80.9</cell><cell>94.9 95.5</cell></row><row><cell>piecewise</cell><cell>bias contextual</cell><cell>80.0 80.9</cell><cell>95.0 95.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Ablation for clip function and piecewise function. The experiments are conducted over DeiT-S [22] model with product shared-head relative position encoding on Im- ageNet [4]. The number of buckets is 50.needs different relative position encoding (RPE for short) to capture different information. In contextual mode, each head can compute its own RPE by the Eq. (15) while in bias mode the shared RPE forces all heads to pay the same atten- tion on patches. For parameter-saving, we adopt the share scheme in our final methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison on ImageNet<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>AP 75 AP S AP M AP L</figDesc><table><row><cell cols="7"># AP 50 1 [1] sinusoid Abs Pos. Rel Pos. #buckets epoch AP none -150 39.5 60.3</cell><cell>41.4</cell><cell>17.5</cell><cell>43.0</cell><cell>59.1</cell></row><row><cell>2</cell><cell>none</cell><cell>none</cell><cell>-</cell><cell>150</cell><cell>30.4(-9.1)</cell><cell>52.5</cell><cell>30.2</cell><cell>9.4</cell><cell>31.2</cell><cell>50.5</cell></row><row><cell>3</cell><cell>sinusoid</cell><cell>bias</cell><cell>9 ? 9</cell><cell>150</cell><cell cols="2">40.6(+1.1) 61.2</cell><cell>42.8</cell><cell>19.0</cell><cell>43.9</cell><cell>60.2</cell></row><row><cell>4</cell><cell>none</cell><cell>contextual</cell><cell>9 ? 9</cell><cell>150</cell><cell>38.7(-0.8)</cell><cell>60.1</cell><cell>40.4</cell><cell>18.2</cell><cell>41.8</cell><cell>56.7</cell></row><row><cell>5</cell><cell>sinusoid</cell><cell>ctx clip</cell><cell>9 ? 9</cell><cell>150</cell><cell cols="2">40.4(+0.9) 60.9</cell><cell>42.4</cell><cell>19.1</cell><cell>43.7</cell><cell>59.8</cell></row><row><cell>6</cell><cell cols="2">sinusoid contextual</cell><cell>9 ? 9</cell><cell>150</cell><cell cols="2">40.8(+1.3) 61.5</cell><cell>42.5</cell><cell>18.5</cell><cell>44.4</cell><cell>60.5</cell></row><row><cell>7</cell><cell cols="3">sinusoid contextual 15 ? 15</cell><cell>150</cell><cell cols="2">40.8(+1.3) 61.7</cell><cell>42.6</cell><cell>18.5</cell><cell>44.2</cell><cell>61.2</cell></row><row><cell cols="2">8 [1] sinusoid</cell><cell>none</cell><cell>-</cell><cell>300</cell><cell>40.6</cell><cell>61.6</cell><cell>-</cell><cell>19.9</cell><cell>44.3</cell><cell>60.2</cell></row><row><cell>9</cell><cell cols="2">sinusoid contextual</cell><cell>9 ? 9</cell><cell>300</cell><cell cols="2">42.3(+1.7) 62.8</cell><cell>44.3</cell><cell>20.7</cell><cell>46.2</cell><cell>61.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">shows our method takes at most 1% extra</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">computational cost with efficient implementation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Component-wise analysis on DETR<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Do we really need explicit position encodings for vision transformers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10882</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaime</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIS-TATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09688</idno>
		<title level="m">Pct: Point cloud transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improve transformer models with better relative position embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP, 2020. 3, 7</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selfattention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv&amp;apos;e J&amp;apos;egou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Standalone axial-attention for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
