<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-aided Articulated Motion Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Skeleton-aided Articulated Motion Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Motion Generation</term>
					<term>Skeleton Aid</term>
					<term>Video Anal- ysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work makes the first attempt to generate articulated human motion sequence from a single image. On one hand, we utilize paired inputs including human skeleton information as motion embedding and a single human image as appearance reference, to generate novel motion frames based on the conditional GAN infrastructure. On the other hand, a triplet loss is employed to pursue appearance smoothness between consecutive frames. As the proposed framework is capable of jointly exploiting the image appearance space and articulated/kinematic motion space, it generates realistic articulated motion sequence, in contrast to most previous video generation methods which yield blurred motion effects. We test our model on two human action datasets including KTH and Human3.6M, and the proposed framework generates very promising results on both datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Object motion prediction and generation in the videos is a key factor in video analysis, and it has potential application to smart surveillance, human-computer interaction and other applications. Generative models such as GAN <ref type="bibr" target="#b0">[1]</ref> have achieved great success on image generation, but how to generate videos with motion dynamics is rarely explored. Although recent developments of convolutional neural network (CNN) and recurrent neural network (RNN/LSTM <ref type="bibr" target="#b1">[2]</ref>) have made great success on action classification task <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[4]</ref>, <ref type="bibr" target="#b6">[5]</ref>, motion generation is still challenging because it often involves high-dimensional data with complex temporal dynamics. In particular, previous video generation methods <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b10">[9]</ref> are only good at simulating rigid movement of objects. In the case of articulated movement (e.g., human motion), these methods mostly yield blurred effects for various body parts.</p><p>Existing video generation methods mainly focus on two tasks. The first one is video prediction <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b7">[6]</ref>, <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref>, <ref type="bibr" target="#b14">[13]</ref>, i.e., the models need to learn the motion patterns from a sequence of observed frames and to predict/generate the next frames. These methods are usually based on a recurrent structure (RNN of LSTM), despite of the good ability of the RNN/LSTM to model sequential data, they usually achieve good results only for short-term predictions where the videos are simple and quiet predictable. While the long-term prediction results usually suffer from low image quality, such as blur and object deformation. The second type of methods aim to directly generate a sequence of frames based on a single input <ref type="bibr" target="#b15">[14]</ref> or only the scene types <ref type="bibr" target="#b9">[8]</ref>. This task is more challenging as the motion patterns can no longer be observed during test phase. These methods employ the GAN model to generate the spatio-temporal cuboids or employ the Variational Autoencoders <ref type="bibr" target="#b16">[15]</ref> to forecast the dense trajectory of pixels in the scene. However, the objects in the scene can move  <ref type="figure">Fig. 1</ref>. Overview of the proposed framework. Previous video generation methods do not explicitly model the articulated structure of the foreground motion objects, the generated videos usually suffer from great deformation (see the top part). The bottom part gives an illustration of our approach, and we utilize the appearance information as well as the skeleton information to generate motion sequence. arbitrarily if no geometric constrains are given to foreground objects, which will result in great deformation on the generated objects. Furthermore, we observe a shared limitation over these two type of methods, i.e., the articulated structures of the foreground motion objects (i.e., human) are not well modeled in the generation model. As previous generative methods only take the whole appearance as input, it will be difficult for the model to learn the structural relationship among the articulations/parts if given no supervision, thus resulting in great deformation during motion. Limited by this constraint, the quality of the generated videos are far from satisfaction.</p><p>In this work, we propose to use skeleton information to help generate articulated motion, which is motivated by the following observations. On the one hand, the articulated motion is usually under strong structure/geometric constrain, which can be well represented by the skeletons. On the other hand, compared to images with high dimension, the skeleton (coordinates of body parts) serves as a very good low dimensional embedding for human motion. Therefore it could be used as underlying status parameters to generate flexible poses. Also, skeletons can be mapped to image oneby-one, this avoids the long-term prediction problem shared by previous methods. Moreover, recent development on human pose estimation techniques has made skeleton data easy to access, thus avoiding heavy human annotations.</p><p>Our problem is defined as follows. Suppose we have a sequence of skeletons and a single image of human appearance, the task is to generate a sequence of articulated motion images.</p><p>This task can be further decomposed into two sub-problems: 1) how to generate realistic articulated motion (i.e., instead of blobs); and 2) how to generate appearance, which is adapted to every generated image frame. On one hand, motion generation (i.e., generating different human pose) is addressed by a GANlike network. Recently, GAN has achieved great success in image generation, domain adaption and most importantly, image-to-image translation. The motion generation process can be naturally transformed into skeleton-to-image translation problem, which can be naturally handled by a conditional GAN model (i.e., in this work, we employ a GAN loss and an L1 loss to ensure smooth image-to-image translation). On the other hand, if given no appearance information (cloth color, bodily form), appearance of the generated image sequences cannot be controlled, i.e., the generated appearances might differ from image to image. This violates the rule that the appearance of an object should be consistent during the entire motion sequence. To address this issue, we choose to generate the motion sequence based on both skeleton sequence and an appearance image, which is realized with a specially designed generator. Furthermore, in order to ensure inter frame continuity, and we also employ a triplet loss which aims to penalize the generation loss if the adjacent frames have larger distance than the non-adjacent frames.</p><p>This work is among the very few works for complete video generation. To the best of our knowledge, this is the first work to employ skeleton information to help generate videos. Our key contribution is that the proposed method can generate images/videos with large scale geometry change, where previous methods achieve little success. We test our model on two human action datasets including KTH and Human3.6M, and the proposed framework generates very promising results on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Human Motion Analysis. In computer vision, human motion analysis is a broad concept which focuses on the understanding and applications of human motion patterns. It has been receiving increasing attention for a long time and has been applied to enormous applications, such as content-based video retrieval, visual surveillance, and mancomputer interfaces <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b19">[18]</ref>. Among the researches of human motion analysis, variant human body models (e.g., stick figure model <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, cardboard model <ref type="bibr" target="#b22">[21]</ref>, 3D volumetric model <ref type="bibr" target="#b23">[22]</ref>) play an important role. These models involve low-level processes on human body structures and cover the kinematic properties of the body, which build the foundation in solving different problems, including human motion tracking <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b26">[25]</ref>, action recognition <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref>, and pose estimation <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b33">[32]</ref>. Motivated by these successful applications based on well-defined human body models, in this paper, we pay attention to another more challenging task that attempts to generate consistent human motion sequences based on the correspondent skeleton and appearance information.</p><p>Image Generation. Early works for image generation usually make efforts on simple texture synthesis with handcrafted features <ref type="bibr" target="#b34">[33]</ref>. During the past few years, two generation models have been attracting more and more attention, i.e., the variational autoencoder (VAE) <ref type="bibr" target="#b16">[15]</ref> and the generative adversarial network (GAN) <ref type="bibr" target="#b0">[1]</ref>. VAE is a classical method which aims to model complicated distribution and it has been widely applied in various generative tasks. Gregor et al. <ref type="bibr" target="#b35">[34]</ref> propose a sequential generative model which extends the original VAE with recurrent neural networks and attention mechanism. Another interesting model is proposed by Yan et al. <ref type="bibr" target="#b36">[35]</ref>, they develop a layered generative model based on conditional VAE. GAN is also a popular generative model and many recent works are built on it. Some works improve the architecture of original GAN for better performances <ref type="bibr" target="#b37">[36]</ref>, <ref type="bibr" target="#b38">[37]</ref>, <ref type="bibr" target="#b39">[38]</ref>, <ref type="bibr" target="#b40">[39]</ref>. Conditional generative adversarial network (CGAN) <ref type="bibr" target="#b41">[40]</ref> gives extra information to the input as condition, and the output is constrained by the input conditions. CGAN has been further extended by <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b43">[42]</ref>, <ref type="bibr" target="#b44">[43]</ref> to solve the image-to-image translation problem, which gives inspiration of our model proposed in this paper. Other applications such as image super-resolution <ref type="bibr" target="#b45">[44]</ref>, image edition <ref type="bibr" target="#b46">[45]</ref> and unsupervised representation learning <ref type="bibr" target="#b47">[46]</ref> also show impressive results.</p><p>Video Generation. Our problem is closely related to video generation or prediction. Video texture based methods <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b50">[49]</ref> can generate periodic motion sequences if an input reference video is given. Lotter et al. <ref type="bibr" target="#b11">[10]</ref> propose a predictive neural network motivated by the concepts from neuroscience. Finn et al. <ref type="bibr" target="#b8">[7]</ref> develop an action-conditioned video prediction model which concentrates on pixel motion and Mathieu et al. <ref type="bibr" target="#b12">[11]</ref> introduce multi-scale architecture to reduce the deformation in the predictions. Instead of focusing on pixel level prediction, Van et al. <ref type="bibr" target="#b51">[50]</ref> attempt to predict the transformations between frames. Some works also utilize GAN or VAE in video generation. Vondrick et al. <ref type="bibr" target="#b9">[8]</ref> propose a GAN model which generates static background and dynamic foreground sequences separately. Xue et al. <ref type="bibr" target="#b52">[51]</ref> introduce conditional VAE and build a cross convolutional network which encodes image and motion information for generation. Although variant models are proposed, the results of these methods are usually limited by two issues: 1) the deformation of the foreground object is serious, 2) and the inter-frame consistency cannot be well maintained. These problems inspire us that in order to generate more realistic videos, strong motion constrains are needed during the generation process. Therefore, we employ skeleton information to guide our model for motion generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our problem is defined as follows. Given an image x containing the foreground person (the appearance reference image), we would like to generate a sequence of images Y = {y 1 , ..., y n } that share the same appearance, and the foreground objects should keep a specific motion pattern as well (e.g., walking, running). In other words, we would like to generate articulated motion from a single static image. This is challenging because a person could have infinite move patterns. The first step is to choose a specific move pattern for the sequence. As skeleton can well represent a person's motion, we employ a sequence of skeletons S = {s 1 , ..., s n } as prior knowledge for the motion. The remaining problem is to generate the output sequence based on the appearance image and skeleton sequence: {x, S} ? Y . Here, we propose a skeleton conditioned GAN to model this mapping. The second problem is to maintain appearance-smoothness between consecutive frames, we employ a triplet loss on the generator for this purpose. The details of the proposed method are given in the following of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Skeleton Conditioned GAN</head><p>Different from the previous image-to-image translation model, where only a single input image is mapped to the output, our generative model is conditioned on two inputs, i.e., the appearance reference image x and the skeleton image s. The value function of the Conditional GAN (CGAN) model is expressed as follows:</p><formula xml:id="formula_0">L c (G, D) = E x,s,y?p data (x,s,y) [log D(x, s, y)] + E x,s?p data (x,s),z?pz(z) [log(1 ? D(x, s, G(x, s, z)))],<label>(1)</label></formula><p>where the generator G tries to produce a new frame, and a discriminator D tries to distinguish real triplets (x, s, y) and synthesized triplets (x, s, G(x, s, z)). The architectures of the generator and discriminator model are illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. Previous methods <ref type="bibr" target="#b42">[41]</ref>, <ref type="bibr" target="#b12">[11]</ref> observe that using the CGAN loss alone will give sharp results with artifacts, and it's beneficial adding a contractive loss such as L1 loss. Although this may cause blur effect, mixing these two losses will generate overall better results. The L1 loss can be expressed as:</p><formula xml:id="formula_1">L L1 (G) = E x,s?p data (x,s),z?pz(z) [ y ? G(x, s, z) 1 ]. (2)</formula><p>Notice that our goal is to generate continuous motions rather than individual images. Thus it is important to consider the  <ref type="figure">Fig. 3</ref>. Loss terms of the model. Despite of the GAN loss, we take two additional loss terms, i.e., the L1 loss to enhance the image-to-image translation quality, and the triplet loss to guarantee the continuity of the generated motion sequence. motion continuity and appearance smoothness of the adjacent frames. Although this can be achieved by training a perfect generator that precisely maps the input appearance image into a new pose specified by the input skeleton, the perfect generator cannot be achieved in practice because a single appearance image does not contain the complete information of the moving object. e.g., for an image of a person, some parts are inevitably occluded, which can't be generated perfectly through training. To address this issue, we propose to utilize a  triplet loss that motivates adjacent frames to be more similar than the far-away frames. First, we need to construct the triplet set T = {t a i , t p i , t n i } m i=1 from the generated samples: Y = {? 1 , ...,? n }, where? j = G(x, s j , z), 1 ? j ? n, and m is the number of triplets selected for training. The anchor of the triplet can be randomly chosen from the generated samples, say t a i =? j , the positive sample can select a sample adjacent to the anchor (e.g., t p i =? j+1 ), and the negative sample can choose a far-away sample (e.g., t n i =? j+5 ). We would like that the distance between anchor and positive is smaller than that of anchor and negative, thus the loss function can be expressed as:</p><formula xml:id="formula_2">L tri (G) = m i=1 [ t a i ? t p i 2 2 ? t a i ? t n i 2 2 + ?] + .<label>(3)</label></formula><p>In our experiments, we also tried to replace the L2 norm with L1 norm in the triplet loss, but we don't observe performance gain. The loss terms are illustrated in <ref type="figure">Figure 3</ref>.</p><p>The overall objective function is:</p><formula xml:id="formula_3">L(G, D) = L c (G, D) + ?L L1 (G) + ?L tri (G),<label>(4)</label></formula><p>where ? and ? are the weights for different loss terms. We aim to solve</p><formula xml:id="formula_4">G * = arg min G max D L(G, D).<label>(5)</label></formula><p>B. Generator architecture</p><p>The basic structure of our network is built upon <ref type="bibr" target="#b42">[41]</ref>, in which the generator usually follows the encoder-decoder structure. Specifically, the U-net structure <ref type="bibr" target="#b53">[52]</ref> has proven to be more effective for image-to-image translation tasks, because it adds skip connections between encoder and decoder. Such a structure enables the information to share between the inputs and outputs, and thus achieving success for tasks such as image colorization, image segmentation, etc. We also adopt this structure for the generator. In our case, the generator needs to translate two inputs (i.e., the appearance reference image x and the skeleton image s) into a single output y. Therefore, the encoder takes two inputs simultaneously and shuttles the information to the decoder.</p><p>Stacked Generator. There are multiple options to design the structure of the encoder. The most straightforward way is to FConv-(512,K4?4,S2), BN, Dropout-(0.5), ReLU 7</p><p>FConv-(256,K4?4,S2), BN, Dropout-(0.5), ReLU 8</p><p>FConv-(128,K4?4,S2), BN, Dropout-(0.5), ReLU stack the two input images as a single input (i.e., i = [x, s]), thus the standard encoding network can be applied directly. We denote this structure as stacked generator 1. We further observe that the skeleton images have completely black ground which is not informative at all, and all the motion information is contained in the pose of the skeleton. Therefore, we can directly draw the skeleton on the appearance image instead. The resulted image sequence can be viewed as a skeleton moving on the appearance image. In this case, the inputs for the encoder are standard images, and have no distinction with the traditional image-to-image task. We denote this structure as stacked generator 2. The two stacked structures are illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>(a). The stacked encoders provide a simple solution for simultaneously encoding two input images by stacking the two input as an ensemble. However, the two inputs usually contain different information, which needs to be modeled separately. To this end, we design a second structure.</p><p>Siamese Generator. In particular, each input image can be modeled by an encoding network, and the features are concatenated at the bottleneck layer for the decoder. In this way, the different information of both the appearance image and the skeleton image can be well modeled respectively. We denote this structure as the Siamese generator, because it has Siamese structure. See <ref type="figure" target="#fig_4">Figure 4(b)</ref> for an illustration.</p><p>The detailed structure of the generator is illustrated in <ref type="table" target="#tab_0">Table I</ref>, where the encoder is composed of convolution (Conv), batch normalization (BN) layers and leaky rectified linear unit (lReLU) layers, and the decoder is composed of fractional length convolutional (FConv) layers, BN layer, Dropout layer and the ReLU layers. For gray-scale inputs, we replicate their channel 3 times so that the network doesn't distinguish RGB inputs and gray-scale inputs. We resize all the input images into a fix size, i.e., 256?256?3. Therefore, the input size for the stacked generators is 256?256?6 because they stack two inputs. And for the Siamese generator, the number of channel is 3 all the inputs. Also notice that each of the Siamese structure has exactly the same structure as the encoder, the output of each encoder is concatenated as inputs (60?60?2) for the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discriminator architecture</head><p>The discriminator needs to be able to classify the realistic triplets (x, s, y) from the synthesized <ref type="figure">triplets (x, s, G(x, s, z)</ref>), thus the inputs for the discriminator network are three images. Similar to the encoding network, we can also stack all the three inputs as a 9-dimensional input, or we can extract features from each inputs, and then combine them to make a decision. However, we observe little difference using these two structures during our experiments, therefore we only report the results of the stacked structure. The discriminator structure is illustrated in <ref type="table" target="#tab_0">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Learning and Implementation</head><p>Our implementation is based on a modified version of image-to-image translation network <ref type="bibr" target="#b42">[41]</ref> on Tensorflow <ref type="bibr" target="#b54">[53]</ref>. We report results for several architectures. For all the models, we alternatively train the discriminator and then the generator. We train the generator and discriminator with stochastic gradient descent, with a fixed learning rate as 0.0002 and the Adam optimizer. All the models are trained for 30 epochs. We find that small batch size leads to more appealing results. So the batch size is set as 10 for all the experiments to balance the generation quality and training time. All the videos are rescaled into range [?1, 1] as normalization. The random noise z is not explicitly sampled from Gaussian distribution, it appears only in the form of dropout, which is in consistent with <ref type="bibr" target="#b42">[41]</ref>. The skeletons are extracted using the real-time human pose estimator <ref type="bibr" target="#b55">[54]</ref>. And we also use the same estimator to detect the generated human motion sequence to compare with the ground truth pose estimation. Because we use three kinds of loss function to train the network, we try different weights of each term in training phase to get optimal results. More detailed analysis on loss function is in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present extensive experimental evaluations and in-depth analysis of the proposed method. The evaluations are performed on the following two human action datasets:</p><p>KTH dataset. This dataset contains several types of human actions in the outdoor environment, and all the videos are gray scale. We experiment on three types of actions, i.e., walking, running and hand waving. And we choose these actions because their motion parts are different, walking mainly involve the movement of legs, hand waving involve the movement of arms, and running involves both the movement of arms and legs. Each type of action contains 100 videos, and we divide the dataset into training set containing 80 videos and test set containing 20 videos.</p><p>Human3.6M dataset. This dataset was collected in an indoor environment, there are 4 cameras working simultaneously, i.e., we have access to 4 views of each action. We use <ref type="figure">Fig. 5</ref>. Examples generated by different generator structure. The first row contains the ground truth motion sequence, the image with red bounding box is the appearance reference image.</p><p>the walking scenario in this dataset. The dataset also contains foreground segmentations, therefore we experiment on both videos with and without background.</p><p>As there exists no standard evaluation protocol for image and video generation tasks, in order to evaluate the quality of the generated videos, we report both qualitative and quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structure Analysis</head><p>We propose the stacked structure and the Siamese structure for the generator. We evaluate the performance of the two structure in this part, and the results of different generator structures are compared in <ref type="figure">Figure 5</ref>. The image with red bounding box is the appearance reference image. Benefitting from the U-Net structure, all the generators achieve to generate realistic motion patterns. The two stacked generators generate similar results. However, they suffer from a major issue, i.e., the generated sequences do not preserve the appearance of the reference image any more. For example, the subject in the reference image is in white T-shirt and black pants. The results generated by the stacked generators are in dark cloth. In contrast, the results generated by the Siamese generator are more similar to the ground truth. This is mainly because that the stacked structure does not distinct the pose and appearance information, which is encoded by the same network. However, the appearance images and the skeleton images are under different distributions, encoding them with two separate networks will better fit the two different distributions. Therefore, the Siamese generator achieves better results. It not only encodes the motion pattern, but also successfully encodes the appearance information, thus the generated sequence shares high appearance similarity with the reference image. In the following of this paper, we report the results of the Siamese generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion Generation</head><p>Some generated sequences on KTH dataset are visualized in <ref type="figure">Figure 6</ref>. For each example, the first row is the ground-truth sequence, and the second row represents the detected skeleton sequence. The generated results are given in the third row. We re-run the skeleton detector on the generated sequence, and the resulted skeleton sequence is shown in the last row. We have three observations: 1) the foreground subjects are naturally generated in the scene. Different from <ref type="bibr" target="#b9">[8]</ref>, which employs a two stream model to separately generate the foreground and background, our model is a unified model and does not distinct foreground from backgrounds. Moreover, for each of the generated image, the boundary of the foreground subject is sharp and looks natural in the scene. Qualitatively, this is better than the results in <ref type="bibr" target="#b9">[8]</ref>, where the people in the scene are often blobs. 2) The model successfully generates motions patterns with high quality. We find that the generated motion sequences are highly recognizable for humans. Moreover, the skeletons extracted from the generated sequence are very similar from ground-truth skeletons. We make two remarks here. On one hand, this demonstrates that the generated pose is close to the objective. On the other hand, it in turn demonstrates that our model has successfully generated humans that can be recognized by the pose detector.</p><p>3) The identity of the appearance reference image is well preserved. For example, in <ref type="figure">Figure 6</ref>(a) and <ref type="figure">Figure 6</ref>(c), the appearance reference image of the subjects are with white and black clothes respectively. We can observe that the generated sequence share similar appearances with their reference images, and the appearance of the person is consistent in the generated sequence.</p><p>The generation results on Human3.6M dataset are visualized in <ref type="figure">Figure 7</ref>. Note that different from KTH, Human3.6M dataset contains color videos which are much more difficult to encode the appearance into latent space. We can observe that the appearance of generated sequence is close to the ground truth. The results demonstrate that the generator effectively transforms the color space into latent space, and finds good representation for the relationship between different body parts and their corresponding color. And we also present the results on Human3.6M without background in <ref type="figure">Figure 7</ref> <ref type="bibr">(b)</ref>. We can observe that the network indeed transforms the appearance from the target image to generated sequence, without referring to the background information.</p><p>Overall, the proposed method shows promising results in generating human motions. For more examples, please refer to our supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Component Analysis</head><p>In this part, we study how the loss terms in <ref type="bibr">Equation 4</ref> influence the generation results. The GAN loss and the L1 loss have been analyzed in <ref type="bibr" target="#b42">[41]</ref> for image-to-image translation task, which demonstrates that using GAN loss alone will generate artifacts, and using L1 loss alone tends to generate the color averaged over the training set. The results in our experiments are illustrated in <ref type="figure">Figure 8</ref>. We can observe that the L1 loss and GAN loss have different effects for the generated videos. Specially, only using the GAN loss will result in severe artifacts in the video, some parts of the person are missing and color of the person keeps changing in the video. While only using L1 loss will degrade the generalization ability of the network. The third row of <ref type="figure">Figure 8</ref> shows these results. No matter how we change the reference image, all the generated sequences have almost the same appearance, i.e., the average color of all the training samples. These adversary effects in these two kinds of losses are critical for training the generator. Combing L1 loss and GAN loss will generate better results, but the color is not consistent along the frames. Furthermore, introducing the triplet loss described in Section3.1 will stabilize the performance of generator. As shown in bottom two rows of <ref type="figure">Figure 8</ref>, the generated sequence in bottom has consistent clothing color compared to the results which lack the help of triplet loss. We also perform quantitative evaluations over the loss terms. We train an action classification framework <ref type="bibr" target="#b6">[5]</ref> on the KTH dataset, and then use it to classify the generated sequences. If the generated sequences are classified into the right action type, it demonstrates that the generated sequences can be recognized by the off-the-shelf classifier. The results are shown in <ref type="table" target="#tab_0">Table III</ref>. As KTH dataset is a relatively easy dataset for action recognition, the trained classifiers achieve very good performance (?100%) on the ground-truth test sequences. We observe that only using L1 or GAN loss leads to great performance drop, and combining the two loss terms significantly improves the recognition accuracy. Further adding triplet loss does not bring in further improvements, because the motion patterns are already clear enough for the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we propose a skeleton-aided articulated motion generation method. In contrast to existing video generation methods, which usually lack geometric constraints for the foreground object, our method utilizes skeleton information as a guidance for the geometric change during the motion. Experimental results show that by giving an appearance reference image and the skeleton sequence, our model can produce highquality video sequences that not only preserve the appearance of the reference image, but also have clear motion pattern as the skeletons. The generated motion sequences are also recognizable by the off-the-shelf action recognition framework. (c) Hand waving. <ref type="figure">Fig. 6</ref>. Generated samples on KTH dataset. The first rows contain the ground truth motion sequences, and the images with red bounding boxes are the appearance reference images. The second rows are the ground truth reference skeletons. The third rows are the samples generated by our model, and the last rows are the detected skeletons of the generated sequences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of the generation and discrimination network. The inputs for the generator are the skeleton-appearance pairs (s, x) and generate the synthesized sequence? = {? 1 , ...,?n}. The discriminator D tries to distinguish real triplets (x, s, y) and synthesized triplets (x, s,?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Structures of the generator. For both the stacked generator and the Siamese generator, we take the U-Net structure for both generators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETAILED</head><label>I</label><figDesc>STRUCTURE OF THE GENERATOR.</figDesc><table><row><cell></cell><cell>Encoder</cell></row><row><cell>Layer</cell><cell>Input size: 256?256?6(3)</cell></row><row><cell>1</cell><cell>Conv-(64,K4?4,S2), lReLU-(0.2)</cell></row><row><cell>2</cell><cell>Conv-(128,K4?4,S2), BN, lReLU-(0.2)</cell></row><row><cell>3</cell><cell>Conv-(256,K4?4,S2), BN, lReLU-(0.2)</cell></row><row><cell>4-8</cell><cell>Conv-(512,K4?4,S2), BN, lReLU-(0.2)</cell></row><row><cell></cell><cell>Decoder</cell></row><row><cell>Layer</cell><cell>Input size: 60?60?1(2)</cell></row><row><cell>1</cell><cell>FConv-(512,K4?4,S2), BN, Dropout-(0.5), ReLU</cell></row><row><cell>2-5</cell><cell>FConv-(1024,K4?4,S2), BN, Dropout-(0.5), ReLU</cell></row><row><cell>6</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II DETAILED</head><label>II</label><figDesc>STRUCTURE OF THE DISCRIMINATOR.</figDesc><table><row><cell></cell><cell>Discriminator</cell></row><row><cell>Layer</cell><cell>Input size: 256?256?9</cell></row><row><cell>1</cell><cell>Conv-(64,K4?4,S2), lReLU-(0.2)</cell></row><row><cell>2</cell><cell>Conv-(128,K4?4,S2), BN, lReLU-(0.2)</cell></row><row><cell>3</cell><cell>Conv-(256,K4?4,S2), BN, lReLU-(0.2)</cell></row><row><cell>4-6</cell><cell>Conv-(512,K4?4,S2), BN, lReLU-(0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III RECOGNITION</head><label>III</label><figDesc>ACCURACIES (%) ON THE GENERATED SEQUENCES. GT DENOTES THE GROUND-TRUTH RESULTS. L1 DENOTES L1 LOSS, G DENOTES GAN LOSS, T DENOTES TRIPLET LOSS.</figDesc><table><row><cell>Action</cell><cell>GT</cell><cell>L1</cell><cell>G</cell><cell>L1+G</cell><cell>L1+G+T</cell></row><row><cell>Walking</cell><cell>100</cell><cell cols="2">83.1 43.1</cell><cell>93.8</cell><cell>93.8</cell></row><row><cell>Running</cell><cell>98.5</cell><cell cols="2">76.9 41.5</cell><cell>92.3</cell><cell>92.3</cell></row><row><cell>Hand waving</cell><cell>100</cell><cell cols="2">95.4 47.7</cell><cell>100</cell><cell>100</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Walking without background</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generation results on Human3.6M dataset. (a) Walking sequence with background. (b) The same sequence without background. Fig. 8. Examples generated with different loss terms</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="677" to="691" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno>abs/1605.08104</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1701.08435</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<idno>abs/1412.6604</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vision-based human motion analysis: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="18" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human motion analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quin</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonrigid and Articulated Motion Workshop</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="90" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Advances in view-invariant human motion analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghai</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cardboard people: A parameterized model of articulated image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on</title>
		<meeting>the Second International Conference on</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian reconstruction of 3d human motion from single-camera video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Nicholas R Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Leventon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="820" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards a system for the interpretation of moving light displays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rashid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page" from="574" to="581" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Markerless tracking of complex human motions from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="209" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model-based human body tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="552" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Viewpoint invariant exemplar-based 3d human tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng-Jon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">S</forename><surname>Micilotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="189" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human body model acquisition and tracking using voxel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Miki?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="223" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Real-time skeleton-tracking-based human action recognition using kinect data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><surname>Georgios Th Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="473" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust human action recognition via long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grushin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Monner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Reggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured time series analysis for human action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemei</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1427" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">At-tribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="776" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1609.03126</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Loss-sensitive generative adversarial networks on lipschitz densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<idno>abs/1701.06264</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
	</analytic>
	<monogr>
		<title level="j">Wasserstein GAN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CoRR</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno>abs/1411.1784</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1611.07004</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1703.10593</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeksoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsu</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<idno>abs/1609.04802</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural photo editing with introspective adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1609.07093</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Sch?dl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Panoramic video textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="821" to="827" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automated video looping with progressive dynamism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<idno>abs/1701.08435</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg ; Martin Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<idno>abs/1603.04467</idno>
		<editor>Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Vi?gas, Oriol Vinyals, Pete Warden,</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<pubPlace>Dan Man?, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<idno>abs/1611.08050</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
