<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Few-Shot Learning with Frozen Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
							<email>jmenick@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
							<email>cabi@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eslami</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Oriol</surname></persName>
							<email>vinyals@deepmind.com</email>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">Hill</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DeepMind University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Few-Shot Learning with Frozen Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Auto-regressive transformers have been shown to be very impressive models of natural language <ref type="bibr" target="#b39">[40]</ref>. Large-scale language transformers exhibit several surprising abilities beyond that of standard text generation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. Perhaps most notably, they are few-shot learners; they can learn to perform a new task from a few examples without any further gradient updates. Equipped with this ability, these models have been shown to rapidly adapt to new tasks and styles of generation via prompting (e.g. switching from formal to informal language) <ref type="bibr" target="#b3">[4]</ref>, to quickly retrieve relevant encyclopedic or general knowledge when primed with a relevant context (e.g. answering questions such as 'When did the French Revolution begin?') <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref> and to use new words in appropriate ways straight after being taught what those words mean (sometimes referred to as 'fast binding') <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Despite these impressive capabilities, such large scale language models are 'blind' to modalities other than text, preventing us from communicating visual tasks, questions or concepts to them. Indeed, philosophers and linguists have questioned whether an un-grounded language model can ever achieve true understanding of the language it processes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>. Here, we present Frozen, a method for giving a pre-trained language model access to visual information in a way that extends its few-shot learning capabilities to a multimodal setting, without changing its weights. Frozen consists of a neural network trained to encode images into the word embedding space of a large pre-trained language model such that the language model generates captions for those images. The weights of the language model are kept frozen, but gradients are back-propagated through it to train the image encoder from With one of these I can drive around a track, overtaking other cars and taking corners at speed</p><p>With one of these I can take off from a city and fly across the sky to somewhere on the other side of the world With one of these I can</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>break into a secure building, unlock the door and walk right in &lt;EOS&gt; <ref type="figure">Figure 1</ref>: Curated samples with about five seeds required to get past well-known language model failure modes of either repeating text for the prompt or emitting text that does not pertain to the image. These samples demonstrate the ability to generate open-ended outputs that adapt to both images and text, and to make use of facts that it has learned during language-only pre-training. scratch ( <ref type="figure">Figure 2</ref>). Although Frozen is trained on single image-text pairs, once trained it can respond effectively to ordered sets of multiple images and words. This allows users to e.g. 'prompt' it with several examples of new multimodal tasks before evaluating its performance, or to 'teach' it the name of a new visual category before immediately asking about that category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model</head><p>Text Embedder   <ref type="figure">Figure 2</ref>: Gradients through a frozen language model's self attention layers are used to train the vision encoder.</p><formula xml:id="formula_0">V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l T t j v 4 Y g j 7 Z c r b t W d g 6 w S L y c V y N H o l 7 9 6 g 5 i l E V f I J D W m 6 7 k J + h n V K J j k 0 1 I v N T y h b E y H v G u p o h E 3 f j a / d 0 r O r D I g Y a x t K S R z 9 f d E R i N j J l F g O y O K I 7 P s z c T / v G 6 K 4 b W f C Z W k y B V b L A p T S T A m s + f J Q G j O U E 4 s o U w L e y t h I 6 o p Q x t R y Y b g L b + 8 S l q 1 q n d Z r d 1 f V O o 3 e R x F O I F T O A c P r q A O d 9 C A J j C Q 8 A y v 8 O Y 8 O i / O u / O x a C 0 4 + c w x / I H z + Q M f 1 Z A I &lt; / l a t e x i t &gt; f ? &lt; l a</formula><formula xml:id="formula_1">V 4 u W k D D n q v d J X t x + z N O I K m a T G d D w 3 Q T + j G g W T f F r s p o Y n l I 3 o g H c s V T T i x s / m 9 0 7 J u V X 6 J I y 1 L Y V k r v 6 e y G h k z C Q K b G d E c W i W v Z n 4 n 9 d J M b z x M 6 G S F L l i i 0 V h K g n G Z P Y 8 6 Q v N G c q J J Z R p Y W 8 l b E g 1 Z W g j K t o Q v O W X V 0 m z W v G u K t W H y 3 L t N o + j A K d w B h f g w T X U 4 B 7 q 0 A A G E p 7 h F d 6 c J + f F e X c + F q 1 r T j 5 z A n / g f P 4 A b d + Q O w = = &lt; / l a t e x i t &gt; v</formula><p>By exploiting its pre-trained language model, Frozen exhibits strong zero-shot performance on multimdodal tasks that it was not trained on, such as visual question answering (VQA). More surprisingly, it gets better at these tasks after seeing a handful of examples "in-context" as in <ref type="bibr" target="#b3">[4]</ref>, and also performs above chance on tests of fast category learning such as miniImageNet <ref type="bibr" target="#b40">[41]</ref>. In each case, comparisons with 'blind' baselines show that the model is adapting not only to the language distribution of these new tasks, but also to the relationship between language and images. Frozen is therefore a multimodal few-shot learner, bringing the aforementioned language-only capabilities of rapid task adaptation, encyclopedic knowledge and fast concept binding to a multimodal setting.</p><p>Our goal in developing Frozen was not to maximise performance on any specific task, and in many cases it is far from state-of-the-art. Nonetheless, it performs well above trivial baselines across a wide range of tasks without ever seeing more than a handful of the training examples provided by these benchmarks. Moreover, as illustrated in <ref type="figure">Figure 1</ref>, Frozen is a system for genuinely open-ended and unconstrained linguistic interpretation of images that often produces compelling output.  To summarise, our contributions are as follows: 1. We present Frozen, a modular, scalable and efficient approach to training vision front-ends for large language models. The resulting combined model retains all of the capabilities of large language models, but can also process text and image inputs in any arbitrary sequence. 2. We show that such models transfer their capacity for rapid task adaptation, encyclopedic knowledge and fast concept binding from a language-only to a multimodal setting, and verify that prompting them with both visual and language information can be strictly more effective than doing so with language information alone. 3. We quantify these capabilities on a range of existing and new benchmarks, paving the way for future analysis of these capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Attention Layers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Frozen method is inspired by lots of recent work. <ref type="bibr" target="#b24">[25]</ref> show that the knowledge encoded in transformer language models can be a valuable prior for tasks involving reasoning and memory across discrete sequences, and even classification of images presented as sequences of spatial regions. In that approach, a small subset of the pre-trained language model weights are fine-tuned to the various final applications. In contrast, applying Frozen to different tasks does not involve any weight updates to the transformer whatsoever; the system adapts to and improves at multimodal (vision and language) tasks as activations propagate through the model. The two studies thus reveal different ways in which knowledge acquired from text can transfer to non-linguistic settings.</p><p>The effectiveness of prefix tuning <ref type="bibr" target="#b21">[22]</ref> or prompt tuning <ref type="bibr" target="#b18">[19]</ref> was another important motivation for Frozen. Prefix tuning is a method for prompting a language model to produce output of a particular style using gradient descent to learn a task-specific bias term which functions like the continuous embedding of a text prompt. Using prefix tuning, language models can be adapted to different natural language generation tasks like summarization. Frozen could also be considered a type of imageconditional prefix tuning, in which this continuous prompt is not a bias but an image-conditional activation produced by an external neural network.</p><p>A large body of work has applied either text-specific or multimodal representation-learning approaches like BERT <ref type="bibr" target="#b7">[8]</ref> to visual question answering (VQA) and captioning (see e.g. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref> and many more). In these approaches, models are first trained with aligned data on task-agnostic cross-modal objectives and then fine-tuned to specific tasks. This approach can yield state-of-the-art performance on a range of classification tasks. Unlike Frozen, the resulting systems are highly specialized to one task, and cannot learn new concepts or adapt to new tasks in a few shots.</p><p>By contrast, <ref type="bibr" target="#b6">[7]</ref> propose text generation as an objective for task-general multimodal models, yielding a system that, like Frozen, produces unconstrained language output. Unlike Frozen, they do not use a pre-trained model trained on text only, and do not consider zero or few-shot learning, instead updating all weights of the system with training data for each task they consider -thus, again, specializing the models to one task at a time. Similarly, <ref type="bibr" target="#b43">[44]</ref> and <ref type="bibr" target="#b5">[6]</ref> show that a large pre-trained language model as decoder can improve a captioning performance when training data is limited. Unlike Frozen, they use pre-trained frozen visual encoders or object extractors and fine-tune the pre-trained weights in the text decoder on the captioning data. Similarly, they do not consider zero or few-shot adaptation across different multimodal tasks. Past work has also explored alternative approaches for post-hoc combination of models for different modalities using latent variables <ref type="bibr" target="#b38">[39]</ref>.</p><p>Multimodal pre-training has recently been shown to enable strong zero-shot generalization in the discriminative setting using large-scale contrastive learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref>. Also in a discriminative setting, <ref type="bibr" target="#b42">[43]</ref> has observed signs of emergent few-shot-learning from large-scale training. In contrast, our work enables strong generalization to new multimodal tasks both zero-shot or few-shot with completely open-ended generative text output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Frozen Method</head><p>Frozen is a method for grounding a large language model without changing its weights, closely related to prefix tuning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref>. Prefix tuning trains a task-specific continuous bias term to function like the embedding of a constant, static text prompt used for all test-time examples. Frozen extends this approach by making this prefix dynamic, in that it is not a constant bias but an input-conditional activation emitted by a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Pre-trained Autoregressive Language Models Our method starts from a pre-trained deep autoregressive language model, based on the Transformer architecture <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b28">29]</ref>, which parametrizes a probability distribution over text y. Text is decomposed into a sequence of discrete tokens y = y 1 , y 2 , ..., y L by the SentencePiece tokenizer <ref type="bibr" target="#b16">[17]</ref>. We use a vocabulary of size 32,000. The language model makes use of an embedding function g ? which independently transforms each token into a continuous embedding t l := g ? (y l ), as well as a transformer neural network f ? whose output is a vector of logits parameterizing a categorical distribution over the vocabulary. The distribution p ? (y) is represented as follows:</p><formula xml:id="formula_2">log p ? (y) = l log p ? (y l |y 1 , y 2 , ..., y l?1 ) = l f ? (t 1 , t 2 , ..., t l?1 ) y l</formula><p>The model we start from is pre-trained, i.e. ? has been optimised via the standard maximum-likelihood objective on a large dataset of text from the internet. We use a 7 billion parameter transformer trained on the public dataset C4 <ref type="bibr" target="#b29">[30]</ref> -previous work has shown that the multi-billion parameter scale is sufficient to exhibit the key capacities we are interested in studying <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Vision Encoder Our vision encoder is based on NF-ResNet-50 <ref type="bibr" target="#b2">[3]</ref>. We define v ? as a function that takes a raw image and emits a continuous sequence to be consumed by the transformer. We use the final output vector of the NF-Resnet after the global pooling layer.</p><p>Visual Prefix One important requirement is to represent images in a form that the transformer already understands: a sequence of continuous embeddings, each having the same dimensionality D as a token embedding t l . We therefore form the visual prefix by linearly mapping the vision encoder's output to D * n channels, and then reshaping the result as a sequence of n embeddings, each with dimensionality D. We call this sequence a visual prefix since it plays the same functional role in the transformer architecture as (part of) an embedding sequence of prefix tokens. We experimented using different number of tokens, specifically 1, 2 and 4 and found that 2 performs best, though certainly this would be sensitive to other architectural details. See Appendix for more details on the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>During training, we update only the parameters ? of the vision encoder using paired image-caption data from the Conceptual Captions dataset <ref type="bibr" target="#b34">[35]</ref>. Our experiments show that fine-tuning ? hurts generalization, as much less paired image-caption data is available than the amount of text-only data used to pre-train ?. Training only the parameters ? makes our system modular -it can use an existing language model off the shelf -and also quite simple: we only train a visual encoder and rely on the capabilities of an existing language model.</p><p>Following standard captioning systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref>, we treat captioning as conditional generation of caption text y given an image x. We represent x as v ? (x) = i 1 , i 2 , ..., i n and train ? to maximise the likelihood:</p><formula xml:id="formula_3">log p ?,? (y|x) = l log p ?,? (y l |x, y 1 , y 2 , ..., y l?1 ) = l f ? (i 1 , i 2 , ..., i n , t 1 , t 2 , ..., t l?1 ) y l</formula><p>Whilst the parameters ? are frozen, each element i k of the visual prefix receives gradients</p><formula xml:id="formula_4">l ? i k f ? (i 1 , i 2 , .</formula><p>.., i n , t 1 , t 2 , ..., t l?1 ) y l , enabling the parameters of the visual encoder to be optimised with standard backpropagation and SGD ( <ref type="figure">Figure 2</ref>).</p><p>As the notation f ? (i 1 , i 2 , ..., i n , t 1 , t 2 , ..., t l?1 ) suggests, we present the visual prefix during training as if it were a sequence of embeddings occurring earlier in time than the caption (token embeddings) t 1 , t 2 , .... We use relative positional encoding <ref type="bibr" target="#b35">[36]</ref>, which enables the transformer to generalize to prompt sequences where an image is not always in the first absolute positions, and where more than one image may be present. We leave improvements of this simple scheme for future work. This is a blicket.</p><p>This is a dax. This is a blicket.</p><p>This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is this?</head><p>A: This is a Question from ImageNet 0-repeats 0-shots 2-way 0-repeats 2-inner-shots</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Answer with dax or blicket.</p><p>inner-shot 1 inner-shot 2 inner-shot 1 inner-shot 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support from ImageNet</head><p>This is a blicket. This is a dax. This is a blicket.</p><p>This is a dax.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interface at Inference Time</head><p>At inference time, a vanilla language model, conditioned upon an arbitrary text prompt or 'prefix' y 1 , y 2 , ..., y p , generates text sequences y p+1 , y p+2 , ... autoregressively. In Frozen it is straightforward to include images in a prompt by placing an image's embedding i 1 , i 2 next to a text embedding subsequence t 1 , t 2 , ..., t p . Because the transformer f ? is modality-agnostic, we can interleave a sub-sequence of text token embeddings with a sub-sequence of image embeddings in any arbitrary order. In <ref type="figure" target="#fig_2">Figure 3</ref>, we show how this can support zero-shot visual question-answering <ref type="figure" target="#fig_2">(Figure 3a</ref>), few-shot visual question-answering ( <ref type="figure" target="#fig_2">Figure 3b</ref>), and few-shot image classification <ref type="figure" target="#fig_2">(Figure 3c</ref>).</p><p>To evaluate these tasks, the model decodes output sequences greedily and these outputs are compared against the ground truth answers of the task following the normalization technique used in <ref type="bibr" target="#b17">[18]</ref>. We do not use short-lists of pre-canned answers to stress test the open-ended capabilities of Frozen, even though in some tasks this may hurt its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Few-Shot Learning Definitions</head><p>The ability of Frozen to be conditioned on a sequence of interleaved images and text allows it not only to be able to perform at different multimodal tasks, but also gives rise to different ways of 'inducing' the task to the model in order to improve its performance. We briefly define the terminology used in our settings, common amongst all the different tasks. See <ref type="figure">Figure 5</ref> in the appendix for a visual illustration of these concepts.</p><p>? Task induction Explanatory text that precedes the sequence of images and text. It is intended to describe the task to the model in natural language, for example 'Please answer the question.' For tasks involving fast concept binding (e.g., few-shot image classification), we define further specific terminology. See also <ref type="figure" target="#fig_3">Figure 4a</ref> and <ref type="figure" target="#fig_6">Figure 6</ref> in the appendix.</p><p>? Number of ways The number of object classes in the task (e.g. dog vs cat).</p><p>? Number of inner-shots The number of distinct exemplars from each category that are presented to the model (i.e. number of images of different dogs). In previous work with MiniImagenet, these were known as shots, but we modify the term here to distinguish from the more general usage of the term described above. ? Number of repeats The number of times each inner-shot is repeated in the context presented to the model. We use this setting as an ablation to explore how the model integrates visual information about a category.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments: A Multi-Modal Few-Shot Learner</head><p>Our experiments are designed to quantify three capacities that should be characteristic of a Multi-Modal Few-Shot Learner: rapid adaptation to new tasks, fast access to general knowledge and fast binding of visual and linguistic elements. We train Frozen on Conceptual Captions, a public dataset that consists of around three million image-caption pairs <ref type="bibr" target="#b34">[35]</ref>. We do early stopping on the validation set perplexity which usually reaches an optimum just after a single epoch with batch size 128. All experiments used the Adam optimizer with ? 1 = 0.9 and ? 2 = 0.95 and a constant learning rate of 3e-4 unless otherwise noted. We operate on 224?224 images at both train and test-time. Images which are not square are first padded with zeroes to square and then resized to 224?224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rapid Task Adaptation</head><p>We first examine zero-shot and few-shot generalization from captioning to visual question-answering. This is a type of rapid adaptation from captioning behaviour to question-answering behaviour with either simple prompting alone or few-shot learning, analogous to transfer from language modelling to open-domain question-answering <ref type="bibr" target="#b32">[33]</ref> in the vision plus language domain. We evaluate on the VQAv2 <ref type="bibr" target="#b9">[10]</ref> validation set.</p><p>Zero-shot transfer from captioning to VQA Captioning training can transfer moderately well to visual question-answering in the zero-shot setting with no training or in-context examples at all. The strength of the pre-trained language model is a double-edged sword. It powers the generalization abilities of Frozen but also enables the model to perform surprisingly well without considering the visual input at all. To guard against this possibility we also train blind baselines, in which the image presented to the visual encoder is blacked out, but the convnet weights are still trained. This amounts to prefix tuning <ref type="bibr" target="#b21">[22]</ref>. We outperform this blind baseline which also inherits the few-shot learning abilities of the language model.</p><p>In these experiments we also include two additional and important baselines: Frozen finetuned in which the language model is instead finetuned starting from the pretrained weights and Frozen scratch , wherein the whole system is trained from scratch end-to-end. These baselines preferred a smaller learning rate of 1e-5. Results in <ref type="table" target="#tab_2">Table 1</ref> show that keeping the language model frozen generalizes substantially better to visual question-answering than finetuning. The model trained from scratch is not able to transfer at all from captioning to VQA; we interpret this to suggest that the tremendous generalization abilities of large language models are reliant upon large-scale training datasets in which the task of predicting the next token mimics the test setting (here question-answering) with non-negligible frequency.</p><p>Improving performance with few-shot learning This zero-shot transfer to visual questionanswering via prompting improves by presenting examples to the model in-context. We repeat the previous experiments with up to four examples of image-question-answer triples shown to the model as conditioning information in the continuous prompt sequence (using the interface in <ref type="figure" target="#fig_2">Figure 3</ref>). We present these few-shot results compared to mixing in data from the VQAv2 training set -for SGD training -in There are two important takeaways from the results presented in this section. First, they show that training a visual encoder through a pretrained and frozen language model results in a system capable of strong out-of-distribution (zero-shot) generalization. Second, they confirm that the ability to rapidly adapt to new tasks given appropriate prompts is inherited from the pretrained language model and transfers directly to multimodal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encyclopedic Knowledge</head><p>Here we study the extent to which Frozen can leverage the encyclopedic knowledge in the language model towards visual tasks. The Conceptual Captions dataset is hypernymed meaning that e.g. proper names are replaced with a general word like person. This enables us to rigorously study the transfer of factual knowledge because all knowledge of named entities comes from language model pretraining.</p><p>Consequently, when we show the model an image of an airplane and ask "who invented this?" <ref type="figure">(Figure 1</ref>), the visual encoder has determined that the image contains an airplane, and the language model has used this to retrieve the factual knowledge that airplanes were invented by the Wright brothers, a fact which is referenced in the C4 training set through (text-only) articles about airplanes. This is a fascinating chain of deduction. A detailed analysis of this behaviour with more examples is included in the Appendix (e.g. <ref type="figure">Figure 9</ref>, <ref type="figure">Figure 10</ref>, <ref type="figure">Figure 11</ref>).</p><p>We bolster this finding quantitatively by evaluating performance on OKVQA <ref type="bibr" target="#b25">[26]</ref>, a visual questionanswering dataset designed to require outside knowledge in order to answer correctly. The pretrained language model's command of factual knowledge is of course dependent upon its scale, so we examine the performance of Frozen using pretrained language models of varying sizes: the base model with 7 billion parameters, and a much smaller 400 million parameter language model pretrained on the same dataset. <ref type="table" target="#tab_3">Table 2</ref> shows the results: task performance scales with model size. Again finetuning performs worse than leaving the model frozen in terms of generalization performance. We stress that Frozen is never trained on OKVQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fast Concept Binding</head><p>In the multi-modal setting, fast-binding refers to a model's ability to associate a word with a visual category in a few shots and immediately use that word in an appropriate way.</p><p>Open-Ended miniImageNet and Real-Name miniImageNet To quantify the fast-binding capacity of of Frozen, we evaluate it on the minImageNet meta-learning task <ref type="bibr" target="#b40">[41]</ref>. Note that there are important differences with how we attempt miniImageNet and how it is approached in previous work. First, unlike standard meta-learning, we do not train Frozen on the (meta) task. Second, we evaluate</p><p>Frozen in an open-ended fashion, where it must successfully generate a correct category name (and then the EOS token) in order to be credited with a correct answer. Finally, although we use the same image classes as the miniImageNet test set, they are at higher resolution (224?224) and with class labels replaced with nonsense words ('dax', 'blicket' etc). This allows the system to express its answers with word-like tokens. We refer to this task as Open-Ended miniImageNet, and it mimics closely the standard miniImagenet setting used elsewhere. To assess how much difficulty is added by binding visual categories to nonsense words versus simply adapting to an image recognition task per se, we also consider a version -Real-Name miniImagenet -in which visual categories in both the support set and the answer retain their original names. See <ref type="figure" target="#fig_3">Figure 4a</ref> for an illustration.</p><p>On both versions of this evaluation, we experiment by exposing the model to different numbers of inner-shots, repeats and task induction. On two-way Open-Ended miniImagenet, we observe that when Frozen is presented with a sequence of images and descriptions of new names for them, it is able to learn new names for the objects presented and then use these new names immediately with substantially above chance accuracy. Importantly, the ability of the model to use these new words improves with with more examples of the corresponding category. Notably, this upward trend is more pronounced when this supporting information involves different exemplars from the visual category (inner-shots) rather than repetitions of a single exemplar (repeats). The fast-binding capacities of the model can thus be improved with richer and more varied visual support or prompting.</p><p>On two-way Real-Name miniImagenet, we observe a similar trend but with higher absolute performance. This underlines the difficulty in Open-Ended miniImagenet introduced by having to assign novel words to categories that may otherwise be already known to the model, and because the real names may carry visual information leveraged from the captioning data the model was trained on.</p><p>In <ref type="table" target="#tab_7">Table 4</ref>, we show that the observed effects on Open-Ended miniImagenet do not transfer to the 5-way setting, where Frozen is not significantly above chance. This shows that learning to bind five new names to five visual categories in a single forward pass is beyond the current capabilities of Frozen. As before, however, we do observe an upward trend in the model's capacity to return the actual name for a visual category among the five possibilities as the number of inner-shots or repeats increases. Further work is required and we look forward to progress in this more challenging setting.   Fast-VQA and Real-Fast-VQA As transformers are trained to model text, their attention weights learn to associate -or 'bind'-pairs of words across sentences. The experiments with miniImageNet show that this capacity can transfer directly to binding visual categories to their names, enabling the system to generate the name on demand. This raises the question of whether Frozen can integrate a newly-acquired visual category (and its names) more fully into the model's language system, so that it can, for instance, describe or answer questions about that category.</p><p>To test this capacity, we constructed a new task -Fast-VQA -out of two well-known datasets, ImageNet <ref type="bibr" target="#b33">[34]</ref> and Visual Genome <ref type="bibr" target="#b15">[16]</ref>. For each question, the model is presented with nonsense words ('dax' and 'blicket') and n images of the referents of those words (e.g. of a 'cat' or a 'dog') taken from ImageNet. It is then asked a question containing at least one of those two words, about a further image (taken from Visual Genome) in which both of the referents appear (see <ref type="figure" target="#fig_3">Figure 4b</ref>). As with miniImagenet, the words 'dax' and 'blicket' (and how they refer) should be new to Frozen, but the corresponding visual categories may be known from the Conceptual Captions training data, albeit by different names.</p><p>To quantify how much harder the introduction of new words for known categories makes this task, we also created a variant (Real-Fast-VQA) in which the original category names ('cat' or 'dog') are used instead of 'dax' and 'blicket'. Real-Fast-VQA is a special case of VQA involving questions from Visual Genome, in which a model is reminded what the important entities in the question look like prior to answering the question. Real-Fast-VQA does not require the same ability to bind categories to new words, but it does measure how well a model can exploit task-relevant multimodal guidance when attempting a new task in an otherwise zero-shot manner.</p><p>Fast-VQA and Real-Fast-VQA are very challenging tasks because they are attempted without taskspecific training, and because the underlying questions come from Visual Genome (VQAv2 images do not come with the necessary meta-data to construct the task). Visual Genome questions are particularly challenging because only a single answer exists for each question. When scoring models, for simplicity we credit only an exact match with the output generated by the model, modulo the same post-processing applied for VQAv2. Because of the inherent difficulty of the task, we use strong baselines to verify strength of observed effects. The Fast-VQA and Real-Fast-VQA evaluation sets will be provided with the camera ready version of this manuscript, as a resource to stimulate further research on multimodal fast-binding, together with training data (not used in this work).</p><p>Fast-VQA Real-Fast-VQA Inner Shots  As shown in <ref type="table" target="#tab_8">Table 5</ref>, the fact that the model improves with more shots in both Fast-VQA and Real-Fast-VQA confirms that Frozen has some capacity to integrate novel words into its general capacity to process and generate natural language in a multimodal context. It is notable that a prefix-tuned model with no access to images improves moderately at Real-Fast-VQA as more concepts are presented, showing that additional linguistic cues (just being reminded of the words involved and the linguistic form of the task) goes some way to preparing for the upcoming question. As exemplified in <ref type="figure" target="#fig_3">Figure 4</ref>, inspection of the model output confirms that in many cases it is indeed the multimodal (and not just linguistic) support that enables Frozen to improve performance as the number of shots increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Limitations</head><p>We believe this work is an important proof-of-concept for a desired, much more powerful system capable of open-ended multimodal few-shot learning. Frozen achieves the necessary capacities to some degree, but a key limitation is that it achieves far from state-of-the-art performance on the specific tasks that it learns in a few shots, compared to systems that use the full training set for those tasks. As such, the main contribution of this work should be seen as a starting point or baseline for this exciting area of research of multimodal few-shot learning.</p><p>Further improvement can make the impressive zero-shot and few-shot generalization we observed more robust as reflected by higher accuracy and fewer seeds required to demonstrate our most compelling samples. Finally, there are many technical questions that were not explored in this proofof-concept study, such as whether performance could be improved with more elaborate architectures for mixing vision and language. We leave the exploration of these possibilities to future investigations. The Open-Ended miniImageNet, Real-Name miniImagenet, Fast-VQA and Real-Fast-VQA benchmarks that we will provide with the camera ready version of this manuscript should facilitate the evaluation and analysis of future systems of this type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Conclusion</head><p>We have presented a method for transforming large language models into multimodal few-shot learning systems by extending the soft-prompting philosophy of prefix tuning <ref type="bibr" target="#b21">[22]</ref> to ordered sets of images and text while preserving text prompting abilities of the language model. Our experiments confirm that the resulting system, Frozen, is capable both of open-ended interpretation of images and genuinely multimodal few-shot learning even though the system is only trained to do captioning. One corollary of these results is that the knowledge required to quickly bind together or associate different words in language is also pertinent to rapidly binding language to visual elements across an ordered set of inputs. This finding extends the conclusion of <ref type="bibr" target="#b24">[25]</ref> -that knowledge in transformer language models can transfer to non-linguistic tasks -to the specific case of knowledge about few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Compute Usage</head><p>The seven billion parameter language model we used as part of Frozen used model parallelism with the strategy from <ref type="bibr" target="#b36">[37]</ref> to partition one instance of the model over four accelerators. Each instance had a batch size of 8. To reach a batch size of 128 in this configuration, we additionally employed data parallelism with 16 synchronous replicas. The whole system was trained on a 4x8 TPUv3 <ref type="bibr" target="#b14">[15]</ref> topology for about 12 hours, which is when validation set performance for Conceptual Captions led us to do early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Frozen Architecture Details</head><p>The pretrained transformer language model we used has a GPT-like architecture <ref type="bibr" target="#b28">[29]</ref>. It consists of a series of identical residual layers, each comprised of a self-attention operation followed by a positionwise MLP. The only deviation from the architecture described as GPT-2 is the use of relative position encodings <ref type="bibr" target="#b35">[36]</ref>. Our seven billion parameter configuration used 32 layers, with each hidden layer having a channel dimensionality of 4096 hidden units. The attention operations use 32 heads each with key/value size dimensionality of 128, and the hidden layer of each MLP had 16384 hidden units. The 400 million parameter configuration used 12 layers, 12 heads, hidden dimensionality of 1536, and 6144 units in the MLP hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Few-Shot Learning Definitions</head><p>As Frozen can be conditioned on a sequence of interleaved images and text, it is capable not only of performing on a variety of multimodal tasks, but also, the same task can be induced in multiple ways to help Frozen to learn and perform better. In order to make it easier to distinguish among these different ways of 'inducing' a task to the model, we have formalized the terminology used in our settings, which is described in section 3.4 of the main text. In <ref type="figure">Figure 5</ref> and <ref type="figure" target="#fig_6">Figure 6</ref> below we provide more visual examples of this terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Tasks to Evaluate Fast-Binding Capacity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Open-Ended MiniImageNet</head><p>To construct the Open-Ended MiniImagenet evaluation we begin with the same subset S of ImageNet classes applied in prior on meta-learning with MiniImagenet (See the appendix of <ref type="bibr" target="#b31">[32]</ref>). All images are taken from the validation set of ImageNet.</p><p>To generate a 2-way question with n inner-shots, the following process is followed:</p><formula xml:id="formula_5">1. Sample two classes c 1 , c 2 from S 2. Sample n images v c1 1 . . . v c1 n+1 from c 1 and n images v c2 1 . . . v c2 n from c 2 3. Interleave into a sequence of 2n support images [v c1 1 , v c2 1 . . . v c1 n , v c2 n ] 4.</formula><p>Assign the nonsense words (dax, blicket) to c 1 , c 2 at random, and interleave support captions "this is a dax" or "this is a blicket" accordingly 5. Select one of c 1 , c 2 at random, c q , and sample a further question image v cq 6. Assign the truncated caption "this is a" to v q and the appropriate nonsense word as the correct answer.</p><p>Note that this process ensures that the image class and nonsense word assigned to the correct answer occur in either first or second place in the support, and the correct answer may be dax or blicket with equal probability.</p><p>To generate a 5-way question, the above process is generalized. In 1. five distinct classes are sampled from S. The set of nonsense words applied in step 4. and 6 is: [dax, blicket, slation, perpo, shously].</p><p>The final three words were taken from a nonsense-word generator 1 and selected because, like dax and blicket and for consistency, they decompose into two tokens in our model's subword vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0-repeats 1-shot</head><p>Question: What is this? Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shots Question</head><p>Please answer the question.</p><p>Question: What is this? Answer: Big Ben</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-repeats 1-shot</head><p>Question: What is this? Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shots Question</head><p>Please answer the question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shots Question</head><p>Please answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0-repeats 2-shots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shots</head><p>Please answer the question.</p><p>Question: What is this? Answer: Big Ben Question: Type of animal? Answer: dog shot 1 shot 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Please answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Please answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Please answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>shot 1</head><p>Question: What is this? Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Task Induction</head><p>Answer with lion or dog.</p><p>0-repeats 0-shots <ref type="figure">Figure 5</ref>: Examples of few-shot learning vocabulary.</p><p>All images are stored at 224 ? 224 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Real-Name miniImageNet</head><p>To generate Real-Name miniImagenet, the same process is followed, except that in steps 4. and 6., instead of using nonsense words to caption the support images (e.g. "this is a dax"), the (first) class name from the ImageNet dataset is used (e.g. "this is a fruit bat").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Fast-VQA</head><p>Unlike Open-Ended miniImageNet, Fast-VQA uses images from all 1,000 classes in the ImageNet dataset. For the evaluations in this paper, we again only take images from the validation set. Denote by W the set of all 1,000 class (first) names, and for each w i ? W , the corresponding set of images c i .</p><p>The Visual Genome (VG) dataset contains meta-data, questions and answers, such that we can consider data in the form (Im, q, a, Ob), where Im is the image, q is the corresponding question, a is the answer and Ob is a list of names for all objects annotated in Im. We first filtered the dataset into a subset V G * such that every question q k contained at least one word w i ? W and such that the corresponding object list Ob k also contained q k and at least one other word w j ? W : w j ! = w i . Thus, we can consider the elements of V G * to be of the form (Im, q, a, Ob, w i , w j ) 0-repeats 0-shots 2-way 1-inner-shot 0-repeats This is a blicket.</p><p>This is a dax. This is a blicket.</p><p>This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is this?</head><p>A: This is a Question 0-repeats 0-shots 2-way 2-inner-shots 0-repeats 0-repeats 0-shots 2-way 1-inner-shot 1-repeat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Answer with dax or blicket. This is a blicket.</p><p>This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is this?</head><p>A: This is a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Task Induction</head><p>Answer with dax or blicket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0-repeats 1-shot 2-way 1-inner-shot 0-repeats</head><p>This is a blicket.</p><p>This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is this?</head><p>A: This is a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Answer with dax or blicket. This is a blicket.</p><p>This is a dax. This is a blicket.</p><p>This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: What is this?</head><p>A: This is a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Question</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Induction</head><p>Answer with dax or blicket. This is a blicket.</p><p>This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q:What is this?</head><p>A:This is a dax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shots</head><p>Answer with dax or blicket. To generate a 2-way, n-shot Fast-VQA question out of an element (Im, q, a, Ob, w i , w j ), we then did the following: 3. Assign the nonsense words (dax, blicket) to w i , w j at random, and interleave support captions "this is a dax" or "this is a blicket" accordingly 4. Transform q and a into modified questions and answers q * and a * by replacing all instances of w i and any instances of w j with the corresponding strings dax or blicket 5. Append the (VG) question (Im, q * , a * ) to the (ImageNet) support from 2. to create the Fast-VQA sample.</p><p>In this work, we only consider 2-way Fast-VQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.4 Real-Fast-VQA</head><p>To generate Real-Fast-VQA, the same process is followed, except that in step 3. the (first) class name from ImageNet is used to caption the support images ("this is a cat", "this is a wolf"), and no string replacement is undertaken in 4.</p><p>Links to download Open-Ended miniImageNet, Real-Name miniImageneNet, Fast-VQA and Real-Fast-VQA will be made available soon.  <ref type="figure">Figure 7</ref>: Example of a Fast-VQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Encyclopedic Knowledge</head><p>Here we add more detail to the claim in subsection 4.2 that the model seems to be performing a sort of multi-hop deduction in the "Wright Brothers" example from <ref type="figure">Figure 1</ref>.</p><p>First, there has been a substantial amount of recent work studying a language model's ability to draw upon factual knowledge, examining the ability of language models to answer factual questions either zero-shot <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref> or after open-domain QA finetuning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. Buoyed by these findings, we here demonstrate rigorously the impressive extent to which Frozen seems to be commanding this factual knowledge and drawing upon it when prompted by an image (here an image of an airplane). We now break down why it is interesting that the model correctly determines that the Wright Brothers invented the object in the image (an airplane), by studying how the model responds to different prompts concerning this same test image in <ref type="figure">Figure 9</ref>.</p><p>Recall that Conceptual Captions is hypernymed so none of the language targets used to train Frozen contain named entities like "The Wright Brothers". Instead, our training signal teaches the model to emit text that would roughly describe an image. The impressive finding is that this scalable, weakly supervised objective generalizes to general information retrieval about an image.</p><p>The top pane in <ref type="figure">Figure 9</ref> shows an example of what the text in the captioning distribution looks like, captioning the image as "an airplane flying over a blue sky -stock photo #". Now, as established in subsection 4.1 we enjoy some amount of zero-shot transfer from captioning to visual questionanswering. This is demonstrated in the second and third rows of <ref type="figure">Figure 9</ref>. But, adhering to the distribution of caption text, the model does not give a named entity when asked who invented the airplane. Instead it completes the prompt vaguely by saying "This was invented by an aerospace engineer and is made by the brand he worked for".</p><p>But we know for certain that the language model has learned plenty of facts about named entities during pre-training and in particular we determined via the C4 dataset search tool <ref type="bibr" target="#b8">[9]</ref> that there are multiple articles concerning the Wright Brothers. It's just that matching the distribution of Conceptual Captions text has taught the model to not emit named entities when prompted with an image. But the model can recover the ability to refer to named entities given an image with few-shot learning (bottom row of <ref type="figure">Figure 9</ref>). We show the model two examples of saying who invented an object depicted in an image by giving a named entity (Zacharias Janssen invented the microscope and Henry Ford invented the model T, an early automobile). With this prompt, Frozen reliably retrieves the correct factual knowledge, having determined in the vision encoder that the image depicts an airplane, and having been demonstrated in-context that the desired output is the name of a person.</p><p>This outcome is robust, in the sense that we observed it in multiple versions of Frozen during development, and in multiple examples, but drawing samples is not always successful and can require 3-4 tries to get past well-known language model failure modes of either repeating prompt text or emitting completely unrelated text. That's why we describe some samples as "curated".</p><p>We reiterate that this is a fascinating chain of deduction and a huge generalization leap from the task the model was trained to do, which is emit a caption for an image. Question: Where is the remote control?</p><p>Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>It is under the bed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blind Completion</head><p>It's in the kitchen</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Answers</head><p>Question: Where is the man? Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Model Completion tennis player</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blind Completion</head><p>He is in the bathroom Question: Which animal is this? Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>Model Completion a dog</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blind Completion</head><p>it is a monkey</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Answers</head><p>Question: What animals are these?</p><p>Answer:  <ref type="figure">Figure 8</ref>: VQA qualitative. This is a greedy sample of our model's prediction on a VQAv2 validation set example. See accuracy numbers in <ref type="table" target="#tab_2">Table 1</ref> for overall robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>This was invented by Zacharias Janssen.</p><p>This was invented by Henry Ford.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This was invented by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>the Wright brothers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This was invented by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>an aerospace engineer and is made by the brand he worked for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>a supersonic aircraft over the skyline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>an airplane flying over a blue sky -stock photo # What is this? <ref type="figure">Figure 9</ref>: Encyclopedic Knowledge. Shows the model retrieving factual knowledge given visual input. Required cherry-picking from around 5 seeds to get past common language model failure modes like simply repeating text from the prompt or emitting text that does not pertain to the test image. This is a .</p><p>This is a . This is a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Model Completion</head><p>. &lt;EOS&gt; <ref type="figure">Figure 10</ref>: Emojis. This sample reliably produced good output within a few attempts but did not work for every seed.</p><p>Question: I have this problem, what can I do? Answer:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Completion</head><p>If you can remove it, gently rub the scuff with a clean white cloth. &lt;EOS&gt; <ref type="figure">Figure 11</ref>: Encyclopedic Knowledge. Demonstrates knowledge from language pre-training being commanded given visual input. Required a few seeds to get a good answer which clearly paid attention to the image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " 1 Q W 3 A O k o Q F 2 7 a O y H A / e m d y y n y u 4 = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K q M e i F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 3 J 0 I J / R N e P C j i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t z O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l z r D f w x F H 2 i 9 X 3 K o 7 B 1 k l X k 4 q k K P R L 3 / 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J p 6 V e a n h C 2 Z g O e d d S R S N u / G x + 7 5 S c W W V A w l j b U k j m 6 u + J j E b G T K L A d k Y U R 2 b Z m 4 n / e d 0 U w 2 s / E y p J k S u 2 W B S m k m B M Z s + T g d C c o Z x Y Q p k W 9 l b C R l R T h j a i k g 3 B W 3 5 5 l b R q V e + y W r u / q N R v 8 j i K c A K n c A 4 e X E E d 7 q A B T W A g 4 R l e 4 c 1 5 d F 6 c d + d j 0 V p w 8 p l j + A P n 8 w c h Y J A J &lt; / l a t e x i t &gt; g ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S s u + f p Z O e O J 2 v I P 8 a C d n W + 2 Q V 9 4 = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m K q M e i F 4 8 V 7 A e 0 o W y 2 m 3 b p Z h N 3 J 0 I J / R N e P C j i 1 b / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " F Q U 2 A w F K v a L 1 P 7 D E l y h C L m Q f s 9 A = " &gt; A A A B 7 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k R 9 V j 0 4 r G C / Y A 2 l M 1 2 0 y 7 d b O L u p F B C / 4 Q X D 4 p 4 9 e 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X J F I Y d N 1 v Z 2 1 9 Y 3 N r u 7 B T 3 N 3 b P z g s H R 0 3 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j u 5 n f G n N t R K w e c Z J w P 6 I D J U L B K F q p P e 5 l 3 W Q o p r 1 S 2 a 2 4 c 5 B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Inference-Time interface for Frozen. The figure demonstrates how we can support (a) visual question answering, (b) outside-knowledge question answering and (c) few-shot image classification via in-context learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Examples of (a) the Open-Ended miniImageNet evaluation (b) the Fast VQA evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>?</head><label></label><figDesc>Number of shots The number of distinct full examples of the task presented to the model prior to the evaluated example. For example, in Visual Question-Answering, a shot is an image along with the question and the answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Frozen train-blind 0.7 0.3 1.3 0.4 1.9 2.3 3.7 3.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Examples of few-shot learning vocabulary for fast-binding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Frozen finetuned 24.0 28.2 29.2 Frozen train-blind 26.2 33.5 33.3 Transfer from Conceptual Captions to VQAv2. The ? column indicates whether a model uses training data from the VQAv2 training set. The row denoted Frozen train-blind is the blind baseline described in subsection 4.1. Frozen VQA is a baseline which mixes in VQAv2 training data.</figDesc><table><row><cell>n-shot Acc.</cell><cell cols="3">n=0 n=1 n=4 ?</cell><cell>n-shot Acc.</cell><cell cols="3">n=0 n=1 n=4 ?</cell></row><row><cell>Frozen</cell><cell cols="3">29.5 35.7 38.2</cell><cell>Frozen</cell><cell>5.9</cell><cell cols="2">9.7 12.6</cell></row><row><cell>Frozen scratch</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>Frozen 400mLM</cell><cell>4.0</cell><cell>5.9</cell><cell>6.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frozen finetuned</cell><cell>4.2</cell><cell>4.1</cell><cell>4.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frozen train-blind</cell><cell>3.3</cell><cell>7.2</cell><cell>0.0</cell></row><row><cell>Frozen VQA</cell><cell>48.4</cell><cell>-</cell><cell>-</cell><cell>Frozen VQA</cell><cell>19.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Frozen VQA-blind 39.1</cell><cell>-</cell><cell>-</cell><cell cols="2">Frozen VQA-blind 12.5</cell><cell>-</cell><cell>-</cell></row><row><cell>Oscar [23]</cell><cell>73.8</cell><cell>-</cell><cell>-</cell><cell>MAVEx [42]</cell><cell>39.4</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Transfer from Conceptual Captions to OKVQA. The ? column indicates if a model uses training data from the OKVQA training set. Frozen does not train on VQAv2 except in the baseline row, and it never trains on OKVQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Of course, few-shot learning on four examples is outperformed by SGD on tens of thousands of examples, but few-shot performance clearly improves with more examples and goes a decent way toward closing the gap from zero-shot performance (29.5%) to full SGD training performance (48.4%). With just four examples the gap is closed almost halfway at 38.2%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance of Frozen and baselines on Open-Ended miniImageNet 2-Way Tasks. Randomly picking between the two class labels (then emitting the EOS token) would yield 50% accuracy. As the model has to generate the answer, and is not counted correct if it paraphrases, this is not the best blind baseline, which is why we include open-ended blind baselines that also generate.</figDesc><table><row><cell>Task Induction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inner Shots</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Repeats</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>5</cell></row><row><cell>Frozen</cell><cell cols="7">18.0 20.2 22.3 21.3 21.4 21.6 20.9</cell></row><row><cell>Frozen (Real-Name)</cell><cell>0.9</cell><cell cols="6">14.5 34.7 33.8 33.8 33.3 32.8</cell></row><row><cell>Frozen test-blind</cell><cell>-</cell><cell cols="4">18.6 19.9 19.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Frozen test-blind (Real-Name) -</cell><cell>4.6</cell><cell cols="3">22.6 20.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell>ANIL Baseline [31]</cell><cell>-</cell><cell cols="4">45.5 57.7 62.6 -</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance of Frozen and baselines on Open-Ended miniImageNet 5-Way Tasks. Randomly picking between the five class labels (then emitting the EOS token) would yield 20% accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Performance of Frozen versus an equivalent blind model on Fast and Real-Fast VQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>1. Sample n images v ci 1 . . . v ci n+1 from c 1 and n images v</figDesc><table><row><cell>cj 1 . . . v n from c 2 cj</cell></row><row><cell>2. Depending on coin toss, form either the support [v ci 1 , v 1 . . . v ci cj n , v n ] or the support cj [v cj 1 , v ci 1 . . . v cj n , v ci n ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Support from ImageNet Question from VisualGenome Correct Answer wood blicket (vase) dax (table)</head><label></label><figDesc></figDesc><table><row><cell>2-way</cell><cell></cell><cell></cell></row><row><cell>1-shot</cell><cell></cell><cell></cell></row><row><cell>0-repeats</cell><cell></cell><cell></cell></row><row><cell>0-episodes</cell><cell></cell><cell></cell></row><row><cell>This is a</cell><cell>This is a dax.</cell><cell>Q: What is the</cell></row><row><cell>blicket.</cell><cell></cell><cell>dax made of? A:</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.soybomb.com/tricks/words/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We wish to thank Sebastian Borgeaud and Jack Rae for preparing the pretraining text dataset and pretraining a selection of transformer language models, as well as Trevor Cai for help with experiments and infrastructure. We also wish to thank Pauline Luc, Jeff Donahue, Malcolm Reynolds, Andy Brock, Karen Simonyan, Jean-Baptiste Alayrac, Antoine Miech, Charlie Nash, Aaron van den Oord, Marc Deisenroth, Aida Nematzadeh, Roman Ring, Francis Song, Eliza Rutherford, Kirsty Anderson, Esme Sutherland, Daan Wierstra, and Nando de Freitas for insightful discussions during the course of the project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards a human-like open-domain chatbot. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Climbing towards nlu: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Emily</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-performance largescale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gpt3 and general intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chalmers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Published in Daily Nous</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Visualgpt: Data-efficient adaptation of pretrained language models for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10407</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02779</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Allen Institute for AI. C4 search</title>
		<ptr target="https://c4-search.apps.allenai.org/" />
		<imprint>
			<biblScope unit="page" from="2021" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Realm: Retrieval-augmented language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Word learning in children: An examination of fast mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Heibeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Markman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child development</title>
		<imprint>
			<biblScope unit="page" from="1021" to="1034" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehensive survey of deep learning for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md Zakir Hossain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CsUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Ferdous Sohel</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Luc Cantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alek</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diemthu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathy</forename><surname>Nix</surname></persName>
		</author>
		<editor>Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and Doe Hyun Yoon</editor>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson; Bo Tian, Horia Toma, Erick Tuttle</pubPlace>
		</imprint>
	</monogr>
	<note>In-datacenter performance analysis of a tensor processing unit</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Georgia Tech Visual Intelligence Lab. Vqa python api and evaluation code</title>
		<ptr target="https://github.com/GT-Vision-Lab/VQA" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual to text: Survey of image and video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="312" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05247</idno>
		<title level="m">Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ok-vqa: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3190" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models as knowledge bases? CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Rapid learning or feature reuse? towards understanding the effectiveness of maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddh</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09157</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How much knowledge can you pack into the parameters of a language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08910</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Latent translation: Crossing modalities by bridging generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Multi-modal answer validation for knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Scaling vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Encoderagnostic adaptation for conditional language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06938</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
