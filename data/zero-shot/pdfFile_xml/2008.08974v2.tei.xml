<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ISSAFE: Improving Semantic Segmentation in Accidents by Fusing Event-based Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">ISSAFE: Improving Semantic Segmentation in Accidents by Fusing Event-based Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensuring the safety of all traffic participants is a prerequisite for bringing intelligent vehicles closer to practical applications. The assistance system should not only achieve high accuracy under normal conditions, but obtain robust perception against extreme situations. However, traffic accidents that involve object collisions, deformations, overturns, etc., yet unseen in most training sets, will largely harm the performance of existing semantic segmentation models. To tackle this issue, we present a rarely addressed task regarding semantic segmentation in accidental scenarios, along with an accident dataset DADA-seg. It contains 313 various accident sequences with 40 frames each, of which the time windows are located before and during a traffic accident. Every 11th frame is manually annotated for benchmarking the segmentation performance. Furthermore, we propose a novel event-based multi-modal segmentation architecture ISSAFE. Our experiments indicate that event-based data can provide complementary information to stabilize semantic segmentation under adverse conditions by preserving fine-grain motion of fast-moving foreground (crash objects) in accidents. Our approach achieves +8.2% mIoU performance gain on the proposed evaluation set, exceeding more than 10 state-of-the-art segmentation methods. The proposed ISSAFE architecture is demonstrated to be consistently effective for models learned on multiple source databases including Cityscapes, KITTI-360, BDD and ApolloScape.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Intelligent Vehicles (IV) and Advanced Driver Assistance Systems (ADAS) benefit from breakthroughs in deep learning algorithms. In particular, image semantic segmentation can provide pixel-wise understanding of driving scenes, containing object categories, shapes, and locations. In recent years, many state-of-the-art segmentation models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> have achieved impressive successes in accuracy on major segmentation benchmarks. Other works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> centered on improving the efficiency of the model, in order to deploy real-time semantic segmentation on mobile platforms.</p><p>Unfortunately, driving environments in the real world are more complicated than most existing datasets, divided into normal, critical and accidental situations. In addition to natural related factors in the normal driving scene, such as weathers and illuminations, many human-centered crisis incidents from other traffic participants may occur. For example, vehicles overtaking irregularly, pedestrians dashing This work was supported in part through the AccessibleMaps project by the Federal Ministry of Labor and Social Affairs (BMAS) under the Grant No. 01KM151112, in part by the University of Excellence through the "KIT Future Fields" project, and in part by Hangzhou SurImage Company Ltd. (Corresponding author: Kailun Yang.) <ref type="bibr" target="#b0">1</ref> Authors are with Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Germany (e-mail: firstname.lastname@kit.edu).</p><p>Code and dataset will be made publicly available at: https:// github.com/jamycheung/ISSAFE . From top to bottom are timestamps before and during an accident, where the t1 frame is the ground truth for quantitative evaluation, and the others are predictions of our model. across the road, or cyclists riding out of lanes, these critical situations are all potential causes of traffic accidents, but never seen in vision datasets. Furthermore, the initial accident scene ahead is also defined as an accidental situation, such as an overturned truck or a knocked down motorcycle lying on the road, which should be correctly recognized by passing vehicles in time, only then can pileups be avoided. However, these abnormalities will result in a large and sharp performance drop of the segmentation models when taken from public training imagery to the wild. To satisfy the rigorous requirements of safety-relevant IV systems, a segmentation model should be thoroughly tested on some edge cases to verify its robustness and reliability. Thus, this paper propose an alternative benchmark based on a new task, namely Semantic Segmentation in Accidents (SSA). Being a supplement to classic benchmarks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, our evaluation samples are collected from real-world traffic accidental situations which involve highly dynamic scenes and extremely adverse factors. Some cases are shown in <ref type="figure">Fig. 1</ref>, covering diverse situations: motion blur while the pedestrian is dashing across the road, overturning of the motorcyclist during the collision, back-lighting at the intersection, and occlusions by windshield reflection. As far as known to us, these factors are still challenging for most segmentation algorithms and even harmful to their performance. The objective of creating this benchmark is to provide a set of edge cases (critical and accidental) for testing the robustness of vision models before deployment in real applications.</p><p>In addition to traditional cameras, event cameras are bioinspired novel sensors, such as the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b8">[9]</ref>, that encode changes of intensity at each pixel asynchronously and have the characteristics like higher dynamic range (&gt; 120dB), high time resolution (1M Hz clock or ?s timestamp), and are not affected by motion blur <ref type="bibr" target="#b9">[10]</ref>. Hence, we consider that event cameras are more sensitive to capture the motion information during driving, especially for fast-moving objects (foreground) in extreme or accident scenarios, where classic cameras delay between frames. In low-lighting environments, event cameras still stably bring sufficient perceptual information. Underlying these assumptions, complementary information can be extracted from the event-based data to address shortcomings of the intensity image in both normal and abnormal scenes.</p><p>Finally, as a preliminary exploration on this new task, we propose a light-weight ISSAFE architecture, which can serve as an Event-aware Fusion (EF) model to process RGB and event data, or an Event-aware Domain Adaptation (EDA) model to bridge the source (normal) and target (accident) datasets. In accordance with our ISSAFE architecture, the robustness of SSA can be significantly improved. In summary, our main contributions are:</p><p>? We present a rarely solved task concerning Semantic Segmentation in Accidents (SSA), with the ultimate goal to robustify the perception algorithm against abnormal situations during highly dynamic driving. ? We provide an accompanying accident dataset DADAseg, of which the evaluation set was manually annotated for benchmarking the robustness of SSA. ? We propose a multi-modal segmentation architecture ISSAFE to exploit complementary features from eventbased data according to two approaches, i.e. EF and EDA. Comprehensive comparisons and ablation studies are conducted between various models, datasets and data modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation</head><p>Since FCN <ref type="bibr" target="#b10">[11]</ref> used fully convolutional layers for pixelwise prediction on images, a massive number of models <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> have achieved remarkable performance in image semantic segmentation. In addition to high accuracy, other works, such as ERFNet <ref type="bibr" target="#b4">[5]</ref> and SwiftNet <ref type="bibr" target="#b5">[6]</ref>, proposed simplified architectures to improve the efficiency. Regarding generalizability, Domain Adaptation (DA) strategies were extensively applied to adapt the segmentation algorithm to new scenes <ref type="bibr" target="#b11">[12]</ref>. For example, the day-night conversions in <ref type="bibr" target="#b12">[13]</ref> and the adaptations between diverse weathers like rainy <ref type="bibr" target="#b13">[14]</ref> and snowy <ref type="bibr" target="#b14">[15]</ref> scenes. However, apart from these natural conditions in real driving scenes, there are many uncontrollable factors during the interaction with other traffic participants. The core purpose of our work is to fill the gap of semantic segmentation in abnormal situations.</p><p>Any ambiguity in machine vision algorithms may cause fatal consequences in autonomous driving, thus the robustness testing conducted in diverse driving conditions is essential. For this reason, WildDash <ref type="bibr" target="#b7">[8]</ref> provided ten different hazards, such as blurs, underexposures or lens distortions, as well as negative test cases against the overreaction of segmentation algorithms. However, in order to extend the robustness test from ordinary to accident scenarios, we create an accident dataset DADA-seg. Those critical or accidental scenes are more difficult by having a large variety of adverse hazards.</p><p>On the other hand of improving robustness, some solutions constructed a multi-modal segmentation model by fusing additional information, such as depth in RFNet <ref type="bibr" target="#b15">[16]</ref>, thermal information in RTFNet <ref type="bibr" target="#b16">[17]</ref> and optical flow in <ref type="bibr" target="#b17">[18]</ref>. Differing from these classic modalities, in this work, eventbased data will be explored as a novel auxiliary modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Event-based Vision</head><p>Event cameras are increasingly used in visual analysis due to their complementary features to traditional cameras, such as High Dynamic Range (HDR), no motion blur, and response in microseconds <ref type="bibr" target="#b9">[10]</ref>. Instead of capturing an image in a fixed rate, event cameras asynchronously encode the intensity change at each pixel with the position, time, and polarity: (x, y, t, p). Typically, for processing in a convolutional network, the original event stream is converted into an image form, such as a two-channel event frame in <ref type="bibr" target="#b18">[19]</ref>, a fourdimensional grid in <ref type="bibr" target="#b19">[20]</ref> and a Discretized Event Volume (DEV) in <ref type="bibr" target="#b20">[21]</ref>.</p><p>Based on these image-like representations, Ev-SegNet <ref type="bibr" target="#b21">[22]</ref> was trained on an extended event dataset DDD17 <ref type="bibr" target="#b22">[23]</ref>, whose labels were generated by a pre-trained model and only contain 6 categories. In contrast, our models are trained with the ground-truth labels of Cityscapes in all 19 classes. Additionally, instead of stacking images in the input stage, event data will be adaptively fused with the RGB image through our attention module, which is more effective for combining two heterogeneous modalities.</p><p>While labeled event data for semantic segmentation is scarce in the state of the art, other works leveraged the existing labeled data of images by simulating their corresponding event data. EventGAN <ref type="bibr" target="#b23">[24]</ref> presented a selfsupervised approach to generate events from associated images using only modern GPUs. In this work, we utilize the EventGAN model to extend the datasets by generating their associated event data, so as to investigate the benefit of event sensing in dynamic accident scenes. Finally, the EDA between both datasets is performed by fusing RGB images and the synthesized events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task Definition</head><p>To evaluate the robustness of semantic segmentation models, we create a new task: Semantic Segmentation in Accidents (SSA). Besides, an associated evaluation set is provided for quantitative analysis. All test samples are edge cases collected from real-world traffic accidents and contain adverse situations. We explicitly study the robustness in challenging accident scenarios based on the assumption that the less performance degradation of the algorithm in this unseen dataset, the better its robustness. B. Accident Dataset Dataset Annotation. Our proposed dataset DADA-seg is selected from the large-scale DADA-2000 <ref type="bibr" target="#b24">[25]</ref> dataset, which was collected from mainstream video sites. To extend it to the semantic segmentation community, we performed additional laboratory work for processing all 2000 sequences in two stages. In the first stage, sequences with large watermarking or low resolution were removed, while most of the typical adverse scenes were retained, such as those with motion blur, over/underexposures, weak illuminations, occlusions, etc. All other different conditions are described in <ref type="table" target="#tab_0">Table I</ref>. Concentrating on accident scenes, we remain the 10 frames before the accident and 30 frames during the accident. After selection, the final DADA-seg dataset composes of 313 sequences with a total of 12,520 frames at a resolution of 1584?660.</p><p>In the second stage, based on the same 19 classes as defined in Cityscapes, we manually perform full pixel-wise annotation on every 11th frame of 313 sequences by using the polygons to delineate individual semantic classes, as shown in the t 1 frame in <ref type="figure">Fig. 1</ref>. After labeling, our DADAseg dataset includes 313 labeled images for the quantitative analysis of SSA and 12,207 unlabeled images for EDA between the normal and abnormal imagery. Comparatively, all images of our dataset are taken in broad regions by different cameras from various viewpoints. Besides, all sequences focus on accident scenarios, composing of normal, critical, and accidental situations. In such a way, the evaluation performed on DADA-seg dataset reflects more thoroughly the robustness of semantic segmentation algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Event-based Data</head><p>Event Representation. Event cameras asynchronously encode an event at each individual pixel (x, y) at the corresponding triggering timestamp t, if the change of logarithmic intensity L in time variance ?t is greater than a preset threshold C:</p><formula xml:id="formula_0">L(x, y, t) ? L(x, y, t ? ?t) ? pC, p ? {?1, +1}<label>(1)</label></formula><p>where polarity p indicates the positive or negative direction of change. A typical volumetric representation of a continuous event stream with size N is a set of 4-tuples:</p><formula xml:id="formula_1">V = {e i } N i=1 , where e i = (x i , y i , t i , p i ).<label>(2)</label></formula><p>However, it is still arduous to transmit the asynchronous event spike to the convolutional network by retaining a sufficient time resolution. Hence, we perform a dimensionality reduction operation in the time dimension <ref type="bibr" target="#b23">[24]</ref>. The original volume is discretized with a fixed length for positive and negative events separately, and each event is locally linearly embedded to the nearest time-series panel. According to the number of positive time bin B + , a discretized spatialtemporal volume V + is represented as: Event Data Synthesis. Bringing event data to SSA task, there is still a lack of event-based dataset with semantic annotations. Thus, we utilize the identical EventGAN <ref type="bibr" target="#b23">[24]</ref> model to synthesize highly reliable event data of all mentioned datasets. For this, two adjacent RGB image frames are required as inputs. Different from the fixed frame rate (17Hz) in Cityscapes <ref type="bibr" target="#b6">[7]</ref>, the sequence in the DADA-seg dataset was acquired with diverse cameras and frame rates, which means that its synthesized event data vary from the intensity of motion due to different time intervals. After verification, the penultimate frame was selected and stacked with its anchor frame for event data synthesis. Two cases of the generated event data are visualized in <ref type="figure" target="#fig_1">Fig. 2</ref>. It can be seen how event data benefits the sensing in driving scenes with moving objects or in low-lighting environments, meanwhile providing higher time resolution in volumetric form.</p><formula xml:id="formula_2">t i = (B + ? 1) (t i ? t 1 ) / (t N ? t 1 ) ,<label>(3)</label></formula><formula xml:id="formula_3">V + (x, y,t i ) = B + i max 0, 1 ? t ?t i ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ISSAFE: Event-aware Fusion</head><p>According to the characteristics of event data flow from sparse to dense or vice versa, two diverse EF approaches are mainly designed and explored to excavate complementary informative features from the event data.</p><p>S2D: Sparse-to-Dense. An intuitive fusion approach is treating the sparse event data as input and extracting its  dense feature, similar to a normal RGB image branch. In this paper, we mainly explore the adaptive fusion of these two different modalities between layers. As shown in <ref type="figure" target="#fig_3">Fig. 3a</ref>, the S2D fusion model includes dual branches, i.e. RGB branch and event branch, constructed with the ResNet-18 <ref type="bibr" target="#b25">[26]</ref> backbone for maintaining a real-time speed. Inspired by the design of SwiftNet <ref type="bibr" target="#b5">[6]</ref> and RFNet <ref type="bibr" target="#b15">[16]</ref>, a channelwise attention module is employed between layers of both branches, in which the motion features are emphasized in the event branch and added element-wise into the RGB branch. In other words, the higher time resolution from event data complements the motion-related features in the blurred RGB image. Additionally, its HDR enhances the over/underexposure image. While the image feature map is termed as F i ? R C?H?W and the event feature map as F e ? R C ?H?W , the S2D fused feature F S2D ? R C?H?W by channel-wise attention is represented as:</p><formula xml:id="formula_4">F S2D = F i ? ? i [f (F i )] + F e ? ? e [g(F e )] ,<label>(5)</label></formula><p>where both f (?) and g(?) are composed of the adaptive global pooling and 1 ? 1 convolution operations, the ? i (?) and ? e (?) denote Sigmoid activate functions for the image and event feature map. After four residual layers, the event feature serves as an additional stream in the Spatial Pyramid Pooling (SPP) module <ref type="bibr" target="#b0">[1]</ref> and will be concatenated with other highlevel features for long-range context sensing. Finally, a lightweight decoder, composing of 3 upsampling modules with 1?1 skip connections from the RGB branch, will align different levels of features for the final prediction. D2S: Dense-to-Sparse. On the other hand, inspired by the video restoration from a single blurred image and the event data like <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we alternatively leverage the D2S fusion approach, as shown in <ref type="figure" target="#fig_3">Fig. 3b</ref>. Varying from the classic residual layer in the previous S2D fusion mode, a more light-weight encoder with 4 layers is constructed as the event branch. Each layer only contains a 3?3 and a 1?1 convolutional kernel, which is more effective to extract features from dense to sparse and capable of processing at a higher spatial resolution. After the initial convolution of ResNet-18 <ref type="bibr" target="#b25">[26]</ref>, while the RGB branch encodes higher-level features at smaller resolutions with {4, 8, 16, 32} downsampling rates and {64, 128, 256, 512} channels, the event branch deactivates the non-event features according to the higherlevel semantic features from the RGB branch. Meanwhile, the event branch gradually shallows event channels in the order of {64, 32, 16, 8} for final event prediction, which also enables event processing at the full resolution. The dense RGB feature map F i ? R C?H?W and the sparse event feature map F e ? R C ?H?W will be merged as a D2S fused feature map F D2S ? R C ?H?W :</p><formula xml:id="formula_5">F D2S = F e ? ? [c(F e , F i )] + F e ,<label>(6)</label></formula><p>where ?(?) and c(?) denote Sigmoid activate function and concatenation operations, respectively. Before the event feature is merged in the SPP module <ref type="bibr" target="#b0">[1]</ref>, standard Binary Cross Entropy (BCE) loss function and the ground-truth event data will be used for supervised learning. Furthermore, aiming to learn the whole model in an end-to-end fashion, the Cross Entropy (CE) loss from RGB branch will be merged with the BCE loss as:</p><formula xml:id="formula_6">L = L BCE (V,V ) + L CE (Y,? ),<label>(7)</label></formula><p>where V ,V , Y and? are the ground-truth and the predicted event volume, segmentation ground truth and prediction, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ISSAFE: Event-aware Domain Adaptation</head><p>The source RGB images have labels and the target images do not, but the event-based data of both domains are available. Thus, we propose Event-aware Domain Adaptation (EDA) to reduce the domain gap of normal and abnormal datasets by fusing the event-aware data, which can also explore the large-scale unlabeled accident images of our DADA-seg. Compared to textured RGB images, the monochromatic event data, capturing only changes of intensity, is semantically more consistent in both domains, that denotes the homogeneous event features and thus can serve as a bridge to assist the RGB modal DA in the feature level.   Based on this assumption, as shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, the ISSAFE-EDA consists of two branches, where the light-weight eventaware branch is the same as that in the aforementioned D2S fusion method and the RGB branch is constructed by the ResNet-101 <ref type="bibr" target="#b25">[26]</ref> backbone referring to the CLAN <ref type="bibr" target="#b11">[12]</ref> model. When the source RGB image X S has ground-truth label Y S , CE loss for the RGB branch G i in <ref type="figure" target="#fig_5">Fig. 4</ref> is:</p><formula xml:id="formula_7">L CE (G i ) = ? h,w c?C Y (h,w,c) S log G i (X (h,w,c) S ) + G e (X (h,w,c) S ) . (8)</formula><p>The source event-based data E S is learned from its ground truth V S via BCE in the event branch G e :</p><formula xml:id="formula_8">L BCE (G e ) = ? h,w b?B V (h,w,b) S log G e (E (h,w,b) S</formula><p>) . <ref type="formula">(9)</ref> The adversary loss between the RGB generator G i and discriminator D i according to the target image X T is:</p><formula xml:id="formula_9">L adv (G i , D i ) = ? E[log(D i (G i (X S ) + G e (X S )))] ? E[log(1 ? D i (G i (X T )))].<label>(10)</label></formula><p>The final loss L EDA of our ISSAFE-EDA model is combined from above 3 loss functions. Then, its training objective is:</p><formula xml:id="formula_10">G * i , G * e , D * i = arg min Gi,Ge max Di L EDA (G i , G e , D i ). (11)</formula><p>In this paper, we are making an early attempt to perform the cross-modal EDA from normal to abnormal scenes between two heterogeneous modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Since the synthesis of event data requires image pairs (I t?1 , I t ), two requirements to select a source dataset are: (i) the anchor image I t has semantic annotation; and (ii) the previous image I t?1 is available. Finally, Cityscapes <ref type="bibr" target="#b6">[7]</ref>, KITTI-360 <ref type="bibr" target="#b28">[29]</ref>, BDD <ref type="bibr" target="#b29">[30]</ref> and ApolloScape <ref type="bibr" target="#b30">[31]</ref> are selected for our experiments. The statistics of the datasets are described in <ref type="table" target="#tab_0">Table II</ref>. The ApolloScape and KITTI-360 datasets have semantic annotations for each frame of their video sequences. Therefore, we only sample one anchor image every 10 frames from the video sequence to prevent overfitting cases. As only partial anchor images have annotations in the BDD dataset, we filtered it out based on the aforementioned two conditions, and termed it as BDD3K in our work.</p><p>Since the category definition of ApolloScape is different, so those models trained on it perform segmentation with only 16 overlapping categories <ref type="bibr" target="#b30">[31]</ref>. As the target domain, our proposed DADA-seg dataset has 313 evaluation images from abnormal driving scenes, and the other unlabeled data were used to perform our EDA. All of these data including synthesized event-based data will be open-sourced to foster future research on accident scene understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Settings</head><p>For efficiency reasons, we choose ResNet-18 <ref type="bibr" target="#b25">[26]</ref> as the backbone and the main architecture from SwiftNet <ref type="bibr" target="#b5">[6]</ref>, which is also selected as the baseline model in this work. These models are trained with the Adam optimizer with a Learning Rate (LR) initialized to 4e-4 and dynamically adjusted by the cosine annealing LR scheduling strategy. The minimum LR of the last epoch is fixed in 1e-6. The weight decay of the LR is set to 1e-4. We use ImageNet pre-trained ResNet-18 to initialize the RGB encoder, and use the Kaiming initialization to initialize the whole Event branch as well as the decoder. For parameters of the pre-trained RGB encoder, we update them with a 4? smaller LR than the initialized parameters of the event encoder and the decoder, and apply a 4? smaller weight decay. The data augmentation operation includes a random scaling factor between 0.5 and 2, random horizontal flipping and random cropping with an output resolution of 1024?512. We trained the model for 200 epochs with a batch size of 4 per GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Gap</head><p>To quantitatively evaluate the robustness of semantic segmentation algorithms, accuracy-and efficiency-oriented models are tested on the target dataset, as shown in <ref type="table" target="#tab_0">Table III</ref>. For a fair comparison, when applicable, the results and model weights are provided by the respective publications. Overall, the large gaps show that SSA is still a challenging task for these top-performance models. As expected, although both large <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr">[</ref>    high accuracy in the source domain, they heavily depend on the consistency between the training and the testing data, which are all normal scenes. It thus hinders their generalization ability and leads to a large performance degradation once taken to the abnormal scenes. Nonetheless, this comparison indicates that higher performance in the source domain still benefits performance in the target domain in most cases. In subsequent subsections, we perform ablation studies to verify the effectiveness of our proposed methods for reducing the large gap and improving the robustness in accident scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation of EF</head><p>Quantitative Analysis. As shown in <ref type="table" target="#tab_0">Table IV</ref>, starting with event-only SwiftNet, where the event data are processed alone S2D without RGB image, the higher time bin B brings better performance, and attains the mIoU of 36.6% in the source domain and 19.8% in the target domain. This indicates that the event data has certain interpretability for the segmentation of driving scenes. As a baseline, we train the SwiftNet with RGB only from scratch, which obtains 20.1% mIoU in the target domain. Compared with it, our ISSAFE-S2D obtains an mIoU improvement of +2.9%, while maintaining better performance in the source domain. When the event data is used as auxiliary information of the RGB branch, the model is improved in the moderate event representation (B=2), because others are too few or sparse for the RGB image. Likewise, we implement the ISSAFE-D2S, which brings over +8.2% gain in the target domain when compared with the RGB-only baseline, meanwhile surpassing more than 10 state-of-the-art segmentation methods listed in <ref type="table" target="#tab_0">Table III</ref>. On Diverse Datasets. For extensive verification of fusing event-based data, we conduct comparisons on four different source datasets, where the contrastive results are shown in <ref type="table" target="#tab_7">Table V</ref>. In general, our ISSAFE-D2S is capable of improving the segmentation robustness by integrating eventbased data. Based on the diversity of data, our ISSAFE-D2S trained from BDD3K gains +4.7% on the DADAseg dataset, compared to the SwiftNet. On the KITTI-360 and ApolloScape datasets, our model has also considerable improvements in the target domain, as +2.4% and +2.8%,  respectively. In particular, the mIoU of our ISSAFE-D2S on the Merge3 dataset reaches the highest score with 32.4%. Overall, the results show that our proposal is consistently and significantly effective for enhancing the reliability of SSA. Qualitative Analysis. As it is shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, our ISSAFE-D2S model concentrates on the motion information, especially the foreground objects, such as the motorcycle and truck in the accident scenes. However, segmentation of night scenes is still challenging, although our method greatly benefits from event data, in contrast to the baseline. A case of the initial accident scene is presented as well. Our model can robustly segment the overturned car lying on the road after fence collision thanks to multi-modal cues.</p><p>Hence, the two data modalities are obviously complementary. When event cameras will not be triggered in static scenes, RGB cameras can perfectly capture the scene and provide sufficient textures. When RGB cameras puzzle over adverse scenes, i.e. fast-moving objects or low-lighting environments, the event camera can provide auxiliary information for robustifying semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiments of EDA</head><p>Evaluation Metrics. Our EDA is performed on two different levels, i.e., feature and/or image level. For a comprehensive quantitative analysis, we have adopted three different metrics <ref type="bibr" target="#b10">[11]</ref>, namely pixel accuracy (Acc), mean intersection over union (mIoU) and frequency weighted intersection over union (fwIoU), as shown in <ref type="table" target="#tab_0">Table VI</ref>.</p><p>Quantitative Analysis. Initially, the CLAN model adapted from virtual to real domain was tested directly on the DADA-seg dataset without any adjustments, also named sourceonly CLAN. Note that here a smaller resolution input can obtain higher accuracy in the target domain. There are two main reasons: images of DADA-seg are originally with low resolution, and a smaller resolution can obtain a larger receptive field with wider context understanding, which indicates that correct classification is more critical in accident scenes than delineating the boundaries. Afterwards, we train the CLAN model from scratch in Cityscapes and DADA-seg datasets to verify the feature-and feature-image-level DA, whereas the latter obtained the highest mIoU of 64.8% in the source domain. To distinguish and eliminate the impact of diverse DA strategies, we utilize the CycleGAN <ref type="bibr" target="#b31">[32]</ref> model to translate style of images from Cityscapes to DADAseg and perform image-level DA between the two domains, which is termed as i in <ref type="table" target="#tab_0">Table VI.</ref> As a result, our ISSAFE-EDA model obtains the highest performance in all three metrics on the DADA-seg dataset, and achieves the top accuracy of 30.0% in mIoU, 42.1% in Acc, and 64.5% in fwIoU at the higher resolution. In order to understand the impact of event-aware motion feature, we list the per-class IoU results of all 10 foreground classes in <ref type="table" target="#tab_0">Table VI</ref>. Those results demonstrate that the foreground classes can indeed benefit more from event data. Besides, compared to the CLAN model without using event data, the improvement of our cross-modal ISSAFE-EDA model is consistent with our assumptions that the monochromatic event data can serve as bridge to adapt textured images of two domains towards robust semantic segmentation.</p><p>Comparison with Optical Flow. We replace the event-based data with Optical Flow (OF). For a fair comparison based on the same sparsity of data, we only utilize the traditional Farneback <ref type="bibr" target="#b32">[33]</ref> method to generate optical flow data.</p><p>Our ISSAFE-OF model also obtains accuracy improvements, which further confirms our assumption regarding the effectiveness of motion features as complementary information for segmenting RGB images. However, our ISSAFE-EDA model can achieve better performance on the foreground classes. Although both data are synthesized, motion features with higher time resolution can still be extracted from event data to boost foreground segmentation. Besides, event cameras have a high dynamic range to enhance perception in lowlight conditions, which better conforms with our ISSAFE subject for improving road safety. Qualitative Analysis. Some semantic segmentation results generated by our ISSAFE-EDA model are presented in <ref type="figure" target="#fig_7">Fig. 6</ref>. These traffic accidents occur under different natural conditions (in day or night time), and include various collided objects (pedestrians, cyclists, or cars). All these qualitative studies help to throw insightful hints on how to obtain reliable perception in accident scenes for IV systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In the paper, we present a new task and its relevant evaluation dataset with pixel-wise annotations, which serves as a benchmark to assess the robustness and applicability of semantic segmentation algorithms. The main objective is to improve the segmentation performance of complex scenes in the application of intelligent vehicles, and ultimately reduce traffic accidents and ensure the safety of all traffic participants. As an initial solution, we have constructed the multi-modal segmentation model based on our ISSAFE architecture by fusing event-based data in different modes. Our experiments show that event data can provide complementary information under normal and extreme driving situations to enhance RGB images, such as fine-grained motion information and low-light sensitivity. Even though our experiments are somewhat limited by the use of synthetic events due to the lack of corresponding event data in common annotated datasets, we have observed consistent and large accuracy gains for models learned on multiple datasets including Cityscapes, KITTI-360, BDD and ApolloScape. Eventually, the SSA task is highly complicated and full of challenges that the current segmentation performance still has large development space. The unlabeled data in the DADA-seg dataset may be explored in future work through other learning paradigms, such as self-supervised and contrastive learning, so that we can gain more insights from the accident scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 Fig. 1 :</head><label>41</label><figDesc>Accident sequences from our DADA-seg dataset include diverse hazards (e.g. motion blur, overturns, back light, object occlusions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Visualization of generated event data in B ? H ? W space, which denote the time bins, image height and width. From left to right are RGB image pair (It?1, It), different event representations: event volume, event polarity frame and event grayscale frame, where blue and red colors indicate positive and negative events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where thet i is the embedded timestamp of event e i according to the time bins B + . When both positive and negative volumes are concatenated along time dimension, the entire volume is represented as V ? R B?W ?H , B = B + + B ? , where B, W and H are the total number of time bins, the width and height of spatial resolution, respectively. The detailed setting of time bins B ? {1, 2, 18} will be discussed in the experiments section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Model architectures of two different event fusion strategies. In (a), event data is fused to RGB branch adaptively from sparse to dense, while in (b) event data is extracted from dense image and learned from the sparse ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>RL</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Architecture of the ISSAFE-EDA model with event-aware fusion in D2S mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Contrastive examples of EF. The event data are presented as gray-scale frames. From top to bottom are accident scenes in different situations: motorist collision, car-truck collision, car collision at night time, and initial accident with an overturned car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Semantic segmentation results of our ISSAFE-EDA model on DADA-seg dataset. The columns correspond to the input images and output predictions of the sequence. These traffic accidents occur under different natural conditions (in day-or night-time), and include various collided objects (pedestrians, cyclists, and cars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Distribution of total 313 sequences from DADA-seg dataset under conditions in terms of light, weather and occasion.</figDesc><table><row><cell>DADA-seg</cell><cell>Light</cell><cell>Weather</cell><cell></cell><cell>Occasion</cell><cell></cell></row><row><cell></cell><cell>day night</cell><cell>sunny rainy</cell><cell cols="3">highway urban rural tunnel</cell></row><row><cell>#sequence</cell><cell>285 28</cell><cell>297 16</cell><cell>32</cell><cell>241 38</cell><cell>2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Statistics of datasets for experiments. The Merge3dataset is combined with the Cityscapes, KITTI-360 and BDD3K, since these three datasets have the identical label mapping.</figDesc><table><row><cell>Datasets</cell><cell>Cityscapes</cell><cell>KITTI-360</cell><cell cols="3">BDD3K ApolloScape Merge3</cell><cell>DADA-seg</cell></row><row><cell>#training</cell><cell>2,975</cell><cell>5,504</cell><cell>3,086</cell><cell>6,056</cell><cell>11,565</cell><cell>-</cell></row><row><cell>#evaluaion</cell><cell>500</cell><cell>612</cell><cell>343</cell><cell>673</cell><cell>14,555</cell><cell>313</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Performance gap of models, which are trained and validated on Cityscapes and then evaluated on DADA-seg, both with 1024?512 resolution.</figDesc><table><row><cell>Network</cell><cell>Backbone</cell><cell cols="3">Cityscapes DADA-seg mIoU Gap</cell></row><row><cell>PSPNet [1]</cell><cell>MobileNetV2</cell><cell>70.2</cell><cell>17.1</cell><cell>-52.5</cell></row><row><cell>ERFNet [5]</cell><cell>ResNet-18</cell><cell>72.1</cell><cell>9.0</cell><cell>-63.1</cell></row><row><cell>SwiftNet [6]</cell><cell>ResNet-18</cell><cell>75.4</cell><cell>20.5</cell><cell>-54,9</cell></row><row><cell cols="2">DeepLabV3+ [2] MobileNetV2</cell><cell>75.2</cell><cell>16.5</cell><cell>-58.7</cell></row><row><cell cols="2">DeepLabV3+ [2] ResNet-50</cell><cell>79.0</cell><cell>19.0</cell><cell>-60.0</cell></row><row><cell cols="2">DeepLabV3+ [2] ResNet-101</cell><cell>79.4</cell><cell>23.6</cell><cell>-55.8</cell></row><row><cell>DNL [4]</cell><cell>ResNet-50</cell><cell>79.3</cell><cell>15.7</cell><cell>-63.6</cell></row><row><cell>DNL [4]</cell><cell>ResNet-101</cell><cell>80.4</cell><cell>19.7</cell><cell>-60.7</cell></row><row><cell>OCRNet [3]</cell><cell>HRNetV2p-W18small</cell><cell>77.1</cell><cell>20.5</cell><cell>-56.6</cell></row><row><cell>OCRNet [3]</cell><cell>HRNetV2p-W18</cell><cell>77.7</cell><cell>23.8</cell><cell>-53.9</cell></row><row><cell>OCRNet [3]</cell><cell>HRNetV2p-W48</cell><cell>80.6</cell><cell>24.9</cell><cell>-55.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison of different event representations and event fusion approaches. All models use ResNet-18 as backbone and are tested with 1024?512 resolution.</figDesc><table><row><cell>Network</cell><cell>Input</cell><cell cols="2">Fusion Event data</cell><cell cols="2">Cityscapes DADA-seg</cell></row><row><cell>SwiftNet [6]</cell><cell>Event</cell><cell>-</cell><cell>B = 1</cell><cell>35.6</cell><cell>2.3</cell></row><row><cell>SwiftNet [6]</cell><cell>Event</cell><cell>-</cell><cell>B = 2</cell><cell>36.0</cell><cell>19.7</cell></row><row><cell>SwiftNet [6]</cell><cell>Event</cell><cell>-</cell><cell>B = 18</cell><cell>36.6</cell><cell>19.8</cell></row><row><cell>SwiftNet [6]</cell><cell>RGB</cell><cell>-</cell><cell>-</cell><cell>69.2</cell><cell>20.1</cell></row><row><cell cols="3">ISSAFE-S2D RGB+Event S2D</cell><cell>B = 1</cell><cell>68.3</cell><cell>16.7</cell></row><row><cell cols="3">ISSAFE-S2D RGB+Event S2D</cell><cell>B = 2</cell><cell>68.4</cell><cell>23.0</cell></row><row><cell cols="3">ISSAFE-S2D RGB+Event S2D</cell><cell>B = 18</cell><cell>67.1</cell><cell>10.4</cell></row><row><cell cols="3">ISSAFE-D2S RGB+Event D2S</cell><cell>B = 1</cell><cell>69.0</cell><cell>24.5</cell></row><row><cell cols="3">ISSAFE-D2S RGB+Event D2S</cell><cell>B = 2</cell><cell>69.4</cell><cell>28.3</cell></row><row><cell cols="3">ISSAFE-D2S RGB+Event D2S</cell><cell>B = 18</cell><cell>68.8</cell><cell>24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Comparison between different source datasets and the mIoU gain on the target dataset.</figDesc><table><row><cell>Dataset</cell><cell>Network</cell><cell cols="3">Source DADA-seg Gain</cell></row><row><cell>Cityscapes [7]</cell><cell>SwiftNet [6] ISSAFE-D2S</cell><cell>69.2 69.4</cell><cell>20.1 28.3</cell><cell>+8.2</cell></row><row><cell>BDD3K [30]</cell><cell>SwiftNet [6] ISSAFE-D2S</cell><cell>30.6 36.5</cell><cell>23.9 28.6</cell><cell>+4.7</cell></row><row><cell>KITTI-360 [29]</cell><cell>SwiftNet [6] ISSAFE-D2S</cell><cell>45.2 46.6</cell><cell>13.7 16.1</cell><cell>+2.4</cell></row><row><cell>ApolloScape [31]</cell><cell>SwiftNet [6] ISSAFE-D2S</cell><cell>61.8 58.8</cell><cell>16.7 19.5</cell><cell>+2.8</cell></row><row><cell>Merge3</cell><cell>SwiftNet [6] ISSAFE-D2S</cell><cell>50.3 61.4</cell><cell>28.5 32.4</cell><cell>+3.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc>Performance comparison of DA strategies, where f and i represent the feature and image level. The results of Source and Target are tested with 1024?512 resolution, while Foreground classes and Target ? are with 512?256. To clearly showcase the effect of event-aware branch, the per-class IoU(%) of ten foreground classes of Target result are listed: Traffic Light, Traffic Sign, Pedestrian, Rider, Car, Truck, Bus, Train, Motorcycle and Bicycle. Note that the target dataset does not have any Train. ISSAFE-EDA f + i 17.0 19.5 10.0 8.8 65.6 39.5 39.7 -6.1 7.0 48.2 33.1 68.2 73.2 63.9 87.5 42.1 30.0 64.5</figDesc><table><row><cell>Network</cell><cell>Level</cell><cell>Foreground classes</cell><cell>Target ?</cell><cell>Source</cell><cell>Target</cell></row><row><cell></cell><cell></cell><cell cols="4">TLi TSi Ped Rid Car Tru Bus Tra Mot Bic Acc mIoU fwIoU Acc mIoU fwIoU Acc mIoU fwIoU</cell></row><row><cell>CLAN [12]</cell><cell>-</cell><cell>15.2 5.3 4.0 3.4 32.6 8.8 28.8</cell><cell cols="3">-4.2 0.1 34.0 19.4 45.5 56.3 43.7 77.2 28.1 16.8 38.3</cell></row><row><cell>CLAN [12]</cell><cell>f</cell><cell>17.2 21.5 8.4 6.3 63.5 33.4 33.1</cell><cell cols="3">-3.7 6.2 46.3 31.7 67.2 70.4 62.4 87.0 40.1 28.8 63.8</cell></row><row><cell>CLAN [12]</cell><cell cols="2">f + i 17.0 20.0 9.4 5.2 64.3 36.8 35.9</cell><cell cols="3">-5.6 7.7 47.3 32.4 66.3 73.2 64.8 87.3 39.4 28.2 60.6</cell></row><row><cell>ISSAFE-OF</cell><cell cols="2">f + i 18.1 17.7 9.5 8.1 64.3 34.8 34.9</cell><cell cols="3">-5.1 7.3 48.3 33.4 69.6 71.6 62.9 87.4 40.9 29.2 64.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disentangled non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-ITS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">In defense of pretrained ImageNet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?ic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WildDash -Creating hazard-aware benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murschitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A 128?128 120 dB 15 ?s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbr?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSSC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Event-based vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">See clearer at night: towards robust nighttime semantic segmentation through day-night image conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain bridge for unpaired image-to-image translation and unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pizzati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaccaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cerri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE WACV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open compound domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time fusion network for RGB-D semantic segmentation incorporating unexpected obstacle detection for road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RA</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RTFNet: RGB-Thermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RA</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Optical flow augmented semantic segmentation networks for automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krizek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Helw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>VISAPP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">EV-FlowNet: Selfsupervised optical flow estimation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaney</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">EV-SegNet: Semantic segmentation for event-based cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01458</idno>
		<title level="m">DDD17: End-to-end DAVIS driving dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">EventGAN: Leveraging large scale image datasets for event cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DADA: Driver attention prediction in driving accident scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-ITS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High frame rate video reconstruction based on an event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to extract a video sequence from a single motion-blurred image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic instance annotation of street scenes by 3d to 2d label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BDD100K: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The ApolloScape dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
