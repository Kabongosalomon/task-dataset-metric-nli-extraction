<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Gu</surname></persName>
							<email>jqgu@utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViTs) have emerged with superior performance on computer vision tasks compared to convolutional neural network (CNN)-based models. However, ViTs are mainly designed for image classification that generate single-scale low-resolution representations, which makes dense prediction tasks such as semantic segmentation challenging for ViTs. Therefore, we propose HRViT, which enhances ViTs to learn semantically-rich and spatially-precise multi-scale representations by integrating high-resolution multi-branch architectures with ViTs. We balance the model performance and efficiency of HRViT by various branch-block co-optimization techniques. Specifically, we explore heterogeneous branch designs, reduce the redundancy in linear layers, and augment the attention block with enhanced expressiveness. Those approaches enabled HRViT to push the Pareto frontier of performance and efficiency on semantic segmentation to a new level, as our evaluation results on ADE20K and Cityscapes show. HRViT achieves 50.20% mIoU on ADE20K and 83.16% mIoU on Cityscapes, surpassing state-of-the-art MiT and CSWin backbones with an average of +1.78 mIoU improvement, 28% parameter saving, and 21% FLOPs reduction, demonstrating the potential of HRViT as a strong vision backbone for semantic segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dense prediction tasks such as semantic segmentation are important computer vision workloads on emerging intelligent computing platforms, e.g., AR/VR devices. Convolutional neural networks (CNNs) have rapidly evolved with significant performance improvement in semantic segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Beyond classical CNNs, vision Transformers (ViTs) have emerged with competitive performance in computer vision tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref>. Benefiting from the self-* Work done during an internship at Meta Reality Labs. attention operations, ViTs embrace strong expressivity with long-distance information interaction and dynamic feature aggregation. However, ViT <ref type="bibr" target="#b11">[12]</ref> produces single-scale and low-resolution representations, which are not friendly to semantic segmentation that requires high position sensitivity and fine-grained image details.</p><p>To cope with the challenge, various ViT backbones that yield multi-scale representations were proposed for semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. However, they still follow a classification-like network topology with a sequential or series architecture. Based on complexity consideration, they gradually downsample the feature maps to extract higher-level low-resolution (LR) representations and directly feed each stage's output to the downstream segmentation head. Such sequential structures lack enough crossscale interaction thus cannot produce high-quality highresolution (HR) representations.</p><p>HRNet <ref type="bibr" target="#b26">[27]</ref> was proposed to solve the problem outside of ViT context, which enhances the cross-resolution interaction with a multi-branch architecture maintaining all resolutions throughout the network. HRNet extracts multiresolution features in parallel and fuses them repeatedly to generate high-quality HR representations with rich semantic information. Such a design concept has achieved great success in various dense prediction tasks. Nevertheless, its expressivity is limited by small receptive fields and strong inductive bias from cascaded convolution operations. To deal with the challenge, some HRNet variants such as Lite-HRNet <ref type="bibr" target="#b34">[35]</ref> and HR-NAS <ref type="bibr" target="#b9">[10]</ref> are proposed. However, those improved HRNet designs are still mainly based on the convolutional building blocks, and their demonstrated performance on semantic segmentation is still far behind the SoTA scores of ViT counterparts.</p><p>Therefore, synergistically integrating HRNet with ViTs is an approach to be explored for further performance improvement. By combining those two approaches, ViTs can obtain rich multi-scale representability from the HR architecture, while HRNet can gain a larger receptive field from the attention operations. However, migrating the success of HRNet to ViT backbones is non-trivial. Given arXiv:2111.01236v2 [cs.CV] 23 Nov 2021 the high complexity of multi-branch HR architectures and self-attention operations, simply replacing all convolutional residual blocks in HRNet with Transformer blocks will encounter severe scalability issues. The inherited high representation power from multi-scale can be overwhelmed by the prohibitive latency and energy cost on hardware without careful architecture-block co-optimization. Therefore, we propose HRViT, an efficient multi-scale high-resolution vision Transformer backbone specifically optimized for semantic segmentation. HRViT enables multi-scale representation learning in ViTs and improves the efficiency based on the following approaches: <ref type="bibr" target="#b0">(1)</ref> HRViT's multi-branch HR architecture extracts multi-scale features in parallel with cross-resolution fusion to enhance the multi-scale representability of ViTs; (2) HRViT's augmented local self-attention removes redundant keys and values for better efficiency and enhances the model expressivity with extra parallel convolution paths, additional nonlinearity units, and auxiliary shortcuts for feature diversity enhancement; (3) HRViT adopts mixed-scale convolutional feedforward networks to fortify the multi-scale feature extraction; (4) HRViT's HR convolutional stem and efficient patch embedding layers maintain more low-level fine-grained features with reduced hardware cost. Also, distinguished from the HRNet-family, HRViT follows a unique heterogeneous branch design to balance efficiency and performance, which is not simply an improved HRNet or a direct ensemble of HRNet and self-attention but a new topology of pure ViTs mainly constructed by self-attention with careful branch-block co-optimization.</p><p>Based on the approaches in HRViT, we make the following contributions:</p><p>? We deeply investigate the multi-scale representation learning in vision Transformers and propose HRViT that integrates multi-branch high-resolution architectures with vision Transformers. ? To enhance the efficiency of HRViT for scalable HR-ViT integration, we propose a set of approaches as follows: exploiting the redundancy in Transformer blocks, developing performance-efficiency co-optimized building blocks, and adopting heterogeneous branch designs. ? We evaluate HRViT on ADE20K and Cityscapes and present results that push the Pareto frontier of performance and efficiency forward as follows:</p><p>HRViT achieves 50.20% mIoU on ADE20K val and 83.16% mIoU on Cityscapes val for semantic segmentation tasks, outperforming state-of-the-art (SoTA) MiT and CSWin backbones with 1.78 higher mIoU, 28% fewer parameters, and 21% lower FLOPs, on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed HRViT Architecture</head><p>Recent advances in vision Transformer backbone designs mainly focus on attention operator innovations. A new topology design can create another dimension to unleash the potential of ViTs with even stronger vision expressivity. Extending the sequential topology of ViTs to the multi-branch structure, inspired by HRNet, is a promising approach for performance improvement. An important question that remains to be answered is whether the success of HRNet can be efficiently migrated to ViT backbones to consolidate their leading position in dense prediction tasks such as semantic segmentation.</p><p>In this section, we delve into the multi-scale representation learning in ViTs and introduce an efficient integration of the HR architecture and Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture overview</head><p>As illustrated in <ref type="figure">Figure 1</ref>, the first part of HRViT consists of a convolutional stem to reduce spatial dimensions while extracting low-level features. After the convolutional stem, HRViT deploys four progressive Transformer stages where the n-th stage contains n parallel multi-scale Transformer branches. Each stage can have one or more modules. Each module starts with a lightweight dense fusion layer to achieve cross-resolution interaction and an efficient patch embedding block for local feature extraction, followed by repeated augmented local self-attention blocks (HRViTAttn) and mixed-scale convolutional feedforward networks (MixCFN). Unlike sequential ViT backbones that progressively reduce the spatial dimension to generate pyramid features, we maintain the HR features throughout the network to strengthen the quality of HR representations via cross-resolution fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Efficient HR-ViT integration with heterogeneous branch design</head><p>We design a heterogeneous multi-branch architecture for efficient multi-scale high-resolution ViTs. A straightforward choice to fuse HRNet and ViTs is to replace all convolutions in HRNet with self-attentions. However, given the high complexity of multi-branch HRNet and self-attention operators, this brute-force combining will quickly cause an explosion in memory footprint, parameter size, and computational cost. Therefore, we need to carefully design the model architecture and building blocks to enable scalable and efficient HR-ViT integration. We discuss our approaches to achieve the scalability and efficiency next. Heterogeneous branch configuration. For the branch architecture in HRViT, we need to determine the number of Transformer blocks assigned for each branch. Simply assigning the same number of blocks with the same local self-attention window size on each module will result in intractably large computational costs. Therefore, we analyze the functionality and cost of each branch in <ref type="table">Table 1</ref>, and we propose a simple design heuristic based on the analysis. We analyze (1) the number of parameters and (2) the number of floating-point operations (FLOPs) in HRViTAttn and MixCFN blocks on the i-th branch (i = 1, 2, 3, 4) as follows:</p><formula xml:id="formula_0">ParamsHRViTAttn,i = O(4 i?1 C 2 + 2 i?1 C), ParamsMixCFN,i = O(4 i?1 C 2 ri + 2 i?1 Cri), FLOPsHRViTAttn,i = O HW C 2 + CHW (H +W )si 4 i?1 , FLOPsMixCFN,i = O riHW C 2 + riHW C 2 i?1 .<label>(1)</label></formula><p>We use Equation 1 to compare the memory cost, the computation, the number of parameters, and computation in <ref type="table">Table 1</ref>.</p><p>Based on the complexity analysis, we observe that the first and second HR branches (i = 1, 2) involve a high memory and computational cost. Hence, those HR branches typically can not afford a large enough receptive field for image-level classification. On the other hand, they are parameter-efficient and able to provide fine-grained detail calibration in segmentation tasks. Thus, we use a narrow attention window size and use a minimum number of blocks on two HR paths.</p><p>We observe that the most important branch is the third one with a medium resolution (MR). Given its medium hardware cost, we can afford a deep branch with a large window size on the MR path to provide large receptive fields and well-extracted high-level features.</p><p>The lowest resolution (LR) branch contains most parameters and is very useful to provide high-level features with a global receptive field to generate coarse segmentation maps. However, its small spatial sizes result in too much loss of image details. Therefore, we only deploy a few blocks with a large window size on the LR branch to improve high-level feature quality under parameter budgets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearly-even block assignment. A unique problem in</head><p>HRViT is to determine how to assign blocks to each module In HRViT, we need to assign 20 blocks to 4 modules on the 3rd path. To maximize the average depth of the network ensemble and help input/gradient flow through the deep Transformer branch, we employ a nearly-even partitioning, e.g., 6-6-6-2, and exclude an extremely unbalanced assignment, e.g., 17-1-1-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Efficient HRViT component design</head><p>We discussed how we enable HR-ViT integration with heterogeneous branch design. Next, we discuss how to further push the efficiency and performance boundary via block optimization. Augmented cross-shaped local self-attention. To achieve high performance with improved efficiency, a hardwareefficient self-attention operator is necessary. We adopt one of the SoTA efficient attention designs, cross-shaped selfattention <ref type="bibr" target="#b10">[11]</ref>, as our baseline attention operator. Based on that, we design our augmented cross-shaped local selfattention HRViTAttn illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, which provides the following benefits: (1) Fine-grained attention: Compared to globally-downsampled attentions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>, this one has fine-grained feature aggregation that preserves detailed information. (2) Approximate global view: By using two parallel orthogonal local attentions, this attention can To balance the performance and efficiency, we introduce our augmented version, denoted as HRViTAttn, with several key optimizations. In <ref type="figure" target="#fig_0">Figure 2a</ref>, we follow the crossshaped window partitioning approach in CSWin that separates the input x ? R H?W ?C into two parts {x H , x V ? R H?W ?C/2 }. x H is partitioned into disjoint horizontal windows, and the other half x V is chunked into vertical windows. The window is set to s ? W or H ? s. Within each window, the patch is chunked into K d k -dimensional heads, then a local self-attention is applied,</p><formula xml:id="formula_1">HRViTAttn(x) = BN ?(W O [y1, ? ? ? , y k , ? ? ? , yK ]) y k = z k + DWConv ?(W V k x) [z 1 k , ? ? ? , z M k ] = z k = H-Attn k (x), 1 ? k &lt; K/2 V-Attn k (x), K/2 ? k ? K z m k = MHSA(W Q k x m , W K k x m , W V k x m ) [x 1 , ? ? ? , x m , ? ? ? , x M ] = x, x m ? R (H/s)?W ?C ,<label>(2)</label></formula><p>where W Q k , W K k , W V k ? R d k ?C are projection matrices to generate query Q k , key K k , and value V k tensors for the k-th head, W O ? R C?C is the output projection matrix, and ? is Hardswish activation. If the image sizes are not a multiple of window size, e.g., s H/s &gt; H, we apply zero-padding to inputs x H or x V to allow a complete K-th window, shown in <ref type="figure" target="#fig_0">Figure 2b</ref>. Then the padded region in the attention map is masked to 0 to avoid incoherent semantic correlation.</p><p>The original QKV linear layers are quite costly in computation and parameters. We share the linear projections for key and value tensors in HRViTAttn to save computation and parameters as follows,</p><formula xml:id="formula_2">MHSA(W Q k x m , W V k x m , W V k x m ) = softmax Q m k (V m k ) T ? d k V m k ,<label>(3)</label></formula><p>In addition, we introduce an auxiliary path with parallel depth-wise convolution to inject inductive bias to facilitate training. Unlike the local positional encoding in CSWin, our parallel path is nonlinear and applied on the entire 4-D feature map W V x without window-partitioning. This path can be treated as an inverted residual module sharing pointwise convolutions with the linear projection layers in selfattention. This shared path can effectively inject inductive bias and reinforce local feature aggregation with marginal hardware overhead.</p><p>As a performance compensation for the above key-value sharing, we introduce an extra Hardswish function to improve the nonlinearity. We also append a BatchNorm (BN) layer that is initialized to an identity projection to stabilize the distribution for better trainability. Motivated by recent studies on the importance of shortcuts in ViTs <ref type="bibr" target="#b22">[23]</ref>, we add a channel-wise projector as a diversity-enhanced shortcut (DES). Unlike the augmented shortcut <ref type="bibr" target="#b24">[25]</ref>, our shortcut has higher nonlinearity and does not depend on hardwareunfriendly Fourier transforms. The projection matrix in our DES P C?C is approximated by Kronecker decompo-</p><formula xml:id="formula_3">sition P = A ? C? ? C ? B ? C? ? C to minimize parameter</formula><p>cost. Then we fold x asx ? R HW ? ? C? ? C and convert (A ? B)x into (AxB T ) to save computations. We further insert Hardswish after the B projection to increase the nonlinearity,</p><formula xml:id="formula_4">DES(x) = A ? Hardswish(xB T ).<label>(4)</label></formula><p>Mixed-scale convolutional feedforward network. Inspired by the MixFFN in MiT <ref type="bibr" target="#b32">[33]</ref> and multi-branch inverted residual blocks in HR-NAS <ref type="bibr" target="#b9">[10]</ref>, we design a mixedscale convolutional FFN (MixCFN) by inserting two multiscale depth-wise convolution paths between two linear layers, shown in <ref type="figure" target="#fig_1">Figure 3</ref>. After LayerNorm, we expand the channel by a ratio of r, then split it into two branches. The 3?3 and 5?5 depth-wise convolutions (DWConvs) are used to increase the multi-scale local information extraction of HRViT. For efficiency consideration, we exploit the channel redundancy by reducing the MixCFN expansion ratio r from 4 <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref> to 2 or 3 with marginal performance loss on medium to large models.  Downsampling stem. In semantic segmentation tasks, images are of high resolution, e.g., 1024?1024. Self-attention operators are known to be expensive as their complexity is quadratic to image sizes. To address the scalability issue when processing large images, we down-sample the inputs by 4? before feeding into the main body of HRViT. We do not use attention operations in the stem since early convolutions are more effective to extract low-level features than self-attentions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref>. As early convolutions, we follow the design in HRNet and use two stride-2 CONV-BN-ReLU blocks as a stronger downsampling stem to extract C-channel features with more information maintained, unlike prior ViTs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> that used a stride-4 convolution. Efficient patch embedding. Before Transformer blocks in each module, we add a patch embedding block (CONV-LayerNorm) on each branch, which is used to match channels and extract patch information with enhanced interpatch communication. However, the patch embedding layers have a non-trivial hardware cost in the HR architecture since each module at stage-n will have n embedding blocks. Therefore, we simplify the patch embedding to be a pointwise CONV followed by a depth-wise CONV <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_5">EffPatchEmbed(x) = LN DWConv(PWConv(x)) . (5)</formula><p>Cross-resolution fusion layer. The cross-resolution fusion layer is critical for HRViT to learn high-quality HR representations, shown in <ref type="figure" target="#fig_2">Figure 4</ref>. To enhance cross-resolution interaction, we insert repeated cross-resolution fusion layers at the beginning of each module following the approach in HRNet <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>To help LR features maintain more image details and precise position information, we merge them with downsampled HR features. Instead of using a progressive convolution-based downsampling path to match tensor shapes <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>, we employ a direct down-sampling path to minimize hardware overhead. In the down-sampling path between the i-th input and j-th output (j &gt; i), we use a depth-wise separable convolution with a stride of 2 j?i to shrink the spatial dimension and match the output channels. The kernel size used in the DWConv is (2 j?i +1) to create patch overlaps. Those HR paths inject more image information into the LR path to mitigate information loss and On the other hand, the receptive field is usually limited in the HR blocks as we minimize the window size and branch depth on HR paths. Hence, we merge LR representations into HR paths to help them obtain higher-level features with a larger receptive field. Specifically, in the up-scaling path (j &lt; i), we first increase the number of channels with a point-wise convolution and up-scale the spatial dimension via a nearest neighbor interpolation with a rate of 2 i?j . When i=j, we directly pass the features to the output as a skip connection. Note that in HR-NAS <ref type="bibr" target="#b9">[10]</ref>, the dense fusion is simplified by a sparse fusion module where only neighboring resolutions are merged. This technique is not considered in HRViT since it saves marginal hardware cost but leads to a noticeable accuracy drop, which will be discussed in subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Architectural variants</head><p>As shown in <ref type="table">Table 2</ref> with three design variants of HRViT, variants of HRViT scale in both network depth and width. We follow the aforementioned design guidance and evenly assign 5-6 Transformer blocks to HR branches, 20-24 blocks to the MR branch, and 4-6 blocks to the LR branch. Window sizes are set to (1,2,7,7) for 4 branches. We use relatively large MixCFN expansion ratios in small variants for higher performance and reduce the ratio to 2 on larger variants for better efficiency. We gradually follow the scaling rule from CSWin <ref type="bibr" target="#b10">[11]</ref> to increase the basic channel C for the highest resolution branch from 32 to 64. #Blocks and #channels can be flexibly tuned for the 3rd/4th branch to match a specific hardware cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We pretrain all models on ImageNet-1K <ref type="bibr" target="#b8">[9]</ref> and conduct experiments on ADE20K <ref type="bibr" target="#b42">[43]</ref> and Cityscapes <ref type="bibr" target="#b6">[7]</ref> for semantic segmentation. We compare the performance and efficiency of our HRViT with SoTA ViT backbones, i.e., Swin <ref type="bibr" target="#b18">[19]</ref>, Twins <ref type="bibr" target="#b4">[5]</ref>, MiT <ref type="bibr" target="#b32">[33]</ref>, and CSWin <ref type="bibr" target="#b10">[11]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic segmentation on ADE20K and Cityscapes</head><p>On semantic segmentation, HRViT achieves the best performance-efficiency Pareto front, surpassing the SoTA MiT and CSWin backbones. HRViT (b1-b3) outperform the previous SoTA SegFormer-MiT (B1-B3) <ref type="bibr" target="#b32">[33]</ref> with +3.68, +2.26, and +0.80 higher mIoU on ADE20K val, and +3.13, +1.81, +1.46 higher mIoU on Cityscapes val. ImageNet-1K pre-training.</p><p>All HRViT variants are pre-trained on ImageNet-1K, shown in <ref type="table">Table 3</ref>. We follow the same pre-training settings as DeiT <ref type="bibr" target="#b25">[26]</ref> and other ViTs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>. We adopt stochastic depth <ref type="bibr" target="#b15">[16]</ref> for all HRViT variants with the max drop rate of 0.1. The drop rate is gradually increased on the deepest 3rd branch, and other shallow branches follow the rate of the 3rd branch within the same module. We use the HRNetV2 <ref type="bibr" target="#b26">[27]</ref> classification head in HRViT on ImageNet-1K pre-training. More details can be found in Appendix A.1. Settings. We evaluate HRViT for semantic segmentation on the Cityscapes and ADE20K datasets. We employ a lightweight SegFormer <ref type="bibr" target="#b32">[33]</ref> head based on the mmsegmentation framework <ref type="bibr" target="#b5">[6]</ref>. We follow the training settings of prior work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>. The training image size for ADE20K and Cityscapes are 512?512 and 1024?1024, respectively. The test image size for ADE20K and Cityscapes is set to 512?2048 and 1024?2048, respectively. We do inference on Cityscapes with sliding window test by cropping 1024?1024 patches. More details are in Appendix A.2.</p><p>Results on ADE20K. We evaluate different ViT backbones in single-scale mean intersection-over-union (mIoU), #Params, and GFLOPs. <ref type="figure">Figure 5</ref> plots the Pareto curves in the #Params and FLOPs space. On ADE20K val, HRViT outperforms other ViTs with better performance and efficiency trade-off. For example, with the SegFormer head, HRViT-b1 outperforms MiT-B1 with 3.68% higher mIoU, 40% fewer parameters, and 8% less computation. Our HRViT-b3 achieves a higher mIoU than the best CSWin-S but saves 23% parameters and 13% FLOPs. Compared with HRNetV2+OCR, our HRViT shows considerable performance and efficiency advantages. We also evaluate HRViT with UperNet <ref type="bibr" target="#b30">[31]</ref> head in Appendix B.1.</p><p>Results on Cityscapes. We summarize the results on Cityscapes in <ref type="table" target="#tab_4">Table 4</ref>. Our small model HRViT-b1 outperforms MiT-B1 and CSWin-Ti by +3.13 and +2.47 higher mIoU. The key insight is that the HR architecture can increase the effective width of models with narrow channels, leading to higher modeling capacity. Hence, the parallel multi-branch topology is especially beneficial for small networks. When training HRViT-b3 on Cityscapes, we set the window sizes to 1-2-9-9. HRViT-b3 outperforms the MiT-b4 with +0.86 higher mIoU, 55.4% fewer parameters, and 30.7% fewer FLOPs. Compared with two SoTA ViT backbones, i.e., MiT and CSWin, HRViT achieves an average of +2.16 higher mIoU with 30.7% fewer parameters and 23.1% less computation. MiT+SegFormer <ref type="bibr" target="#b32">[33]</ref> CSWin+FPN <ref type="bibr" target="#b10">[11]</ref> FCN-R50 <ref type="bibr" target="#b19">[20]</ref> PVT+FPN <ref type="bibr" target="#b27">[28]</ref> HRNet-W48+OCR <ref type="bibr" target="#b26">[27]</ref> DeepLabV3+R101 <ref type="bibr" target="#b3">[4]</ref> Swin <ref type="bibr" target="#b18">[19]</ref> b1 b2 b3 CSWin+SegFormer <ref type="bibr" target="#b10">[11]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HRViT (ours)</head><p>Twins <ref type="bibr" target="#b4">[5]</ref> MiT+SegFormer <ref type="bibr" target="#b32">[33]</ref> CSWin+FPN <ref type="bibr" target="#b10">[11]</ref> FCN-R50 <ref type="bibr" target="#b19">[20]</ref> PVT+FPN <ref type="bibr" target="#b27">[28]</ref> HRNet-W48+OCR <ref type="bibr" target="#b26">[27]</ref> DeepLabV3 +R101 <ref type="bibr" target="#b3">[4]</ref> Swin <ref type="bibr" target="#b18">[19]</ref> b1 b2 b3 CSWin+SegFormer <ref type="bibr" target="#b10">[11]</ref> HRViT (ours) Twins <ref type="bibr" target="#b4">[5]</ref> mIoU MiT-B1 42.20</p><p>CSWin-Ti 41.43</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HRViT-b1 45.88</head><p>MiT-B2 46.50</p><p>CSWin-T 47.88</p><p>HRViT-b2 48.76</p><p>MiT-B3 49.40</p><p>CSWin-S 49.93</p><p>HRViT-b3 50.20</p><p>SegFormer Head <ref type="figure">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation studies</head><p>In <ref type="table">Table 5</ref>, we first compare with a baseline where all block optimization techniques are removed. Our proposed key techniques can synergistically improve the ImageNet accuracy by 0.73% and the Cityscapes mIoU by +1.18 with 20% fewer parameters and 13% less computation. Then we independently remove each technique from HRViT to validate their individual contribution.</p><p>Sharing key-value. When removing key-value sharing, i.e., using independent keys and values, HRViT-b1 shows the same ImageNet-1K accuracy but at the cost of lower Cityscapes segmentation mIoU, 9% more parameters, and 4% more computations. Patch embedding. Changing our EffPathEmbed to the CONV-based counterpart <ref type="bibr" target="#b32">[33]</ref> leads to 22% more parameters and 17% more FLOPs without accuracy/mIoU benefits. ware overhead is negligible due to the high efficiency of the Kronecker decomposition-based projector. Vanilla HRNet-ViT baselines vs. HRViT. In <ref type="table">Table 6</ref>, we directly replace residual blocks in HRNetV2 with MiT/C-SWin Transformer blocks, which we refer to as a vanilla baseline. When comparing HRNet-MiT with the sequential MiT, we notice the HR variants have comparable mIoUs while significantly saving hardware cost. This shows that the multi-branch architecture is indeed helpful to boost the multi-scale representability. However, the vanilla HRNet-ViT baseline overlooks the expensive cost of Transformers and is not efficient as the hardware cost quickly outweighs its performance gain. In contrast, HRViT benefits from heterogeneous branches and optimized components with less computation, fewer parameters, and enhanced model representability than the vanilla HRNet-ViT baselines. Different window sizes. In <ref type="table">Table 7</ref>, we evaluate HRViT-b3 on Cityscapes with different window sizes on the 3rd (MR) and 4th (LR) paths. In general, different window sizes give similar mIoUs, while window sizes of 7 and 9 show the best performance-efficiency trade-off. Increasing the window size from 7 to 9 helps HRViT-b3 achieve +0.34 mIoU improvement with only 0.8% more FLOPs. However, overly-large window sizes bring no performance benefits with unnecessary computation overhead. For example, further enlarging the window size from 9 to 15 causes 0.26 mIoU drop and 3.7% more FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Multi-scale representation learning for semantic segmentation. Previous segmentation frameworks progressively down-sample the feature map to compute the LR representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref>, and recover the HR features via up-sampling, e.g., SegNet <ref type="bibr" target="#b0">[1]</ref>, UNet <ref type="bibr" target="#b23">[24]</ref>, Hourglass <ref type="bibr" target="#b21">[22]</ref>. HRNet <ref type="bibr" target="#b26">[27]</ref> maintains the HR representations throughout the network with cross-resolution fusion. Lite-HRNet <ref type="bibr" target="#b34">[35]</ref> proposes conditional channel weighting blocks to exchange information across resolutions. HR-NAS <ref type="bibr" target="#b9">[10]</ref> searches the channel/head settings for inverted residual blocks and the auxiliary Transformer branches. HRFormer <ref type="bibr" target="#b37">[38]</ref> improves HRNetV2 by replacing residual blocks with Swin Transformer blocks. Different from the convolutional HRNetfamily, HRViT is a pure ViT backbone with a novel multibranch topology that benefits both from HR architectures and self-attentions. Distinguished from the direct CONVto-Attention substitution in HRFormer, we explore a novel heterogeneous branch design and various block optimization techniques with higher performance and efficiency. Multi-scale ViT backbones. Several multi-scale ViTs adopt hierarchical architectures to generate progressively down-sampled pyramid features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref>. For example, PVT <ref type="bibr" target="#b27">[28]</ref> integrates a pyramid structure into ViTs for multi-scale feature extraction. Twins <ref type="bibr" target="#b4">[5]</ref> interleaves local and global attentions to learn multi-scale representations. SegFormer <ref type="bibr" target="#b32">[33]</ref> proposes an efficient hierarchical encoder to extract coarse and fine features. CSWin <ref type="bibr" target="#b10">[11]</ref> further improves the performance with multi-scale crossshaped local attentions. However, they still follow the design concept of classification networks with a sequential topology. There is no information flow from LR to HR path inside those sequential ViTs, and the HR features are still very shallow ones of relatively low quality. In contrast, our HRViT adopts a multi-branch topology with enhanced multi-scale representability and improved efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we delve into the multi-scale representation learning in ViTs and present an efficient multi-scale high-resolution ViT backbone design, named HRViT, for semantic segmentation. We enhance ViTs with a multibranch architecture to learn high-quality HR representations via cross-scale interaction. To scale up HRViT with high efficiency, we introduce heterogeneous branch designs and jointly optimize key building blocks with efficient embedding layers, augmented cross-shaped attentions, and mixedscale convolutional FFNs. In our evaluation, we observe that the multi-branch architecture can effectively boost the semantic segmentation performance of ViTs. Besides, we find that branch-block co-optimization is the key to improving the efficiency of HR-ViT integration. Experiments show that HRViT outperforms SoTA ViT backbones on semantic segmentation with significant performance improvement and efficiency boost. As a future direction, we look forward to evaluating HRViT on more dense prediction vision tasks, e.g., object detection, to thoroughly demonstrate the potential of HRViT as a strong vision backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Image classification on ImageNet-1K</head><p>We pre-train HRViT on ImageNet-1K for image classification. To generate logits for classification, we append a classification head from HRNetV2 at the end of the HRViT backbone. Four multi-scale outputs from HRViT are fed into convolutional bottleneck blocks, and the channels are mapped to 128, 256, 512, and 1024. Then, we use stride-2 CONV3x3 to down-sample the highest resolution by 2? and double the channels by 2?, and we add it to the next smaller resolution. We repeat this process until we have the smallest resolution features. Finally, we project the 1024channel feature to 2048 channels via CONV1x1, followed by a global average pooling and a linear classifier.</p><p>We adopt a default image resolution of 224?224 and train HRViT with AdamW <ref type="bibr" target="#b20">[21]</ref> optimizer for 300 epochs using a cosine learning rate decay schedule, 20 epochs of linear warm-up, and an initial learning rate of 1e-3, a minibatch size of 1,024, a weight decay rate of 0.05. We set the weight decay rate for BatchNorm layers to 0. We employ gradient clipping with a maximum magnitude of 1. We employ label smoothing with a rate of 0.1 and various data augmentation methods used in DeiT <ref type="bibr" target="#b25">[26]</ref>, including Ran-dAugment <ref type="bibr" target="#b7">[8]</ref>, Cutmix <ref type="bibr" target="#b38">[39]</ref>, Mixup <ref type="bibr" target="#b39">[40]</ref>, and random erasing <ref type="bibr" target="#b41">[42]</ref>. Note that model EMA and repeated augmentation are not employed here. We use a stochastic drop path rate of 0.1 for all backbones. For the classification head in HRViT, we use a dropout rate of 0.1 for all variants. Models are trained on 32 NVIDIA V100 GPUs with 32 images per GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Semantic segmentation on ADE20K and Cityscapes</head><p>ADE20K <ref type="bibr" target="#b42">[43]</ref> is a semantic segmentation dataset with 150 semantic categories. It contains 25K images in total, including 20K images for training, 2K for validation, and the rest 3K for testing. Cityscapes <ref type="bibr" target="#b6">[7]</ref> dataset contains 5000 fine-annotated high-resolution images with 19 categories. The training image size for ADE20K and Cityscapes are cropped to 512?512 and 1024?1024, respectively. For data augmentation, we use random horizontal flipping, random re-scaling within the ratio range of [0.5, 2.0], and random photometric distortion. On ADE20K, the stochastic drop path rates are set to 0.1, 0.1, 0.25 for HRViT-b1, HRViT-b2, and HRViT-b3, respectively. On Cityscapes, the stochastic drop path rates are set to 0.15, 0.15, 0.25 for HRViT-b1, HRViT-b2, and HRViT-b3, respectively. We use an AdamW optimizer for 160 k iterations using a 'poly' learning rate schedule, 1,500 steps of linear warm-up, an initial learning rate of 6e-5, and a weight decay rate of 0.01. The mini-batch size is set to 16 and 8 for ADE20K and Cityscapes, respectively. We set the weight decay rate for BatchNorm layers to 0 and increase the initial learning rate for the segmentation head by 10?. Models on ADE20K are trained on 8 NVIDIA V100 GPUs with 2 images per GPU. Models on Cityscapes are trained on 8 NVIDIA V100 GPUs with 1 image per GPU. During inference, the image size for ADE20K val and Cityscapes val is set to 512?2048 and 1024?2048, respectively. We do inference on Cityscapes with sliding window test by cropping 1024?1024 patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More experiments B.1. Semantic segmentation on ADE20K with Uper-Net head</head><p>Besides the lightweight SegFormer head, UperNet <ref type="bibr" target="#b30">[31]</ref> is another segmentation framework that is widely used. We evaluate HRViT on ADE20K val with SegFormer and UperNet head in <ref type="table" target="#tab_7">Table 9</ref>. Our HRViT can mostly maintain the advantages, but the computation benefits are not as significant as with the SegFormer head. The reason is that the UperNet head dominates the computations (&gt;89%) and parameters (&gt;54%) in the cost breakdown. Hence any slimming on backbones will be considerably diluted. Moreover, we observe similar performance with lightweight SegFormer head and heavy UperNet head on HRViT. Moreover, we do not observe more performance benefits from UperNet head than the SegFormer head on HRViT. One explanation is that HRViT already has enough multi-resolution fusion, which makes the additional pyramid fusion in UperNet head less effective than used in the sequential architectures. Hence, the lightweight SegFormer head is more suitable to HRViT. In <ref type="table" target="#tab_6">Table 8</ref>, we compare different block assignment strategies on the 3rd low-resolution path in HRViT-b1 on ImageNet-1K and Cityscapes. We observe a clear trend that when concentrating more blocks in one module, e.g., from 6-6-6-2 to 17-1-1-1, the benefits from the crossresolution fusion in the HR architecture diminish accordingly, leading to degraded ImageNet classification accuracy and Cityscapes segmentation mIoU. This phenomenon can be attributed to the information loss in the deep LR module, SegFormer Head <ref type="bibr" target="#b32">[33]</ref> UperNet Head <ref type="bibr">[</ref> which validates the effectiveness of our even block assignment strategy with better information interaction and detail preservation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Different block assignment strategies</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) HRViTAttn: augmented cross-shaped local selfattention with a parallel convolution path and an efficient diversityenhanced shortcut. (b) Window zero-padding with attention map masking. collect global information. (3) Scalable complexity: one dimension of the window is fixed, which avoids quadratic complexity to image sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>MixCFN with multiple depth-wise convolution paths to extract multi-scale local information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Cross-resolution fusion layers with channel matching, up-scaling, and down-sampling. fortify gradient flows during backpropagation to facilitate the training of deep LR branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>MixCFN ratio r Channel C Head dim d k</figDesc><table><row><cell cols="2">Variant Window s HRViT-b1 Architecture design 1 2 7</cell><cell>4 4 4</cell><cell>32 64 128</cell><cell>16 32 32</cell></row><row><cell></cell><cell>7</cell><cell>4</cell><cell>256</cell><cell>32</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>48</cell><cell>24</cell></row><row><cell>HRViT-b2</cell><cell>2 7</cell><cell>3 3</cell><cell>96 240</cell><cell>24 24</cell></row><row><cell></cell><cell>7</cell><cell>3</cell><cell>384</cell><cell>24</cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>64</cell><cell>32</cell></row><row><cell>HRViT-b3</cell><cell>2 7</cell><cell>2 2</cell><cell>128 256</cell><cell>32 32</cell></row><row><cell></cell><cell>7</cell><cell>2</cell><cell>512</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .Table 3</head><label>23</label><figDesc></figDesc><table><row><cell>Variant</cell><cell>Image Size</cell><cell>#Params (M)</cell><cell>GFLOPs</cell><cell>IMNet-1K top-1 acc.</cell></row><row><cell>HRViT-b1</cell><cell>224</cell><cell>19.7</cell><cell>2.7</cell><cell>80.5</cell></row><row><cell>HRViT-b2</cell><cell>224</cell><cell>32.5</cell><cell>5.1</cell><cell>82.3</cell></row><row><cell>HRViT-b3</cell><cell>224</cell><cell>37.9</cell><cell>5.7</cell><cell>82.8</cell></row></table><note>Architecture variants of HRViT. The number of Transformer blocks is marked in each module, followed by per branch settings.. ImageNet-1K pre-training results of HRViT. FLOPs are measured on an image size of 224?224. #Params includes the classification head as used in HRNetV2 [27].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>HRViT achieves the best performance-efficiency trade-off among all models on ADE20K val. The table on the right shows ADE20K val mIoUs of MiT, CSWin, and HRViT with the SegFormer<ref type="bibr" target="#b32">[33]</ref> head. Comparison on the Cityscapes val segmentation dataset.</figDesc><table><row><cell>Backbone</cell><cell cols="3">SegFormer Head [33] #Param. (M)? GFLOPs? mIoU (%)?</cell></row><row><cell>MiT-B0 [33]</cell><cell>3.8</cell><cell>8.4</cell><cell>76.20</cell></row><row><cell>MiT-B1 [33]</cell><cell>13.7</cell><cell>15.9</cell><cell>78.50</cell></row><row><cell>CSWin-Ti [11]</cell><cell>5.9</cell><cell>11.4</cell><cell>79.16</cell></row><row><cell>HRViT-b1</cell><cell>8.1</cell><cell>14.1</cell><cell>81.63</cell></row><row><cell>MiT-B2 [33]</cell><cell>27.5</cell><cell>62.4</cell><cell>81.00</cell></row><row><cell>CSWin-T [11]</cell><cell>22.4</cell><cell>28.3</cell><cell>81.56</cell></row><row><cell>HRViT-b2</cell><cell>20.8</cell><cell>27.4</cell><cell>82.81</cell></row><row><cell>MiT-B3 [33]</cell><cell>47.3</cell><cell>79.0</cell><cell>81.70</cell></row><row><cell>MiT-B4 [33]</cell><cell>64.1</cell><cell>95.7</cell><cell>82.30</cell></row><row><cell>CSWin-S [11]</cell><cell>37.3</cell><cell>78.1</cell><cell>82.58</cell></row><row><cell>HRViT-b3</cell><cell>28.6</cell><cell>66.8</cell><cell>83.16</cell></row><row><cell>Avg improv.</cell><cell>-30.7%</cell><cell>-23.1%</cell><cell>+2.16</cell></row></table><note>We reduce the channels (64?32) of CSWin-T and name it CSWin-Ti. FLOPs are based on the image size of 512?512.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 7 .</head><label>57</label><figDesc>Ablation on proposed techniques. Each entry removes one technique independently. The last one removes all techniques. Evaluate HRViT-b3 on Cityscapes val with different window sizes on the MR and LR paths.</figDesc><table><row><cell>Variants</cell><cell>#Params (M)</cell><cell>FLOPs (G)</cell><cell>IMNet top-1 acc.</cell><cell>City mIoU</cell></row><row><cell>HRViT-b1</cell><cell>8.1</cell><cell>14.1</cell><cell>80.52</cell><cell>81.63</cell></row><row><cell>? Key-value sharing</cell><cell>8.8</cell><cell>14.7</cell><cell>80.52</cell><cell>81.00</cell></row><row><cell>? Eff. patch embed</cell><cell>9.9</cell><cell>16.5</cell><cell>80.19</cell><cell>81.18</cell></row><row><cell>? MixCFN</cell><cell>7.9</cell><cell>13.6</cell><cell>79.86</cell><cell>80.52</cell></row><row><cell>? Parallel CONV path</cell><cell>8.1</cell><cell>14.0</cell><cell>80.06</cell><cell>80.82</cell></row><row><cell>? Nonlinearity/BN</cell><cell>8.1</cell><cell>14.1</cell><cell>80.37</cell><cell>81.12</cell></row><row><cell>? Dense fusion</cell><cell>8.0</cell><cell>14.0</cell><cell>79.95</cell><cell>81.26</cell></row><row><cell>? DES</cell><cell>8.1</cell><cell>14.0</cell><cell>80.36</cell><cell>81.38</cell></row><row><cell>? All block opt.</cell><cell>10.1</cell><cell>16.3</cell><cell>79.79</cell><cell>80.45</cell></row><row><cell cols="5">MixCFN. Replacing the MixCFN block with the origi-</cell></row><row><cell cols="5">nal FFNs [12] directly leads to ?0.66% ImageNet accu-</cell></row><row><cell cols="5">racy drop and 0.11 Cityscapes mIoU loss with marginal</cell></row><row><cell cols="5">efficiency improvement. By adding multi-scale local fea-</cell></row><row><cell cols="5">ture extraction in feedforward networks, MixCFN can in-</cell></row><row><cell cols="3">deed boost the performance of HRViT.</cell><cell></cell><cell></cell></row><row><cell cols="5">Parallel convolution path. The embedded inverted resid-</cell></row><row><cell cols="5">ual path in the HRViTAttn block is very lightweight and</cell></row><row><cell cols="5">contributes 0.46% higher ImageNet accuracy as well as 0.81</cell></row><row><cell cols="2">higher mIoU on Cityscapes.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Additional nonlinearity/BN. The extra Hardswish and BN</cell></row><row><cell cols="5">introduce negligible overhead but boost expressivity and</cell></row><row><cell cols="5">trainability, bringing 0.15% higher ImageNet-1K accuracy</cell></row><row><cell cols="3">0.51 higher mIoU on Cityscapes val.</cell><cell></cell><cell></cell></row><row><cell cols="5">Dense vs. sparse fusion layers. The sparse fusion layer</cell></row><row><cell cols="5">proposed in HR-NAS [10] is not very effective in HRViT</cell></row><row><cell cols="5">as it saves tiny hardware cost (&lt;1%) but leads to 0.57%</cell></row><row><cell cols="3">accuracy drop and 0.37 mIoU loss.</cell><cell></cell><cell></cell></row><row><cell cols="5">Diversity-enhanced shortcut. As an auxiliary path, the</cell></row><row><cell cols="5">proposed shortcut (DES) helps enhance the feature diver-</cell></row><row><cell cols="5">sity and effectively boosts the performance to a higher level</cell></row><row><cell cols="5">both on classification and segmentation tasks. The hard-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Compare different block assignment strategies on the third LR branch in HRViT-b1.</figDesc><table><row><cell>Block Assignment</cell><cell>ImageNet-1K</cell><cell>Cityscapes val</cell></row><row><cell>on the 3rd Branch</cell><cell>Top-1 Acc</cell><cell>mIoU</cell></row><row><cell>6-6-6-2</cell><cell>80.53</cell><cell>81.63</cell></row><row><cell>8-8-2-2</cell><cell>80.51</cell><cell>81.50</cell></row><row><cell>9-9-1-1</cell><cell>80.50</cell><cell>81.25</cell></row><row><cell>17-1-1-1</cell><cell>80.11</cell><cell>81.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Performance and efficiency comparison of different ViT backbones on the ADE20K val segmentation dataset. Average improvements of HRViT over baselines are summarized for each framework.</figDesc><table><row><cell>31]</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seg-Net: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-End object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CrossViT: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papandreou Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the Design of Spatial Attention in Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR, 2021. 1</title>
		<meeting>ICLR, 2021. 1</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiscale Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LeViT: a Vision Transformer in Con-vNet&apos;s Clothing for Faster Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking Depthwise Separable Convolutions: How Intra-Kernel Correlations Lead to Improved MobileNets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Haase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Amthor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14588" to="14597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">LocalViT: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas Kokkinos Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08810,2021.4</idno>
		<title level="m">Do Vision Transformers See Like Convolutional Neural Networks? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Augmented Shortcuts for Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2021</title>
		<meeting>NeurIPS, 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers: distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep High-Resolution Representation Learning for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00154</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Early Convolutions Help Transformers See Better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Co-scale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lite-HRNet: A Lightweight High-Resolution Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02277</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Glance-and-Gaze Vision Transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token ViT: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HRFormer: High-Resolution Transformer for Dense Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiscale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
