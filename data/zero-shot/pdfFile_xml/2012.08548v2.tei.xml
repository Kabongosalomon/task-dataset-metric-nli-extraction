<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zhang</surname></persName>
							<email>zhang-g19@mails.tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
							<email>yinchangqing@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
							<email>liquanquan@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Equalization Loss v2: A New Gradient Balance Approach for Long-tailed Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently proposed decoupled training methods emerge as a dominant paradigm for long-tailed object detection. But they require an extra fine-tuning stage, and the disjointed optimization of representation and classifier might lead to suboptimal results. However, end-to-end training methods, like equalization loss (EQL), still perform worse than decoupled training methods. In this paper, we reveal the main issue in long-tailed object detection is the imbalanced gradients between positives and negatives, and find that EQL does not solve it well. To address the problem of imbalanced gradients, we introduce a new version of equalization loss, called equalization loss v2 (EQL v2), a novel gradient guided reweighing mechanism that rebalances the training process for each category independently and equally. Extensive experiments are performed on the challenging LVIS benchmark. EQL v2 outperforms origin EQL by about 4 points overall AP with 14 ? 18 points improvements on the rare categories. More importantly, it also surpasses decoupled training methods. Without further tuning for the Open Images dataset, EQL v2 improves EQL by 7.3 points AP, showing strong generalization ability. Codes have been released at https: //github.com/tztztztztz/eqlv2 arXiv:2012.08548v2 [cs.CV] 1 Apr 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a fundamental computer vision task that aims to recognize and locate objects of a set of predefined categories. Modern object detectors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref> have shown promising results on some conventional benchmarks such as COCO <ref type="bibr" target="#b25">[26]</ref> and PASCAL VOC <ref type="bibr" target="#b8">[9]</ref>. Collected images in these datasets have been carefully selected and the quantities of each category are relatively balanced. However, in natural images, quantities of categories subject to a long-tailed Zipfian distribution. It means that, in <ref type="figure">Figure 1</ref>: Visualization of accumulative gradients ratio of different trained models. Best view in color. The x-axis is the sorted category index of 1203 categories of LVIS dataset. The y-axis is the accumulative gradient ratio of positives to negatives. Here gradient is the gradient of the output logits with respect to classification loss. AP r and AP c are the AP for rare and common categories. a realistic scenario, we are confronted with a more complex situation that the obtained objects show an extreme imbalance in different categories.</p><p>The difficulty of training detectors on a long-tailed dataset mainly comes from two aspects. First, deep learning methods are hungry for data, but annotations of tail classes (classes with few samples) might be insufficient for training. Second, the model tends to bias towards head classes (classes with many samples) since the head class objects are the overwhelming majority in the entire datasets.</p><p>Current state-of-the-art approaches are based on decoupled training schema <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>. In general, decoupled training involves a two-stage pipeline that learns representations under the imbalance dataset at the first stage, then re-balances the classifier with frozen representation at the second stage. Despite the success of the decoupled training, it needs an extra fine-tuning stage in training phase. In addition, the representation could be suboptimal since it is not jointly learned with the classifier. So a natural question to ask is: could end-to-end training methods match or surpass the accuracy of decoupled training methods?</p><p>Recently, Tan et al. <ref type="bibr" target="#b35">[36]</ref> propose the Equalization Loss (EQL) <ref type="bibr" target="#b35">[36]</ref>, an end-to-end re-weighing loss function, to protect the learning of tail categories by blocking some negative gradients. Although EQL makes improvements to long-tailed training, the accuracy gap between the end-toend and decoupled training approaches still exists. To take a step forward, we analyze the gradient statistics of EQL.</p><p>Here we plot the positive gradient to negative gradient ratio accumulated in the entire training process for each category classifier, as present in <ref type="figure">Figure 1</ref>. The key observation is: for head categories, the ratio is close to 1, which means the positive gradients and the negative gradients have similar magnitude; for tail categories, the gradients are near 0, which means the positive gradients are overwhelmed by the negative gradients. Therefore the gradient ratio could indicate whether a classifier is trained in balance. Compared with the baseline (blue line), the gradient ratio of EQL (orange line) just increases slightly.</p><p>In this paper, we propose a new version of equalization loss, called equalization loss v2 (EQL v2) which improves the long-tailed object detection by balancing the positive to negative gradient ratio. In EQL v2, we first model the detection problem as a set of independent sub-tasks, each task for one category. Next, we propose a gradient guided re-weighing mechanism to balance the training process of each task independently and equally. Specifically, the accumulated gradient ratio is used as an indicator to up-weight the positive gradients and down-weight the negative gradients. It dynamically controls the training of all sub-tasks and each sub-task is treated equally with the same simple re-weighing rule. The positive to negative gradient ratio of EQL v2 are shown in <ref type="figure">Figure 1</ref> (green line). Compared to the baseline and EQL, EQL v2 achieves a more balanced training for most categories.</p><p>We conduct experiments on two long-tailed object detection dataset, LVIS <ref type="bibr" target="#b11">[12]</ref> and OpenImages <ref type="bibr" target="#b19">[20]</ref>. On LVIS, compared to the baseline models, including Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> and Cascade Mask R-CNN <ref type="bibr" target="#b0">[1]</ref>, it increases overall AP by about 6 points and gains 17 ? 20 points AP for tail categories. It outperforms EQL by about 4 points AP. In addition, EQL v2 surpasses all of the existing longtailed object detection methods, including end-to-end training and decoupled training methods. On OpenImages, EQL v2 achieves a 9 points AP gain over the baseline model with the same hyper-parameters as on LVIS, which shows the good generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>General object detection. Modern object detection frameworks rely on the outstanding ability of classification powered by convolutional neural networks (CNN) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b14">15]</ref>. They can be divided into region-based detectors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4]</ref> and anchor-free detectors <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b44">45]</ref> depending on the concept they want to classify. However, all those frameworks are developed under the condition of balanced data. When it comes to the long-tailed distribution of data, the performance deteriorates severely due to the imbalanced among categories.</p><p>Long-tailed image classification. Common solutions for long-tailed image classification are data re-sampling and loss reweighing. However, data re-sampling <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref> methods have to access the pre-computed statistics of data distribution and might make models under the risks of overfitting for tail classes and under-fitting for head classes. For the loss re-weighing methods, including instance-level <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref> ones and class-level ones <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b1">2]</ref>, they suffer from the sensitive hyper-parameters, the optimal setting for different dataset might vary largely and finding them takes too many efforts. There are also some works trying to transfer the knowledge from head classes to tail classes. OLTR <ref type="bibr" target="#b27">[28]</ref> designs a memory module to augment the feature for tail classes. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref> augment the under-represented classes in the feature space by using the knowledge learned from head classes. Recently, the decoupled training <ref type="bibr" target="#b17">[18]</ref> schema attracts much attention. They argue that universal representations can be learned without re-sampling, and the classifier should be re-balanced in the second fine-tuning stage with representations frozen. In spite of excellent results, the extra fine-tuning stage seems unnatural and we can not explain why the representation and the classifier have to be learned separately.</p><p>Long-tailed object detection. Long-tailed object detection is a more difficult problem than long-tailed classification. It has to find all objects with various scale in every possible location. Li et al. <ref type="bibr" target="#b22">[23]</ref> empirically find methods that are designed for long-tailed image classification can not achieve good results in object detection. Tan et al. <ref type="bibr" target="#b35">[36]</ref> first shows the tail classes are heavily suppressed by the head classes and they propose an equalization loss to tackle this problem by ignoring the suppressing part for tail categories. However, they think the negative suppressing comes from competition of foreground categories and ignore the impact of background proposal. Instead, we treat background and foreground uniformly. EQL also has to access the frequency of categories and uses a threshold function to explicitly split head and tail categories. LST <ref type="bibr" target="#b15">[16]</ref> models the learning for long-tailed distribution as a kind of incremental learning, the learning switches from head classes to tail classes in several cascaded stages. SimCal <ref type="bibr" target="#b38">[39]</ref> and Balanced Group-Softmax (BAGS) <ref type="bibr" target="#b22">[23]</ref> follow the spirit of decoupled training. For SimCal, they train an extra classification branch with class-balanced proposals in the fine-tuning stage and combine its score with a normal trained softmax classification branch via dual inference. BAGS divides all categories into several groups based on the instance count and does softmax separately in each group to avoid the domination of head classes. In contrast, our method does not have to split categories into different groups and treat all categories equally. Moreover, we do not need the fine-tuning stage and can be trained end-to-end. Tang et al. <ref type="bibr" target="#b36">[37]</ref> shows that SGD momentum makes the classifier biased towards head classes. They introduce causal intervention in training and remove the biased part for tail classes in inference. On the contrary, our method is simpler and more efficient, and keeps consistent between training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Equalization Loss v2</head><p>In this section, we introduce the Equalization Loss v2. We begin by revisiting the entanglement of instances and tasks in Section 3.1, then present our novel gradient guided reweighing strategy in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Entanglement of Instances and Tasks</head><p>Suppose we have a batch of instances I and their representations. To output logits Z for C categories, a weight matrix W is used as a linear transformation of representations. Each weight vector in W, which we refers as a category classifier, is responsible for a specific category, i.e. a task. Then the output logits are transformed to an estimated probability distribution P by the sigmoid function. We expect that for each instance, only the corresponding classifier gives the high score while others give a low score. That is saying, one task with positive label and C?1 tasks with negative labels are introduced by a single instance. Hence, we can calculate the actual number of positive samples m pos j and negative samples m neg j for classifier j:</p><formula xml:id="formula_0">m pos j = i?I y i j , m neg j = i?I (1 ? y i j )<label>(1)</label></formula><p>Where the y i is the one-hot groud truth label for the ith instance, and usually we have j y i j = 1. The ratio of expectation of positive samples to the negative samples over the dataset is then:</p><formula xml:id="formula_1">E|m pos j | E|m neg j | = 1 N nj ? 1<label>(2)</label></formula><p>Where n j is the instance number of category j and N is the total instance number over the dataset. Equation 2 shows that if we consider each classifier separately, the ratio of the positive samples to the negative samples could have a big difference for different classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Gradient Guided Reweighing</head><p>Obviously, we have E|m pos j | E|m neg j | especially when category j is a rare category. But the ratio in Equation 2 might not be a good indicator of how balanced the training is. The reason behind it is that the influence of each sample is different. For example, the negative gradients accumulated by large quantities of easy negatives might be smaller than the positive gradients generated by a few hard positives. Therefore, we directly choose gradient statistics as a metric to indicate whether a task is in balanced training. The positive and negative gradients for each classifier's output z j with respect to the loss L are formulated as:</p><formula xml:id="formula_2">? pos zj (L) = 1 |I| i?I y i j (p i j ? 1) (3) ? neg zj (L) = 1 |I| i?I (1 ? y i j )p i j<label>(4)</label></formula><p>p i j is the estimated probability of category j for the ith instance. The basic idea of gradient guided balanced reweighing is that we up-weight the positive gradients and down-weight negative gradients for each classifier independently according to their accumulated gradient ratio of positives to negatives.</p><p>To achieve this, we first define g (t) j as the ratio of accumulated positive gradients to negative gradients of task j until the iteration t. Then the weight for positive gradients q t j and negative gradients r t j at this iteration can be computed by:</p><formula xml:id="formula_3">q (t) j = 1 + ?(1 ? f (g (t) j )), r (t) j = f (g t j )<label>(5)</label></formula><p>Where f (?) is a mapping function:</p><formula xml:id="formula_4">f (x) = 1 1 + e ??(x??)<label>(6)</label></formula><p>After obtaining q t j and r t j , we apply them to the positive gradient and negative gradient for the current batch, so the re-weighted gradients become:</p><formula xml:id="formula_5">? pos zj (L (t) ) = q (t) j ? pos zj (L (t) ) (7) ? neg zj (L (t) ) = r (t) j ? neg zj (L (t) )<label>(8)</label></formula><p>Finally we update the ratio of accumulated positive gradients to negative gradients for the next iteration t + 1:  <ref type="table" target="#tab_1">Table 2</ref>: Effect of different components. obj for adding the category-agnostic task, neg for reweighing negative gradients, pos for reweighing positive gradients. Models are trained with random samplers by standard 1x schedule. AP and AP b denotes mask AP and box AP respectively.</p><formula xml:id="formula_6">g t+1 j = method #sampler #epoch AP AP r AP c AP f AP b End-to-end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metric</head><p>LVIS <ref type="bibr" target="#b11">[12]</ref> is a new benchmark for long-tailed object recognition. It provides precise bounding box and mask annotations for various categories with long-tailed distribution. We mainly perform experiments on the recently released challenging LVIS v1.0 dataset. It consists of 1203 categories. We train our models on the train set, which contains about 100k images with 1.3M instances. In addition to widely-used metric AP across IoU threshold from 0.5 to 0.95, LVIS also reports AP r (rare categories with 1-10 images), AP c (common categories with 11-100 images), AP f (frequent categories with &gt; 100 images). Since LVIS is federated dataset, categories are not annotated exhaustively. Each image have two more types of labels: pos category ids and neg category ids, indicating which categories are or are not present in that image. Detection results that do not belong to those categories will be ignored for that image. We report results on the val set of 20k images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We implement our method using MMDetection <ref type="bibr" target="#b4">[5]</ref>. Models are trained using SGD with a momentum of 0.9 and a weight decay of 0.0001. The ResNet <ref type="bibr" target="#b14">[15]</ref> backbones are initialized by ImageNet pre-trained models. Following the convention, scale jitter and horizontal flipping are used in training and no test time augmentation is used. We use a total batch size of 16 on 16 GPUs (1 image per GPU), and set the initial learning rate to 0.02. Following <ref type="bibr" target="#b11">[12]</ref>, the maximum number of detection per image are up to 300, and the minimum score threshold are reduced to 0.0001 from 0.01. For our method, we set ? = 12, ? = 0.8 and ? = 4. More details about hyper-parameters are showcased in section 4.7. Since EQL v2 uses sigmoid loss function, we initialize the bias of the last fully-connected classification layer(fc-cls) with values of 0.001 to stabilize the training at the beginning.</p><p>Following <ref type="bibr" target="#b22">[23]</ref>, we also add a branch for detecting objectiveness instead of concrete category to reduce falsepositives, which we refer as category-agnostic task. In the training phase, this task treats all other tasks' positive samples as its positive samples. In the inference phase, the estimated probability of other sub-tasks becomes:</p><formula xml:id="formula_7">p j = p j * p obj<label>(10)</label></formula><p>Where p obj is the probability for a proposal being a object. The proposed gradient guided reweighing are not applied on the category-agnostic task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We take Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> equipped with ResNet-50 and FPN <ref type="bibr" target="#b23">[24]</ref> as our baseline model. The effect of each component is shown in  gap between head and tail classes are very large. Adding a category-agnostic task helps all categories to some extent, improving the overall AP by 2.0% but not very much for the rare categories since the main problem for them is the unbalanced positive and negative gradients, i.e. their positive gradients are overwhelmed by negative gradients cause by a vast number of negative samples. By down-weighting the influence of negative gradients, their accuracy is boosted significantly (5.4% for rare categories). Up-weighting the positive gradient helps to achieve a more balanced ratio of positive to negative gradients. It brings a 7.6% performance boosting for rare categories. With these three components, we achieve a 23.7% AP, outperforming the baseline model 16.1% AP by a large margin without any re-sampling techniques. These ablation experiments verified the effectiveness of our proposed loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Main Results</head><p>Comparison with Decoupled Training methods. We mainly compare our method with three decoupled training methods (cRT <ref type="bibr" target="#b17">[18]</ref>, LWS <ref type="bibr" target="#b17">[18]</ref>, and BAGS <ref type="bibr" target="#b22">[23]</ref>). The results are present in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with State-of-the-Art Methods</head><p>In this section, we compare our method with other work that report state-of-the-art results on LVIS v0.5 and LVIS v1.0. The results on LVIS v0.5 is present in <ref type="table" target="#tab_5">Table  4</ref>, including RFS <ref type="bibr" target="#b11">[12]</ref> for re-sampling, EQL <ref type="bibr" target="#b35">[36]</ref> for reweighing, LST <ref type="bibr" target="#b15">[16]</ref> for incremental learning, SimCal <ref type="bibr" target="#b38">[39]</ref> and BAGS <ref type="bibr" target="#b22">[23]</ref> for decoupled training, Forest R-CNN <ref type="bibr" target="#b40">[41]</ref> for hierachy classification, De-cofound-TDE <ref type="bibr" target="#b36">[37]</ref> for causal inference. EQL v2 achieves better results than all those methods. With ResNet-50-FPN backbone, it outperforms the winner of last year's challenge EQL by 4.3%. We also compare the results under large models. With the same Cascade Mask R-CNN <ref type="bibr" target="#b0">[1]</ref> framework equipped with ResNet-101-FPN, EQL v2 still has a 1.8% higher AP than Deconfound-TDE. With the same Hybrid Cascade R-CNN <ref type="bibr" target="#b3">[4]</ref> framework with ResNeXt-64x4d-FPN <ref type="bibr" target="#b41">[42]</ref>, EQL v2 outperform BAGS by 0.8% AP. Since LVIS v1.0 is recently proposed, not much work has reported their results on it. We mainly compare EQL v2 with De-confound-TDE. In addition, we also re-implement the equalization loss. The original EQL chooses a hyper-parameter ? of 1.76 ? 10 ?3 for LVIS v0.5, we found this is not optimal for LVIS v1.0, so we tune this hyper-parameter and report the results with best value of ? = 1.1 ? 10 ?3 . The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Our method achieves higher overall AP across different backbones and frameworks. EQL v2 outperforms the De-confound-TDE by 6.3% for rare categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Model Analysis</head><p>Do we have a more balanced gradient ratio? We visualize the gradient ratio of our method (   Whether the classifiers are balanced? Decoupled training methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> have shown that if models are trained with long-tailed distributed data, the weight norm in the last classifier layer (fc-cls) is heavily biased. Those methods rebalance the classifier at the second fine-tuning stage, resulting in balanced weight norms. We also visualize the weight norm of the fc-cls of three models: baseline, RFS and EQL v2 <ref type="figure">(Table 1 (a)</ref>, (c) and (h)), in <ref type="figure" target="#fig_1">Figure 3</ref>. The model trained with repeat factor sampling still suffers from biased weight norm. On the contrary, the model trained with EQL v2 has a more balanced weight norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do we have a better representation?</head><p>To evaluate the quality of representations trained with our method. We adopt models trained with our method and standard training as pre-trained model. Then we follow the classic decoupled training recipe to re-train the classifier with frozen representations. The results are shown in   increase 0.3 % after using BAGS re-training, compared to the 23.7% AP of EQL v2 (Table 1(h)), and AP drops 1.3 % and 0.6 % after using cRT and LWS respectively. It shows that decoupled training is not always necessary, we can train models with both a balanced classifier and better representations in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Influence of hyper-parameters</head><p>In <ref type="table" target="#tab_0">Table 12</ref> we investigate how the shape of mapping function f (?) in equation 6 affects the training. There are two hyper-parameters in the function, ? and ?. We can see that the detection result is not sensitive to the shape of the mapping function, and the AP increases stably when those two hyper-parameters move in a wide range. The visualization of mapping functions is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The effect of ? is present in <ref type="table" target="#tab_10">Table 8</ref>.     estimate the probability. We show another choice of the classifier: 2C Softmax which uses 2 output logits for each task and adopts softmax function to estimate the probability. The extra output logit for each task can be regarded as a concept of others category and introduces competition when doing inference so it helps reducing false-positives. In <ref type="table" target="#tab_12">Table 9</ref>, we compare the results of C-sigmoid and 2C-softmax under different settings. When only adding the objectiveness task and down-weighting the negative gradients, 2Csoftmax achieves higher accuracy than C-sigmoid. These two designs reach comparable results after up-weighing the positive gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Experiments on Open Images Detection</head><p>To verify the generalization ability to other datasets, we conduct experiments on the OpenImages <ref type="bibr" target="#b19">[20]</ref>. OpenImages is another large-scale object detection dataset with longtailed distributed categories. We use the data split of challenge 2019, which is a subset of OpenImages V5. The train set consists of 1.7M images of 500 categories. We evaluate our models on the 41k val set. In addition to the standard mAP@IOU=0.5 metric, we also group categories into five groups (100 categories per group) according to their instance numbers and report the mAP within each group respectively. The results are shown in <ref type="table" target="#tab_0">Table 10</ref>. EQL v2 reaches an AP of 52.6, outperforming the baseline model and EQL by 9.5 AP and 7.3 AP respectively. For the tail group (AP1), the EQL v2 increases the AP by 22.3 point, which is much more than the improvement of EQL (6.4 AP). EQL v2 also outperform EQL considerably on the larger ResNet-101 backbone. For both baseline and EQL models, there is still a large performance gap between head and tail classes. EQL v2 brings all categories into a   more equal status. It achieves similar accuracy for all categories groups. It is worth noting that we tune the hyperparameter ? in EQL which puts 250 categories into tail group for OpenImage. In contrast, the hyper-parameters of EQL v2 are kept the same as that on LVIS. Those experiments not only show the effectiveness but also good generalization ability of EQL v2. We also report the further tuned results of EQL v2 on Open Images. The values of ?, ? and ? are 0.9, 12, 8 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose the key of improving performance for long-tailed object detection is to maintain balanced gradients between positives and negatives. An improved version of EQL, EQL v2 is then proposed to dynamically balance the gradient ratio between positives to negatives in the training phase. It brings large improvements with notably boosting on tail categories across various frameworks. As an end-to-end training method, it beats all existing methods on the challenging LVIS benchmark, including the dominant decoupled training schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Mapping Function Types</head><p>In <ref type="table" target="#tab_0">Table 11</ref>, we make the comparison among several variant mapping functions. Results show that our proposed sigmoid-like mapping function achieves the highest AP.  We have conducted more ablation studies of ? and ?, and the experiment results are presented in <ref type="table" target="#tab_0">Table 12</ref>. Since ? represents the value which we think as a high enough gradient ratio, lowering its values significant degrades the accuracy. It is better to choose a higher value for it, e.g., 0.8, 0.9. ? is more robust when ? is in a reasonable range.  <ref type="table" target="#tab_0">Table 12</ref>: More Ablations of ? and ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The accumulated gradients ratio of positives to negatives. Models are trained with total 75k iterations. We show the values at different training iteration. We compare the accumulated gradients of two models, Mask R-CNN with sigmoid loss and EQL v2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The L2 weight norm of the fc-cls layer of models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Mapping functions with different ? and ? method EQL v2 AP AP r AP c AP f AP b cRT 22.0 13.5 20.8 27.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>AP f AP APr APc AP f sqrt (y = ? x) 18.4 2.1 17.6 28.2 21.4 6.2 21.0 28.6 linear (y = x) 18.8 2.0 16.9 28.3 22.6 10.0 22.0 28.7 exp (y = x 2 ) 19.1 2.1 17.6 28.2 23.2 11.9 22.7 28.8 ours 19.7 7.3 17.6 27.6 23.7 14.9 22.8 28.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison with end-to-end and decoupled training methods on LVIS v1.0 val set with ResNet-50-FPN Mask R-CNN by 1x schedule. For cRT and LWS, they use class-balance sampler to fine-tune their model in the second stage, and BAGS uses a random sampler following the origin paper. Instead, our method train models in an end-to-end fashion without any fine-tuning stage. .3 17.6 27.6 20.5 23.7 14.9 22.8 28.6 24.2</figDesc><table><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(a) Softmax</cell><cell></cell><cell>random</cell><cell>12</cell><cell>16.1</cell><cell>0.0</cell><cell>12.0</cell><cell>27.4</cell><cell>16.7</cell></row><row><cell>(b) Sigmoid</cell><cell></cell><cell>random</cell><cell>12</cell><cell>16.5</cell><cell>0.0</cell><cell>13.1</cell><cell>27.3</cell><cell>17.2</cell></row><row><cell>(c) EQL [36]</cell><cell></cell><cell>random</cell><cell>12</cell><cell>18.6</cell><cell>2.1</cell><cell>17.4</cell><cell>27.2</cell><cell>19.3</cell></row><row><cell>(d) RFS [12]</cell><cell></cell><cell>repeat factor</cell><cell>12</cell><cell>22.2</cell><cell>11.5</cell><cell>21.2</cell><cell>28.0</cell><cell>22.9</cell></row><row><cell>Decoupled Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(e) LWS [18]</cell><cell></cell><cell>random/balance</cell><cell>12+12</cell><cell>17.0</cell><cell>2.0</cell><cell>13.5</cell><cell>27.4</cell><cell>17.5</cell></row><row><cell>(f) cRT [18]</cell><cell></cell><cell>random/balance</cell><cell>12+12</cell><cell>22.1</cell><cell>11.9</cell><cell>20.2</cell><cell>29.0</cell><cell>22.2</cell></row><row><cell>(g) BAGS [23]</cell><cell></cell><cell>random/random</cell><cell>12+12</cell><cell>23.1</cell><cell>13.1</cell><cell>22.5</cell><cell>28.2</cell><cell>23.7</cell></row><row><cell>(h) EQL v2 (Ours)</cell><cell></cell><cell>random</cell><cell>12</cell><cell>23.7</cell><cell>14.9</cell><cell>22.8</cell><cell>28.6</cell><cell>24.2</cell></row><row><cell>16.1</cell><cell>0</cell><cell>12.0 27.4 17.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">18.1 1.9 16.4 28.3 19.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">19.7 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>obj? neg? pos? AP AP r AP c AP f APb</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The baseline model performs poorly on the tail classes, resulting in 0% and 12.0% AP for rare and common categories. And the performance</figDesc><table><row><cell>method</cell><cell>AP</cell><cell>AP r</cell><cell>AP c</cell><cell>AP f</cell></row><row><cell>(a) Mask-R50</cell><cell>20.5</cell><cell>2.0</cell><cell>19.0</cell><cell>30.3</cell></row><row><cell>(b) +EQL v2</cell><cell>26.2</cell><cell>19.1</cell><cell>25.0</cell><cell>30.7</cell></row><row><cell>(c) Mask-R101</cell><cell>21.7</cell><cell>1.6</cell><cell>20.7</cell><cell>31.7</cell></row><row><cell>(d) +EQL v2</cell><cell>27.5</cell><cell>20.5</cell><cell>26.2</cell><cell>32.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of larger backbones with a longer 3x schedule. A Random sampler is used. The models are trained with totally 36 epochs, and the learning rate is divided by 10 at the 28th epoch and 34th epoch respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Those decoupled training methods all improve the AP, mainly for tail classes. The improvements brought by LWS is limited. We conjecture it is because that LWS only learns a scaling factor to adjust the decision boundary of the classifier but the classifier itself is not good and imbalanced. Our method achieves a overall 23.7% AP, increasing AP r by 14.9%, AP c by 10.8%. It is worth noting that EQL v2 does not require the extra fine-tuning stage, and the representation and classifier are learned jointly. More importantly, it has already surpassed the decoupled training methods. margin, 7.6% and 5.1% respectively. Note that RFS repeats images that contains tail categories in each epoch, so it increases the total training time. Instead, our method only uses a random sampler and does not increase the training time, and achieves better results, 23.7% vs. 22.2%.</figDesc><table><row><cell>Larger Model &amp; Longer Training. To verify the gen-</cell></row><row><cell>eralization ability across different backbones and training</cell></row><row><cell>schedule. We conduct experiments with larger models by</cell></row><row><cell>a 3x schedule. The results are present in Table 3. Note</cell></row><row><cell>that training Mask R-CNN with longer schedule does not</cell></row><row><cell>help rare categories a lot (Table 1 (a) vs. Table 3 (a)), the</cell></row><row><cell>AP of rare categories is still bad because rare categories are</cell></row><row><cell>heavily suppressed by the negative gradients caused by the</cell></row><row><cell>entanglement of instances and tasks. In contrast, with the</cell></row><row><cell>proposed EQL v2, the performance of rare categories can</cell></row><row><cell>be further improved from 14.9% to 19.1% (Table 1 (h) and</cell></row><row><cell>Table 3 (b)). When using large ResNet-101 backbone, we</cell></row><row><cell>do not observe the over-fitting of tail classes in such a long</cell></row><row><cell>schedule, and the gap between Mask R-CNN and EQL v2</cell></row><row><cell>holds.</cell></row></table><note>. The decoupled training models (Table 1 (e) (f) (g)) are first initialized from naive softmax baseline (Table 1 (a)), then re-train their classifier layer (fc- cls) for another 12 epoch with other layers frozen, result- ing in a total 24 epoch training.Comparison with End-to-End Training methods. Ta- ble 1 compares EQL v2 with two popular end-to-end train- ing methods, Repeat Factor Sampling [12] (re-sampling) and Equalization Loss [36] (re-weighting). With a random sampler, our method outperforms naive softmax and EQL by a large</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>(h)) and base-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods on LVIS v0.5 val set. ? indicates that the reported result is directly</figDesc><table><row><cell cols="7">copied from referenced paper. 'Mask-R50' indicates Mask R-CNN [14] with ResNet50-FPN [15, 24], 'Cascade' is for</cell></row><row><cell cols="7">Cascade Mask R-CNN [1], 'HTC' is for Hybrid Task Cascade [4]. Models are trained with the corresponding LVIS v0.5</cell></row><row><cell>train set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>framework</cell><cell>AP</cell><cell>AP r</cell><cell>AP c</cell><cell>AP f</cell><cell>AP b</cell></row><row><cell>De-confound  ? [37]</cell><cell>Cascade-R101</cell><cell>23.5</cell><cell>5.2</cell><cell>22.7</cell><cell>32.3</cell><cell>25.8</cell></row><row><cell>De-confound-TDE  ? [37]</cell><cell>Cascade-R101</cell><cell>27.1</cell><cell>16.0</cell><cell>26.9</cell><cell>32.1</cell><cell>30.0</cell></row><row><cell>Mask R-CNN</cell><cell>Mask-R50</cell><cell>19.2</cell><cell>0</cell><cell>17.2</cell><cell>29.5</cell><cell>20.0</cell></row><row><cell>EQL *</cell><cell>Mask-R50</cell><cell>21.6</cell><cell>3.8</cell><cell>21.7</cell><cell>29.2</cell><cell>22.5</cell></row><row><cell>EQL v2 (Ours)</cell><cell>Mask-R50</cell><cell>25.5</cell><cell>17.7</cell><cell>24.3</cell><cell>30.2</cell><cell>26.1</cell></row><row><cell>Mask R-CNN</cell><cell>Mask-R101</cell><cell>20.8</cell><cell>1.4</cell><cell>19.4</cell><cell>30.9</cell><cell>21.7</cell></row><row><cell>EQL *</cell><cell>Mask-R101</cell><cell>22.9</cell><cell>3.7</cell><cell>23.6</cell><cell>30.7</cell><cell>24.2</cell></row><row><cell>EQL v2 (Ours)</cell><cell>Mask-R101</cell><cell>27.2</cell><cell>20.6</cell><cell>25.9</cell><cell>31.4</cell><cell>27.9</cell></row><row><cell>Cascade Mask R-CNN</cell><cell>Cascade-R101</cell><cell>22.6</cell><cell>2.0</cell><cell>22.0</cell><cell>32.5</cell><cell>25.2</cell></row><row><cell>EQL *</cell><cell>Cascade-R101</cell><cell>24.5</cell><cell>4.1</cell><cell>25.8</cell><cell>32.0</cell><cell>27.2</cell></row><row><cell>EQL v2 (Ours)</cell><cell>Cascade-R101</cell><cell>28.8</cell><cell>22.3</cell><cell>27.8</cell><cell>32.8</cell><cell>32.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art methods on LVIS v1.0 val set. ? indicates that the reported result is directly copied from the referenced papers. * indicates our re-implementation. Models are trained using LVIS v1.0 train set. We train our models with a standard 2x schedule.</figDesc><table><row><cell>line model (Table 1 (b)) during the training process, see Fig-</cell></row><row><cell>ure 2. The baseline model does not have a balanced ratio</cell></row><row><cell>for all categories. The positive gradients are overwhelmed</cell></row><row><cell>by the negative gradients, especially for tail classes, which</cell></row><row><cell>makes it hard to detect them. And training longer does not</cell></row><row><cell>help a lot. In contrast, EQL v2 preserves a more balanced</cell></row><row><cell>gradient ratio in the entire training phase.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>There are two main observations: Firstly, models initialized with EQL v2 always achieve a higher AP, resulting in 22.4 vs. 22.0 for cRT, 23.1 vs. 17.0 for LWS, 24.0 vs. 23.1 for BAGS. It verifies that we obtain a better representation by adopting EQL v2 compared to standard training. This result doubts the claim [18] that re-weighing will hurt the representation.</figDesc><table /><note>Secondly, the models get marginal improvements or even worse performance after decoupled training. The AP only</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :Table 7 :</head><label>67</label><figDesc>Results of various decoupled training methods with different pre-trained models. EQL v2 indicates the pretrained models are trained with EQL v2, otherwise with standard training. Only random samplers are used. 23.6 13.9 22.7 28.8 24.1 10 0.8 23.6 13.8 22.9 28.7 24.3 10 0.9 23.5 14.0 22.8 28.5 24.0 12 0.8 23.7 14.9 22.8 28.6 24.2 12 0.9 23.6 15.5 22.6 28.2 24.0 Varying ? and ? for mapping function. The positive up-weighting parameter ? is set to 4.</figDesc><table><row><cell>The idea of balancing gradient in EQL v2 is not restricted</cell></row><row><cell>to sigmoid classifier. Recall that sigmoid uses a single out-</cell></row><row><cell>put logit to represent each task and use the function ? to</cell></row></table><note>4.8. C Sigmoid vs. 2C Softmax</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Varying ?. ? and ? is set to 12 and 0.8 respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>obj? neg? pos? AP APr APc AP f</figDesc><table><row><cell>C-sigmoid</cell><cell>18.1 1.9 16.4 28.3</cell></row><row><cell>2C-softmax</cell><cell>19.0 2.0 17.3 28.4</cell></row><row><cell>C-sigmoid</cell><cell>19.7 7.3 17.6 27.6</cell></row><row><cell>2C-softmax</cell><cell>20.7 9.5 18.9 27.7</cell></row><row><cell>C-sigmoid</cell><cell>23.7 14.9 22.8 28.6</cell></row><row><cell>2C-softmax</cell><cell>23.7 14.9 22.7 28.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison between C-sigmoid and 2C-softmax under different components.</figDesc><table><row><cell></cell><cell>AP</cell><cell>AP1 AP2 AP3 AP4 AP5</cell></row><row><cell>Faster-R50</cell><cell cols="2">43.1 26.3 42.5 45.2 48.2 52.6</cell></row><row><cell>EQL</cell><cell cols="2">45.3 32.7 44.6 47.3 48.3 53.1</cell></row><row><cell>EQL v2  ?</cell><cell cols="2">52.6 48.6 52.0 53.0 53.4 55.8</cell></row><row><cell>EQL v2  ?</cell><cell cols="2">53.8 49.6 53.3 54.5 54.9 56.6</cell></row><row><cell cols="3">Faster-R101 46.0 29.2 45.5 49.3 50.9 54.7</cell></row><row><cell>EQL</cell><cell cols="2">48.0 36.1 47.2 50.5 51.0 55.0</cell></row><row><cell>EQL v2  ?</cell><cell cols="2">55.1 51.0 55.2 56.6 55.6 57.5</cell></row><row><cell>EQL v2  ?</cell><cell cols="2">55.6 51.5 55.5 57.5 55.8 57.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Results on Open Images Challenge 2019 val set. The model Faster R-CNN<ref type="bibr" target="#b30">[31]</ref> with ResNet-FPN is trained with a schedule of 120k/160k/180k. Categories are grouped into five groups according to the instance number. AP1 is the AP of the first group, where categories have least annotations, AP5 is the AP of the last group, where categories have most annotations. ?means that we directly use the hyper-parameters searched in LVIS, ?means that we tune them in OpenImages.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Comparison between different mapping functions Appendix B. More Ablations of Hyper-Params</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t t * =0 |? pos zj (L (t * ) )| t t * =0 |? neg zj (L (t * ) )|(9)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03673</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Borderline-smote: a new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="878" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5375" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09217</idno>
		<title level="m">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2537" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="196" />
		</imprint>
	</monogr>
	<note>Ashwin Bharambe, and Laurens van der Maaten</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07379</idno>
		<title level="m">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Long-tailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12991</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The devil is in classification: A simple framework for long-tail instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11978</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Forest r-cnn: Large-vocabulary long-tailed object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05676</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5704" to="5713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with longtailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
