<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Multimodal Bitransformers for Classifying Images and Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrat</forename><surname>Bhooshan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Firooz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
							<email>perez@nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Testuggine</surname></persName>
							<email>davidet@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Facebook</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">; ?</forename><surname>Nyu</surname></persName>
						</author>
						<title level="a" type="main">Supervised Multimodal Bitransformers for Classifying Images and Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised bidirectional transformer models such as BERT have led to dramatic improvements in a wide variety of textual classification tasks. The modern digital world is increasingly multimodal, however, and textual information is often accompanied by other modalities such as images. We introduce a simple yet effective baseline for multimodal BERT-like architectures, a supervised multimodal bitransformer that jointly finetunes unimodally pretrained text and image encoders by projecting image embeddings to text token space. We approach or match state-of-theart accuracy on several text-heavy multimodal classification tasks, outperforming strong baselines, including on hard test sets specifically designed to measure multimodal performance. Surprisingly, our method is competitive with ViLBERT, a self-supervised multimodal "BERT for vision-and-language" approach, while being much simpler and more easily extendible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many of the classification problems that we face in the modern digital world are multimodal in nature: textual information on the web rarely occurs alone, and is often accompanied by images, sounds, videos, or other modalities. Recent advances in representation learning for natural language processing, such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, have led to dramatic improvements in text-only classification problems. Following BERT's success, various multimodal architectures have been proposed as well-including ViLBERT , Vi-sualBERT , LXMERT <ref type="bibr" target="#b39">(Tan and Bansal, 2019)</ref>, VL-BERT <ref type="bibr" target="#b37">(Su et al., 2019)</ref> and several others-which advocate pretraining on intermediary or proxy multimodal tasks before finetuning on the multimodal task at hand.</p><p>In this work, we describe a simple yet highly effective baseline architecture for BERT-like multimodal architectures. We demonstrate that supervised bidirectional transformers with unimodally pretrained components are excellent at performing multimodal fusion, outperforming a variety of alternative fusion techniques. Moreoever, we find that their performance is competitive with, and can be extended to outperform, multimodally pretrained ViLBERT models on various multimodal classification tasks.</p><p>Our proposed approach offers several advantages. Unimodally pretrained models are simpler and easier to adapt to unimodal advances, i.e., it is straightforward to replace the text or image encoders with better alternatives and directly finetune, without requiring multimodal retraining. Furthermore, our method does not rely on a particular feature extraction pipeline since it does not require e.g. region or bounding box proposals, and is modalityagnostic: it works for any sequence of dense vectors. Hence, it can be used to compute raw image features, rather than pre-extracting them, and backpropagate through the entire encoder. Concretely, our model is BERT-first, learning to map dense multimodal features to BERT's token embedding space. We show that this approach works well on three text-heavy multimodal classification tasks: MM-IMDB <ref type="bibr" target="#b1">(Arevalo et al., 2017)</ref>, Food101 <ref type="bibr" target="#b45">(Wang et al., 2015)</ref> and V-SNLI <ref type="bibr" target="#b43">(Vu et al., 2018)</ref>. Evaluating on these tasks offers several benefits. Many real-world multimodal tasks on internet data have similar characteristics, in that text is often clearly the dominant modality and the goal is to predict a single classification label rather than answer a question. Importantly, contrary to e.g. VQA <ref type="bibr" target="#b0">(Antol et al., 2015)</ref>, these tasks have not yet been studied extensively in the multimodal transformer literature. Our work thus allows us to check whether multimodal advances on VQA extend to <ref type="table" target="#tab_1">w2  w3  w4  w5  w6  w7  w8  I1  I2  I3  I4  I5  I6  I7   0  position  1  2  3  4  5  6  7  0  1  2  3</ref>  tasks such as these. Finally, a desired characteristic of multimodal models is improved performance on cases where high-quality multimodal information is available-i.e., the whole should strictly outperform the sum of its parts. We use these tasks to construct novel hard test sets specifically designed to measure the multimodal performance of a system, consisting of examples that unimodal methods fail to classify correctly.</p><formula xml:id="formula_0">Image image . . . ResNet . . . pool ? w1 token</formula><p>Our findings indicate that the proposed supervised multimodal bitransformer model outperforms various other competitive fusion techniques, even if we give those strictly more parameters. We argue that this is due to the multimodal bitransformer's ability to employ self-attention over both modalities simultaneously, providing earlier and more fine-grained multimodal fusion. We find that our straightforward method approaches or matches multimodally pretrained ViLBERT models on our tasks. Put another way, we can match the performance of multimodally pretrained models, without any multimodal pretraining. These results show that the proposed method constitutes a powerful baseline for future work in multimodal classification, as it is straightforward to implement, easy to extend (to different modalities, or different encoders) and performs competitively with more sophisticated methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multimodal Bitransformers</head><p>There is a long history, both in natural language processing and computer vision, of transfer learning from pre-trained representations. Self-supervised word and sentence embeddings <ref type="bibr">(Collobert and We-ston, 2008;</ref><ref type="bibr" target="#b22">Kiros et al., 2015)</ref> have become ubiquitous in natural language processing. In computer vision, transferring from supervised ImageNet features is the de facto standard in computer vision <ref type="bibr" target="#b27">(Oquab et al., 2014;</ref><ref type="bibr" target="#b35">Razavian et al., 2014)</ref>.</p><p>While supervised data in NLP has also proven useful for universal sentence representations <ref type="bibr" target="#b8">(Conneau et al., 2017)</ref>, the field was revolutionized by the idea of fine-tuning self-supervised language modeling systems <ref type="bibr" target="#b9">(Dai and Le, 2015)</ref>. Language modeling enables systems to learn embeddings in a contextualized fashion, leading to improved performance on a variety of tasks <ref type="bibr" target="#b32">(Peters et al., 2018;</ref><ref type="bibr" target="#b18">Howard and Ruder, 2018)</ref>. Training transformers <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> on large quantities of data yielded even better results <ref type="bibr" target="#b34">(Radford et al., 2018)</ref>. BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> improved on this further by training transformers bidirectionally (which we refer to as bitransformers) and changing the objective to masking, leading to state-of-the-art performance on many tasks.</p><p>We introduce a straightforward yet highly effective multimodal bitransformer model that combines the text-only self-supervised representations from natural language processing with the power of state-of-the-art convolutional neural network architectures from computer vision. See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration of the architecture. In what follows, we describe the different components in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Encoder</head><p>In computer vision it is common to transfer the final fully connected layer of a pre-trained convolutional neural network <ref type="bibr" target="#b35">(Razavian et al., 2014)</ref>, where the  <ref type="bibr" target="#b45">(Wang et al., 2015)</ref> Multiclass 60101 5000 21695 2 101 V-SNLI <ref type="bibr" target="#b43">(Vu et al., 2018)</ref> Multiclass 545620 9842 9842 3 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Transformer Input Layer</head><p>We use a bidirectional transformer model initialized with pre-trained BERT weights. The architecture takes contextual embeddings as input, where each contextual embedding is computed as the sum of separate D-dimensional segment, position and token embeddings. We learn weights W n ? R P ?D to project each of the N image embeddings to Ddimensional token input embedding space:</p><formula xml:id="formula_1">I n = W n f (img, n),<label>(1)</label></formula><p>where f (?, n) is the n-th output of the image encoder's final pooling operation.</p><p>For tasks that consist of a single text and single image input, we assign text one segment ID and image embeddings the other. We use 0-indexed positional coding, i.e., we start counting from 0, for each segment. The architecture can be straightforwardly generalized to an arbitrary number of modalities, as we show for the V-SNLI task, which consists of three inputs. Since pre-trained BERT itself has only two segment embeddings, in those cases we initialize additional segment embeddings as s i = 1 2 (s 0 + s 1 ) + where s i is a segment embedding for i ? 2 and ? N (0, 1e ?2 ). Note that our method is compatible with scenarios where not every modality is present in each example (i.e., if we only have text, or only an image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Classification</head><p>We use the first output of the final layer as the input to a classification layer clf(x) = W x + b where W ? R D?C , with D as the transformer dimensionality and C as the number of classes. For multilabel tasks, which can have more than one right answer, we apply a sigmoid on the logits and train with a binary cross-entropy loss for each output class (during inference time, we set the threshold at 0.5); for multiclass tasks we apply a softmax on the logits and train with a regular cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pre-training</head><p>The image encoder was pre-trained on ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009)</ref>. We use the ResNet-152 <ref type="bibr" target="#b17">(He et al., 2016)</ref> implementation and weights available in PyTorch <ref type="bibr" target="#b28">(Paszke et al., 2017)</ref> through torchvision. We use the pre-trained 12-layer 768dimensional base-uncased model for BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, trained on English Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Fine-tuning</head><p>Our architecture consists of a mixture of pre-trained and randomly initialized components. In NLP, BERT is commonly fine-tuned in its entirety, and not transfered as an encoder with fixed parameters, as used to be the case in e.g. SkipThought <ref type="bibr" target="#b22">(Kiros et al., 2015)</ref> and InferSent <ref type="bibr" target="#b8">(Conneau et al., 2017)</ref>. In computer vision, the convolutional network is often kept fixed (Razavian et al., 2014), although it has been found that unfreezing the convolutional network during later stages of training leads to significant improvements, e.g. in image-caption retrieval <ref type="bibr" target="#b13">(Faghri et al., 2017)</ref>.</p><p>Multimodal optimization is not trivial <ref type="bibr" target="#b44">(Wang et al., 2019)</ref>. In our model, image embeddings are mapped to BERT's token space using a set of randomly initialized mappings W n . Here, we explore a simple solution for optimization across multiple modalities, namely freezing and unfreezing encoding components at different stages, which we treat as a hyperparameter. If we first learn to map image embeddings to an appropriate subspace of the text encoder's input space, we may expect the network  to make more use of visual information than otherwise. Since the text modality is likely to dominate, we want to give the visual modality a chance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we describe the datasets, the baselines and provide other experimental details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation</head><p>We evaluate on a diverse set of multimodal classification tasks. We compare against two tasks also used in <ref type="bibr" target="#b20">(Kiela et al., 2018)</ref>: MM-IMDB <ref type="bibr" target="#b1">(Arevalo et al., 2017)</ref> and FOOD101 <ref type="bibr" target="#b45">(Wang et al., 2015)</ref>. To illustrate that the architecture generalizes beyond two input types, we additionally evaluate on V-SNLI <ref type="bibr" target="#b43">(Vu et al., 2018)</ref>, which consists of (premise, hypothesis, image) triplets. See <ref type="table" target="#tab_1">Table 1</ref> for dataset statistics and <ref type="table" target="#tab_3">Table 2</ref> for examples.</p><p>MM-IMDB The MM-IMDB dataset <ref type="bibr" target="#b1">(Arevalo et al., 2017)</ref> consists of movie plot outlines and movie posters. The objective is to classify each movie by genre. This is a multilabel prediction problem, i.e., one movie can have multiple genres.</p><p>The dataset was specifically introduced by <ref type="bibr" target="#b1">(Arevalo et al., 2017)</ref> to address the relative scarcity of highquality multimodal classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FOOD101</head><p>The UPMC FOOD101 dataset <ref type="bibr" target="#b45">(Wang et al., 2015)</ref> contains textual recipe descriptions for 101 food labels. The recipes were scraped from web pages and subsequently cleaned to extract text data. Each page was matched with a single image, where the images were obtained by querying Google Image Search for the given (possibly noisy) category. The objective is to find the corresponding food label for each recipe-image combination.</p><p>V-SNLI The V-SNLI dataset is based on the SNLI dataset <ref type="bibr" target="#b5">(Bowman et al., 2015)</ref>. The objective is to classify a premise and hypothesis, with associated image, into one of three categories: entailment, neutral or contradition. The SNLI dataset was created by having Turkers provide hypotheses for premises that were derived from captions in the Flickr30k dataset <ref type="bibr" target="#b47">(Young et al., 2014)</ref>. <ref type="bibr" target="#b43">(Vu et al., 2018)</ref> put the original images and the premisehypothesis pairs back together in order to create a grounded entailment task, called V-SNLI. V-SNLI also comes with a hard subset of the test set, originally created for SNLI, where a hypothesis-only classifier fails (Gururangan et al., 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare against strong unimodal baselines, as well as the highly competitive, more sophisticated multimodal fusion methods. In all cases we use a single linear classifier, fine-tuning the entire model end-to-end. We describe each of the baselines:</p><p>Bag of words (Bow) We sum 300-dimensional GloVe embeddings <ref type="bibr" target="#b29">(Pennington et al., 2014</ref>) (Common Crawl) for all words in the text, ignoring the visual features, and feed it to the classifier.</p><p>Text-only BERT (Bert) We take the first output of the final layer of a pre-trained base-uncased BERT model, and feed it to the classifier.</p><p>Image-only (Img) We take a standard pretrained ResNet-152 with average pooling as output, yielding a 2048-dimensional vector for each image, and classify it in the same way as the other systems.</p><p>Concat Bow + Img (ConcatBow) We concatenate the outputs of the Bow and the Img baselines. Concatenation is often used as a strong baseline in multimodal methods. In this case, the input to the classifier is 2048+300-dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Late Fusion</head><p>We average the scores of our best Bert and Img classifiers to get the final prediction.</p><p>FiLMBert We combine FiLM <ref type="bibr" target="#b30">(Perez et al., 2018)</ref> with BERT, where the BERT model predicts feature-wise gains and biases for a ConvNet classifier. We use fixed ResNet-152 features as input to the ConvNet, similar to <ref type="bibr" target="#b30">Perez et al. (2018)</ref>.</p><p>Concat BERT + Img (ConcatBert) We concatenate the outputs of the Bert and the Img baselines. In this case, the input to the classifier is 2048+768-dimensions. This is a competitive baseline, since it combines the best encoder for each modality such that the classifier has direct access to the encoder outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Making the Problem Harder</head><p>While we evaluate on a diverse set of multimodal classification tasks, there are actually surprisingly few high-quality tasks of this nature. In many cases, the textual modality is overly dominant (this is even a problem in VQA; see <ref type="bibr" target="#b15">Goyal et al., 2019)</ref>, making it difficult to tease apart differences between different multimodal methods, or to identify if it is actually worthwhile to incorporate multimodal information in the first place. As we observed earlier, Gururangan et al. <ref type="formula" target="#formula_1">(2018)</ref>   predicted answer and t is the correct answer. We take the top 10% of the most-different examples as the hard cases in the new test sets. The idea is that these are the examples that require more sophisticated multimodal reasoning, allowing us to better examine multimodal-specific performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other Implementation Details</head><p>For all models, we sweep by over the learning rate (in {1e ?4 , 5e ?5 }) and early stop on validation accuracy for the multiclass datasets, and Micro-F1 for the multilabel dataset. We additionally sweep over the number of epochs to keep the text and visual encoders fixed, as well as the number of image embeddings to use as input. For the Bert models, we use BertAdam <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> with a warmup rate of 0.1; for the other models we use regular Adam (Kingma and Ba, 2014). Since not all datasets are balanced, we weigh the class labels by their inverse frequency. Code and models are available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The main results can be found in <ref type="table" target="#tab_5">Table 3</ref>. In each case, we show mean performance over 5 runs with random seeds together with the standard deviation. We compare against the results of (Kiela et al., 2018) on MM-IMDB and FOOD101. They found that a bilinear-gated model worked best, meaning that one of the two input modalities is sig- On MM-IMDB, we also compare against Gated Multimodal Units <ref type="bibr" target="#b1">(Arevalo et al., 2017)</ref>, which are a special recurrent unit specifically designed for multimodal fusion (which similarly has one modality gate over the other). In addition, we compare to CentralNet <ref type="bibr" target="#b42">(Vielzeuf et al., 2018)</ref>, a multilayer approach for multimodal fusion that currently holds the state of the art on this dataset. For FOOD101, we include the original results from the paper <ref type="bibr" target="#b45">(Wang et al., 2015)</ref>, which were obtained by concatenating word2vec and VGGNet features and classifying. For V-SNLI, we compare to the state-of-the-art Visual Bilateral Multi-Perspective Matching (V-BiMPM) model of <ref type="bibr" target="#b43">(Vu et al., 2018)</ref>.</p><p>We find that the multimodal bitransformer (MMBT) outperforms the baselines by a significant margin. Late fusion, FiLMBert and Concat-Bert perform similarly. We speculate that the cause of MMBT's improvement over ConcatBert is its ability to let information from different modalities interact at different levels, via self-attention, rather than only at the final layer. Part of the improvement comes from Bert's superior performance (which makes sense, given text's dominance), but even then MMBT improves over Bert by e.g. ?3% on MM-IMDB Macro-F1 and an impressive ?6% on Food101 (i.e., an additional 1300 examples). In all cases, multimodal models outperform their direct unimodal counterparts.   <ref type="table" target="#tab_7">Table 4</ref> reports the results on the hard test sets. Recall that these were created by selecting examples where unimodal (Bert and Img) classifiers differed the most from the ground truth, meaning that these results provide insight into true multimodal performance. We also report results on VSNLI hard <ref type="bibr" target="#b16">(Gururangan et al., 2018)</ref>. We observe a similar pattern to the main results, with MMBT outperforming the alternatives. Note that on V-SNLI hard , <ref type="bibr" target="#b43">Vu et al. (2018)</ref> report a score of 73.75 for their best-performing architecture, compared to our 80.4. It is also interesting to observe that on that hard test set, the imageonly classifier already outperforms the text-only one, which is definitely not the case for the normal (non-hard) V-SNLI test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hard Testsets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Freezing Strategy</head><p>We conduct an analysis of whether it helps to initially freeze different pre-trained components. Freezing can help when learning to map from visual space to the expected token input space of the transformer. In other words, the randomly initialized components can be trained first. We can then unfreeze the image encoder, to make the image information maximally useful, before we unfreeze  the bitransformer to tune the entire system on the task. <ref type="figure" target="#fig_1">Figure 2</ref> shows the results, and indeed corroborates the intuition that it is useful to first learn to put the components together, then unfreeze the image encoder, and only after that unfreeze the pre-trained bitransformer. The optimal number of epochs is task-dependent, while unfreezing the image encoder early works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Number of Parameters</head><p>A possible explanation for the superior performance of the multimodal bitransformer over Con-catBert could be that it has slightly more parameters (i.e., an additional 2048 ? D versus 2048 ? N , where D is the embedding dimensionality and N is the number of classes), although the difference is small: 168M vs 170M parameters. To investigate this, we also compare against a ConcatBert with a 2-layer and 3-layer multi-layer perceptron (MLP) classifier on top, of 174M and 175M parameters respectively, rather than the single-layer logistic regression in MMBT. For MM-IMDB, ConcatBert-2 and ConcatBert-3 get a Macro-F1 of 60.21 ? .5 and 59.71 ? .4 and a Micro-F1 of 65.08 ? .3 and 64.82 ? .2 respectively; while for Food101 they get 91.13 ? .2 and 90.27 ? .2. This clearly demonstrates (cf. <ref type="table" target="#tab_5">Table 3</ref>) that MMBT is superior to ConcatBert, even when we give an already highly competitive baseline even more parameters and a deeper classifier. The results suggest that ConcatBert is more prone to overfitting 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Robustness to Missing Modalities</head><p>We compare ConcatBert and MMBT in a setting where only a subset of the dataset has images. To our knowledge, this setting has not been explored thoroughly in the literature. It is unclear a priori which of the two models would be more robust to this data regime, and this experiment provides a useful extra dimension for comparing mid-level <ref type="bibr">2</ref> The result was the same with more image embeddings. fusion with the more sophisticated type of fusion provided by MMBT. <ref type="figure" target="#fig_2">Figure 3</ref> shows that performance drops with fewer images. It is interesting to observe that MMBT is much more robust to missing images than ConcatBert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison to ViLBERT</head><p>We examine the effectiveness of fusing unimodally pretrained components by comparing to selfsupervised multimodally pretrained models. We take ViLBERT  as the canonical example of that class of models. ViLBERT was trained multimodally on images and captions, and is meant to be the "BERT of vision and language". It uses Faster RCNN-extracted bounding boxes , kept fixed during training. Our focus on these somewhat out-of-the-ordinary tasks now proves fruitful, since it allows us to compare these models on a level playing field. <ref type="table" target="#tab_9">Table 5</ref> shows the results. We compare against a variety of ViLBert models, both the standard pretrained version as well as the versions fine-tuned for particular tasks like VQA. The latter approach is not proposed in the original ViLBert paper, but similar "two-stage pre-training" approaches have proven effective for fine-tuning BERT on unimodal tasks <ref type="bibr" target="#b33">(Phang et al., 2018)</ref>. We tune using the hyperparameter sets used in that paper: (batch size, learn-ing rate) ? {(64, 2e ?5 ), (256, 4e ?5 )}. We observe that our straightforward MMBT model is surprisingly competitive. On MM-IMDB, it matches the task-specific ViLBERT models on Macro-F1. On the Hard subset of that dataset, which more accurately measures multimodal performance, MMBT matches ViLBert's performance. For FOOD-101, we observe a similar story, with performance being remarkably close, occasionally outperforming taskspecific models, in particular on the Hard subset. Our results suggest that self-supervised multimodal pre-training has more room for improvement, and that the supervised fusion of unimodally-pretrained components is remarkably competitive.</p><p>Our method may be more preferable depending on the constraints: with new models coming out every month, these will be easy to incorporate into the architecture. To illustrate this point (obviously not a fair comparison), we use a BERT-Large model instead to make MMBT outperform ViLBERT. This is trivial to do in our setting, but for ViLBERT would require retraining from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Transformers <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref> have been used to encode sequential data for classification with great success when pre-trained for language modeling or language masking and subsequently finetuned <ref type="bibr" target="#b34">(Radford et al., 2018;</ref><ref type="bibr" target="#b11">Devlin et al., 2019)</ref>.</p><p>The question of how to effectively do multimodal fusion has a long history <ref type="bibr" target="#b2">(Baltru?aitis et al., 2019)</ref>. While concatenation can be considered the default, other fusion methods have been explored e.g. for lexical representation learning <ref type="bibr" target="#b6">(Bruni et al., 2014;</ref><ref type="bibr" target="#b23">Lazaridou et al., 2015)</ref>. In classification, <ref type="bibr" target="#b20">Kiela et al. (2018)</ref> examine various fusion methods for pre-trained fixed representations, and find that a bilinear combination of data with gating worked best. Our supervised multimodal bitransformer has fusion between the modalities via self-attention over many different layers.</p><p>Applications in multimodal NLP range from classification to cross-modal retrieval <ref type="bibr" target="#b46">(Weston et al., 2011;</ref><ref type="bibr" target="#b14">Frome et al., 2013;</ref><ref type="bibr" target="#b36">Socher et al., 2013)</ref> to image captioning <ref type="bibr" target="#b4">(Bernardi et al., 2016)</ref> to visual question answering <ref type="bibr" target="#b0">(Antol et al., 2015)</ref> and multimodal machine translation <ref type="bibr" target="#b12">(Elliott et al., 2017)</ref>. Multimodal information is also useful in learning human-like meaning representations <ref type="bibr" target="#b3">(Baroni, 2016;</ref><ref type="bibr" target="#b19">Kiela, 2017)</ref>. Multimodal bitransformers provide what is effectively a deep fusion method. Related deep fusion methods include multimodal transformers <ref type="bibr" target="#b40">(Tsai et al., 2019)</ref>, CentralNet <ref type="bibr" target="#b42">(Vielzeuf et al., 2018)</ref>, MFAS <ref type="bibr" target="#b31">(P?rez-R?a et al., 2019)</ref> and Tensor Fusion Networks <ref type="bibr" target="#b48">(Zadeh et al., 2017)</ref>.</p><p>There has been a large number of self-supervised multimodal architectures published recently, e.g. ViLBERT , VisualBERT , LXMERT <ref type="bibr" target="#b39">(Tan and Bansal, 2019)</ref>, VL-BERT <ref type="bibr" target="#b37">(Su et al., 2019)</ref>, VideoBERT <ref type="bibr" target="#b38">(Sun et al., 2019)</ref>, and others. Our model differs from these self-supervised architectures in that the individual components are pretrained only unimodally. This has pros and cons: our method is straightforward and intuitive, easy to implement even for existing self-supervised encoders, and obtains impressive improvements. If a new and better text or vision model comes out, it is trivial to replace components. On the other hand, it is not able to fully leverage multimodal information during self-supervised pretraining. That said, it does potentially have access to orders of magnitude more unimodal data. In other words, if anything, these supervised multimodal bitransformers should provide a strong baseline for gauging if and how much self-supervised multimodal pretraining actually helps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduced a supervised multimodal bitransformer model. We compared against several baselines on a variety of tasks, including on hard test sets created specifically for examining multimodal performance (i.e., where unimodal performance fails). We find that the proposed architecture significantly outperforms the existing state of the art, as well as strong baselines. We then conducted an analysis of multimodal optimization, exploring a freezing/unfreezing strategy, and looked at the number of parameters, showing that the strong baseline with more parameters and a deeper classifier was still outperformed.</p><p>Our architecture consists of components that were pre-trained individually as unimodal tasks, which already showed great improvements over alternatives. It is as of yet unclear if multimodal self-supervised models are going to be generally useful. We compared to ViLBERT and showed that the proposed model performs competitively, while being much simpler. The methods outlined here should serve as a useful and powerful baseline to gauge the performance of self-supervised multimodal models. Supervised multimodal bi-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of the multimodal bitransformer architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Analysis of freezing pre-trained text and image components for N epochs of training. moided and then gates over the other input bilinearly, i.e. by taking an outer product. Note that in our case, with 2048-dimensional ResNet outputs and 768-dimensional Bert outputs, bilinear gated would need a 2048?768?101-dimensional output layer (approximately 158M parameters just for the classifier on top), which is not practical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Performance (MicroF1) on MM-IMDB when we drop the image for a percentage of the training set, measuring robustness to missing images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation tasks used for evaluating performance.</figDesc><table><row><cell>output is often the result of a pooling operation</cell></row><row><cell>over feature maps. For multimodal bitransform-</cell></row><row><cell>ers, however, this pooling is not necessary, since</cell></row><row><cell>they can handle arbitrary numbers of dense inputs.</cell></row><row><cell>Thus, we experiment with having the pooling yield</cell></row><row><cell>not one single output vector, but N separate image</cell></row><row><cell>embeddings, unlike in a regular convolutional neu-</cell></row><row><cell>ral network. In this case we use a ResNet-152 (He</cell></row><row><cell>et al., 2016) with average pooling over K?M grids</cell></row><row><cell>in the image, yielding N = KM output vectors</cell></row><row><cell>of 2048 dimensions each, for every image. Images</cell></row><row><cell>are resized, center-cropped and normalized.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>in a stable on Christmas, right next to You Know Who. The wise men appear and begin to distribute gifts. The star moves further, so they take it all back and move on. This is how Brian's life goes. [...] He joins the Peoples' Front of Judea, one of several dozen separatist groups who actually do nothing, but really hate the Romans. While not about Jesus, it is about those who hadn't time, or interest to listen to his message. Many Political and Social comments.</figDesc><table><row><cell>Dataset</cell><cell>Label</cell><cell>Image</cell><cell>Text</cell></row><row><cell cols="4">MM-IMDB Brian is born FOOD101 Comedy Cup cakes [...] simple and oh so delicious these basic cupcakes make a lovely birthday</cell></row><row><cell></cell><cell></cell><cell></cell><cell>treat makes 24 ingredients 200g unsalted butter softened 1 teaspoon vanilla</cell></row><row><cell></cell><cell></cell><cell></cell><cell>extract 1 cup caster sugar 3 eggs 2 1 2 cups self raising flour [...] bake for 15</cell></row><row><cell></cell><cell></cell><cell></cell><cell>to 17 minutes alternatively for 1 tablespoon capacity mini muffin pans use</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1 tablespoon mixture bake for 10 to 12 minutes 4 stand cakes in pans for 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>minutes transfer to a wire rack to cool 5 decorate to suit your party theme</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[...]</cell></row><row><cell>V-SNLI</cell><cell>Entailment</cell><cell></cell><cell>Premise: Children smiling and waving at camera. Hypothesis: There are children present.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Example data for each of the datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>= t|I)p(a = t|T ), where I and T are the image and textual information respectively, a is the</figDesc><table><row><cell>created hard subsets of the SNLI dataset where a hypothesis-only baseline was unable to correctly classify the example, rectifying artifacts in the original SNLI test set. Here, we fol-low a similar approach, and create hard multimodal test sets for our other two tasks. We construct hard test sets by taking the ex-amples where the Bert and Img classifier predic-tions are most different from the ground truth classes in the test set, i.e. examples that maxi-mize p(a MM-IMDB FOOD-101 V-SNLI GMU 51.4/63.0 --CentralNet 56.1/63.9 --W+V -85.1 -BG -/62.3 90.8 -V-BiMPM --86.99 Bow 38.1?.2/45.6?.2 72.4?.3 48.6?.3 Img 32.5?.7/44.4?.3 63.2?.6 33.8?.3 Bert 59.9?.3/65.4?.1 87.2?.1 90.1?.3 Late Fusion 59.4?.1/66.2?.0 91.1?.1 90.1?.0 ConcatBow 43.8?.4/53.6?.4 79.0?.9 49.5?.1 FiLMBert 59.7?.4/65.1?.2 90.2?.3 90.2?.2 ConcatBert 60.5?.3/65.9?.2 90.0?.6 90.2?.4 MMBT 61.6?.2/66.8?.1 92.1?.1 90.4?.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Main Results. MM-IMDB is Macro F1 /</cell></row><row><cell>Micro F1; others are Accuracy. Compared against</cell></row><row><cell>GMU (Arevalo et al., 2017), CentralNet (Vielzeuf</cell></row><row><cell>et al., 2018), Word2vec+VGGNet (W+V) (Wang et al.,</cell></row><row><cell>2015), Bilinear-gated (BG) (Kiela et al., 2018) and V-</cell></row><row><cell>BiMPM (Vu et al., 2018).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Hard Subsets (marked ?). Late is Late Fusion. Concat is ConcatBert. MM-IMDB is Macro F1 / Micro F1; others are Accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>6?.2 / 66.8?.1 65.3?.4 / 68.6?.4 92.1?.1 92.4?.5 MMBT-Large 63.2?.2 / 68.0?.2 68.2?.5 / 70.3?.4 93.2?.1 93.4?.3 ViLBert-VQA 60.0?.3 / 66.4?.2 62.7?.6 / 66.2?.4 92.1?.1 92.4?.3 ViLBert-VCR 61.6?.3 / 67.6?.2 63.4?.9 / 66.9?.4 92.1?.1 92.1?.3 ViLBert-Refcoco 61.4?.3 / 67.7?.1 63.4?.5 / 67.1?.4 92.2?.1 92.1?.3 ViLBert-Flickr30k 61.4?.3 / 67.8?.1 63.4?.9 / 67.0?.5 92.2?.1 92.2?.3 ViLBert 63.0?.2 / 68.6?.1 65.4?1. / 68.6?.4 92.9?.1 92.9?.3</figDesc><table><row><cell></cell><cell>MM-IMDB</cell><cell>-Hard FOOD-101</cell><cell>-Hard</cell></row><row><cell>MMBT</cell><cell>61.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison of MMBT to ViLBert on MM-IMDB and FOOD-101.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/facebookresearch/mmbt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>transformers are straightforward and intuitive, and importantly, are easy to implement even for existing self-supervised encoders.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01992</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grounding distributional semantics in the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic description generation from images: A survey of models, datasets, and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruket</forename><surname>Cakici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="409" to="442" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">Vse++: Improving visualsemantic embeddings with hard negatives</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="398" to="414" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02324</idno>
		<title level="m">Annotation artifacts in natural language inference data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep Embodiment: Grounding Semantics in Perceptual Modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient large-scale multimodal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02598</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">PyTorch: Tensors and dynamic neural networks in python with strong GPU acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Soumith Chintala, and Gregory Chanan. PyTorch</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Harm de Vries, Vincent Dumoulin, and Aaron Courville</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MFAS: multimodal fusion architecture search. Proceedings of CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arxiv preprint 1903.06496</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>abs/1811.01088</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training. Technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cnn features offthe-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pretraining of generic visual-linguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01766</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>arxiv preprint 1906.00295</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Centralnet: a multilayer approach for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grounded textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trong</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliia</forename><surname>Erofeeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayeh</forename><surname>Jafaritazehjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Linders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Testoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2354" to="2368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12681</idno>
		<title level="m">What makes training multi-modal networks hard? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recipe recognition with large multimodal food dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devinder</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Precioso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Second International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
