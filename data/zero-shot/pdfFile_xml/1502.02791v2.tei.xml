<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-05-27">27 May 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<email>mingsheng@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">TNList Lab for Info. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep3">Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<email>yue-cao14@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">TNList Lab for Info. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep3">Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">TNList Lab for Info. Sci. &amp; Tech</orgName>
								<orgName type="department" key="dep3">Institute for Data Science</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan@berkeley</forename><surname>Edu</surname></persName>
						</author>
						<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-05-27">27 May 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The generalization error of supervised learning machines with limited training samples will be unsatisfactorily large, while manual labeling of sufficient training data for diverse application domains may be prohibitive. Therefore, there is incentive to establishing effective algorithms to reduce the labeling cost, typically by leveraging off-the-shelf labeled data from relevant source domains to the target domains. Domain adaptation addresses the problem that we have data from two related domains but under different distributions. The domain discrepancy poses a major obstacle in adapting predictive models across domains. For example, an object recognition model trained on manually annotated images may not generalize well on testing images under substantial variations in the pose, occlusion, or illumination. Domain adaptation establishes knowledge transfer from the labeled source domain to the unlabeled target domain by exploring domain-invariant structures that bridge different domains of substantial distribution discrepancy <ref type="bibr" target="#b22">(Pan &amp; Yang, 2010)</ref>.</p><p>One of the main approaches to establishing knowledge transfer is to learn domain-invariant models from data, which can bridge the source and target domains in an isomorphic latent feature space. In this direction, a fruitful line of prior work has focused on learning shallow features by jointly minimizing a distance metric of domain discrepancy <ref type="bibr" target="#b23">(Pan et al., 2011;</ref><ref type="bibr" target="#b19">Long et al., 2013;</ref><ref type="bibr" target="#b1">Baktashmotlagh et al., 2013;</ref><ref type="bibr" target="#b11">Gong et al., 2013;</ref><ref type="bibr" target="#b33">Zhang et al., 2013;</ref><ref type="bibr" target="#b8">Ghifary et al., 2014;</ref><ref type="bibr" target="#b30">Wang &amp; Schneider, 2014)</ref>. However, recent studies have shown that deep neural networks can learn more transferable features for domain adaptation <ref type="bibr" target="#b9">(Glorot et al., 2011;</ref><ref type="bibr" target="#b6">Donahue et al., 2014;</ref><ref type="bibr" target="#b32">Yosinski et al., 2014)</ref>, which produce breakthrough results on some domain adaptation datasets. Deep neural networks are able to disentangle exploratory factors of variations underlying the data samples, and group features hierarchically in accordance with their relatedness to invariant factors, making representations robust to noise.</p><p>While deep neural networks are more powerful for learning general and transferable features, the latest findings also reveal that the deep features must eventually transition from general to specific along the network, and feature transferability drops significantly in higher layers with increasing domain discrepancy. In other words, the features computed in higher layers of the network must depend greatly on the specific dataset and task <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>, which are task-specific features and are not safely transferable to novel tasks. Another curious phenomenon is that disentangling the variational factors in higher layers of the network may enlarge the domain discrepancy, as different domains with the new deep representations become more "compact" and are more mutually distinguishable <ref type="bibr" target="#b9">(Glorot et al., 2011)</ref>. Although deep features are salient for discrimination, enlarged dataset bias may deteriorate domain adaptation performance, resulting in statistically unbounded risk for the target tasks <ref type="bibr" target="#b20">(Mansour et al., 2009;</ref><ref type="bibr" target="#b3">Ben-David et al., 2010)</ref>.</p><p>Inspired by the literature's latest understanding about the transferability of deep neural networks, we propose in this paper a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. The main idea of this work is to enhance the feature transferability in the task-specific layers of the deep neural network by explicitly reducing the domain discrepancy. To establish this goal, the hidden representations of all the task-specific layers are embedded to a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. As mean embedding matching is sensitive to the kernel choices, an optimal multi-kernel selection procedure is devised to further reduce the domain discrepancy. In addition, we implement a linear-time unbiased estimate of the kernel mean embedding to enable scalable training, which is very desirable for deep learning. Finally, as deep models pre-trained with large-scale repositories such as ImageNet <ref type="bibr" target="#b24">(Russakovsky et al., 2014)</ref> are representative for generalpurpose tasks <ref type="bibr" target="#b32">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b16">Hoffman et al., 2014)</ref>, the proposed DAN model is trained by fine-tuning from the AlexNet model  pre-trained on ImageNet, which is implemented in Caffe . Comprehensive empirical evidence demonstrates that the proposed architecture outperforms state-of-the-art results evaluated on the standard domain adaptation benchmarks.</p><p>The contributions of this paper are summarized as follows. (1) We propose a novel deep neural network architecture for domain adaptation, in which all the layers corresponding to task-specific features are adapted in a layerwise manner, hence benefiting from "deep adaptation." (2) We explore multiple kernels for adapting deep representations, which substantially enhances adaptation effectiveness compared to single kernel methods. Our model can yield unbiased deep features with statistical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A related literature is transfer learning <ref type="bibr" target="#b22">(Pan &amp; Yang, 2010)</ref>, which builds models that bridge different domains or tasks, explicitly taking domain discrepancy into consideration. Transfer learning aims to mitigate the effort of manual labeling for machine learning <ref type="bibr" target="#b23">(Pan et al., 2011;</ref><ref type="bibr" target="#b11">Gong et al., 2013;</ref><ref type="bibr" target="#b33">Zhang et al., 2013;</ref><ref type="bibr" target="#b30">Wang &amp; Schneider, 2014)</ref> and computer vision <ref type="bibr" target="#b25">(Saenko et al., 2010;</ref><ref type="bibr" target="#b10">Gong et al., 2012;</ref><ref type="bibr" target="#b1">Baktashmotlagh et al., 2013;</ref><ref type="bibr" target="#b19">Long et al., 2013)</ref>, etc. It is widely recognized that the domain discrepancy in the probability distributions of different domains should be formally measured and reduced. The major bottleneck is how to match different domain distributions effectively. Most existing methods learn a new shallow representation model in which the domain discrepancy can be explicitly reduced. However, without learning deep features which can suppress domain-specific factors, the transferability of shallow features could be limited by the task-specific variability.</p><p>Deep neural networks learn nonlinear representations that disentangle and hide different explanatory factors of variation behind data samples <ref type="bibr" target="#b4">(Bengio et al., 2013)</ref>. The learned deep representations manifest invariant factors underlying different populations and are transferable from the original tasks to similar novel tasks <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>. Hence, deep neural networks have been explored for domain adaptation <ref type="bibr" target="#b9">(Glorot et al., 2011;</ref><ref type="bibr" target="#b5">Chen et al., 2012)</ref>, multimodal and multi-source learning problems <ref type="bibr" target="#b21">(Ngiam et al., 2011;</ref><ref type="bibr" target="#b7">Ge et al., 2013)</ref>, where significant performance gains have been obtained. However, all these methods depend on the assumption that deep neural networks can learn invariant representations that are transferable across different tasks. In reality, the domain discrepancy can be alleviated, but not removed, by deep neural networks <ref type="bibr" target="#b9">(Glorot et al., 2011)</ref>. Dataset shift has posed a bottleneck to the transferability of deep networks, resulting in statistically unbounded risk for target tasks <ref type="bibr" target="#b20">(Mansour et al., 2009;</ref><ref type="bibr" target="#b3">Ben-David et al., 2010)</ref>.</p><p>Our work is primarily motivated by <ref type="bibr" target="#b32">Yosinski et al. (2014)</ref>, which comprehensively explores feature transferability of deep convolutional neural networks. The method focuses on a different scenario where the learning tasks are different across domains, hence it requires sufficient target labeled examples such that the source network can be finetuned to the target task. In many real problems, labeled data is usually limited especially for a novel target task, hence the method cannot be directly applicable to domain adaptation. There are several very recent efforts in learning domain-invariant features in the context of shallow neural networks <ref type="bibr" target="#b0">(Ajakan et al., 2014;</ref><ref type="bibr" target="#b8">Ghifary et al., 2014)</ref>. Due to the limited capacity of shallow architectures, the performance of these proposals does not surpass deep CNN . <ref type="bibr" target="#b29">Tzeng et al. (2014)</ref> proposed a DDC model that adds an adaptation layer and a dataset shift loss to the deep CNN for learning a domain-invariant representation. While performance was improved, DDC only adapts a single layer of the network, which may be restrictive in that there are multiple layers where the hidden features are not transferable <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>. DDC is also limited by suboptimal kernel matching of probability distributions <ref type="bibr" target="#b13">(Gretton et al., 2012b)</ref> and its quadratic computational cost that restricts transferability and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Adaptation Networks</head><p>In unsupervised domain adaptation, we are given a source domain D s = {(x s i , y s i )} ns i=1 with n s labeled examples, and a target domain D t = {x t j } nt j=1 with n t unlabeled examples. The source domain and target domain are characterized by probability distributions p and q, respectively. We aim to construct a deep neural network which is able to learn transferable features that bridge the cross-domain discrepancy, and build a classifier y = ?(x) which can minimize target risk ? t (?) = Pr (x,y)?q [? (x) = y] using source supervision. In semi-supervised adaptation where the target has a small number of labeled examples, we denote by D a = {(x a i , y a i )} the n a annotated examples of source and target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MK-MMD</head><p>Domain adaptation is challenging in that the target domain has no (or only limited) labeled information.</p><p>To approach this problem, many existing methods aim to bound the target error by the source error plus a discrepancy metric between the source and the target <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>. Two classes of statistics have been explored for the two-sample testing, where acceptance or rejection decisions are made for a null hypothesis p = q, given samples generated respectively from p and q: energy distances and maximum mean discrepancies (MMD) <ref type="bibr" target="#b26">(Sejdinovic et al., 2013)</ref>. In this paper, we focus on the multiple kernel variant of MMD (MK-MMD) proposed by <ref type="bibr" target="#b13">Gretton et al. (2012b)</ref>, which is formalized to jointly maximize the two-sample test power and minimize the Type II error, i.e., the failure of rejecting a false null hypothesis.</p><p>Denote by H k be the reproducing kernel Hilbert space (RKHS) endowed with a characteristic kernel k. The mean embedding of distribution p in H k is a unique element</p><formula xml:id="formula_0">? k (p) such that E x?p f (x) = f (x) , ? k (p) H k for all f ? H k . The MK-MMD d k (p, q)</formula><p>between probability distributions p and q is defined as the RKHS distance between the mean embeddings of p and q. The squared formulation of MK-MMD is defined as</p><formula xml:id="formula_1">d 2 k (p, q) E p [? (x s )] ? E q ? x t 2 H k .<label>(1)</label></formula><p>The most important property is that p = q iff d 2 k (p, q) = 0 <ref type="bibr" target="#b12">(Gretton et al., 2012a)</ref>. The characteristic kernel associated with the feature map ?, k (x s ,</p><formula xml:id="formula_2">x t ) = ? (x s ) , ? (x t ) , is defined as the convex combination of m PSD kernels {k u }, K k = m u=1 ? u k u : m u=1 ? u = 1, ? u 0, ?u ,<label>(2)</label></formula><p>where the constraints on coefficients {? u } are imposed to guarantee that the derived multi-kernel k is characteristic. As studied theoretically in <ref type="bibr" target="#b13">Gretton et al. (2012b)</ref>, the kernel <ref type="table">input  conv1  conv2  conv3  conv4  conv5  fc6  fc7  fc8</ref> source output target output frozen frozen frozen finetune finetune learn learn learn learn <ref type="figure">Figure 1</ref>. The DAN architecture for learning transferable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MK-MMD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MK-MMD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MK-MMD</head><p>Since deep features eventually transition from general to specific along the network, (1) the features extracted by convolutional layers conv1-conv3 are general, hence these layers are frozen, (2) the features extracted by layers conv4-conv5 are slightly less transferable, hence these layers are learned via fine-tuning, and (3) fully connected layers f c6-f c8 are tailored to fit specific tasks, hence they are not transferable and should be adapted with MK-MMD.</p><p>adopted for the mean embeddings of p and q is critical to ensure the test power and low test error. The multi-kernel k can leverage different kernels to enhance MK-MMD test, leading to a principled method for optimal kernel selection.</p><p>One of the feasible strategies for controlling the domain discrepancy is to find an abstract feature representation through which the source and target domains are similar <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>. Although this idea has been explored in several papers <ref type="bibr" target="#b23">(Pan et al., 2011;</ref><ref type="bibr" target="#b33">Zhang et al., 2013;</ref><ref type="bibr" target="#b30">Wang &amp; Schneider, 2014)</ref>, to date there has been no attempt to enhance the transferability of feature representation via MK-MMD in deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Adaptation Networks (DAN)</head><p>In this paper, we explore the idea of MK-MMD-based adaptation for learning transferable features in deep networks. We start with deep convolutional neural networks (CNN) ), a strong model when it is adapted to novel tasks <ref type="bibr" target="#b16">Hoffman et al., 2014)</ref>. The main challenge is that the target domain has no or just limited labeled information, hence directly adapting CNN to the target domain via fine-tuning is impossible or is prone to over-fitting. With the idea of domain adaptation, we are targeting a deep adaptation network (DAN) that can exploit both source-labeled data and target-unlabeled data. <ref type="figure">Figure 1</ref> gives an illustration of the proposed DAN model.</p><p>We extend the AlexNet architecture , which is comprised of five convolutional layers (conv1-conv5) and three fully connected layers (f c6f c8). Each f c layer ? learns a nonlinear mapping</p><formula xml:id="formula_3">h ? i = f ? W ? h ??1 i + b ? , where h ?</formula><p>i is the ?th layer hidden representation of point x i , W ? and b ? are the weights and bias of the ?th layer, and f ? is the activation, taking as rectifier units f ? (x) = max(0, x) for hidden layers or softmax units f ? (x) = e x / |x| j=1 e xj for the output layer. Letting</p><formula xml:id="formula_4">? = W ? , b ? l ?=1 denote the set of all CNN parameters, the empirical risk of CNN is min ? 1 n a na i=1 J (? (x a i ) , y a i ),<label>(3)</label></formula><p>where J is the cross-entropy loss function, and ? (x a i ) is the conditional probability that the CNN assigns x a i to label y a i . We will not discuss how to compute the convolutional layers as we will not impose distribution-adaptation regularization in those layers, given that the convolutional layers can learn generic features that tend to be transferable in layers conv1-conv3 and are slightly domain-biased in conv4-conv5 <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>. Hence, when adapting the pre-trained AlexNet to the target, we opt to freeze conv1-conv3 and fine-tune conv4-conv5 to preserve the efficacy of fragile co-adaptation .</p><p>In standard CNNs, deep features must eventually transition from general to specific by the last layer of the network, and the transferability gap grows with the domain discrepancy and becomes particularly large when transferring the higher layers f c6-f c8 <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>. In other words, the f c layers are tailored to their original task at the expense of degraded performance on the target task, hence they cannot be directly transferred to the target domain via fine-tuning with limited target supervision. In this paper, we fine-tune CNN on the source labeled examples and require the distributions of the source and target to become similar under the hidden representations of fully connected layers f c6f c8. This can be realized by adding an MK-MMD-based multi-layer adaptation regularizer (1) to the CNN risk <ref type="formula" target="#formula_4">(3)</ref>:</p><formula xml:id="formula_5">min ? 1 n a na i=1 J (? (x a i ) , y a i ) + ? l2 ?=l1 d 2 k D ? s , D ? t ,<label>(4)</label></formula><p>where ? &gt; 0 is a penalty parameter, l 1 and l 2 are layer indices between which the regularizer is effective. In our implementation of DAN, we set l 1 = 6 and l 2 = 8, although different configurations are also possible, depending on the size of the labeled source dataset and the number of parameters in the layers that are to be fine-tuned. D ? * = h * ? i is the ?th layer hidden representation for the source and target examples, and d 2 k D ? s , D ? t is the MK-MMD between the source and target evaluated on the ?th layer representation.</p><p>Training a deep CNN requires a large amount of labeled data, which is prohibitive for many domain adaptation problems, hence we start with an AlexNet model pretrained on ImageNet 2012 and fine-tune it as in <ref type="bibr" target="#b32">Yosinski et al. (2014)</ref>. With the proposed DAN optimization framework (4), we are able to learn transferable features from a source domain to a related target domain. The learned representation can both be salient benefiting from CNN, and unbiased thanks to MK-MMD. Two important advantages that distinguish DAN from relevant literature are: (1) multilayer adaptation. As revealed by <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>, feature transferability gets worse on conv4-conv5 and significantly drops on f c6-f c8, hence it is critical to adapt multiple layers instead of only one layer. In other words, adapting a single layer cannot undo the dataset bias between the source and the target, since there are other layers that are not transferable. Another benefit of multi-layer adaptation is that by jointly adapting the representation layers and the classifier layer, we could essentially bridge the domain discrepancy underlying both the marginal distribution and the conditional distribution, which is crucial for domain adaptation .</p><p>(2) multi-kernel adaptation. As pointed out by <ref type="bibr" target="#b13">Gretton et al. (2012b)</ref>, kernel choice is critical to the testing power of MMD since different kernels may embed probability distributions in different RKHSs where different orders of sufficient statistics can be emphasized. This is crucial for moment matching, which is not well explored by previous domain adaptation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Algorithm</head><p>Learning ? Using the kernel trick, MK-MMD (1) can be computed as the expectation of kernel functions d 2</p><formula xml:id="formula_6">k (p, q) = E x s x ? s k(x s , x ? s ) + E x t x ?t k(x t , x ? t ) ? 2E x s x t k(x s , x t ), where x s , x ? s iid ? p, x t ,</formula><p>x ? t iid ? q, and k ? K. However, this computation incurs a complexity of O(n 2 ), which is rather undesirable for deep CNNs, as the power of deep neural networks largely derives from learning with large-scale datasets. Moreover, the summation over pairwise similarities between data points makes mini-batch stochastic gradient descent (SGD) more difficult, whereas mini-batch SGD is crucial to the training effectiveness of deep networks. While prior work based on MMD <ref type="bibr" target="#b23">(Pan et al., 2011;</ref><ref type="bibr" target="#b29">Tzeng et al., 2014)</ref> rarely addresses this issue, we believe it is critical in the context of deep learning. In this paper, we adopt the unbiased estimate of MK-MMD <ref type="bibr" target="#b13">(Gretton et al., 2012b)</ref> which can be computed with linear complexity. More specifically,</p><formula xml:id="formula_7">d 2 k (p, q) = 2 ns ns/2 i=1 g k (z i ), where we denote quad-tuple z i (x s 2i?1 , x s 2i , x t 2i?1 , x t 2i )</formula><p>, and evaluate multi-kernel function k on each quad-tuple z i by</p><formula xml:id="formula_8">g k (z i ) k(x s 2i?1 , x s 2i ) + k(x t 2i?1 , x t 2i ) ? k(x s 2i?1 , x t 2i ) ? k(x s 2i , x t 2i?1 )</formula><p>. This approach computes an expectation of independent variables as in (1) with cost O(n).</p><p>When we train deep CNN by mini-batch SGD, we only need to consider the gradient of objective (4) with respect to each data point x i . Since the linear-time MK-MMD takes a nice summation form that can be readily decoupled into the sum of g k (z i )'s, we only need to compute the gradients</p><formula xml:id="formula_9">?g k (z ? i ) ?? ? for the quad-tuple z ? i = h s? 2i?1 , h s? 2i , h t? 2i?1 , h t?<label>2i</label></formula><p>of the ?th layer hidden representation. To be consistent with the gradient of MK-MMD, we need to compute the corresponding gradient of CNN risk ?J(zi) ?? ? , where J (z i ) = i ? J (? (x a i ? ) , y a i ? ), and {(x a i ? , y a i ? )} indicates the labeled examples in quad-tuple z i -for instance, in unsupervised adaptation where the target domain has no labeled data, we have {(x a i ? , y a i ? )} = {(x s 2i?1 , y s 2i?1 ), (x s 2i , y s 2i )}. To perform a mini-batch update, we compute the gradient of objective (4) with respect to the ?th layer parameter ? ? as</p><formula xml:id="formula_10">? ? ? = ?J (z i ) ?? ? + ? ?g k z ? i ?? ? .<label>(5)</label></formula><p>Such a mini-batch SGD can be easily implemented within the Caffe framework for CNNs . Given kernel k as the linear combination of m Gaussian kernels</p><formula xml:id="formula_11">{k u (x i , x j ) = e ? xi?xj 2 /?u }, the gradient ?g k (z ? i ) ?? ?</formula><p>can be readily computed using the chain rule. For instance,</p><formula xml:id="formula_12">?k(h s? 2i?1 , h t? 2i ) ?W ? = ? m u=1 2? u ? u k u h s? 2i?1 , h t? 2i ? h s? 2i?1 ? h t? 2i ? I h s(??1) 2i?1 ? I h t(??1) 2i T ,<label>(6)</label></formula><p>where the last row computes the gradient of the ?th layer rectifier units, with I being defined as an indicator such that</p><formula xml:id="formula_13">I h ??1 ji = h ??1 ji if W ? j? h ??1 i + b ? j 0, else I h ??1 ji = 0.</formula><p>Learning ? The proposed multi-layer adaptation regularizer performs layerwise matching by MK-MMD, hence we seek to learn optimal kernel parameter ? for MK-MMD by jointly maximizing the test power and minimizing the Type II error <ref type="bibr" target="#b13">(Gretton et al., 2012b)</ref>, leading to the optimization</p><formula xml:id="formula_14">max k?K d 2 k D ? s , D ? t ? ?2 k ,<label>(7)</label></formula><p>where ? 2</p><formula xml:id="formula_15">k = E z g 2 k (z) ? [E z g k (z)] 2 is estimation variance. Letting d = (d 1 , d 2 , . . . , d m ) T , each d u is MMD via kernel k u . Covariance Q = cov (g k ) ? R m?m can be computed in O(m 2 n) cost, i.e. Q uu ? = 4 ns ns/4 i=1 g ? ku (z i ) g ? k u ? (z i ), wherez i (z 2i?1 , z 2i ) and g ? ku (z i ) g ku (z 2i?1 ) ? g ku (z 2i</formula><p>). Hence (7) reduces to a quadratic program (QP),</p><formula xml:id="formula_16">min d T ?=1,? 0 ? T (Q + ?I) ?,<label>(8)</label></formula><p>where ? = 10 ?3 is a small regularizer to make the problem well-defined. By solving (8), we obtain a multi-kernel k = m u=1 ? u k u that jointly maximizes the test power and minimizes the Type II error.</p><p>We note that the DAN objective (4) is essentially a minimax problem; i.e., we compute min</p><formula xml:id="formula_17">? max K d 2 k D ? s , D ? t ? ?2 k .</formula><p>The CNN parameter ? is learned by minimizing MK-MMD as a domain discrepancy, while the MK-MMD parameter ? is learned by minimizing the Type II error. Both criteria are dedicated to an effective adaptation of domain discrepancy, aiming to consolidate the transferability of DAN features. We accordingly adopt an alternating optimization that updates ? by mini-batch SGD (5) and ? by QP <ref type="formula" target="#formula_16">(8)</ref> iteratively. Both updates cost O(n) and are scalable to large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis</head><p>We provide an analysis of the expected target-domain risk of our approach, making use of the theory of domain adaptation <ref type="bibr" target="#b2">(Ben-David et al., 2007;</ref><ref type="bibr" target="#b20">Mansour et al., 2009)</ref> and the theory of kernel embedding of probability distributions <ref type="bibr" target="#b27">(Sriperumbudur et al., 2009;</ref><ref type="bibr" target="#b12">Gretton et al., 2012a;</ref>.</p><p>Theorem 1 Let ? ? H be a hypothesis, ? s (?) and ? t (?) be the expected risks of source and target respectively, then</p><formula xml:id="formula_18">? t (?) ? s (?) + 2d k (p, q) + C,<label>(9)</label></formula><p>where C is a constant for the complexity of hypothesis space and the risk of an ideal hypothesis for both domains.</p><p>Proof sketch: A result from <ref type="bibr" target="#b2">Ben-David et al. (2007)</ref> shows <ref type="figure">q)</ref> is the H-divergence between p and q, which is defined as</p><formula xml:id="formula_19">that ? t (?) ? s (?) + d H (p, q) + C 0 , where d H (p,</formula><formula xml:id="formula_20">d H (p, q) 2 sup ??H Pr x s ?p [?(x s ) = 1] ? Pr x t ?q ?(x t ) = 1 .</formula><p>(10) The H-divergence relies on the capacity of the hypothesis space H to distinguish distributions p from q, and ? ? H can be viewed as a two-sample classifier. By choosing ? as a (kernel) Parzen window classifier <ref type="bibr" target="#b27">(Sriperumbudur et al., 2009)</ref>, d H (p, q) can be bounded by the empirical estimate</p><formula xml:id="formula_21">d H (p, q) d H (D s , D t ) + C 1 2 1? inf ??H ns i=1 L[?(x s i )=1] ns + n t j=1 L [ ?(x t j )=?1 ] n t +C1 = 2 (1 + d k (p, q)) + C 1 ,<label>(11)</label></formula><p>where L(?) is the linear loss function of the Parzen window classifier ?, L[? = 1] ??, L[? = ?1] ?. By explicitly minimizing MK-MMD in multiple layers, the features and classifier learned by the proposed DAN model can decrease the upper bound on target risk. The source classifier and the two-sample classifier together provide a way to assess the adaptation performance, and can facilitate model selection. Note that we maximize MK-MMD w.r.t. ? (7) to minimize Type II test error, and to help the Parzen window classifier achieve minimal risk of two-sample discrimination in (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare the DAN model to state-of-the-art transfer learning and deep learning methods on both unsupervised and semi-supervised adaptation problems, focusing on the efficacy of multi-layer adaptation with multi-kernel MMD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Office-31 <ref type="bibr" target="#b25">(Saenko et al., 2010)</ref> This dataset is a standard benchmark for domain adaptation. It consists of 4,652 images within 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which are images taken by web camera and digital SLR camera in an office with different environment variation, respectively. We evaluate our method across the 3 transfer tasks, A ? W, D ? W and W ? D, which are commonly adopted in deep learning methods <ref type="bibr" target="#b29">Tzeng et al., 2014)</ref>. For completeness, we further include the evaluation on the other 3 transfer tasks, A ? D, D ? A and W ? A. Office-10 + Caltech-10 <ref type="bibr" target="#b10">(Gong et al., 2012)</ref>. This dataset consists of the 10 common categories shared by the Office-31 and Caltech-256 (C) <ref type="bibr" target="#b14">(Griffin et al., 2007)</ref> datasets and is widely adopted in transfer learning methods <ref type="bibr" target="#b19">(Long et al., 2013;</ref><ref type="bibr" target="#b1">Baktashmotlagh et al., 2013)</ref>. We can build another 6 transfer tasks: A ? C, W ? C, D ? C, C ? A, C ? W, and C ? D. With more transfer tasks, we are targeting an unbiased look at the dataset bias <ref type="bibr" target="#b28">(Torralba &amp; Efros, 2011)</ref>.</p><p>We compare to a variety of methods: TCA <ref type="bibr" target="#b23">(Pan et al., 2011)</ref>, GFK <ref type="bibr" target="#b10">(Gong et al., 2012)</ref>, CNN , LapCNN <ref type="bibr" target="#b31">(Weston et al., 2008)</ref>, and DDC . Specifically, TCA is a conventional transfer learning method based on MMD-regularized PCA. GFK is a widely-adopted method for our datasets which interpolates across intermediate subspaces to bridge the source and target. CNN was the leading method in the ImageNet 2012 competition, and it turns out to be a strong model for learning transferable features <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>. LapCNN is a semi-supervised variant of CNN based on Laplacian graph regularization. Finally, DDC is a domain adaptation variant of CNN that adds an adaptation layer between the f c7 and f c8 layers that is regularized by single-kernel MMD. We implement the CNN-based methods, i.e., CNN, LapCNN, DDC, and DAN based on the Caffe  implementation of AlexNet  trained on the ImageNet dataset. In order to study the efficacy of multi-layer adaptation and multi-kernel MMD, we evaluate several variants of DAN:</p><p>(1) DAN using only one hidden layer, either f c7 or f c8 for adaptation, termed DAN 7 and DAN 8 respectively; (2) DAN using single-kernel MMD for adaptation, termed DAN SK .</p><p>We mainly follow standard evaluation protocol for unsupervised adaptation and use all source examples with labels and all target examples without labels <ref type="bibr" target="#b11">(Gong et al., 2013)</ref>. To make our results directly comparable to most published results, we report a classical protocol <ref type="bibr" target="#b25">(Saenko et al., 2010)</ref> in that we randomly down-sample the source examples, and further require 3 labeled target examples per category for semi-supervised adaptation. We compare the averages and standard errors of classification accuracy for each task. For baseline methods, we follow the standard procedures for model selection as explained in their respective papers. For MMD-based methods (i.e., TCA, DDC, and DAN), we use a Gaussian kernel k (x i , x j ) = e ? xi?xj 2 /? with the bandwidth ? set to the median pairwise distances on the training data-the median heuristic <ref type="bibr" target="#b13">(Gretton et al., 2012b)</ref>. We use multi-kernel MMD for DAN, and consider a family of m Gaussian kernels {k u } m u=1 by varying bandwidth ? u between 2 ?8 ? and 2 8 ? with a multiplicative step-size of 2 1/2 <ref type="bibr" target="#b13">(Gretton et al., 2012b)</ref>. As minimizing MMD is equivalent to maximizing the error of classifying the source from the target (two-sample classifier) <ref type="bibr" target="#b27">(Sriperumbudur et al., 2009)</ref>, we can automatically select the MMD penalty parameter ? on a validation set (comprised of source-labeled instances and target-unlabeled instances) by jointly assessing the test errors of the source classifier and the two-sample classifier. We use the finetuning architecture <ref type="bibr" target="#b32">(Yosinski et al., 2014)</ref>, however, due to limited training examples in our datasets, we fix convolutional layers conv1-conv3 that were copied from pretrained model, fine-tune conv4-conv5 and fully connected layers f c6-f c7, and train classifier layer f c8, both via back propagation. As the classifier is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use stochastic gradient descent (SGD) with 0.9 momentum and the learning rate annealing strategy implemented in Caffe, and cross-validate base learning rate between 10 ?5 and 10 ?2 with a multiplicative step-size 10 1/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Discussion</head><p>The unsupervised adaptation results on the first six Office-31 transfer tasks are shown in <ref type="table">Table 1</ref>, and the results on the other six Office-10 + Caltech-10 transfer tasks are shown in <ref type="table">Table 2</ref>. To directly compare with DDC, we report semi-supervised adaptation results of the same tasks used by DDC in <ref type="table">Table 3</ref>. We can observe that DAN significantly outperforms the comparison methods on most transfer tasks, and achieves comparable performance on the easy transfer tasks, D ? W and W ? D, where source and target are similar <ref type="bibr" target="#b25">(Saenko et al., 2010)</ref>. This is reasonable as the adaptability may vary across different transfer tasks. The performance boost demonstrates that our architecture of multi-layer adaptation via multi-kernel MMD is able to transfer pre-trained deep models across different domains.</p><p>From the experimental results, we can make the following observations. (1) Deep learning based methods significantly outperform conventional shallow transfer learning methods by a large margin. (2) Among the deep learning methods, the semi-supervised LapCNN provides no improvement over CNN, suggesting that the challenge of domain discrepancy cannot be readily bridged by semi- <ref type="table">Table 1</ref>. Accuracy on Office-31 dataset with standard unsupervised adaptation protocol <ref type="bibr" target="#b11">(Gong et al., 2013)</ref>.  <ref type="table">Table 2</ref>. Accuracy on Office-10 + Caltech-10 dataset with standard unsupervised adaptation protocol <ref type="bibr" target="#b11">(Gong et al., 2013)</ref>.    in which fine-tuning was not carried out, it does not yield a large gain over Caffe AlexNet using fine-tuning. This shows the limitation of single-layer adaptation via single-kernel MMD, which cannot explore the strengths of deep networks and multiple kernels for domain adaptation.</p><formula xml:id="formula_22">Method A ? W D ? W W ? D A ? D D ? A W ? A</formula><formula xml:id="formula_23">Method A ? C W ? C D ? C C ? A C ? W C ? D</formula><p>To dive deeper into DAN, we present the results of three variants of DAN: (1) DAN 7 and DAN 8 achieve better accuracy than DDC, which highlights that multi-kernel MMD can bridge the domain discrepancy more effectively than single-kernel MMD. The reason is that multiple kernels with different bandwidths can match both the low-order moments and high-order moments to minimize the Type II error <ref type="bibr" target="#b13">(Gretton et al., 2012b)</ref>.</p><p>(2) DAN SK also attains higher accuracy than DDC, which confirms the capability of deep architecture for distribution adaptation. The rationale is similar to that of deep networks: each layer of deep network is intended to extract features at a different abstraction level, and hence we need to match the distributions at each task-specific layer to consolidate the adaptation quality at all levels. The multi-layer architecture is one of the most critical contributors to the efficacy of deep learning, and we believe it is also important for MMD-based adaptation. The evidence of comparable performance between the multilayer variant DAN SK and multi-kernel variants DAN 7 and DAN 8 shows their equal importance for domain adaptation. As expected, DAN obtains the best performance by jointly exploring multi-layer adaptation with multi-kernel MMD. Another benefit of DAN is that it uses a linear-time unbiased estimate of the kernel embedding, which makes it an order more efficient than existing methods TCA and DDC. <ref type="bibr">Though Tzeng et al. (2014)</ref> speed up DDC by computing the MMD within each mini-batch of the SGD, this leads to a biased estimate of MMD and lower adaptation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Empirical Analysis</head><p>Feature Visualization To demonstrate the transferability of the DAN learned features, we follow  and <ref type="bibr" target="#b29">Tzeng et al. (2014)</ref>   in task C ? W with DDC features and DAN features, respectively. We make the following observations: (1) With DDC features, the target points are not discriminated very well, while with DAN features, the points are discriminated much better.</p><p>(2) With DDC features, the categories between the source and the target are not aligned very well, while with DAN features, the categories are aligned much better between domains. Both these observations can explain the superior performance of DAN over DDC: (1) implies that the target points are more easily discriminated with DAN features, and (2) implies that the target points can be better discriminated with the source classifier. DAN can learn more transferable features for effective domain adaptation.</p><p>A-Distance A theoretical result in Ben-David et al. <ref type="formula" target="#formula_1">(2010)</ref> suggests A-distance as a measure of domain discrepancy. As computing the exact A-distance is intractable, an approximate distance is defined asd A = 2 (1 ? 2?), where ? is the generalization error of a two-sample classifier (kernel SVM in our case) trained on the binary problem to distinguish input samples between the source and target domains. <ref type="figure" target="#fig_2">Figure 3(a)</ref>   <ref type="bibr" target="#b9">Glorot et al. (2011)</ref>. However, domain adaptation may be deteriorated by the enlarged domain discrepancy <ref type="bibr" target="#b3">(Ben-David et al., 2010)</ref>. It is desirable thatd A on DAN feature is smaller thand A on CNN feature, which guarantees more transferable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Sensitivity</head><p>We investigate the effects of the parameter ?. <ref type="figure" target="#fig_2">Figure 3(b)</ref> gives an illustration of the variation of transfer classification performance as ? ? {0.1, 0.4, 0.7, 1, 1.4, 1.7, 2} on tasks A ? W and C ? W. We can observe that the DAN accuracy first increases and then decreases as ? varies and demonstrates a bell-shaped curve. This confirms the motivation of jointly learning deep features and adapting distribution discrepancy, since a good trade-off between them can enhance feature transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a novel Deep Adaptation Network (DAN) architecture to enhance the transferability of features from task-specific layers of the neural network. We confirm that while general features can generalize well to a novel task, specific features tailored to an original task cannot bridge the domain discrepancy effectively. We show that feature transferability can be enhanced substantially by mean-embedding matching of the multi-layer representations across domains in a reproducing kernel Hilbert space. An optimal multi-kernel selection strategy further improves the embedding matching effectiveness, while an unbiased estimate of the mean embedding naturally leads to a lineartime algorithm that is very desirable for deep learning from large-scale datasets. An extensive empirical evaluation on standard domain adaptation benchmarks demonstrates the efficacy of the proposed model against previous methods.</p><p>As deep features transition from general to specific along the network, it is interesting to study the principled way of deciding the boundary of generality and specificity, and the application of distribution adaptation to the convolutional layers of CNN to further enhance the feature transferability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>? 0.0 26.4 ? 0.0 36.4 ? 0.0 56.2 ? 0.0 43.7 ? 0.0 42.0 ? 0.0 41.0 CNN 83.8 ? 0.3 76.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>and plot in Figures 2(a)-2(b) and 2(c)-2(d) the t-SNE embeddings of the images Feature visualization: t-SNE of DDC features on source (a) and target (b); t-SNE of DAN features on source (c) and target (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Empirical analysis: (a) A-Distance of CNN &amp; DAN features; (b) sensitivity of ? (dashed lines show best baseline results).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>displaysd A on transfer tasks A ? W and C ? W using Raw features, CNN features, and DAN features, respectively. It reveals a surprising observation that thed A on both CNN and DAN features are larger than thed A on Raw features. This implies that ab-stract deep features can be salient both for discriminating different categories and different domains, which is consistent with</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain-adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Transfer and Multi-task learning: Theory Meets Practice</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Analysis of representations for domain adaptation. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for information trustworthiness estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Domain adaptive neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6041</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>COLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">L. ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Equivalence of distance-based and rkhs-based statistics in hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2263" to="2291" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernel choice and classifiability for rkhs embeddings of probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flexible transfer learning under support and model shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rattle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
