<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
							<email>mahaoyu@connect.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
							<email>xrlin2@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
							<email>wuzifeng@deepwise.com</email>
							<affiliation key="aff1">
								<orgName type="department">Deepwise AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
							<email>yizhouy@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coarse-to-Fine Domain Adaptive Semantic Segmentation with Photometric Alignment and Category-Center Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) in semantic segmentation is a fundamental yet promising task relieving the need for laborious annotation works. However, the domain shifts/discrepancies problem in this task compromise the final segmentation performance. Based on our observation, the main causes of the domain shifts are differences in imaging conditions, called image-level domain shifts, and differences in object category configurations called category-level domain shifts. In this paper, we propose a novel UDA pipeline that unifies image-level alignment and category-level feature distribution regularization in a coarse-to-fine manner. Specifically, on the coarse side, we propose a photometric alignment module that aligns an image in the source domain with a reference image from the target domain using a set of image-level operators; on the fine side, we propose a category-oriented triplet loss that imposes a soft constraint to regularize category centers in the source domain and a self-supervised consistency regularization method in the target domain. Experimental results show that our proposed pipeline improves the generalization capability of the final segmentation model and significantly outperforms all previous state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is a fundamental computer vision task that aims to assign a semantic category label to every pixel in an image. It has been widely used in many important downstream tasks such as autonomous driving <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref> and medical image analysis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b9">10]</ref>. Recent state-of-the-art methods on semantic segmentation are primarily deep learning based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref> and require a large number of high quality annotated ground-truth data which are difficult to obtain especially in practical applications. Unsupervised domain adaptation semantic segmenta-* These authors have equal contribution. ? Corresponding author tion is an alternative method to solve the data scarcity problem where it generalizes models trained on the source domain composed of synthetic images and labels to perform well on the target domain composed of real world images only <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>. However, the problem is that semantic segmentation models trained merely on synthetic data exhibit poor performance on real world images due to the differences in multiple aspects (also called domain shifts/discrepancies), including exposure, contrast, lighting, object shape and surface textures, between the source domain and the target domain. Therefore, matching the distributions between the source and target domains to learn domain-invariant representations is crucial to solve the domain shifts.</p><p>Although the domain shifts could be caused by multiple factors, based on our observation, the primary causes can be summarized into two groups, namely image-level domain shifts and category-level domain shifts. For the imagelevel domain shifts, these refer to the differences in imaging conditions, such as lighting and settings in the camera imaging pipeline. Existing works on solving image-level domain shifts through image style transfer generally utilize deep models such as generative models and image-to-image translation models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref> while another line of research focuses on using Fourier transformation <ref type="bibr" target="#b32">[33]</ref>. These methods have proven that transferring image style of one domain to another domain can bring the two domains closer. However, the downside of these methods is that they either require to carry out a computationally expensive training process for the deep models or generate inferior styletransferred output images as shown in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>Despite the fact that the domain gap can be minimized by global alignment methods such as the above, there is no guarantee that samples from different object categories in the target domain can be well separated. This is because some categories are naturally close to others in terms of body shape, pose and textures. To solve this problem, existing methods adopt category anchors computed on the source domain to guide the alignment between the two do-mains <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref>, which can be regarded as a hard constraint on the category centers. The problem of this design is that it does not regularize the distance between different category features, and categories with similar feature distributions in the source domain also have similar distributions in the target domain, which results in erroneous classification results especially when no supervision information is available in the target domain. Our experimental results have demonstrated that imposing soft regularization methods on category distributions can improve the model's capacity to adjust the relative magnitude of inter-category and intra-category feature distances.</p><p>According to the analysis above, performing alignment from either image-level perspective or category-level perspective alone will not solve the domain shifts reasonably. Therefore, we approach the problem from a different perspective and propose a novel and efficient pipeline that unifies image-level alignment and category-level feature distribution regularization in a coarse-to-fine manner. In general, on the coarse side, we propose a novel and efficient image-level alignment module to coarsely align the two domains; on the fine side, we introduce a new categoryoriented triplet loss to softly regularize the category centers in the source domain and propose a self-supervised consistency regularization method in the target domain. By addressing both level of domain shifts simultaneously, we can significantly improve the performance of our proposed domain adaptation method.</p><p>Coarse Alignment. To solve the image-level domain shifts discussed above, we propose a global photometric alignment (GPA) module that aligns an image in the source domain with a reference image from the target domain using a set of image-level operators. Our method is superior to other generative methods and Fourier transformation based methods in two aspects: first, compared to the generative counterparts, our method requires no extra training process and produces stochastic image results; second, the quality of the translated image and the performance of our method is comparable to its generative counterpart and is superior to that of Fourier transformation based methods.</p><p>Category-level Feature Distribution Regularization. To address category-level domain shifts on the fine side, in addition to the common strategy of using pseudo labels for the target domain, we propose two novel regularization methods for the source and target domains respectively. First, considering the fact that there are annotated ground truth labels in the source domain, we propose a category-oriented triplet loss (CTL) that imposes a soft constraint to regularize category centers calculated using the source image pixel features, which actively enlarges the distances among category centers, making inter-category distances in a high-level feature space larger than intracategory distances by a predefined margin. Second, inspired by the commonly used self-supervised learning methods: consistency regularization and pseudo-labeling, we propose a simple yet effective consistency regularizer for the target domain, called target domain consistency regularization (TCR), which constrains the prediction on an augmented target image to be consistent with the pseudo label of the corresponding non-augmented image, forcing the class labels of similar semantic contents to be consistent in the target domain.</p><p>In conclusion, this paper has the following contributions:</p><p>? We propose a novel coarse-to-fine domain adaptive semantic segmentation pipeline that seamlessly combines coarse image-level alignment with finer category-level feature distribution regularization. ? We introduce two novel and effective category-level regularization methods for the source and target domains respectively. The first one is called categoryoriented triplet loss that regularizes category centers in the source domain while the second one performs target domain consistency regularization. ? Our method outperforms all previous methods, achieving new state-of-the-art performance on both GTA5?Cityscapes and SYNTHIA?Cityscapes benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since our proposed domain adaptation pipeline is mostly related to photometric alignment based <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref> and category-based domain adaptation methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>, we focus on these two types of work in this section.</p><p>Photometric Alignment. Previous works on domain adaptation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8]</ref> have applied adversarial models, such as GAN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref> and CycleGAN <ref type="bibr" target="#b36">[37]</ref>, to achieve photometric alignment results. Adversarial training makes a model capable of transferring image styles from one domain to another to significantly reduce the photometric differences between the two domains in the original image space <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>. Then a segmentation model trained on (style transferred) source domain images can be applied to target domain images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>. However, adversarial models are hard to train. Many researchers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref> have also shown that models based on adversarial training generally align distributions from different domains, but do not actually obtain mappings between features from different domains. Other types of photometric alignment methods for unsupervised semantic segmentation are rare. One method was proposed in <ref type="bibr" target="#b32">[33]</ref> to align the source and target domains by simply replacing the low frequency component in a source domain image with its counterpart in the target domain reference image. However, such simple substitution of frequency components leaves unsatisfactory visual artifacts, and the performance of the model trained on the aligned samples relies heavily on a multi-band ensem-ble. On the contrary, our method is different from previous method in that it has a light-weight photometric alignment strategy which does not require to carry out a computationally expensive training process and more importantly, produces comparable (superior) performance and image quality with respect to its generative (Fourier transformation based) counterpart.</p><p>Category-Based Methods. Category labels/predictions were introduced in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> to enforce global semantic constraints on the distribution of predicted labels. The proposed methods in <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b29">[30]</ref> take one step further. They map penultimate target domain image features, that are used for generating pseudo labels in the output layer, to the corresponding features of the source domain image. However, in their work, category feature centroids <ref type="bibr" target="#b33">[34]</ref> or instance features <ref type="bibr" target="#b29">[30]</ref> in the source domain serve as anchors for category-based feature alignment, which does not explicitly enlarge the margins between the centers. This alignment strategy can be problematic because category anchors close in the source domain are likely hard to separate in the target domain as well. Our work differs from theirs in the following aspects: first, we propose a category-oriented triplet loss for the source domain that imposes a soft constraint to regularize category centers, actively making intercategory distances in a high-level feature space larger than intra-category distances by a specified margin; second, to further constrain category-level feature distributions in the target domain, we force the predictions on augmented target domain images to be consistent with the pseudo labels, generated by the segmentation model, of the corresponding non-augmented images, which is a self-supervision based consistency regularization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Coarse-to-Fine Pipeline</head><p>The key idea underlying our domain adaptation pipeline is intuitive: first, we exploit the photometric differences in the two domains and coarsely align the source domain images with the target domain images to minimize the domain shift; then, we regularize category-level feature distributions by setting constraints on inter-class center distances and intra-class feature variations.</p><p>Step 0: Coarse Alignment. Define M = {m k } Ns k=1 as the source domain training set, where m k is a source domain image and N s is the number of images in the source domain training set. Similarly, the target domain training set is defined as N = {n k } Nt k=1 . Our proposed GPA module converts a source domain image m in the training batch and a randomly selected target domain reference image n into Lab color space as (L m , a m , b m ) and (L n , a n , b n ). Then the histogram mapping function f match is applied to a m and b m , and gamma correction function f gamma is applied to</p><formula xml:id="formula_0">L m to form (f gamma (L m ), f match (a m ), f match (b m )).</formula><p>The image is then converted to RGB space as aligned image m to construct aligned source domain training set M . Then, a stochastic function ? is applied to produce an augmented version of every image in M . A segmentation model T 0 is trained based on all style-transferred images ? (M ) with segmentation loss L seg .</p><p>Step 1: Category-level Feature Distribution Regularization. In this step, we train a segmentation model T 1 with ? (M ) and N. We apply the segmentation model T 0 to all images in the target domain to produce a feature vector and a class probability vector at every pixel. The category corresponding to the largest value of the probability vector is defined to be the pseudo label at the pixel, and the largest probability value itself defines the confidence of the pseudo label. We further pre-define the pair of probability threshold P h and percentage threshold p for all categories. The latter gives rise to a category specific probability threshold P s,c , meaning p% pixels in the category have confidence above P s,c . Thus the final confidence threshold for category c is t c = min(P h , P s,c ), and any pseudo labels in this category with a confidence higher than t c are considered valid and added to the segmentation loss L seg . The remaining pixels are left out during backpropagation. Then category center f c for every category c are also calculated as the L2 normalized mean of all pixel features with category c as the ground truth label in the source domain. In addition to aligned training set M and cross-entropy loss L seg , we impose a category-oriented triplet loss L triplet on the segmentation model T 1 in the source domain to enlarge inter-category distances, and a target domain consistency loss L consist to regularize category-level feature distributions in the target domain. Then we finetune model T 0 U iterations to produce the model T 1 by minimizing L seg + L triplet + L consist .</p><p>Step 2 to K: Iterative Self-Supervised Training. Model T 1 trained in Step 1 can be further improved with iterative steps similar to Step 1. Such an iterative approach is frequently called self-supervised training in the area of unsupervised domain adaptation for semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref>. The same Step 1 is executed except that model T i?1 instead of T 0 is used as the pretrained model to generate pseudo-labels and category centers f c . This process is repeated for K?1 times. The overall pipeline of our proposed coarse-to-fine method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Global Photometric Alignment</head><p>Since the global domain shift mostly affects low-level pixel attributes, which are irrelevant to pixel-wise category labels, we propose global photometric alignment (GPA) to align images from the source with images from the target domain. We observe that the spatial lightness distribution of an image can be very complicated under certain circum- stances while the spatial color distribution of a and b have similar bell-shaped histograms. Therefore, we treat lightness and color differently and perform classic histogram matching <ref type="bibr" target="#b5">[6]</ref> between the source domain image and the target domain reference image only on color channels a and b to avoid introducing artifacts commonly seen in histogram matching results.</p><p>Lightness Gamma Correction. L channel, on the other hand, is much more diversified among images. This is because light interacts with the 3D structure of a scene in a complicated manner. Simple histogram matching function gives rise to large areas of overexposure and fake structures. Thus, instead of strictly enforcing the mapping constraint prescribed by histogram matching for every histogram bin, we choose to only constrain the mean value of the lightness channel in the source domain image and make it equal to the mean value of the target domain reference image. Here, we choose the power-law function. The difference between our proposed method and the classic gamma correction is that our function coefficients are automatically calculated with given source-target image pairs rather than user-defined. Specifically, we define f gamma (L) = L ? , where L is the normalized lightness value. Then the mean value constraint can be written as</p><formula xml:id="formula_1">L Lp m s (f gamma (L)) = L Lp m s (L ? ) = L Lp n t (L)<label>(1)</label></formula><p>, where p m s is the lightness histogram of source image m, and p n t is the lightness histogram of target reference image n. This is a nonlinear equation and ? can be solved numerically. ? = 1 when it is an identical transformation. In practice, to prevent ? from deviating too much away from 1, we introduce a regularization term into the following minimization,</p><formula xml:id="formula_2">? * = arg min ? L Lp m s (L ? ) ? L Lp n t (L) 2 +?(??1) 2 ,<label>(2)</label></formula><p>which is a simple convex optimization problem with only one variable ?, and can be easily solved with few steps of gradient descent. The process of proposed GPA module is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. </p><formula xml:id="formula_3">? ? ? (a) (b) (d) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Loss</head><p>The only training loss during coarse alignment step is the segmentation cross-entropy loss. The overall loss function we use during the category-level stage consists of three parts: the cross-entropy classification loss, a categoryoriented triplet loss, and a target domain consistency regularization loss.</p><p>Category-oriented Triplet Loss. Even though the features learned with the GPA module are domain-invariant to some extent, the cross entropy losses used in previous training does not explicitly control the category-wise feature distribution. Therefore, the model learned with the GPA module using cross-entropy losses is coarsely aligned. Pixel features are distributed unevenly among different categories and some category centers are close to each other. To tackle this issue, we propose a category-oriented triplet loss that aims to further push the category-wise features closer to the corresponding category centers and further from other category centers. Let x i,j be the pixel-wise features in the feature map of the second last layer, and y i,j be the ground truth pixel-wise labels of a source domain image. The category center of category c is calculated as follows,</p><formula xml:id="formula_4">f c = G( 1 N c s i j 1 (y i,j = c) x i,j )<label>(3)</label></formula><p>where N c is the total number of pixels in category c and s is the source domain image index, and G is a L2 normalization function. Note that this L2 normalization is crucial to keep the category centers on the unit sphere to avoid scaling issue. The centers are updated after the training and this allows the centers become further and further from each other on the sphere surface.</p><p>Our category-oriented triplet loss is formulated as fol-lows,</p><formula xml:id="formula_5">L triplet = 1 N s s C i j max( G(x i,j ) ? f c=C ? G(x i,j ) ? f c =C + ?, 0),<label>(4)</label></formula><p>where N is the total number of pixels in all images, and ? is a prescribed margin. The loss would be zero if every feature x i,j is at least ? closer to its own category center than other category centers. Note that we only have reliable category labels for the source domain, thus we only apply the category-oriented triplet loss to the source domain images.  The working principals of our proposed categoryoriented triplet loss is illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. In cooperation with proposed photometric alignment and data augmentation in the source domain, our proposed triplet loss exploits hard samples in the coarsely aligned source domain and further improves the generalization capability of the trained model, which serves as complementary to crossentropy loss.</p><p>Target Domain Consistency Regularization. Category-oriented triplet loss is designed to regularize category-wise features in the source domain where the annotated ground truth labels are available. However, this is not the case in the target domain where no labels are provided. Consistency regularization is an important component of many recent state-of-the-art self-supervised learning algorithms, which utilizes unlabeled data by relying on the assumption that the model should output similar predictions when fed perturbed versions of the same image <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Motivated by this, we propose a target domain consistency regularization method shown in <ref type="figure" target="#fig_0">Figure 1</ref> to perform category-level feature distribution regularization in the target domain.</p><p>The idea of our proposed consistency regularization is simple: given a target domain image n k , with the trained segmentation model T i?1 , we extract a pseudo label? k j at every location j by feeding n k to T i?1 followed by applying arg max(.); and the corresponding pixel prediction is converted to a hard label vector 1 [c=? k j ] ; then, we apply the stochastic function ? to n k to obtain a perturbed version n k ; then, we feed n k to T i to obtain a prediction p k j at every location j in the perturbed image; finally, p k j is forced to be consistent with? k j by using a cross entropy loss function at pixel locations whose largest class probability is above the previously defined category-level confidence threshold t c . By doing this, category-level feature distributions in the target domain are regularized under the supervision of valid pseudo labels. The overall formula is defined as follows,</p><formula xml:id="formula_6">L cst = j 1(max(T i?1 (n k )| j ) ? t c )CELoss(1 [c=? k j ] , p k j ), y k j = arg max(T i?1 (n k )| j ), p k j = T i (n k )| j .</formula><p>(5) It is important to use trained model T i?1 rather than model T i to generate pseudo labels. This is because T i is still being trained and unstable. Fluctuating pseudo labels generated by T i would be catastrophic to the training process. Experimental results show this consistency regularization method is very effective even though the idea is simple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation Details</head><p>We follow the evaluation settings used in <ref type="bibr" target="#b33">[34]</ref>, and evaluate our proposed method with the source domain datasets GTA5 <ref type="bibr" target="#b20">[21]</ref> and Synthia <ref type="bibr" target="#b21">[22]</ref>, and the target domain dataset Cityscapes <ref type="bibr" target="#b2">[3]</ref>. The GTA5 dataset shares 19 common categories with the Cityscapes dataset and all the irrelevant categories are ignored during training; the Synthia dataset shares 16 common categories with the Cityscapes dataset. Some previous works only train and test on a 13-category subset of the Synthia dataset, or train two models on both subset and the whole set for better performance. Here we follow the practice in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref> to train a model only on the whole set and test it on both settings.</p><p>According to <ref type="figure" target="#fig_0">Figure 1</ref>, we first use the photometrically aligned source domain images to train an initial segmentation model T 0 in the coarse alignment step. Then, the model is trained in an iterative self-supervision manner with K = 6 and U = 20k, and the total number of training iterations is 140k which is comparable to all previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref>. In our experiments, P h = 0.9 and p = 10 for the pseudo-labels (as in <ref type="bibr" target="#b13">[14]</ref>), and the regularization term ? in (5) is 0.01. The margin ? is 0.2 for the triplet loss. We use the standard color-jittering as the stochastic function ? (.) in both source and target domains as in <ref type="bibr" target="#b28">[29]</ref>. Following the same experimental settings in CAG <ref type="bibr" target="#b33">[34]</ref>, we adopt DeepLab V3+(Resnet101) <ref type="bibr" target="#b0">[1]</ref> 1 as our segmentation model. Our proposed method has been implemented in Py-Torch <ref type="bibr" target="#b18">[19]</ref>, and all experiments are conducted on 4 NVIDIA GeForce 2080Ti GPUs with 1 sample on each GPU. In the coarse alignment step, the stochastic gradient descent is 1 https : //github.com/RogerZhangzz/CAG UDA/issues/6 used with momentum of 0.9 and weight decay of 1e?4. The learning rate is initially set to 5e ? 4 and is decreased using the polynomial learning rate policy with power of 0.9. The setting for the following iterative finetuning steps are exactly the same except we halve the learning rate to 2.5e ? 4 to fine-tune previously trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art Methods</head><p>In this section, we compare our method against all the existing state-of-the-art methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref>, on both GTA5?Cityscapes and Synthia?Cityscapes tasks.</p><p>For the GTA5?Cityscapes task, according to <ref type="table">Table 1</ref>, it is clear that our proposed method outperforms all previous methods, achieving a new state-of-the-art mIoU at 56.1% which is 5.9% higher than previous state-of-the-art methods <ref type="bibr" target="#b33">[34]</ref>. In general, our method achieves the best performance in many important categories, including 'road', 'sidewalk', 'building', 'light', 'sky', 'car', 'person', 'train', 'motor', and 'bike'. In particular, our model delivers a very good classification performance over 'road', 'sidewalk', 'motor' and 'bike' although some of these categories share very similar local appearances. This is because our category-oriented triplet loss focuses on the most confusing samples in different classes, and improves the generalization capability of the model. Moreover, the target consistency regularization in the target domain improves the classification accuracy of categories with a large intra-category variance, such as 'building' and 'sky'.</p><p>The performance of the proposed method on Synthia?Cityscapes is shown in <ref type="table">Table 2</ref>. The Synthia dataset has a larger domain shift caused by perspective and layout in addition to photometric differences in comparison to the GTA5 dataset. But the overall performance of our model across all categories still surpasses the performance of other state-of-the-art methods, which demonstrates the effectiveness of our proposed techniques.</p><p>In comparison to CAG <ref type="bibr" target="#b33">[34]</ref> using the same segmentation model, our proposed modules achieve a significant performance improvement, which is 5.9% in the GTA5?Cityscapes task and 3.7% in the Synthia?Cityscapes task. We further show some of the segmentation samples in <ref type="figure" target="#fig_5">Figure 4</ref> to qualitatively demonstrate the superiority of our method. Please refer to the supplementary document for more qualitative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Component Analysis. Most previous works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30</ref>] require a segmentation model pre-trained on the original source domain training set only, and we call this model the source-only model. Although we do not use the source-only model during training, we train one to provide a baseline to demonstrate that the primary performance gain comes from our proposed modules and pipeline. As shown in <ref type="table">Table 3</ref>  <ref type="table">Table 1</ref>. Performance comparison with state-of-the-art methods on the GTA5?Cityscapes task. Results after only coarse alignment and whole coarse-to-fine pipeline are both presented.  the performance of the source-only baseline using Deeplab v3+ is 37.6%, which is only slightly higher than that of the baseline using Deeplab v2 (36.6%) reported in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>, and our proposed pipeline improves the baseline performance by 18.5%. Following the same settings in previous stateof-the-art methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29]</ref>, we further evaluate the impact of each proposed component on the performance of our model in the GTA5?Cityscapes task by removing one component at a time. According to our experimental results, the performance of the segmentation model has the most deterioration when the global photometric alignment module is removed. This is because in our coarse-to-fine pipeline, removing photometric alignment literally removes the first coarse alignment stage, and the resulting erroneous pseudo-labels are very detrimental to category-level feature distribution regularization. This also validates the necessity of a coarse alignment stage. Interestingly, although our target domain consistency regularization is simple, it has been proved to be very effective. This is because there are fewer training images in the target domain than the source domain, and filtering pseudo-labels with low confidence makes them even fewer. Our target domain consistency regularization increases the number of training samples in the target domain, therefore, giving rise to such a performance gain. The category-oriented triplet loss applied on the source domain also boosts the performance by 2.9% as it exploits hard samples in the source domain.</p><p>Photometric Alignment. There are currently other methods, which can achieve the goal of coarse alignment, such as the GAN-based method in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref> and the frequency-based method in <ref type="bibr" target="#b32">[33]</ref>. We substitute our proposed global photometric alignment with these two methods, and retrain our whole pipeline. The result is shown in <ref type="table" target="#tab_1">Table 4</ref>. We also visualize some representative aligned images produced with different methods in <ref type="figure" target="#fig_6">Figure 5</ref>. Our proposed GPA can generate the aligned image according to a randomly chosen target domain reference image, while the GAN-based model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b1">2]</ref> performs deterministically and generates aligned images with a similar style, only covering part of the actual target domain image span. This explains why our proposed model works even better than the pre-trained deep adversarial model. Although the frequency-based method proposed in <ref type="bibr" target="#b32">[33]</ref> can generate style-transferred images randomly, the concatenation of frequencies usually introduces significant noises during training, which largely limits its final performance.</p><p>Based on our observation, gamma correction on all three channels does not have sufficient alignment capability, while histogram matching on all three channels results in image artifacts. We have run a comparison for the coarse alignment stage and the result (  <ref type="table">Table 2</ref>. Performance comparison with state-of-the-art methods on the Synthia?Cityscapes task (mIoU: 16-class; mIoU*: 13-class).   <ref type="table">Table 3</ref>. Ablation study of the proposed components on the GTA5?Cityscapes task. GPA: global photometric alignment, CTL: category-oriented triplet loss, TCR: target domain consistency regularization. labels but not pseudo-labels in the target domain. Although target domain images with pseudo-labels can be used as supplementary samples when the pseudo-labels are of high confidence, our proposed triplet loss aims to deal with hard samples, and pseudo-labels of hard samples in the target domain are not reliable. In order to verify this, we include pseudo-labels in our category-oriented triplet loss, and the result is shown in <ref type="table" target="#tab_1">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel coarse-to-fine pipeline for domain adaptation semantic segmentation that smoothly integrates image-level alignment with category-level feature distribution regularization. In particular, we introduce</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modules</head><p>Methods mIoU Image Align. Frequency Align <ref type="bibr" target="#b32">[33]</ref>.</p><p>52.0 BDL-GAN <ref type="bibr" target="#b13">[14]</ref> 54.5 Photometric Align. 56.1 GPA Scheme Lab Gamma Correction 44.5 Lab Histogram Match 43.3 Hybrid 47.3 Pseudo-labels Triplet loss with pseudo-labels 53. <ref type="bibr" target="#b2">3</ref> Triplet loss w/o pseudo-labels 56.1 <ref type="table" target="#tab_1">Table 4</ref>. Ablation studies of the image alignment plan, photometric alignment scheme, and using pseudo-labels for the categoryoriented triplet loss on the GTA5?Cityscapes task. a novel and efficient global photometric alignment module to coarsely align the source and target domains, and then, we propose a category-oriented triplet loss for the source domain and a target domain consistency regularization method to regularize the category-level feature distributions from a fine-grained category perspective. Experiments demonstrate that each of our proposed techniques improves the generalization capability of our model. And integrating them together results in a significant performance improvement in comparison to existing state-of-the-art unsupervised domain adapted semantic segmentation methods, demonstrating that solving image-level and category-level domain shifts simultaneously deserves more attention.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) First, the global photometric alignment (GPA) module is used to coarsely align the source and target domain images to train the initialized segmentation model T0. (b) Then, we train the category-level feature distribution regularization step with the calculated category feature center fc and pseudo-label threshold tc for each category c. The category-oriented triplet loss is applied to the source domain and the consistency regularization is used in the target domain to jointly regularize the category-level feature distribution. (c) The overall pipeline is trained in an iterative self-supervised manner with Ti, fc, and tc updated at each step i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a)Input source domain image and (b) a randomly chosen target domain image is aligned in (c) Lab channels to generate (d) aligned image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>:</head><label></label><figDesc>Aligned feats of cat. Green and Blue : Tgt. feats of cat. Green and Blue : Aug. feats of cat. Green and Blue : Feat centers of cat. Green and Blue</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Our proposed category-oriented triplet loss exploits hard samples and further enlarge category margins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative analysis on GTA5?Cityscapes task. (a) Input images, (b) CAG [34], (c)Ours, (d) Labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative analysis on global photometric alignment. (a) Input images, (b) reference image, (c) BDL-GAN<ref type="bibr" target="#b13">[14]</ref>, (d) Fourier Adaptation<ref type="bibr" target="#b32">[33]</ref>, (e) Global photometric alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>44.7 84.2 34.6 27.6 30.2 36.0 36.0 85.0 43.6 83.0 58.6 31.6 83.3 35.3 49.7 3.3 28.8 35.6 48.5 IDA [18] 90.6 36.1 82.6 29.5 21.3 27.6 31.4 23.1 85.2 39.3 80.2 59.3 29.4 86.4 33.6 53.9 0.0 32.7 37.6 46.3 DTST [30] 90.6 44.7 84.8 34.3 28.7 31.6 35.0 37.6 84.7 43.3 85.3 57.0 31.5 83.8 42.6 48.5 1.9 30.4 39.0 49.2 FGGAN [29] 91.0 50.6 86.0 43.4 29.8 36.8 43.4 25.0 86.8 38.3 87.4 64.0 38.0 85.2 31.6 46.92.5 58.3 86.5 27.4 28.8 38.1 46.7 42.5 85.4 38.4 91.8 66.4 37.0 87.8 40.7 52.4 44.6 41.7 59.0 56.1</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege</cell><cell>.</cell><cell>terrace</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motor</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>BDL [14]</cell><cell cols="18">91.0 1 6.5</cell><cell cols="3">25.4 37.1 50.1</cell></row><row><cell>FDA [33]</cell><cell cols="21">92.5 53.3 82.3 26.5 27.6 36.4 40.5 38.8 82.2 39.8 78.0 62.6 34.4 84.9 34.1 53.1 16.8 27.7 46.4 50.4</cell></row><row><cell>CAG [34]</cell><cell cols="21">90.4 51.6 83.8 34.2 27.8 38.4 25.3 48.4 85.4 38.2 78.1 58.6 34.6 84.7 21.9 42.7 41.1 29.3 37.2 50.2</cell></row><row><cell>coarse align. (ours)</cell><cell cols="21">83.9 37.5 82.7 28.7 18.9 35.3 41.3 31.1 85.2 29.5 86.6 62.8 30.9 82.4 23.0 39.3 33.0 26.0 39.7 47.3</cell></row><row><cell>coarse-to-fine (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 )</head><label>4</label><figDesc>shows our hybrid scheme performs the best.Pseudo-labels. In our proposed method, we only apply the category-oriented triplet loss to source domain category<ref type="bibr" target="#b14">15</ref>.8 80.5 81.8 59.9 33.1 70.2 37.3 28.5 45.8 -52.1 FGGAN [29] 84.5 40.1 83.1 4.8 0.0 34.3 20.1 27.2 84.8 84.0 53.5 22.6 85.4 43.7 26.8 27.8 45.2 86.5 78.1 66.3 28.1 81.8 21.8 22.9 49.0 -52.6 CAG (16 classes) [34] 84.7 40.8 81.7 7.8 0.0 35.1 13.3 22.7 84.5 77.6 64.2 27.8 80.9 19.7 22.7 48.3 44.5 coarse align. (ours) 64.0 25.7 73.9 9.6 0.8 33.3 12.3 25.9 81.6 85.5 62.4 26.2 80.6 30.9 26.8 23.8 41.5 47.7 coarse-to-fine (ours) 75.7 30.0 81.9 11.5 2.5 35.3 18.0 32.7 86.2 90.1 65.1 33.2 83.3 36.5 35.3 54.3 48.2 55.5</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>vege</cell><cell>.</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>motor</cell><cell>bike</cell><cell>mIoU mIoU*</cell></row><row><cell>BDL [14]</cell><cell cols="4">86.0 46.7 80.3 -</cell><cell>-</cell><cell>-</cell><cell cols="12">14.1 11.6 79.2 81.3 54.1 27.9 73.7 42.2 25.7 45.3 -</cell><cell>51.4</cell></row><row><cell>IDA [18]</cell><cell cols="4">84.3 37.7 79.5 5.3</cell><cell cols="3">0.4 24.9 9.2</cell><cell>8.4</cell><cell cols="10">80.0 84.1 57.2 23.0 78.0 38.1 20.3 36.5 41.7</cell><cell>48.9</cell></row><row><cell>DTST [30]</cell><cell cols="4">83.0 44.0 80.3 -</cell><cell>-</cell><cell>-</cell><cell cols="12">17.1 52.5</cell></row><row><cell>FDA [33]</cell><cell cols="4">79.3 35.0 73.2 -</cell><cell>-</cell><cell>-</cell><cell cols="12">19.9 24.0 61.7 82.6 61.4 31.1 83.9 40.8 38.4 51.1 -</cell><cell>52.5</cell></row><row><cell cols="5">CAG (13 classes) [34] 84.8 41.7 85.5 -</cell><cell>-</cell><cell>-</cell><cell cols="2">13.7 23.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Cheng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for bayesian brain mri segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Adrian V Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Eugenio</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iglesias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="356" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Haase-Sch?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudius</forename><surname>Glaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Digital image processing using MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Eugene</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven L</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eddins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">brain segmentation using spatially localized atlas network tiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoubing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Aboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Parvathaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunxing</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camilo</forename><surname>Bermudez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><forename type="middle">E</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennett A Landman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="119" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>3d whole</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-domain weakly-supervised object detection through progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tsit: A simple and versatile framework for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12072</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-training and adversarial background regularization for unsupervised domain adaptive one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6092" to="6101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3764" to="3773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Discriminative transfer joint matching for domain adaptation in hyperspectral image classification. IEEE Geoscience and Remote Sensing Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="972" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Stephan R Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1163" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLITS, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic brain tumor segmentation based on cascaded convolutional neural networks with uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">56</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09222</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12635" to="12644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dcan: Dual channel-wise alignment networks for unsupervised scene adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><forename type="middle">Gokhan</forename><surname>Uzunbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning semantic representations for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5423" to="5432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Label propagation with augmented anchors: A simple semi-supervised learning baseline for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07695</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
