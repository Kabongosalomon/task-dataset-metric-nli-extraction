<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KEPLER: Keypoint and Pose Estimation of Unconstrained Faces by Learning Efficient H-CNN Regressors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
							<email>akumar14@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">CFAR</orgName>
								<orgName type="institution" key="instit2">UMIACS University of Maryland-College Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Alavi</surname></persName>
							<email>azadeh@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">CFAR</orgName>
								<orgName type="institution" key="instit2">UMIACS University of Maryland-College Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">CFAR</orgName>
								<orgName type="institution" key="instit2">UMIACS University of Maryland-College Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KEPLER: Keypoint and Pose Estimation of Unconstrained Faces by Learning Efficient H-CNN Regressors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Keypoint detection is one of the most important pre-processing steps in tasks such as face modeling, recognition and verification. In this paper, we present an iterative method for Keypoint Estimation and Pose prediction of unconstrained faces by Learning Efficient H-CNN Regressors (KEPLER) for addressing the face alignment problem. Recent state of the art methods have shown improvements in face keypoint detection by employing Convolution Neural Networks (CNNs). Although a simple feed forward neural network can learn the mapping between input and output spaces, it cannot learn the inherent structural dependencies. We present a novel architecture called H-CNN (Heatmap-CNN) which captures structured global and local features and thus favors accurate keypoint detecion. H-CNN is jointly trained on the visibility, fiducials and 3D-pose of the face. As the iterations proceed, the error decreases making the gradients small and thus requiring efficient training of DCNNs to mitigate this. KEPLER performs global corrections in pose and fiducials for the first four iterations followed by local corrections in a subsequent stage. As a by-product, KEPLER also provides 3D pose (pitch, yaw and roll) of the face accurately. In this paper, we show that without using any 3D information, KEPLER outperforms state of the art methods for alignment on challenging datasets such as AFW [40] and AFLW <ref type="bibr" target="#b17">[18]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Keypoint detection on unconstrained faces is one of the most studied topics in the past decade, as accurate localization of fiducials is a vital pre-processing task for variety of applications. In the last five years, keypoint localization using Convolution Neural Networks (CNN) has received great attention from computer vision researchers. This is mainly due to the availability of large scale annotated unconstrained face datasets such as AFLW <ref type="bibr" target="#b17">[18]</ref>. Works such as <ref type="bibr" target="#b32">[33]</ref> have hypothesized that as the network gets deeper more abstract information such as identity, pose, attributes are retained while immediate local features are lost. However, various methods <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, and <ref type="bibr" target="#b35">[36]</ref> directly use CNNs as regressors or use deep features from CNNs to design regressors for predicting keypoints.</p><p>On the other hand, an earlier method of Explicit Shape Regression (ESR) proposed by Cao et al. <ref type="bibr" target="#b7">[8]</ref> achieved superior results by introducing the important concept of nonparametric shape regression for facial keypoint localization. Following <ref type="bibr" target="#b7">[8]</ref>, unconstrained face alignment received great deal of attention and many of its variants <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b18">[19]</ref> were published later, using a variety of features producing incremental improvements over <ref type="bibr" target="#b7">[8]</ref>. However, they are all limited by the fixed number of points on the face. In real life applications, there are more challenging datasets such as IJBA <ref type="bibr" target="#b16">[17]</ref> and AFW <ref type="bibr" target="#b39">[40]</ref>, which do not always have 68 or 49 fixed points mainly due to occlusion or pose variations. As alternatives, researchers moved towards more sophisticated techniques, incorporating 3D shape models <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b12">[13]</ref>, domain learning <ref type="bibr" target="#b37">[38]</ref>, recurrent autoencoderdecoder <ref type="bibr" target="#b0">[1]</ref> and many others. However, one question still remains unanswered: Can cascaded shape regression be applied for an arbitrary face with no prior knowledge ?</p><p>The motivation for this work is to adapt cascaded regression for predicting landmarks of arbitrary faces, while taking advantage of CNNs. We transform the cascaded regression formulation into an iterative scheme for arbitrary faces. In each iteration the regressor predicts the increment for the next stage jointly for all the points while maintaining the shape constraint. As by-products of KEPLER, we get the visibility confidence of each keypoint and 3D pose (pitch, yaw and roll) for the face image. The main contributions of this paper are:</p><p>? We design a novel GoogLenet-based <ref type="bibr" target="#b25">[26]</ref> architecture with a channel inception module which pools features from intermediate layers and concatenates them similar to inception module. We call the proposed architecture Channeled Inception in the rest of the paper. This network is used in all the stages of KEPLER. ? Inspired by <ref type="bibr" target="#b8">[9]</ref>, we present an iterative method for estimating the face landmarks using the fixed point consolidation scheme inspired by <ref type="bibr" target="#b8">[9]</ref>. We observe that estimating landmarks on a face is more challenging than estimating keypoints on a human body. The overview of the pipeline is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. ? After each iteration, the error from ground-truth decreases, making the gradient smaller and hence different training policies are employed in every stage for the efficient training of H-CNN. <ref type="bibr">?</ref> We evaluate the performance of our keypoint estimation method on challenging datasets such as AFLW and AFW, which include faces in diverse poses and expressions. We also introduce a new protocol for evaluating the facial keypoint localization scheme on the AFLW dataset which is more challenging and usually left out while evaluating unconstrained face alignment methods. The rest of the paper is organized as follows. Section II reviews closely related works. Section III presents the proposed method in detail. Section IV describes the experiments and comparisons, which are then followed by conclusions and suggestions for future works in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Following <ref type="bibr" target="#b7">[8]</ref>, we classify previous works on face alignment into two basic categories.</p><p>Part-Based Deformable models: These methods perform alignment by maximizing the confidence of part locations in a given input image. One of the major works in this category was done by Zhu and Ramanan <ref type="bibr" target="#b39">[40]</ref>, where they used a part-based model for face detection, pose estimation and landmark localization assuming the face shape to be a tree structure. <ref type="bibr" target="#b4">[5]</ref> by Asthana et al., learned a dictionary of probability response maps followed by linear regression in a Constrained Local Model (CLM) framework. Hsu et al. <ref type="bibr" target="#b10">[11]</ref> extended the mixture of tree model <ref type="bibr" target="#b39">[40]</ref> to achieve better accuracy and efficiency. However, their method again assumes face shape to be a tree structure, enforcing strong constraints specific to shape variations. Regression-based approaches: Since face alignment is naturally a regression problem, a multitude of regressionbased approaches has been proposed in recent years. Methods reported in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[36]</ref> are based on learning a regression model that directly maps image appearances to target outputs. However, these methods along with methods from <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b24">[25]</ref> were mostly evaluated either in a lab setting or on face images where all the facial keypoints are visible. Wu et al. <ref type="bibr" target="#b29">[30]</ref> proposed an occlusion-robust cascaded regressor to handle occlusion. Xiong et al. <ref type="bibr" target="#b30">[31]</ref> pointed out that standard cascaded regression approaches such as Supervised Descent Method (SDM) <ref type="bibr" target="#b31">[32]</ref> tend to average conflicting gradient directions resulting in reduced performance. Hence, <ref type="bibr" target="#b30">[31]</ref> suggested domain dependent descent maps. Inspired by this, Cascade Compositional Learning (CCL) <ref type="bibr" target="#b37">[38]</ref> and Ensemble of Model Regression Trees (EMRT) <ref type="bibr" target="#b36">[37]</ref> developed head pose based and domain selective regressors respectively. <ref type="bibr" target="#b37">[38]</ref> partitioned the optimization domain into multiple directions based on head pose and learned to combine the results of multiple domain regressors through composition estimator function. Similarly <ref type="bibr" target="#b36">[37]</ref> trained an ensemble of random forests to directly predict the locations of keypoints whereafter face alignment is achieved by aggregating the consensus of different models.</p><p>Recently, methods using 3D models for face alignment have been proposed. PIFA <ref type="bibr" target="#b12">[13]</ref> by Jourabloo et al. suggested a 3D approach that employed cascaded regression to predict the coefficients of 3D to 2D projection matrix and the base shape coefficients. Another recent work from Jourabloo et al. <ref type="bibr" target="#b13">[14]</ref> formulated the face alignment problem as a dense 3D model fitting problem, where the camera projection matrix and 3D shape parameters were estimated by a cascade of CNN-based regressors. However, <ref type="bibr" target="#b37">[38]</ref> suggests that optimizing the base shape coefficients and projection is indirect and sub-optimal since smaller parameter errors are not necessarily equivalent to smaller alignment errors. 3DDFA <ref type="bibr" target="#b38">[39]</ref> by Zhu et al. fitted a dense 3D face model to the image via CNN, where the depth data is modeled in a Z-Buffer.</p><p>Our work principally falls in the category of regressionbased approaches and addresses the issue of adapting the cascade shape regression to unconstrained settings. KEPLER performs joint training on three fundamental tasks, namely, 3D pose, visibility of each keypoint and the location of keypoints, using only 2D color image. It also demonstrates that efficient joint training on the three tasks achieves superior performance. One of the closely related work is <ref type="bibr" target="#b34">[35]</ref> where the authors used multi-tasking for many attributes, but did not leverage the intermediate features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. KEPLER</head><p>KEPLER is an iterative method which at its core consists of three modules. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the basic building blocks of KEPLER. The first module is a rendering module h which models the structure in an N-dimensional input space, with N being the maximum number of keypoints on a face. The current location of the keypoints are represented by the </p><formula xml:id="formula_0">vector y t = {y 1 t . . . y N t }.</formula><p>The output of the rendering module is concatenated to the raw RGB input image I, along the third dimension which is then fed to the function f. The second module is the function f which calculates the correction to be made at the next stage. The function f is modeled by a convolution neural network whose architecture is described in section III-A. The third module is the correction stage which adds the increments, predicted by f, to the current locations. The output goes again into the rendering module h which prepares the rendered data for the next iteration. The rendering function is not learned in this work, but represented by a 2D Gaussian with fixed variance and centered at current keypoint locations in each of the N channels. Finally, the Gaussian rendered images are stacked together with image I. Therefore the overall method can be summarized by the following set of equations.</p><formula xml:id="formula_1">? t = f t (X t , ? t ) (1) y t+1 = y t + ? t (2) X t+1 = h(y t+1 )<label>(3)</label></formula><p>where f is a function with learned parameters ? t , predicting the increments ? t . The prediction function f is indexed by t as it is trained separately for every iteration. In the first iteration, the function h renders Gaussians at y 0 , which is the mean shape. In this work we set t = 5 iterations. We perform the last iteration only to take into effect the improper bounding box across different datasets (see <ref type="figure" target="#fig_0">Figure 1</ref>). The loss functions for each task is mentioned below. Keypoint localization Keypoint localization is the task of predicting the keypoints in a face. In this paper, we consider predicting the locations of N = 21 keypoints on the face. With each point is associated the visibility of that point. The loss function for this task is given by</p><formula xml:id="formula_2">L 1 (y, g) = N i=1 v i (y i t ? g i ) 2 ,<label>(4)</label></formula><p>where y i t and g i are the predicted and the ground truth locations of the i th keypoint resprectively at time t. v i is the ground truth visibility associated with each keypoint. We discuss this loss function and its variant in section III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Prediction</head><p>Pose prediction refers to the task of estimating the 3D pose of the face. We use the Euclidean loss function for pose prediction.</p><formula xml:id="formula_3">L 2 (p p , g p ) = (p yaw ?g yaw ) 2 +(p pitch ?g pitch ) 2 +(p roll ?g roll ) 2</formula><p>(5) where p stands for predicted and g for the ground-truth. Visibility This task is associated with estimating the visibility of each keypoint.The number of keypoints visible on the face varies with pose. Hence, we use the Euclidean loss to estimate the visibility confidence of each point.</p><formula xml:id="formula_4">L 3 (v p , v g ) = N i=1 (v p,i ? v g,i ) 2 ,<label>(6)</label></formula><p>Therefore the net loss in the network is the weighted linear combination of the above loss functions.</p><formula xml:id="formula_5">L(p, g) = ?L 1 (y, g) + ?L 2 (p p , g p ) + ?L 3 (v p , v g ) (7)</formula><p>where ?, ? and ? are the weight parameters suitably chosen depending on the iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>For the modeling function f we design a unique ConvNet architecture based on GoogLenet <ref type="bibr" target="#b25">[26]</ref> by pruning the inception network after inception 4c. As PReLU has shown better performance in many vision tasks such as object recognition <ref type="bibr" target="#b9">[10]</ref>, in this pruned network we first replace the ReLU non-linearity with PReLU. We pool the intermediate features from the pruned GoogLenet. Then convolutions are performed from the output of each branch, and the output maps are concatenated similar to the inception module. We call this module the Channeled Inception module. Since the output maps after conv 1 are larger in size, we first perform 4X4 convolution and then again a 4X4 convolution, both with the stride of 3 to finally match the dimension of the output to 7X7. Similarly after conv 2 we first perform 4X4 convolution and then 3X3 convolution to match the output to 7X7. The former uses a stride of 4 and the latter uses 2. The most n?ive way of combining features is by concatenation. However, the concatenated output blob can be very high dimensional and hence we perform 1X1 convolution for dimensionality reduction. This lets the network decide the weights to effectively combine the pooled features into lower dimension. It has been shown in <ref type="bibr" target="#b32">[33]</ref> that adjacent layers are correlated and hence, we only pool features from alternate layers.</p><p>Next the network is trained on three tasks namely, pose, visibilities and the bounded error using ground truth. The joint training is helpful since it models the inherent relationship between visible number of points, pose and the amount of correction needed for a keypoint in particular pose. Choosing an architecture like GoogLenet is based on the fact that due to fewer number of parameters the training of GoogLenet is faster and adding to it batch normalization, even speeds up the training process. In order to further speed up the process we only use convolution layers till the last layer where we use a fully connected layer to get the final output. The architecture of the whole network is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Iteration 1 and 2: Constrained Training</head><p>In this section, we explain the first stage training for keypoint estimation. The first stage is the most crucial one for face alignment. Since the network is trained from scratch, precautions have to be taken on what the network should learn. Directly learning the locations of keypoints from a network is difficult because when the network gets deeper it loses the localization capability. This is due to the fact that the outputs of the final convolution layers have a larger receptive field on the input image. We devise a strategy in which the corrections for the first two stages are bounded. Let us suppose the key-points are represented by their 2D coordinates y : {y i ? 2 , i ? [1, . . . , N ]} where N is the number of keypoints and y i denotes the i th keypoint. The bounded corrections were calculated using (8) given below.</p><formula xml:id="formula_6">? i t (g i , y i t ) = min(L, u ).?<label>(8)</label></formula><p>where L denotes the bound of correction. u = g ? y t and? = u u represent the error vector and error unit vector respectively. In our experiments we set the bound L to a maximum of 20 pixels. This simplifies the learning problem for the network in the first stage. According to this formulation, error correction for points for which the ground truth is far away, gets bounded by L. The interesting property of this formulation is that in the first and second stage the network only learns the direction in which the points have to shift. This can be thought of as learning the direction of the unit error vector, to which the magnitude will be added later. In addition to just having keypoint location we also have access to facial 3D pose and the visibility of each point. One-shot prediction of the location of keypoints is difficult since the input space of the ConvNet is typically nonlinear. Also, learning small corrections should be easier, when the network is being trained for the first time. Hence, to impart prior knowledge to the network we jointly learn the pose and visibility of each point. The loss functions used for the three tasks are described in the previous section. The function f for second iteration is trained in a similar fashion with the weights initialized from the first iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Iteration 3: Variant of Euclidean loss</head><p>We show the outputs of the network after the second stage of training in <ref type="figure" target="#fig_3">Figure 4</ref>. Physical inspection of the outputs shows that for many of the faces, the network has already learned the magnitude and direction of the correction vector. However, there are misalignments in some images or in some keypoints in the images. But repeating the training methodology exactly as second iteration revealed that our architecture suffered from vanishing gradients. While back propagating the gradients, the loss is averaged over a batch and if there are few misalignments in a batch, there is very little gradient to be propagated. To maintain consistency we stick with the same architecture. Even though GoogLenet <ref type="bibr" target="#b25">[26]</ref> claims to not have vanishing gradient problem, KEPLER faced it because of the dataset being small. This motivated us to design a loss function that satisfies both of these conditions: on the one hand, the loss function should minimize the error between prediction and the ground truth; on the other hand, it should have sufficient gradients to be propagated to make the learning process reach global minima. Towards this end, we use the following loss function.</p><formula xml:id="formula_7">L 1 (y, g) = 1 n N i=1 v i (y i ? g i ) 2 + ? N i=1 v i | y i ? g i | (9) ?L 1 (y, g) ?y = 1 n 2 N i=1 v i (y i ? g i ) + ? N i=1 v i | y i ? g i | y i ? g i<label>(10)</label></formula><p>where ? is a parameter which controls the strength of the gradient and n is the number of samples in a batch. We would like to emphasize that the additional term is not a regularizer as it is added to the objective function and does not directly regularize the weights. However, this is able to provide substantial gradients for the training of ConvNet. The representation function h in this stage does not render any Gaussian in the channel for which the predicted visibility is below the threshold ? . In this work we set this threshold ? to 0.03 and ? to 0.2 obtained by cross validation. We do not constrain the amount of error corrections for the third stage training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Iteration 4: Hard sample mining</head><p>Recently, Kabkab et al. <ref type="bibr" target="#b14">[15]</ref> suggested that by efficiently sampling the data one can make an optimal use of training set while training ConvNets leading to improved performance. <ref type="bibr" target="#b14">[15]</ref> developed an online data sampling method based on a convex optimization formulation and showed how their formulation can make the classifier robust in class imbalanced problem. In our case, although after the third iteration, most of the images are aligned, they lack precision in local alignment. Inspired by <ref type="bibr" target="#b14">[15]</ref>, we reuse the hard samples of the dataset to build a more robust keypoint localization system. Using the keypoints predicted after the third iteration, we plot the histogram <ref type="figure" target="#fig_4">(Fig.5</ref>) of normalized mean error (NME), after calculating it for all the training samples. We denote the NME on x-axis around which the maximum number of samples are centered, as C. In an ideal case, the value of C should be low, implying that the average alignment error is less. Therefore, the objective of this stage is to lower the value of C by hard sample mining. We select a threshold ? (0.03 in our experiments), towards the right of C, after which at least 30 ? 40% of the samples lie, as the threshold for hard samples. Using ?, we partition the dataset into two groups of hard and easy samples. We first select equal number of samples from both groups to form a batch which is then presented to ConvNet for training. This effectively results in reusing the hard samples. Then, to counter the group imbalance we finetune the network with entire dataset again with a lower learning rate. We use the loss function as in (9) with ? = 0.1 for this stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Iteration 5: Local Error Correction</head><p>There is a lot of inconsistency among the bounding boxes provided by different datasets. AFLW <ref type="bibr" target="#b17">[18]</ref> provides larger bounding box annotations compared to AFW <ref type="bibr" target="#b39">[40]</ref>.</p><p>Regression-based alignment methods are dependent on the mean shape initialization, which is scaled to the bounding box size. Also it is impractical to come up with a heuristic which tries to determine compatible bounding boxes. Almost all the existing methods perform data augmentation by randomly perturbing the bounding boxes by some amount. However, it is not clear by how much the bounding boxes should be perturbed to obtain reasonably good bounding boxes during testing which is consistent with the dataset the network was trained on. We train our networks on a larger bounding box provided by AFLW. AFLW bounding boxes tend to be square and for almost all the images the nose tip appears at the center of the bounding box. This is a big limitation for the deployment of the system in real world scenarios. It is worthy to note that the previous four stages are trained on full images and hence produce global corrections. Our last stage of local correction is optional, which depends upon the test set and the bounding box annotations that it comes with. We train an exactly similar network as in <ref type="figure" target="#fig_2">Fig  3.</ref> but only for the tasks of predicting the visibility and corrections in the local patches. Predicting the pose with a local patch of say W XW pixels is difficult which can lead the network to learn improper weights. We choose all the N patches irrespective of the visibility factor. Learning visibility and corrections is important because we do not want the network to propagate any gradient if the point is invisible. We observe during experimentation that training the ConvNet on two tasks together achieves significantly better performance than when the network is trained only for the task of error correction. We again partition the dataset into easy and hard sample groups according to the strategy explained in the previous section. We finally finetune the network with the whole dataset with lower learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND COMPARISON</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We select two challenging datasets with their most recent benchmarks.</p><p>In-the-wild datasets: To make the system robust for images in real life scenarios such as challenging shape variations and significant view changes, we select AFLW <ref type="bibr" target="#b17">[18]</ref> for training and, AFLW and AFW <ref type="bibr" target="#b39">[40]</ref> as the main test sets.</p><p>AFLW contains 24, 386 in-the-wild faces (obtained from Flickr) with head pose ranging from 0 ? to 120 ? for yaw and upto 90 ? for pitch and roll with extremely challenging shape variations and deformations. Along with this AFLW also demonstrates external-object occlusion. There are a total of 21% invisible landmarks caused by occlusion, larger than 13% on COFW <ref type="bibr" target="#b6">[7]</ref> where only internal object-occlusion is exhibited. In addition, one important point to note is that COFW also provides the annotations for the invisible landmarks while in the case of AFLW the invisble landmarks are absent. AFW is a popular benchmark for the evaluation of face alignment algorithms. AFW contains 468 in-the-wild faces (obtained from Flickr) with yaw degree up to 90 ? . The images are well diverse in terms of pose, expression and illumination. The number of visible points also varies depending on the image, but the location of occluded points are to be predicted as well. AFLW provides at most 21 points for each face. It excludes coordinates for invisible landmarks, which we consider to be the best, because there is no way of correctly knowing the exact location of those points. In many cases such invisible points are mostly hallucinated and annotated thereafter.</p><p>Testing Protocols:</p><p>(I)AFLW-PIFA: We follow the protocol used in PIFA <ref type="bibr" target="#b12">[13]</ref>. We randomly select 23, 386 images for training and the remaining 1, 000 for testing. We divide the testing images in three groups as done in <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_8">[0 ? , 30 ? ], [30 ? , 60 ? ] and [60 ? , 90 ? ]</formula><p>where the number of images in each group are taken to be equal. (II)AFLW-Full: We also test on the full test set of AFLW of sample size 1, 000. (III)AFLW-All variants: In the next experiment, to have more rigorous analysis, we perform the test on all variants of images from (I) above. To create all variants images, we first rotate the whole images from (I) at angles of 15 ? ,30 ? ,45 ? and 60 ? . We do the same with the horizontally flipped version of these images. We then rotate the bounding box coordinates and the key-points also at the same angles and crop the faces. This is done for all the images following the AFLW-PIFA protocol. One important effect of this rotation is that some of the images have smaller face compared to others due to rotated bounding box. This experiment tests the robustness of the algorithm on faces of different effective sizes and orientations. (IV)AFW: We only use AFW for testing purposes. We follow the protocol as stated in <ref type="bibr" target="#b39">[40]</ref>. AFW provides 468 images in total, out of which 341 faces have height greater than 150 pixels. We only evaluate on those 341 images following the protocol of <ref type="bibr" target="#b39">[40]</ref>.</p><p>Evaluation metric: Following most previous works, we obtain the error for each test sample via averaging normalized errors for all annotated landmarks. We demonstrate our results with mean error over all samples, or via Cumulative Error Distribution (CED) curve. For pose, we evaluate on continuous pose predictions as well as their discretized versions rounded to nearest 15 ? . We report the continuous mean absolute error for the AFLW testset and plot the Cumulative Error Distribution curve for AFW dataset. All the experiments including training and testing were performed using the Caffe <ref type="bibr" target="#b11">[12]</ref> framework and two Nvidia TITAN-X GPUs. Our method can process upto 3-4 frames per second, which can be higher in batch mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFLW</head><p>AFW Method NME NME TSPM <ref type="bibr" target="#b39">[40]</ref> -11.09 CDM <ref type="bibr" target="#b1">[2]</ref> 12.44 9.13 RCPR <ref type="bibr" target="#b6">[7]</ref> 7.85 -ESR <ref type="bibr" target="#b7">[8]</ref> 8.24 -PIFA <ref type="bibr" target="#b12">[13]</ref> 6.8 8.61 3DDFA <ref type="bibr" target="#b38">[39]</ref> 5.32 -LPFA-3D <ref type="bibr" target="#b13">[14]</ref> 4.72 7.43 EMRT <ref type="bibr" target="#b36">[37]</ref> 4.01 3.55 CCL <ref type="bibr" target="#b37">[38]</ref> 5     <ref type="table" target="#tab_1">Table III</ref> shows the mean error in degrees, in estimating the 3D pose of a face image. <ref type="figure" target="#fig_6">Figures 7 and 8</ref> show the cumulative error distribution in predicting keypoints on the AFLW and AFW test sets. <ref type="figure" target="#fig_8">Figure 9</ref> shows the cumulative error distribution in pose estimation on AFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Comparison with CCL <ref type="bibr" target="#b37">[38]</ref>: It is clear from the tables that KEPLER outperforms all state of the art methods on the AFLW dataset. It also outperforms all state of the art methods except CCL <ref type="bibr" target="#b37">[38]</ref> on the AFW datatset. Visual inspection of our results suggests that KEPLER is a little farther from ground truth on invisible points. We note that CCL <ref type="bibr" target="#b37">[38]</ref> manually annotates the AFLW dataset with 19 landmarks along with the invisible landmarks, leaving the earpoints. In our experiments we prefer to use the dataset as provided by AFLW <ref type="bibr" target="#b17">[18]</ref>, although we believe that CCLkind of reannotation may boost the performance(since during AFW evaluation the locations of occluded points also need to be predicted). In KEPLER there is no loss propagated for the invisible points. We believe that training KEPLER on the revised annotation by <ref type="bibr" target="#b37">[38]</ref> would make the prediction of occluded points more precise.   We also verify our claim that iteration 5 is optional and only required for transferring the algorithm to other datasets with different bounding box annotations. To support our claim we calculate the normalized mean error after iteration 4 for both datasets and compare with the error obtained after iteration 5. The error after iteration 4 for AFLW testset was 0.0369 (which is already lower than all existing works) and after fifth iteration it was 0.0299, bringing the performance up by 18%. On the other hand the improvement in AFW (whose bounding box annotation is different from AFLW) was close to 60%. The error after iteration 4 on AFW dataset was 0.0757 which decreases to 0.0301 after fifth iteration. We demonstrate some qualitative results from AFLW and AFW test sets in <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORKS</head><p>In this work we show that by efficiently capturing the structure of face through additional channels, we can obtain precise keypoint localization on unconstrained faces. We propose a novel Channeled Inception deep network which pools features from intermediate layers and combines them in the same manner to the Inception module. We show how cascade regressors can outperform other recently developed works. As a byproduct of KEPLER, 3D facial pose is also generated which can be used for other tasks such as pose dependent verification methods, 3D model generation and many others. In conclusion, KEPLER demonstrates that by improved initialization and multitask training, cascade regressors outperforms state of the art methods not only in predicting the keypoints but also for head pose estimation.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Sample results generated by the proposed method. White dots represent the location of keypoints after each iteration. The first row shows an image from the AFLW dataset. The points move at subpixel level after fourth iteration. The second row is a sample image from the AFW dataset, which shows how the last stage of error correction can effectively mitigate the inconsistency of the bounding box across datasets. The numbers in red are the predicted 3D pose P:Pitch Y:Yaw R:Roll</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the architecture of KEPLER. The function f() predicts visibility, pose and the corrections for the next stage. The representation function h() forms the input representation for the next iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The KEPLER network architecture. The dotted line shows the channeled inception network. The intermediate features are convolved and the responses are concatenated in a similar fashion as the inception module. Tasks such as pose are abstract and contained in deeper layers, however, the localization property is in the shallower layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative results of KEPLER after second stage. The green dots represent the predicted points after second stage. Red dots represent the ground truth. It can be seen that the visible points have taken the shape of input face image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Error Histogram of training samples after stage 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Red dots in the left image represent the ground truth while green dots represent the predicted points after the fourth iteration. Local patches centered around predicted points are extracted and fed to the network. The network shown inFig 3 (see section III-E for details) is trained on the task of local fiducial correction and visibility of fiducials inside the patch. The image on the right shows the predictions after local correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Cumulative error distribution curves for landmark localization on the AFLW dataset. The numbers in the legend are the average normalized mean error normalized by the face size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Cumulative error distribution curves for landmark localization on the AFW dataset. The numbers in the legend are the fraction of testing faces that have average error below (5%) of the face size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Cumulative error distribution curves for pose estimation on AFW dataset. The numbers in the legend are the percentage of faces that are labeled within ?15 ? error tolerance VI. ACKNOWLEDGMENT This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results of KEPLER after last stage. The green dots represent the final predicted points after fifth iteration. First row are the test samples from AFLW. Second row shows the samples from AFW dataset. The last two rows are the results of KEPLER after last stage from AFLW testset for all variants protocol. The green dots represent the final predicted points after fifth iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparison of KEPLER with other state of the art methods. NME stands for normalized mean error. For AFLW, numbers for other methods are taken from respective papers following the PIFA protocol. For AFW, numbers are taken from respective works published following the protocol of<ref type="bibr" target="#b39">[40]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table I compares the performance of KEPLER compared to other existing methods. Table II summarises the performance of KEPLER under different protocols of AFLW testset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Summary of performance on different protocols of AFLW and AFW by KEPLER.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AFLW</cell><cell></cell><cell>AFW</cell></row><row><cell>Method</cell><cell>Yaw</cell><cell>Pitch</cell><cell>Roll</cell><cell>MAE</cell><cell>Accuracy(? 15 ? )</cell></row><row><cell>Random Forest [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>12.26 ?</cell><cell>83.54%</cell></row><row><cell>KEPLER</cell><cell>6.45 ?</cell><cell>5.85 ?</cell><cell>8.75 ?</cell><cell>6.45 ?</cell><cell>96.67%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison of Mean error in 3D pose estimation by KEPLER on AFLW testset. For AFLW<ref type="bibr" target="#b28">[29]</ref> only compares mean average error in Yaw. For AFW we we compare the percentage of images for which error is less than 15 ? .</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A recurrent autoencoder-decoder for sequential face alignment</title>
		<ptr target="http://arxiv.org/abs/1608.05477" />
		<imprint>
			<biblScope unit="page" from="2016" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pose-free Facial Landmark Fitting via Optimized Part Mixtures and Cascaded Deformable Shape Model</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5435" to="5444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive cascaded regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP&apos;16</title>
		<meeting><address><addrLine>Phoenix, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, CVPR &apos;13</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regressive tree structured model for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dcnns on a diet: Sampling strategies for reducing the training set size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1606.04232</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Face alignment by local deep descriptor regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1601.07950</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face alignment using cascade gaussian process regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="4204" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face alignment via componentbased discriminative search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>D. A. Forsyth, P. H. S. Torr, and A. Zisserman</editor>
		<imprint>
			<biblScope unit="volume">5303</biblScope>
			<biblScope unit="page" from="72" to="85" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multitask learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 FPS via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, CVPR &apos;13</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mnemonic descent method: A recurrent process applied for end-to-end face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gauss-newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Head-Pose Estimation In-the-Wild Using a Random Forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vald?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="24" to="33" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="3658" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised descent method and its application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xuehan-Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename><surname>De La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coarse-to-fine autoencoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<biblScope unit="volume">8690</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Face alignment by coarseto-fine shape searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Towards arbitrary-view face alignment by recommendation trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1511.06627</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unconstrained face alignment via cascaded compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1511.07212</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
