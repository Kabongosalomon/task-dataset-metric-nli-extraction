<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STC: Spatio-Temporal Contrastive Learning for Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
							<email>zhengkjiang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangxuan</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlong</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent Youtu Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">STC: Spatio-Temporal Contrastive Learning for Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Instance Segmentation</term>
					<term>Spatio-Temporal Contrastive Learning</term>
					<term>Temporal Consistency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Instance Segmentation (VIS) is a task that simultaneously requires classification, segmentation, and instance association in a video. Recent VIS approaches rely on sophisticated pipelines to achieve this goal, including RoI-related operations or 3D convolutions. In contrast, we present a simple and efficient single-stage VIS framework based on the instance segmentation method CondInst by adding an extra tracking head. To improve instance association accuracy, a novel bi-directional spatio-temporal contrastive learning strategy for tracking embedding across frames is proposed. Moreover, an instance-wise temporal consistency scheme is utilized to produce temporally coherent results. Experiments conducted on the YouTube-VIS-2019, YouTube-VIS-2021, and OVIS-2021 datasets validate the effectiveness and efficiency of the proposed method. We hope the proposed framework can serve as a simple and strong baseline for other instance-level video association tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While significant progress <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> has been made in instance segmentation with the development of deep neural networks, less attention has been paid to its challenging variant in the video domain. The video instance segmentation (VIS) <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17]</ref> task requires not only classifying and segmenting instances but also capturing the instance associations across frames. Such technology can benefit a great variety of scenarios, e.g., video editing, video surveillance, autonomous driving, and augmented reality. As a result, it is in great need of accurate, robust, and fast video instance segmentation approach in practice.</p><p>Previous researchers have developed sophisticated pipelines for tackling this problem <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">12]</ref>. Generally speaking, previous studies can be divided into the categories of two-stage <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12]</ref>, feature-aggregation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2]</ref> ? Equal contributions. This work was done while Zhangxuan Gu was interning at Tencent Youtu Lab. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP (%)</head><p>Ours-50</p><p>Ours-101</p><p>CompFeat <ref type="bibr">'21</ref> MaskTrack R-CNN <ref type="bibr">'19</ref> TraDeS <ref type="bibr">'21</ref> SipMask <ref type="bibr">'20</ref> QueryInst'21 CrossVIS'21</p><p>VisSTG <ref type="bibr">'21</ref> STEm-Seg <ref type="bibr">'20</ref> VisTR'21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>Single-Stage Two-Stage Transformer 3D <ref type="figure">Fig. 1</ref>. Speed-Accuracy trade-off curve on the YouTube-VIS-2019 validation set. The baseline results are compared with the same ResNet-50 backbone for fair comparison. We achieve best tradeoff between speed and accuracy. In particular, STC exceeds recent CrossVIS <ref type="bibr" target="#b10">[11]</ref> 1.9% mAP with similar running speed.</p><p>inspired from video object detection domain <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref>, 3D convolution-based <ref type="bibr" target="#b0">[1]</ref>, transformer-based <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b16">17]</ref>, and single-stage <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref> methods. Two-stage methods, e.g., MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref> and CompFeat <ref type="bibr" target="#b11">[12]</ref>, usually rely on the RoIAlign operation to crop the feature and obtain the representation of an instance for further binary mask prediction. Such the RoIAlign operation would lead to great computational inefficiency. 3D convolution-based STEm-Seg <ref type="bibr" target="#b0">[1]</ref> holds huge complexity and could not achieve good performance. Transformer-based VisTR <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b16">17]</ref> could not handle long videos due to largely increasing memory usage and needs a much longer training time for convergence. Feature-aggregation methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref> enhance features through pixel-wise or instance-wise aggregation from adjacent frames similarly to other video tasks, like video object detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45]</ref>. Although some attempts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref> have been made to tackle VIS in a simple single-stage manner, their performances are still not satisfying.</p><p>The key difference between video and image instance segmentation lies in the need of capturing robust and accurate instance association across frames. However, most previous works such as MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, and CMask-Track R-CNN <ref type="bibr" target="#b32">[33]</ref> formulate instance association as a multi-label classification problem, focusing only on the intrinsic relationship within instances while ignoring the extrinsic constraint between different ones. Thus different instances with similar distributions may be wrongly associated by using previous tracking embeddings only through such multi-label classification loss constraint.</p><p>Alternatively, we propose an efficient single-stage fully convolutional network for video instance segmentation task, considering that single-stage instance segmentation is simpler and faster. Based on the recent instance segmentation method CondInst <ref type="bibr" target="#b36">[37]</ref>, an extra tracking head is added to simultaneously learn instance-wise tracking embeddings for instance association besides original classification head, box head, and mask head by dynamic filter. To improve instance association accuracy between adjacent frames, a spatio-temporal con-trastive learning strategy is utilized to exploit relations between different instances. Specifically, for a tracking embedding query, we densely sample hundreds of negative and positive embeddings from reference frames based on the label assignment results, acting as a contrastive manner to jointly pull closer to the same instances and push away from different instances. Different from previous metric learning based instance association methods i.e., Triplet Loss, the proposed contrastive strategy enables efficient many-to-many relations learning across frames. We believe this contrast mechanism enhances the instance similarity learning, which provides more substantial supervision than using only the labels. Moreover, this contrastive learning scheme is applied in a bi-directional way to better leverage the temporal information from both forward and backward views. At last, we further propose a temporal consistency scheme for instance encoding, which contributes to both the accuracy and smoothness of the video instance segmentation task.</p><p>In summary, our main contributions are:</p><p>-We propose a single-stage fully convolutional network for video instance segmentation task with an extra tracking head to simultaneously generate instance-specific tracking embeddings for instance association. -To achieve accurate and robust instance association, we propose a bi-directional spatio-temporal contrastive learning strategy that aims to obtain representative and discriminative tracking embeddings. In addition, we present a novel temporal consistency scheme for instances encoding to achieve temporally coherent results.</p><p>-Comprehensive experiments are conducted on the YouTube-VIS-2019, YouTube-VIS-2021, and OVIS-2021 benchmark. Without bells and whistles, we achieve 36.7% AP and 35.5% AP with ResNet-50 backbone on YouTube-VIS-2019 and YouTube-VIS-2021 datasets, which is the best performance among all listed single-model methods with high efficiency. We also achieve best performance on recent proposed OVIS-2021 dataset. In particular, compared to the first VIS method named MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, our proposed method (STC) achieves 36.7% AP on YouTube-VIS-2019, outperforming it by 6.4% AP with the advantage of being much simpler and faster. Compared with recent method CrossVIS <ref type="bibr" target="#b51">[52]</ref>, STC outperforms it by 1.9% AP with a slightly faster speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instance Segmentation</head><p>Instance segmentation aims to represent objects at a pixel level, which is a finergrained representation compared with object detection. There are mainly two kinds of instance segmentation methods, i.e., two-stage <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16]</ref>, and singlestage <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37]</ref>. Two-stage methods first detect objects, then crop their region features to further classify each pixel into the foreground or background, while the framework of single-stage instance segmentation is much simpler. For</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference Frame</head><p>Input Frame</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Backbone</head><p>Mask Branch +Coord.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mask head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convs Convs Convs</head><p>Binary masks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Generator</head><p>Tracking Head</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convs</head><p>Convs Convs </p><formula xml:id="formula_0">F !"#$ " F %&amp;'(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attract Repel</head><p>Positive Embeddings <ref type="figure">Fig. 2</ref>. The overview of our proposed framework. The framework contains the following components: a shared CNN backbone for encoding frames to feature maps, kernel generators with mask heads for instance segmentation, a mask branch to combine multi-scale FPN features, and a shared tracking head with a bi-directional spatiotemporal contrastive learning strategy (the bi-directional learning scheme is omitted here for simplicity) for instance association. A temporal consistency constraint is applied to the kernel weights, as the blue line shows. Best viewed in color.</p><p>example, YOLACT <ref type="bibr" target="#b3">[4]</ref> is proposed to generate a set of prototype masks and predict per-instance mask coefficients. The instance masks are then produced by linearly combining the prototypes with the mask coefficients. SOLO <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> reformulates the instance segmentation as two simultaneous category-aware prediction problems, i.e., location prediction, and mask prediction, respectively. Inspired by dynamic filter network <ref type="bibr" target="#b17">[18]</ref>, CondInst <ref type="bibr" target="#b36">[37]</ref> proposes to dynamically predict instance-aware filters for mask generation. SOLOv2 <ref type="bibr" target="#b41">[42]</ref> further incorporates dynamic filter scheme to dynamically segments each instance in the image with a novel matrix non-maximum suppression (NMS) technique. Compared to the image instance segmentation, video instance segmentation aims not only to segment object instances in individual frames but also to associate the predicted instances across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video Instance Segmentation</head><p>Video instance segmentation <ref type="bibr" target="#b49">[50]</ref> aims to simultaneously classify, segment, and track instances of the videos. Various complicated pipelines are designed by state-of-the-art methods to solve it. To better introduce the related methods, we separate them into the following groups. (1) The two-stage method Mask-Track R-CNN <ref type="bibr" target="#b49">[50]</ref>, as the pioneering work for VIS, extends image instance segmentation method Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> to video domain by introducing an extra tracking branch for instance association. Another method in the two-stage group is MaskProp <ref type="bibr" target="#b1">[2]</ref>, which first uses Hybrid Task Cascade (HTC) <ref type="bibr" target="#b6">[7]</ref> to generate the predicted masks and propagates them temporally to the other frames in a video. Recently, CompFeat <ref type="bibr" target="#b11">[12]</ref> proposed a feature aggregation approach based on MaskTrack R-CNN, which refines features by aggregating multiple adjacent frames features. (2) Relying on 3D convolutions, STEm-Seg [1] models a video clip as a single 3D spatial-temporal volume and separates object instances by clustering.</p><p>(3) Based on feature-aggregation, STMask <ref type="bibr" target="#b22">[23]</ref> proposes a simple spatial feature calibration to detect and segment object masks frame-by-frame, and further introduces a temporal fusion module to track instances across frames.</p><p>(4) More recently, a transformer-based method VisTR <ref type="bibr" target="#b43">[44]</ref> is proposed to reformulate VIS as a parallel sequence decoding problem. (5) There also exist some single-stage VIS methods, e.g., SipMask <ref type="bibr" target="#b4">[5]</ref>, and TraDeS <ref type="bibr" target="#b45">[46]</ref>. SipMask <ref type="bibr" target="#b4">[5]</ref> proposes a spatial preservation module to generate spatial coefficients for the mask predictions while recently proposed TraDeS <ref type="bibr" target="#b45">[46]</ref> presents a joint detection and tracking model by propagating the previous instance features with the predicted tracking offset. CrossVIS <ref type="bibr" target="#b51">[52]</ref> proposes cross-frame instance-wise consistency loss for video instance segmentation. Although current methods have made good progress, their complicated pipelines or unsatisfying performance prohibit practical application. In contrast, the proposed framework acts in a fully convolutional manner with decent performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>Contrastive learning has lead to considerable progress in many real-world applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9]</ref>. For example, MOCO <ref type="bibr" target="#b12">[13]</ref> builds image-level large dictionaries for unsupervised representation learning using contrastive loss. Sim-CLR <ref type="bibr" target="#b7">[8]</ref> utilizes the elaborate data augmentation strategies and a large batch, which outperforms MOCO by a large margin on self-supervised learning Ima-geNet <ref type="bibr" target="#b33">[34]</ref> classification task. Different from the above methods, which focus on image-level contrastive learning for unsupervised representation learning, we use modified multiple-positives contrastive learning to learn instance-level tracking embeddings accurately for video instance segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first briefly review the instance segmentation method CondInst <ref type="bibr" target="#b36">[37]</ref> for mask generation of still-image. Then, we introduce the proposed whole framework for the video instance segmentation task. Next, we present a novel spatiotemporal contrastive learning strategy for tracking embeddings to achieve accurate and robust instance association. In addition, we further propose a bidirectional spatio-temporal contrastive learning strategy. At last, the temporal consistency scheme aiming to achieve temporally coherent results is introduced in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mask Generation for Still-image</head><p>For still-image instance segmentation, we use the dynamic conditional convolutions method CondInst <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18]</ref>. Specifically, instance mask at location (x, y) can be generated by convolving an instance-agnostic feature mapF x,y mask from mask branch and instance-specific dynamic filter ? x,y , which is calculated as follows:</p><formula xml:id="formula_1">m x,y = MaskHead(F x,y mask ; ? x,y ),<label>(1)</label></formula><p>whereF x,y mask is the combination of multi-scale fused feature map F mask from FPN features {P 3 , P 4 , P 5 } and relative coordinates O x,y . The MaskHead consists of three 1 ? 1 conv-layers with dynamic filter ? x,y at location (x, y) as convolution kernels. m x,y ? R H?W is the predicted binary mask at location (x, y) as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Framework for VIS</head><p>The overall framework of the proposed method is illustrated in <ref type="figure">Figure 2</ref>. Based on the instance segmentation method CondInst <ref type="bibr" target="#b36">[37]</ref>, we add a tracking head for instances association. The whole architecture mainly contains following components: (1) A shared CNN backbone (e.g. ResNet-50 <ref type="bibr" target="#b14">[15]</ref>) is utilized to extract compact visual feature representations with FPN <ref type="bibr" target="#b24">[25]</ref>. <ref type="formula" target="#formula_2">(2)</ref> Multiple heads including a classification head, a box regression head, a centerness head, a kernel generator head, and a mask head as same as CondInst <ref type="bibr" target="#b36">[37]</ref>. Since the architectures of the above classification, box regression, and centerness heads are not our main concerns, we omit them here (please refer to <ref type="bibr" target="#b37">[38]</ref> for the details). <ref type="formula" target="#formula_3">(3)</ref> A tracking head where spatio-temporal contrastive learning strategy is proposed to associate instances across frames with comprehensive relational cues in the tracking embeddings. (4) Temporal consistency scheme on instance-wise kernel weights across frames aims to generate temporally coherent results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatio-Temporal Contrastive Learning</head><p>To associate instances from different frames, an extra lightweight tracking head is added to obtain the tracking embeddings <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> in parallel with the original kernel generator head. The tracking head consists of several convolutional layers which take multi-scale FPN features {P 3 , P 4 , P 5 } as input. And the outputs are fused to obtain the feature map of tracking embedding. As shown in <ref type="figure">Figure 2</ref>, given an input frame I for training, we randomly select a reference frame I ref from its temporal neighborhood. A location is defined as a positive sample if it falls into any ground-truth box and the class label c of the location is the class label of the ground-truth box. If a location falls into multiple bounding boxes, it is considered as the positive sample of the bounding box with minimal area <ref type="bibr" target="#b37">[38]</ref>. Thus, two locations formulate a positive pair if they are associated with the same instance across two frames and a negative pair otherwise.</p><p>During training, for a given frame, the model first predicts the object detection results. Then, the tracking embedding of each instance can be extracted from the tracking feature map by the center of the predicted bounding box. For a training sample with extracted tracking embedding q, we can obtain positive embeddings k + and negative embeddings k ? according to label assignment results at reference frame. Note that traditional unsupervised representation learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b7">8]</ref> with contrastive learning only uses one positive sample and multiple negative samples as follows:</p><formula xml:id="formula_2">L q = ? log exp(q ? k + ) exp(q ? k + ) + k ? exp(q ? k ? ) .<label>(2)</label></formula><p>Since there are many positive embeddings at reference frame for each training sample, instead of randomly selecting one positive embedding at reference frames, we optimize the objective loss with multiple positive embeddings and multiple negative embeddings as:</p><formula xml:id="formula_3">L contra = ? k + log exp(q ? k + ) exp(q ? k + ) + k ? exp(q ? k ? ) = k + log[1 + k ? exp(q ? k ? ? q ? k + )].<label>(3)</label></formula><p>Suppose there are N pos training samples at input frame, the objective track loss with multiple samples is:</p><formula xml:id="formula_4">L track = 1 N pos Npos i=1 L i contra .<label>(4)</label></formula><p>Bi-directional Spatio-Temporal Learning. Many video-related tasks have shown the effectiveness of bi-directional modeling <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b34">35]</ref>. To fully exploit such temporal context information, we further propose a bi-directional spatio-temporal learning scheme to learn instance-wise tracking embeddings better. Note that we only utilize this scheme in the training stage, and thus it does not affect the inference speed. Similar to <ref type="bibr">Equation 4</ref>, the objective function of bi-directional spatio-temporal contrastive learning can be denoted asL track by reversing input frame and reference frame. Thus, the final bi-directional spatio-temporal contrastive loss is:</p><formula xml:id="formula_5">L bi?track = 1 2 (L track +L track ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Temporal Consistency</head><p>Compared with image data, the coherent property between frames is also crucial to video-related researches. Thus, we add a temporal consistency constraint on the kernel weights, marked as the blue line in <ref type="figure">Figure 2</ref>, to capture such prior during training so that the predicted masks will be more accurate and robust across frames. Given an instance at location (x, y) appearing at both input and reference frames, we use (x, y) and (x,?) to denote its positive candidate positions from two frames, respectively. Formally, the temporal consistency constraint during training can be formulated as an L2-loss function:</p><formula xml:id="formula_6">L consistency = ||? x,y ? ? ref x,? || 2 + ||m x,y ? m ref x,? || 2 ,<label>(6)</label></formula><p>where ? ref x,? is the dynamic filter at reference frame, m ref x,? is the predicted instance mask by reference dynamic filter. With such a simple constraint, our kernel generator can obtain accurate, robust and coherent mask predictions across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Inference</head><p>Training Scheme. Formally, the overall loss function of our model can be formulated as follows:</p><formula xml:id="formula_7">L overall = L condinst + ? b L bi?track + ? c L consistency ,<label>(7)</label></formula><p>where L condinst denotes the original loss of CondInst <ref type="bibr" target="#b36">[37]</ref> for instance segmentation. We refer readers to <ref type="bibr" target="#b36">[37]</ref> for the details of L condinst . ? b and ? c are the hyper-parameters. Inference on Frame. For each frame, we forward it through the model to get the outputs, including classification confidence, centerness scores, box predictions, kernel weights, and tracking embeddings. Then we obtain the box detections by selecting the positive positions whose classification confidence is larger than a threshold (set as 0.03), similar to FCOS <ref type="bibr" target="#b37">[38]</ref>. After that, following previous work MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, the NMS <ref type="bibr" target="#b3">[4]</ref> with the threshold being 0.5 is used to remove duplicated detections. In this step, these boxes are also associated with the kernel weights and tracking embeddings. Supposing that there remain T boxes after the NMS, thus we have T groups of the generated kernel weights. Then T groups of kernel weights are used to produce T mask heads. These instance-specific mask heads are applied to the positions encoded mask feature to predict the instance masks following <ref type="bibr" target="#b36">[37]</ref>. T is 10 in default following previous work MaskTrack R-CNN. Inference on Video. Given a testing video, we first construct an empty memory bank for the predicted instance embeddings. Then our model processes each frame sequentially in an online scheme. Our network generates a set of predicted instance embeddings at each frame. The association with identified instances from previous frames relies on the cues of embedding similarity, box overlap, and category label similar to the MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>. All predicted instance embeddings of the first frame are directly regarded as identified instances and saved into the memory bank. After processing all frames, our method produces a set of instances sequence. The majority votes are utilized to decide the unique category label of each instance sequence. <ref type="table">Table 1</ref>. Comparisons with some state-of-the-art approaches on YouTube-VIS-2019 val set. ?indicates using extra data augmentation (e.g., random crop, higher resolution input, multi-scale training) <ref type="bibr" target="#b1">[2]</ref> or additional data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b43">44]</ref>. ? indicates the method that reaches higher performance by stacking multiple networks, and we regard it an unfair competitor in general setting. Note that STMask <ref type="bibr" target="#b22">[23]</ref> uses deformable convolution network (DCN) <ref type="bibr" target="#b9">[10]</ref> as the backbone, which is still inferior to our method at both accuracy and speed, demonstrating the superiority of our proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To verify the effectiveness of our approach, we evaluate it on recent three video instance segmentation benchmarks, YouTube-VIS-2019 <ref type="bibr" target="#b49">[50]</ref>, YouTube-VIS-2021 <ref type="bibr" target="#b48">[49]</ref> and OVIS-2021 <ref type="bibr" target="#b32">[33]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>The evaluation metrics are average precision (AP) and average recall (AR), with the video Intersection over Union (IoU) of the mask sequences as the threshold <ref type="bibr" target="#b49">[50]</ref>. Specifically, for a predicted maskm i and a ground-truth mask m j , we first extend them to the whole video with length T by padding empty mask. Then,</p><formula xml:id="formula_8">IoU(i, j) = T t=1m i t ? m j t T t=1m i t ? m j t .<label>(8)</label></formula><p>According to the definition, if the model detects object masks successfully but fails to associate the objects across frames, it still gets a low IoU. Thus, accurate and robust instance association across frames is very crucial for achieving high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Model Settings. In our experiments, we choose the ResNet-50 <ref type="bibr" target="#b14">[15]</ref> and ResNet-101 with FPN <ref type="bibr" target="#b24">[25]</ref> as the backbone in the proposed method. Our model is pretrained on COCO train2017 <ref type="bibr" target="#b25">[26]</ref> with 1? schedule following previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b49">50]</ref>. We implement the proposed method with PyTorch <ref type="bibr" target="#b31">[32]</ref> and the FPS is measured on an RTX 2080 Ti GPU including the pre-and post-processing steps for fair comparison following previous work <ref type="bibr" target="#b51">[52]</ref>. The optimizer of the proposed method is SGD, with a learning rate 5e-3 and a weight decay 1e-4. The models are trained with 1? schedule for 12 epoch, and we decay the lr with the ratio 0.1, in the 8-th and 11-th epoch. The input frames are resized to 640?360 following previous works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Hyper-parameters. There exists some hyper-parameters in our proposed framework, i.e., bi-directional contrastive learning loss ? b , and temporal consistency loss ? c . In this paper, we set ? b = 0.2 and ? c = 10 in default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>Here we compare our method with two-stage <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12]</ref>, single-stage <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref>, 3D convolution-based <ref type="bibr" target="#b0">[1]</ref>, feature aggregation-based <ref type="bibr" target="#b22">[23]</ref>, and transformer-based <ref type="bibr" target="#b43">[44]</ref> methods. For some differences in the training settings (e.g., resolution, training epochs) vary from different methods, we strictly follow MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, <ref type="table">Table 2</ref>. Comparisons with some recent VIS methods on the YouTube-VIS-2021 val set. We use ResNet-50 backbone and 1? schedule for all experiments for fair comparison.</p><p>Methods AP AP50 AP75 AR1 AR10 SipMask <ref type="bibr" target="#b4">[5]</ref> 28. <ref type="bibr" target="#b5">6</ref>    <ref type="bibr" target="#b43">[44]</ref>, we also argue that the performance of MaskProp <ref type="bibr" target="#b1">[2]</ref> relies heavily on stacking multiple networks, e.g., Spatio-temporal Sampling Network <ref type="bibr" target="#b2">[3]</ref> and Hybrid Task Cascade Network <ref type="bibr" target="#b6">[7]</ref>, not to mention the larger resolution and more training epochs. Our model also beats the recently proposed CompFeat <ref type="bibr" target="#b11">[12]</ref> by 1.4 % in AP with a significant improvement on the performance of speed. Meanwhile, it outperforms STEm-Seg <ref type="bibr" target="#b0">[1]</ref> and VisTR <ref type="bibr" target="#b43">[44]</ref> with the same backbone on the accuracy, which indicates the superiority of our method. Note that VisTR utilizes multi-scale training and takes a week on 8 NVIDIA Tesla V100 for training. Furthermore, compared with the single-stage methods SipMask <ref type="bibr" target="#b4">[5]</ref> and TraDeS <ref type="bibr" target="#b45">[46]</ref>, our method obtains about 4.2 % and 4.1 % improvement in AP, respectively. Compared with the feature aggregation-based method STMask <ref type="bibr" target="#b22">[23]</ref> which uses multi-frames to obtain more robust features, our method surpasses it by 3.2 % in AP for ResNet-50 backbone, and even it uses a stronger ResNet-50-DCN backbone. When compared with recent work CrossVIS <ref type="bibr" target="#b51">[52]</ref>, our method still shows the superiority of the performance on both performance and speed.</p><p>As shown in <ref type="table">Table 1</ref>, we also compare the FPS (frames per second) with other state-of-the-art methods. Our method achieves 36.7% AP at a 40.3 FPS, which is the best tradeoff for the single model. In addition, our method can run an online mode which is crucial for practical usages.</p><p>YouTube-VIS-2021. We evaluate the recently proposed MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, SipMask <ref type="bibr" target="#b4">[5]</ref> and CrossVIS <ref type="bibr" target="#b51">[52]</ref> on YouTube-VIS-2021 using the official implementation for comparison. As shown in <ref type="table">Table 2</ref>, our method surpasses MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref> and CrossVIS <ref type="bibr" target="#b51">[52]</ref> by 3.8 % and 1.3 % in AP , which verifies the effectiveness of our method. <ref type="table" target="#tab_4">Table 3</ref> we can observe that all methods meet a large performance degradation due to the complexity and occlusions in OVIS-2021 dataset. Our method achieves the best 15.5% AP, surpassing all methods under the same experimental conditions. We hope that our proposed method can serve as a strong baseline for this challenging benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OVIS-2021. From</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>We conduct experiments on the YouTube-VIS-2019 validation set with the ResNet-50 backbone and 1? schedule for the ablation studies.</p><p>Analysis for Each Component. As shown in <ref type="table">Table 4</ref>, we first use CondInst <ref type="bibr" target="#b36">[37]</ref> to obtain the instance masks instead of utilizing RoIAlign and mask head in MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, which achieves 3.4 % in AP improvements (33.7% vs. 30.3%). Besides the performance improvement, this component also changes the two-stage model to a simple single-stage and fully convolutional one with faster speed. Note that our temporal consistency constraint for the kernel generator successfully gains 0.7 % in AP by digging deeper into the temporal information in the video sequence. For the instances association across frames, we conduct experiments to verify the effectiveness of two components ("Contrastive" and "Bi-direction"). Specifically, when only using spatio-temporal contrastive learning module, we could achieve 1.9% in AP improvement. When using the bi-directional contrastive learning strategy, we finally obtain 36.7% in AP, surpassing "Contrastive" baseline by 0.4% in AP, demonstrating the effectiveness of the bi-directional learning strategy.</p><p>Kernel Generator. Kernel generator from CondInst <ref type="bibr" target="#b36">[37]</ref> plays a critical role in our method. Thus, we conduct ablation studies to show the impact of parameters in kernel generator head. As presented in <ref type="table">Table 6</ref>, with the number of convolutions in kernel generator head increasing, the performance improves  steadily and achieves the peak 36.7% AP with three stacked convolutions. Temporal consistency obtains 0.5% in AP, which demonstrates the effectiveness.</p><p>Mask Branch. To enhance the expressiveness of the mask feature, we further explore the channel number and relative coordinate map ("Coord.") used in the mask branch. As illustrated in <ref type="table">Table 7</ref>, the 8-channel mask feature achieve 36.2% AP without the coordinate map, and extra channels cannot improve the performance. We set the channel number of the mask feature to 8 by default as a result. Relative coordinates are attached to the mask feature for better performance (about 0.5% in AP improvement).</p><p>Tracking Embedding. The tracking embedding is crucial for VIS since AP relies heavily on the accuracy of instance association. We compare with different tracking embedding dimensions. As shown in <ref type="table">Table 5</ref>, AP improves as the embedding dimension increases. However, we can not afford the complexity cost when the embedding dimension is larger than 256 considering the speed. Thus, we set the embedding dimension as 256 by default.</p><p>Negative Sampling. The designed contrastive learning strategy aims to obtain representative and discriminative tracking embeddings. Thus, we further explore different numbers of negative embeddings and how they are selected in <ref type="table" target="#tab_6">Table 8</ref>. "Inbox" means we randomly select the negative embeddings within boxes from negative locations according to label assignment results. We find that choosing 128 negative embeddings is a good balance of total training time and accuracy. Moreover, randomly selecting negative embeddings from the whole feature map of the reference frame is much better than "Inbox". This observation verifies that the model can learn more discriminative representations from background stuff or objects. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Visualizations</head><p>Instance Embedding. To verify the effectiveness of the proposed method qualitatively, we visualize the instance embeddings of the same video sequence using t-SNE <ref type="bibr" target="#b28">[29]</ref>, which is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. Comparing with <ref type="figure" target="#fig_1">Figure 3(a)</ref>, the instance embeddings of <ref type="figure" target="#fig_1">Figure 3</ref>(b) is more separable, which indicates that our proposed STC module helps to distinguish different instances in the embedding space. Thus, compared with the original multi-class classification loss <ref type="bibr" target="#b49">[50]</ref>, we could obtain more accurate instance association accuracy for video instance segmentation task.</p><p>Video Visualization. The visualization of the proposed method on the YouTube-VIS-2019 validation dataset is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Compared with baseline method MaskTrack R-CNN <ref type="bibr" target="#b49">[50]</ref>, as shown in the first row and the second row, STC achieve more accurate segmentation results. From the last two rows, STC could achieve more coherent tracking results compared with MaskTrack R-CNN baseline, which demonstrates the effectiveness of the proposed spatio-temporal contrastive learning strategy. In conclusion, our method can segment and associate instances better with more accurate boundary results in challenging situations while MaskTrack R-CNN suffers from the missing instances or identity mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we introduced a effective architecture for video instance segmentation. Our model is conceptually simple without requiring RoIAlign operation or 3D convolutions. Moreover, it achieves state-of-the-art single-model results (i.e., ResNet-50 backbone) on the YouTube-VIS-2019, YouTube-VIS-2021, and OVIS-2021 datasets in a fully convolutional fashion. We hope our work could serve an strong baseline, which could inspire designing more efficient framework and rethinking the embeddings loss for challenging video instance segmentation task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Visualizations of instance embeddings without or with bi-directional contrastive learning module using t-SNE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of our proposed method and MaskTrack R-CNN on the YouTube-VIS-2019 val set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? ? means transformer on top of ResNet-50 or ResNet-101.</figDesc><table><row><cell>Method</cell><cell>Publication</cell><cell>Augmentations</cell><cell>Backbone FPS AP AP50 AP75 AR1 AR10</cell></row><row><cell cols="2">MaskTrack R-CNN [50] ICCV'19</cell><cell>?</cell><cell>ResNet-50 33 30.3 51.1 32.6 31.0 35.5</cell></row><row><cell>SipMask [5]</cell><cell>ECCV'20</cell><cell>?</cell><cell>ResNet-50 34 32.5 53.0 33.3 33.5 38.9</cell></row><row><cell>STEm-Seg [1]</cell><cell>ECCV'20</cell><cell>?</cell><cell>ResNet-50 4.4 30.6 50.7 33.5 31.6 37.1</cell></row><row><cell>CompFeat [12]</cell><cell>AAAI'21</cell><cell>?</cell><cell>ResNet-50 &lt; 33 35.3 56.0 38.6 33.1 40.3</cell></row><row><cell>TraDeS [46]</cell><cell>CVPR'21</cell><cell>?</cell><cell>ResNet-50 26 32.6 52.6 32.8 29.1 36.6</cell></row><row><cell>QueryInst [11]</cell><cell>ICCV'21</cell><cell>?</cell><cell>ResNet-50 32 34.6 55.8 36.5 35.4 42.4</cell></row><row><cell>CrossVIS [52]</cell><cell>ICCV'21</cell><cell>?</cell><cell>ResNet-50 39.8 34.8 54.6 37.9 34.0 39.0</cell></row><row><cell>VisSTG [40]</cell><cell>ICCV'21</cell><cell>?</cell><cell>ResNet-50 22 35.2 55.7 38.0 33.6 38.5</cell></row><row><cell>PCAN [22]</cell><cell>NeurIPS'21</cell><cell>?</cell><cell>ResNet-50 -36.1 54.9 39.4 36.3 41.6</cell></row><row><cell>Ours (STC)</cell><cell>-</cell><cell>?</cell><cell>ResNet-50 40.3 36.7 57.2 38.6 36.9 44.5</cell></row><row><cell>STMask [23]</cell><cell cols="3">CVPR'21 DCN backbone [10] ResNet-50 29 33.5 52.1 36.9 31.1 39.2</cell></row><row><cell>SG-Net [27]</cell><cell cols="3">CVPR'21 multi-scale training ResNet-50 23 34.8 56.1 36.8 35.8 40.8</cell></row><row><cell>VisTR [44]</cell><cell cols="3">CVPR'21 random-crop training ResNet-50 30 35.6 56.8 37.0 35.2 40.2</cell></row><row><cell>QueryInst [11]</cell><cell>ICCV'21</cell><cell cols="2">multi-scale training ResNet-50 32 36.2 56.7 39.7 36.1 42.9</cell></row><row><cell>CrossVIS [52]</cell><cell>ICCV'21</cell><cell cols="2">multi-scale training ResNet-50 39.8 36.3 56.8 38.9 35.6 40.7</cell></row><row><cell>VisSTG [40]</cell><cell>ICCV'21</cell><cell cols="2">multi-scale training ResNet-50 22 36.5 58.6 39.0 35.5 40.8</cell></row><row><cell>Ours (STC)</cell><cell>-</cell><cell cols="2">multi-scale training ResNet-50 40.3 37.6 58.9 39.7 38.2 46.2</cell></row><row><cell cols="2">MaskTrack R-CNN [50] ICCV'19</cell><cell>?</cell><cell>ResNet-101 33 30.3 51.1 32.6 31.0 35.5</cell></row><row><cell>SRNet [53]</cell><cell>ACMMM'21</cell><cell>?</cell><cell>ResNet-101 35 32.3 50.2 34.8 32.3 40.1</cell></row><row><cell>STEm-Seg [1]</cell><cell>ECCV'20</cell><cell>?</cell><cell>ResNet-101 2.1 34.6 55.8 37.9 34.4 41.6</cell></row><row><cell>PCAN [22]</cell><cell>NeurIPS'21</cell><cell>?</cell><cell>ResNet-101 -37.6 57.2 41.3 37.2 43.9</cell></row><row><cell>Ours (STC)</cell><cell>-</cell><cell>?</cell><cell>ResNet-101 36.6 37.8 58.5 40.6 38.5 46.3</cell></row><row><cell>SipMask [5]</cell><cell cols="3">ECCV'20 multi-scale training ResNet-101 24 35.8 56.0 39.0 35.4 42.4</cell></row><row><cell>STMask [23]</cell><cell cols="3">CVPR'21 DCN backbone [10] ResNet-101 23 36.8 56.8 38.0 34.8 41.8</cell></row><row><cell>SG-Net [27]</cell><cell cols="3">CVPR'21 multi-scale training ResNet-101 20 36.3 57.1 39.6 35.9 43.0</cell></row><row><cell>VisTR [44]</cell><cell cols="3">CVPR'21 random-crop training ResNet-101 28 38.6 61.3 42.3 37.6 44.2</cell></row><row><cell>Ours (STC)</cell><cell>-</cell><cell cols="2">multi-scale training ResNet-101 36.6 39.2 61.5 42.4 39.7 47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>datasets. Following previous works<ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b51">52]</ref>, we evaluate our method on the validation sets of YouTube-VIS-2019, YouTube-VIS-2021 and OVIS-2021. YouTube-VIS-2019 dataset contains 40 class annotations, including many common objects. The official dataset consists of three subsets: 2238 training videos, 302 validation videos, and 343 test videos. YouTube-VIS-2021 dataset is an improved version of YouTube-VIS-2019 containing 40 class annotations. It collects more videos and high-quality annota-tions. This dataset also consists of three subsets: 2985 training videos, 421 validation videos, and 453 test videos. OVIS-2021 is a new large scale benchmark dataset for video instance segmentation task with 25 common semantic categories. It is designed with object occlusions in videos, which could reveal the complexity of real-world scenes. It consists of 607 training videos, 140 validation videos, and 154 testing videos as the official split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with some recent VIS methods on very challenging OVIS-2021 val set. We use ResNet-50 backbone and 1? schedule for all experiments for fair comparison.</figDesc><table><row><cell>Methods</cell><cell>AP AP50 AP75 AR1 AR10</cell></row><row><cell>SipMask [5]</cell><cell>10.3 25.4 7.8 7.9 15.8</cell></row><row><cell cols="2">MaskTrack R-CNN [50] 10.9 26.0 8.1 8.3 15.2</cell></row><row><cell>STEm-Seg [1]</cell><cell>13.8 32.1 11.9 9.1 20.0</cell></row><row><cell>CrossVIS [52]</cell><cell>14.9 32.7 12.1 10.3 19.8</cell></row><row><cell>Ours (STC)</cell><cell>15.5 33.5 13.4 11.0 20.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Ablation studies for each component of the proposed framework on YouTube-VIS-2019 validation set. Comparisons among different settings of the track embedding on the YouTube-VIS-2019 validation set.</figDesc><table><row><cell cols="5">Baseline Consistency Contrastive Bi-direction AP</cell><cell cols="4">Contrastive Bi-direction Embedding dim AP</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>33.7</cell><cell>?</cell><cell>?</cell><cell>256</cell><cell>34.5</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>34.4</cell><cell>?</cell><cell>?</cell><cell>256</cell><cell>36.2</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>36.3</cell><cell>?</cell><cell>?</cell><cell>256</cell><cell>35.4</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>36.7</cell><cell>?</cell><cell>?</cell><cell>256</cell><cell>36.7</cell></row><row><cell cols="5">Table 6. Comparisons among different</cell><cell cols="4">Table 7. Comparisons among differ-</cell></row><row><cell cols="5">settings of the kernel generator head on</cell><cell cols="4">ent settings of the mask branch on the</cell></row><row><cell cols="5">the YouTube-VIS-2019 validation set.</cell><cell cols="3">YouTube-VIS-2019 validation set.</cell><cell></cell></row><row><cell></cell><cell cols="3">Consistency # Conv AP</cell><cell></cell><cell cols="3">Coord. # Channel AP</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>1</cell><cell>31.5</cell><cell></cell><cell>?</cell><cell>1</cell><cell>28.7</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>2</cell><cell>33.7</cell><cell></cell><cell>?</cell><cell>4</cell><cell>33.6</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>3</cell><cell>36.2</cell><cell></cell><cell>?</cell><cell>8</cell><cell>36.2</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>4</cell><cell>36.0</cell><cell></cell><cell>?</cell><cell>16</cell><cell>36.1</cell><cell></cell></row><row><cell></cell><cell>?</cell><cell>3</cell><cell>36.7</cell><cell></cell><cell>?</cell><cell>8</cell><cell>36.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Comparisons among different settings of the negative sampling methods of contrastive learning on the YouTube-VIS-2019 validation set.</figDesc><table><row><cell>Inbox</cell><cell cols="2"># Negative</cell><cell>AP</cell></row><row><cell>?</cell><cell>0</cell><cell></cell><cell>34.9</cell></row><row><cell>?</cell><cell>64</cell><cell></cell><cell>36.5</cell></row><row><cell>?</cell><cell>128</cell><cell></cell><cell>36.7</cell></row><row><cell>?</cell><cell>256</cell><cell></cell><cell>36.4</cell></row><row><cell>?</cell><cell>128</cell><cell></cell><cell>35.2</cell></row><row><cell cols="2">w/o bi-directional contrastive learning</cell><cell cols="2">w. bi-directional contrastive learning</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatiotemporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="158" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9739" to="9748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="331" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Blendmask: Top-down meets bottom-up for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8573" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6910" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03400</idno>
		<title level="m">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6409" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video instance segmentation using interframe communication transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video object detection with locally-weighted deformable neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning where to focus for efficient video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01028</idno>
		<title level="m">Hard negative mixing for contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical crossattention networks for multiple object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial feature calibration and temporal fusion for effective one-stage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11215" to="11224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video instance segmentation with a proposereduce paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1739" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sg-net: Spatial granularity network for onestage video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9816" to="9825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rank &amp; sort loss for object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3009" to="3018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="164" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<title level="m">Occluded video instance segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end video instance segmentation via spatial-temporal graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10797" to="10806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="649" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10152</idno>
		<title level="m">Solov2: Dynamic and fast instance segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Centermask: single shot instance segmentation with point representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9313" to="9321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-toend video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8741" to="8750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Track to detect and segment: An online multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12352" to="12361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12193" to="12202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01342</idno>
		<title level="m">Loco: Local contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://youtube-vos.org/dataset/vis(2021" />
		<title level="m">Youtube-vis dataset 2021 version</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6499" to="6507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05970</idno>
		<title level="m">Crossover learning for fast online video instance segmentation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Srnet: Spatial relation network for efficient singlestage instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2653" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
