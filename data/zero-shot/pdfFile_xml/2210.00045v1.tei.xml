<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CALIBRATING SEQUENCE LIKELIHOOD IMPROVES CONDITIONAL LANGUAGE GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
							<email>yaozhaoyz@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Khalman</surname></persName>
							<email>khalman@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
							<email>rishabhjoshi@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<email>shashinarayan@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
							<email>msaleh@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
							<email>peterjliu@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CALIBRATING SEQUENCE LIKELIHOOD IMPROVES CONDITIONAL LANGUAGE GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conditional language models are predominantly trained with maximum likelihood estimation (MLE), giving probability mass to sparsely observed target sequences. While MLE trained models assign high probability to plausible sequences given the context, the model probabilities often do not accurately rank-order generated sequences by quality. This has been empirically observed in beam search decoding as output quality degrading with large beam sizes, and decoding strategies benefiting from heuristics such as length normalization and repetition-blocking. In this work, we introduce sequence likelihood calibration (SLiC) where the likelihood of model generated sequences are calibrated to better align with reference sequences in the model's latent space. With SLiC, decoding heuristics become unnecessary and decoding candidates' quality significantly improves regardless of the decoding method. Furthermore, SLiC shows no sign of diminishing returns with model scale, and presents alternative ways to improve quality with limited training and inference budgets. With SLiC, we exceed or match SOTA results on a wide range of generation tasks spanning abstractive summarization, question generation, abstractive question answering and data-to-text generation, even with modest-sized models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Conditional language generation aims to generate natural language text based on input context, and includes many useful and hard tasks such as abstractive summarization <ref type="bibr" target="#b26">(Mani, 2001;</ref><ref type="bibr" target="#b28">Nenkova and McKeown, 2011)</ref>, generative question answering <ref type="bibr" target="#b2">(Bajaj et al., 2016)</ref>, question generation <ref type="bibr" target="#b50">(Zhou et al., 2017)</ref> and data-to-text <ref type="bibr" target="#b44">(Wiseman et al., 2017;</ref><ref type="bibr" target="#b7">Gardent et al., 2017)</ref> tasks. Pretraining large Transformer encoder-decoder models and fine-tuning them on downstream tasks is the common paradigm to address these tasks <ref type="bibr" target="#b32">(Raffel et al., 2020;</ref><ref type="bibr" target="#b19">Lewis et al., 2019;</ref><ref type="bibr" target="#b41">Tay et al., 2022;</ref><ref type="bibr" target="#b48">Zhang et al., 2019a)</ref>.</p><p>Conditional language generation tasks are modeled by learning the probability of a target sequence y given a context sequence x. Since directly modeling sequence probability P (y|x) over all possible generated text sequences is intractable, the canonical solution is to auto-regressively factor the probability and share the parameters at all token prediction steps as P ? (y|x) = l t=0 P ? (y t |y 0 ...y t?1 , x), where l is the sequence length. These models are often trained with maximum likelihood estimation (MLE) over observed target sequences. The learning objective thus becomes L = N i ?log(P ? (y i |x i )) = N i l t=0 ?log(P ? (y t i |y 0 i ...y t?1 i , x i )), where N is the number of training instances. It is also referred to as next token prediction loss as it is mathematically equivalent.</p><p>In the ideal setting of MLE training, a large number of target sequences are observed for each context, and the relative frequencies of output sequences can calibrate the assigned model probabilities. However, in practice most language generation training datasets have only a single target sequence given the context. While the subsequent MLE trained models learn to assign relatively high probability to plausible sequences, they lack the direct supervision to compare such sequences, and solely rely on models' generalization capability. We refer to this phenomenon as models' sequence likelihood not being calibrated. Prior works <ref type="bibr" target="#b23">(Liu and Liu, 2021;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref> has shown that the correlation between sequence probability and its quality for MLE trained models can be low. <ref type="bibr" target="#b24">Liu et al. (2022)</ref> attributed this similarly as the deterministic (one-point) target distribution problem. Exposure bias <ref type="bibr" target="#b34">(Ranzato et al., 2016)</ref> further aggravates the problem, as sequence likelihood estimation is noisier when models' decoded sequences shift from exposed training data distribution.</p><p>Many effective heuristics have been proposed during training and decoding to combat the problem of uncalibrated sequence likelihood. Label smoothing <ref type="bibr" target="#b40">(Szegedy et al., 2016)</ref> prevents the network from becoming over-confident towards the observed target. This is particularly necessary in language generation, since the gold target represents just one of many possibilities. It has been observed that increasing number of decoding candidates past a certain point leads to worse quality for beam search decoding <ref type="bibr" target="#b47">(Yang et al., 2018;</ref><ref type="bibr" target="#b16">Koehn and Knowles, 2017)</ref> and sampling <ref type="bibr" target="#b0">(Adiwardana et al., 2020)</ref>. An optimal number of decoding candidates is often determined empirically by decoding models on the validation set and measuring their performance. Using length normalization is also essential for beam search decoding <ref type="bibr" target="#b46">(Wu et al., 2016)</ref> and sampling <ref type="bibr" target="#b0">(Adiwardana et al., 2020)</ref> as models tend to underestimate sequence likelihood of longer sentences. Repetition is another common failure mode when models overestimate the probability of repeated sequences <ref type="bibr" target="#b12">(Holtzman et al., 2019)</ref>. Trigram blocking <ref type="bibr" target="#b31">(Paulus et al., 2018)</ref> and nucleus sampling <ref type="bibr" target="#b11">(Holtzman et al., 2020)</ref> have been used to interrupt repeating sequences. These techniques are pervasive and often the default in modern Transformer libraries <ref type="bibr" target="#b45">(Wolf et al., 2020;</ref><ref type="bibr" target="#b19">Lewis et al., 2019;</ref><ref type="bibr" target="#b32">Raffel et al., 2020;</ref><ref type="bibr" target="#b48">Zhang et al., 2019a)</ref>.</p><p>Since the lack of observed target sequences in MLE training is the root problem, solutions involving learning with multiple sequence candidates have been proposed to directly address it. They can be loosely put in three categories: (1) reinforcement learning with sequence-level rewards <ref type="bibr" target="#b31">(Paulus et al., 2018;</ref><ref type="bibr" target="#b51">Ziegler et al., 2019;</ref><ref type="bibr" target="#b38">Stiennon et al., 2020)</ref>; (2) two-stage systems that generate and rerank candidates <ref type="bibr" target="#b23">(Liu and Liu, 2021;</ref><ref type="bibr" target="#b36">Ravaut et al., 2022b;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>; and (3) multi-task learning with sequence-level losses <ref type="bibr" target="#b6">(Edunov et al., 2018;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>. Refer to Related Works (section 4) for a more comprehensive discussion.</p><p>In this paper, we propose to first decode candidates from a fine-tuned model on its own training dataset, and then continue training the model with a new objective. The new objective aims to align candidates' sequence likelihoods according to their similarities to the target sequence in the model's latent space. We refer to this process as sequence likelihood calibration (SLiC). Our approach is related to multi-task learning with sequence-level losses in <ref type="bibr" target="#b24">Liu et al. (2022)</ref>. However, we propose a simple yet effective recipe that eliminates decoding heuristics and doesn't risk directly optimizing the same metrics that are used to report text generation quality. Unlike reinforcement learning, it is a one-time offline process that avoids costly online decoding processes. Also, when compared to two-stage reranking systems, it doesn't require a separate reranking model that incurs additional complexity and compute. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, our calibration stage naturally extends the current paradigm of pretraining and fine-tuning, and we show that calibrated models have strong improvements over fine-tuned-only models across model sizes.</p><p>Our main contributions include:</p><p>? Proposed a sequence likelihood calibration (SLiC) stage that consistently improves model quality, exceeding or matching state-of-the-art results on abstractive summarization, generative question answering, question generation and data-to-text generation tasks.</p><p>? Proposed a novel calibration similarity metric between model decodes and targets measured in the model's latent space rather than resorting to external metrics or human feedback.</p><p>? Demonstrated that SLiC eliminates the need for popular decoding heuristics, such as beam size optimization, length normalization and repetition prevention for the calibrated models.</p><p>? Demonstrated that SLiC has persistent significant benefits on model performance even as the number of model parameters scales up. Under the same inference budget, smaller calibrated models might outperform larger counterparts by decoding more candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CALIBRATING SEQUENCE LIKELIHOOD</head><p>We extend the common paradigm of pretraining and fine-tuning by introducing a third calibration stage, SLiC. As shown in Algorithm 1, we first decode m candidates {?} m from a fine-tuned model P ? f t (y|x) on fine-tuning dataset {x,?} n and then calibrate the fine-tuned model by continuing training on our proposed loss:</p><formula xml:id="formula_0">L(?) = b L cal (?, s; x,?, {?} m ) + ?L reg (?, ? f t ; x,?) ,</formula><p>where L cal and L reg are the calibration and regularization losses. s = s(?,?; x) measures the similarity between the candidate? and the target? conditioned on the context x. We discuss choices of s, L cal , L reg and decode strategies? ? P ? (y|x) in the following sections.</p><formula xml:id="formula_1">Algorithm 1 Calibrating Sequence Likelihood for x,? ? {x,?} n do sample m candidates from the fine-tuned model {? ? P ? f t (y|x)} m ? ? ? f t initialized from the fine-tuned model for {x,?, {?} m } b ? {x,?, {?} m } n do</formula><p>train with calibration and regularization loss ? ? ? ? lr? ? L(?)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SIMILARITY FUNCTION</head><p>For a given output sequence y, we take the decoder output hidden states e L?D = emb(y, x) as its representations, where L is the number of tokens and D is the hidden states dimension. Between a candidate?'s representations? and the target?'s representations?, we calculate their cosine similarities on spans of n tokens and aggregate them across the sequences with a F-measured based function F n . Notation of F n , P n , R n are same as in BERTScore <ref type="bibr" target="#b49">(Zhang et al., 2019b)</ref>.</p><formula xml:id="formula_2">s ? (?,?; x) = n F n (?,?) = n F n (emb(?, x), emb(?, x)) F n = 2 P n ? R n P n + R n P n (?,?) = 1 |?| ? i:i+n max ej:j+n? T i:i+n?j:j+n R n (?,?) = 1 |?| ? j:j+n max ei:i+n? T i:i+n?j:j+n</formula><p>Compared to BERTScore, we use our models' decoder output representations instead of BERT encoder representations and also consider matching on spans of n = 1, 2, 4, 8 tokens rather than 1.</p><p>Compared to using external metrics, such as ROUGE, BERTScore, this scoring function has a few advantages: (1) it adds very little compute cost, does not require extra model or out-of-graph computation;</p><p>(2) it differs from the metrics that we evaluate the generation systems with and mitigates the risk of directly optimizing towards those imperfect metrics <ref type="bibr" target="#b31">(Paulus et al., 2018;</ref><ref type="bibr" target="#b38">Stiennon et al., 2020)</ref>; (3) it is conditioned on the context s(?,?; x), as opposed to metrics in the form of s(?,?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CALIBRATION LOSS</head><p>The calibration loss L cal (?, s; x,?, {?} m ) aims to align models' decoded candidates' sequence likelihood P ? (?|x) according to their similarity with the target sequence s(?,?; x). Given the context x, target? and a set of candidates {?} m , we consider the following 4 loss types. Rank loss optimizes the ranking order of positive and negative candidates pairs? + ,? ? uniformly sampled from {?} m where s(? + ,?; x) &gt; s(? ? ,?; x). Margin loss maximizes the sequence probability gap of positive and negative candidates pairs. List-wise rank loss optimizes the ranking orders of a list of candidates, where i, j are positions of? i ,? j in the set {?} m sorted by s(?,?; x). It is the contrastive loss used in BRIO <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>. Expected reward loss (or expected minimum risk) maximizes the expected similarity of a list of candidates <ref type="bibr" target="#b6">(Edunov et al., 2018)</ref>.</p><formula xml:id="formula_3">L cal rank = max(0, ? ? log P ? (? + |x) + log P ? (? ? |x)) L cal margin = max(0, ?(s(? + ,?; x) ? s(? ? ,?; x)) ? log P ? (? + |x) + log P ? (? ? |x)) L cal list rank = ? i&lt;j max (0, ?|i ? j| ? log P ? (? i |x) + log P ? (? j |x)) L cal reward = ? i ?s(? i ,?; x) * P ? (? i |x) i P ? (? i |x)<label>(1)</label></formula><p>? values for all losses are chosen empirically for each loss type in subsection 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">REGULARIZATION LOSS</head><p>We consider two alternate types of regularization loss L reg to prevent models from deviating significantly from their fine-tuned MLE objective: Cross entropy is the standard fine-tuning MLE objective used in <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>. KL divergence directly minimizes the probability distribution distance between the calibrated model and the fine-tuned model at each token on observed target sequence. The regularization losses are both on token level.</p><formula xml:id="formula_4">L reg ce = t ? log P ? (? t |? t?1 , x) L reg kl = t P ? (? t |? t?1 , x) log P ? (? t |? t?1 , x) P ? f t (? t |? t?1 , x)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">CANDIDATES DECODING METHODS</head><p>We consider the following decoding methods for SLiC:</p><p>Beam Search is the standard best-first algorithm to solve the intractable maximum likelihood optimization for sequence-to-sequence models <ref type="bibr" target="#b42">(Tillmann and Ney, 2003;</ref><ref type="bibr" target="#b44">Wiseman et al., 2017;</ref><ref type="bibr" target="#b4">Chen et al., 2018)</ref>.</p><p>Diverse Beam Search (DBS; Vijayakumar et al., 2016) generates a list of diverse outputs by dividing the beam search budget into groups and enforcing dissimilarity between groups of beams. It strikes balance between quality and diversity and is often the best strategy for two-stage reranking systems <ref type="bibr" target="#b23">(Liu and Liu, 2021;</ref><ref type="bibr" target="#b36">Ravaut et al., 2022b;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>.</p><p>Nucleus Sampling <ref type="bibr" target="#b11">(Holtzman et al., 2020)</ref> only samples high-probable tokens within cumulative probability p at each step of the decoding. It produces diverse candidates while preventing sampling very low quality ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TASKS AND DATASETS</head><p>For abstractive summarization tasks, we choose CNN/DailyMail <ref type="bibr" target="#b10">(Hermann et al., 2015;</ref><ref type="bibr" target="#b37">See et al., 2017)</ref>, XSUM <ref type="bibr" target="#b27">(Narayan et al., 2018)</ref>, RedditTIFU-long <ref type="bibr" target="#b15">(Kim et al., 2019)</ref> and SAMSum <ref type="bibr" target="#b9">(Gliwa et al., 2019)</ref> due to their diversity in domain, style, abstractiveness, and summary lengths. For question answering related tasks, we choose generative question answering given context MSMARCO NLG <ref type="bibr" target="#b2">(Bajaj et al., 2016)</ref> and its reverse problem of question generation SQuAD QG <ref type="bibr" target="#b50">(Zhou et al., 2017;</ref><ref type="bibr" target="#b5">Du et al., 2017)</ref> . For data-to-text tasks, we choose text generation given structured data WebNLG-en <ref type="bibr" target="#b7">(Gardent et al., 2017)</ref> and common concepts reasoning CommonGen <ref type="bibr" target="#b21">(Lin et al., 2020)</ref>. More details of datasets can be found at Appendix A along with their statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODEL TRAINING AND EVALUATION DETAILS</head><p>We follow the PEGASUS pretraining <ref type="bibr" target="#b48">(Zhang et al., 2019a</ref>) and extend transformer model sizes to PEGASUS SMALL (50M), PEGASUS BASE (200M), PEGASUS LARGE (500M) and PEGASUS 2B (2B). 1 Different from the original paper, we use a sentencepiece 96k vocabulary with byte-fallback <ref type="bibr" target="#b17">(Kudo, 2018)</ref> and pretraining batch size of 4096 across all models. See Appendix B for model dimensions.</p><p>In all experiments, we use learning rate lr = 10 ?4 , and batch sizes of 512 to finetune and 64 to calibrate models. We use beam search to generate calibration candidates and evaluate the calibrated models, unless specified otherwise.</p><p>In our ablation studies (subsection 3.3), benefits analysis (subsection 3.4), and scaling experiments (subsection 3.5), we use models pretrained to 500,000 steps and conduct experiments on 4 datasets (CNN/DailyMail, XSUM, RedditTIFU-long and SAMSum). For ablation studies and benefits analysis, we use PEGASUS LARGE . We report ROUGE 1/2/L <ref type="bibr" target="#b22">(Lin, 2004</ref>) 2 for each dataset on validation splits and their overall score R m defined as geometric mean of ROUGE 1/2/L averaged across datasets,</p><formula xml:id="formula_5">R m = 1 4 d 3 ? R 1 R 2 R L .</formula><p>For the final results (subsection 3.6), we pretrain PEGASUS 2B model to 2.5M steps, fine-tune it on all 8 datasets, calibrate them using the same recipe and report numbers on the test split (unless specified otherwise). We use corresponding standard evaluation scripts for each dataset. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ABLATION STUDIES OF CALIBRATION</head><p>Ablation experiment results discussed below can be found in <ref type="table" target="#tab_0">Table 1</ref>. 2 Using pypi package rouge-score. We report rougeLsum for ROUGE-L. <ref type="bibr">3</ref> For summarization datasets, we use pypi package rouge-score. For SQuAD QG and MSMARCO NLG, we use the original evaluation scripts provided by <ref type="bibr" target="#b5">Du et al. (2017)</ref> and <ref type="bibr" target="#b2">Bajaj et al. (2016)</ref>, respectively. For WebNLG-en and CommonGen, we use the versions from the GEM benchmark <ref type="bibr" target="#b8">(Gehrmann et al., 2021)</ref> and report using the GEM evaluation framework. Those scripts mainly differ in text tokenization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Function</head><p>We compare our proposed similarity function, using models' latent states at decoder output representation s ? (?,?; x) (subsection 2.1), to directly optimizing the evaluation metric ROUGE. They perform similarly on all datasets even when evaluation metrics are ROUGE scores. We also test a variant of our similarity function by replacing decoder representation emb(y, x) with token embeddings. This variant has lower performance, which suggests benefits of contextualized and input-dependent representations.</p><p>Calibration Loss Calibrated models with all loss types significantly improve over fine-tuned-only models. Rank loss performs the best followed by margin, list rank and then reward. Reward maximization has the advantage of no hyper-parameters ? (Equation 1) to sweep while rank and margin loss have smaller training memory footprints. Rank loss showing the best gain indicates that relative ordering of candidates is more important than the absolute value of their similarity to the target.</p><p>Regularization Loss Cross entropy and KL divergence regularization perform similarly. About 85% of the calibration gain remains if regularization is removed.</p><p>Calibration Candidates Decoding Method We choose hyper-parameters for calibration candidates decoding methods based on validation set. The optimal decoding method is dataset dependent, however the differences between methods are small and the worst method achieves 90% of the gains of the best one. Beam search yields the highest average quality. This is opposite to the findings in the two-stage reranking systems <ref type="bibr" target="#b23">(Liu and Liu, 2021;</ref><ref type="bibr" target="#b36">Ravaut et al., 2022b;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>, where more diverse decoding strategies are preferred.</p><p>Checkpoint Selection for Fine-tuned Model We compare ROUGE-selected and perplexityselected checkpoints. The experiments show that starting calibration from the perplexity-selected checkpoint yields same or better performance with the biggest gap on CNN/DailyMail dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TL;DR:</head><p>We recommend a simple recipe: select the fine-tuned model's checkpoint by its validation set perplexity; decode candidates using beam search; calibrate the model with rank loss and KL divergence regularization. Calibrated models' quality monotonically improves as the number of decoding candidates increase, 4 regardless of the calibration-decoding and evaluation-decoding methods, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. On the other hand, fine-tuned-only models suffer from decreased quality when the number of decodes exceeds an optimal value. Once a model is calibrated with either decoding method, it performs well with both at evaluation time. Decoding with beam search yields higher scores, verified up to 20 decodes. When the calibration-decoding and the evaluation-decoding method align, the final quality is slightly better than the mismatched settings. CNN/DailyMail, XSUM, and SAMSum datasets work best with beam search, however RedditTIFU-long works better with nucleus sampling and decoding it with a larger number of candidates may achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BENEFITS</head><p>Calibrated models do not require length normalization. As shown in <ref type="table" target="#tab_1">Table 2</ref>, length normalization (commonly implemented as ? for beam search) is essential for fine-tuned-only models which bias towards longer sequences at decoding time. In contrast, length normalization has minimal effect on calibrated models.</p><p>Calibrated models suffer from far fewer repetitions. The repetition rate (rep%) measures a common mode of model failures. It is defined as the percentage of examples that contain any kind of consecutive repeated word n-grams, While length normalization helps general quality on the finetuned-only models, it leads to a side-effect of higher repetitions. Calibrated models, with or without length normalization, have a much lower repetition rate. When we compare with the repetition rate in the gold reference (repetition may occur naturally), calibrated models without length normalization have similar or lower repetition rate. TL;DR: Calibrated models do not require decoding heuristics such as beam size optimization, length normalization and repetition blocking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">SCALING PROPERTIES OF CALIBRATED MODELS</head><p>Scaling properties are important for projecting a technique's future relevance as models scale up <ref type="bibr">(Kaplan et al., 2020a)</ref>. In <ref type="figure" target="#fig_2">Figure 3</ref>, we compare generation quality versus inference compute at different model sizes and number of decoding candidates using beam search. Appendix F describes the method to estimate inference compute FLOPs. As mentioned earlier in subsection 3.4, fine-tuned-only models have optimal decoding beam sizes while calibrated models' performance monotonically increase with larger decoding beam sizes. Even in the case of greedy decoding (beam size of 1), the calibrated models' performance exceeds the fine-tuned-only models, by a large margin for some datasets (CNN/DailyMail and RedditTIFUlong). Their gaps grow larger with increasing number of beam sizes.</p><p>The magnitude of quality improvement from calibration persists over models sizes spanning from 50M to 2B. There is no obvious sign of diminishing return as model size scales up.</p><p>Inference compute may be used for decoding rather than on larger models. A calibrated model, once trained, can improve its performance by decoding more candidates, usually more effectively in the beginning, although returns diminish over 10 candidates. In some cases (SAMSum and especially CNN/DailyMail), a smaller model decoding more candidates can beat a larger one at both quality and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TL;DR:</head><p>Calibration benefits persist as model sizes scale up. Smaller calibrated models can outperform larger ones under the same inference compute budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">FINAL RESULTS</head><p>We calibrate the fine-tuned PEGASUS 2B models on 8 language generation tasks using the simple recipe identified in subsection 3.3 and evaluate them with beam search without decoding heuristics (subsection 3.4). The only hyper-parameter we optimize for SLiC is learning rate lr (Appendix H). We use beam size 5 for fine-tuned-only models and 10 for calibrated models.</p><p>As shown in <ref type="table" target="#tab_2">Table 3</ref>, calibrated models show consistent improvement over fine-tuned-only models across datasets and tasks. Overall, our calibrated models exceed or match the SOTA models on all datasets. On XSUM, SAMSum, WebNLG-en and CommonGen, our calibrated 2B models are ten to a hundred times smaller than the SOTA models.  <ref type="bibr" target="#b24">(Liu et al., 2022)</ref>, ULL b <ref type="bibr" target="#b41">(Tay et al., 2022)</ref>, ST-MoE c <ref type="bibr">(Zoph et al., 2022)</ref>, UniLMv2 d <ref type="bibr" target="#b3">(Bao et al., 2020)</ref>, Masque e <ref type="bibr" target="#b29">(Nishida et al., 2019)</ref>, and BART+R3F f <ref type="bibr" target="#b1">(Aghajanyan et al., 2021)</ref>  <ref type="bibr" target="#b38">Stiennon et al. (2020)</ref> collects human judgements on fine-tuned models' decodes to train a reward model that ranks candidates according to human preferences. The supervised policy is then fine-tuned against the reward model using PPO. The authors found that optimizing their reward model results in better quality summaries than directly optimizing ROUGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TWO-STAGE RERANKING APPROACHES</head><p>SimCLS <ref type="bibr" target="#b23">(Liu and Liu, 2021)</ref> proposes formulating text generation as a reference-free quality estimation problem assisted by contrastive learning. The first stage decodes candidates with diverse beam search and a RoBERTa based model is used to rank them in the second stage.</p><p>SummaReRanker <ref type="bibr" target="#b35">(Ravaut et al., 2022a)</ref> observes improved performance when training the generation and the reranking models on two non-overlapping halves of the fine-tuning data compared to training two models on the same data. <ref type="bibr" target="#b18">Lee et al. (2021)</ref> trains a discriminative reranker for neural machine translation that predicts the observed distribution of BLEU scores over the n-best list.</p><p>BRIO <ref type="bibr" target="#b24">(Liu et al., 2022)</ref> includes a two-stage reranking system that uses sequence-to-sequence generation models. It is shown that the sequence-to-sequence reranker has better performance than encoder-only models in providing ranking scores. <ref type="bibr" target="#b6">Edunov et al. (2018)</ref> surveys a range of classical objective functions for structured prediction and apply them to sequence-to-sequence models. Their experiments showed that combining sequencelevel objectives with token-level objectives yields improved performance on translation and summarization datasets. <ref type="bibr" target="#b39">Sun and Li (2021)</ref> combines contrastive learning objective with negative log-likelihood to decrease the likelihood of the model generated "silver" summaries meanwhile increasing the likelihood of the "gold" references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MULTI TASK LEARNING WITH SEQUENCE-LEVEL LOSS</head><p>BRIO <ref type="bibr" target="#b24">(Liu et al., 2022)</ref> demonstrates that multi task learning of sequence candidates with contrastive reranking and token-level generation has better performance compared to a two-stage reranking system. The ranking order is determined by similarity to target using external metrics (ROUGE, BERTScore). Models trained to rank by ROUGE also perform well measured on BERTScore and vice versa. <ref type="bibr" target="#b25">Lukasik et al. (2020)</ref> extends label smoothing from classification tasks to semantic label smoothing for sequence-to-sequence learning. Their technique adds sequence-level losses that smooth over well-formed relevant sequences that are similar to the target sequence semantically and on n-gram level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose adding a third stage of sequence likelihood calibration (SLiC) after the pretraining and fine-tuning stages for conditional language generation. The calibration process decodes candidates from the fine-tuned model, and continues training to align their sequence likelihood according to their similarity to the target sequence in the model's latent space. A simple yet effective recipe for SLiC is selecting the fine-tuned model's checkpoint by perplexity, decoding candidates with beam search, calibrating with rank loss and KL divergence regularization. We are able to eliminate all decoding heuristics for calibrated models. The benefits of calibration persist as models scale up in size. Smaller calibrated models might outperform larger ones under the same inference compute budget. By calibrating a PEGASUS 2B model, we exceed or match state-of-the-art results on 8 datasets spanning abstractive summarization, generative question answering, question generation and data-to-text tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank David Grangier for early and engaging discussions, and Noah Fiedel for feedback on the paper. SQuAD QG <ref type="bibr" target="#b50">(Zhou et al., 2017;</ref><ref type="bibr" target="#b5">Du et al., 2017)</ref> is the task of generating a question from a passageanswer pair extracted from the SQuAD dataset <ref type="bibr" target="#b33">(Rajpurkar et al., 2016)</ref>. In particular, we use the split of <ref type="bibr" target="#b5">Du et al. (2017)</ref>, consisting of 75,722, 10,570, and 11,877 examples for training, validation, and testing, respectively. 9</p><p>MSMARCO NLG <ref type="bibr" target="#b2">(Bajaj et al., 2016</ref>) is a large scale dataset focused on machine reading comprehension and question answering. The original QA dataset consists of 1,010,916 queries. However, we work on the NLGEN data that is a subset of the QA data consisting of 182,669 queries, each with a well formed answer. The task is to generate a well formed answer to an input query and a set of answering passages. 10</p><p>WebNLG-en <ref type="bibr" target="#b7">(Gardent et al., 2017)</ref> consists of 16,095 data inputs in the from of sets of RDF triples extracted from DBpedia. Each data point was verbalized by humans in more-than-one natural texts, leading to a total of 38,872 data-text pairs. 11</p><p>CommonGen <ref type="formula">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MODEL ARCHITECTURE</head><p>Model sizes and their configurations are reported in <ref type="table" target="#tab_6">Table 5</ref>.   <ref type="table" target="#tab_8">Table 7</ref>. At evaluation time, models are decoded with 1, 2, 5, 10 and 20 candidates. ROUGE numbers in <ref type="figure" target="#fig_1">Figure 2</ref> are reported in <ref type="table" target="#tab_9">Table 8</ref>.   <ref type="bibr">11/21.15/42.34 46.18/22.84/38.07 27.78/8.40/22.18 52.86/27.89/43.85 2 44.54/21.62/41.73 46.94/23.87/38.89 27.75/8.98/22.37 53.42/29.11/44.56 5 44.78/21.99/41.93 47.26/24.38/39.24 26.88/9.09/21.95 53.47/29.25/44.53 10 44.58/21.86/41.71 47.29/24.60/39.41 25.51/8.78/21.04 53.70/29.22/44.63 20 44.33/21.64/41.43 47.13/24.62/39.36 24.10/8.32/20.06</ref>   <ref type="table" target="#tab_11">Table 9</ref>. Brevity penalty ? is chosen as the best value for fine-tuned models' ROUGE performance on validation dataset or disabled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F MODEL FLOPS ESTIMATION</head><p>We extends formulations in <ref type="table" target="#tab_0">Table 1</ref> of <ref type="bibr" target="#b14">Kaplan et al. (2020b)</ref> to estimate FLOPs of our transformer encoder decoder models following the formula:</p><formula xml:id="formula_6">total C = C enc ? n enc?ctx + C dec ? n dec?ctx ? m C enc = 2N enc + 2n enc?layer n enc?ctx d enc?attn C dec = 2N dec + n dec?layer n dec?ctx d dec?attn<label>(3)</label></formula><p>where m is the number of decoder candidates, other notations can be referenced in <ref type="table" target="#tab_0">Table 1</ref> of <ref type="bibr" target="#b14">Kaplan et al. (2020b)</ref>. Because of upper triangle attention masking, the effective decoder attention context length is half of sequence lengths instead of full sequence lengths as in the encoder. Extra computation incurred by different decoding methods are omitted as they are much smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G SCALING</head><p>SLiC method for scaling curves are reported in <ref type="table" target="#tab_0">Table 10</ref>. At evaluation time, models are decoded with 1, 2, 5, 10, and maybe 15, 20 candidates. ROUGE numbers in <ref type="figure" target="#fig_2">Figure 3</ref> are reported in <ref type="table" target="#tab_0">Table 11</ref>.   <ref type="table" target="#tab_0">Table 12</ref>. We choose the SLiC best based on subsection 3.3. There are in total 3 hyper-parameters: learning rate lr (Algorithm 1), ranking constant ? (Equation 1), and regularization strength ? (Equation 2). We fix two of the them: ? is set to 10, and lr * ? is set to 1e ? 5. Best learning rate lr is determined with hyper-parameter tuning on validation set and reported in <ref type="table" target="#tab_0">Table 13</ref>.  10 ?5 10 ?5 10 ?5 10 ?6 MSMARCO NLG SQuAD QG WebNLG-en CommonGen lr 3 ? 10 ?6 10 ?5 10 ?6 10 ?5</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Calibrating sequence likelihood improves language generation across model scales. Scores are averaged ROUGE across 4 datasets (R m in subsection 3.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Effect of decoding methods on calibrated and fine-tuned only models. Colors indicate calibration method. Markers indicate evaluation decoding method. Hyper-parameters at Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Quality and inference compute trade-off comparison between fine-tuned only and calibrated models. Inference compute is scaled by increasing model parameters (different colors) and number of decoding candidates (dots on the same line). Hyper-parameters at Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b10">(Hermann et al., 2015;</ref><ref type="bibr" target="#b37">See et al., 2017)</ref> summarization dataset contains 313k articles from the CNN and Daily Mail newspapers with bullet point summaries. The summaries are on average 3-4 sentences and relatively extractive. 5 XSUM (Narayan et al., 2018) summarization dataset consists of 227k BBC articles from 2010 to 2017 with a single sentence highly abstractive summary. Sometimes the summary contains information not present in the article. 6 RedditTIFU-long (Kim et al., 2019) summarization dataset contains 42k posts of informal stories from sub-reddit TIFU from 2013-Jan to 2018-Mar with author written summaries. The style and length of the summaries are very diverse. 7SAMSum<ref type="bibr" target="#b9">(Gliwa et al., 2019)</ref> summarization dataset contains 16k high-quality chat-dialogues and their summaries written by linguists. 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>21.83/41.92 47.23/24.31/39.12 26.84/9.08/21.92 53.67/29.35/44.75  0.00% similarity function ROUGE 46.47/22.49/43.63 47.86/24.55/39.58 29.92/9.83/23.93 54.82/30.15/45.30 3.26% decoder repr 46.55/22.50/43.69 47.88/24.62/39.62 29.86/9.84/23.91 54.72/29.96/45.10 3.20% token emb 46.51/22.48/43.67 47.04/23.63/38.39 29.78/9.69/23.46 53.71/29.38/44.75 1.64% calibration loss rank 46.73/22.70/43.85 48.11/24.80/40.06 30.34/9.80/24.32 55.19/30.46/46.32 4.27% margin 46.11/22.46/43.30 47.62/24.81/39.89 30.84/9.97/24.37 54.58/30.10/45.92 3.63% list rank 46.62/22.88/43.76 47.93/24.57/39.67 30.87/9.65/24.46 54.56/29.81/45.17 3.49% reward 46.49/22.55/43.63 47.77/24.48/39.49 30.99/9.95/24.39 54.42/29.98/45.56 3.47% regularization loss none 46.54/22.44/43.68 47.51/24.70/39.82 30.73/9.68/24.05 55.07/30.07/45.60 3.48% cross entropy 46.73/22.70/43.85 48.11/24.80/40.06 29.96/9.72/23.82 55.19/30.46/46.32 4.06% KL divergence 46.80/22.83/43.98 47.96/24.92/40.09 30.73/9.68/24.05 54.87/30.20/45.95 4.09% candidates decoding method beam search 46.50/22.48/43.66 47.82/24.65/39.67 31.04/9.96/24.37 54.66/30.27/45.46 3.70% diverse beam 46.31/22.48/43.47 47.79/24.53/39.51 31.00/9.95/24.08 54.57/29.67/45.55 3.26% nucleus 46.45/22.46/43.54 47.67/24.50/39.47 31.09/10.01/24.31 54.61/30.04/45.63 3.51% calibration checkpoint selection ROUGE 46.66/22.66/43.84 48.03/24.78/39.79 30.94/9.98/24.43 54.63/30.03/45.79 3.96% perplexity 47.36/24.02/44.45 47.96/24.74/39.78 31.04/10.08/24.53 54.65/30.11/46.00 4.93%</figDesc><table><row><cell>Ablation</cell><cell>CNN/DailyMail</cell><cell>XSUM</cell><cell>RedditTIFU-long</cell><cell>SAMSum</cell><cell>?</cell></row><row><cell></cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>avg</cell></row><row><cell>fine-tuned</cell><cell>44.74/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Ablation of the sequence likelihood calibration method. Shared hyper-parameters are held as constant within each comparison group but vary between groups (Appendix C). ? is the relative improvements of overall score R m compared with the fine-tuned model.1 Approximated size, accurate sizes are reported in Appendix B.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison between fine-tuned only models and calibrated models with or w/o brevity penalty ? on overall quality (R1 / R2 / RL) and repetitions' occurrence percentage (rep%). Hyperparameters at Appendix E. 19.67/36.89 0.03 46.96/24.29/39.19 0.03 26.62/8.91/21.77 0.26 50.28/27.25/42.69 0.00 -5.15% 44.74/21.83/41.92 0.13 47.23/24.31/39.12 0.07 26.84/9.08/21.92 0.90 53.67/29.35/44.75 0.20 0.00% 46.44/22.38/43.57 0.02 47.57/24.42/39.46 0.03 30.99/9.95/24.39 0.03 54.42/29.98/45.56 0.00 3.31% 46.49/22.55/43.63 0.03 47.77/24.48/39.49 0.03 30.98/9.96/24.30 0.12 54.64/30.01/45.17 0.08 3.42%</figDesc><table><row><cell>SLiC ?</cell><cell cols="2">CNN/DailyMail</cell><cell>XSUM</cell><cell></cell><cell cols="2">RedditTIFU-long</cell><cell>SAMSum</cell><cell></cell><cell>?</cell></row><row><cell></cell><cell>R1 / R2 / RL</cell><cell>rep%</cell><cell>R1 / R2 / RL</cell><cell>rep%</cell><cell>R1 / R2 / RL</cell><cell>rep%</cell><cell>R1 / R2 / RL</cell><cell>rep%</cell><cell>avg</cell></row><row><cell>gold reference</cell><cell>-</cell><cell>0.03</cell><cell>-</cell><cell>0.01</cell><cell>-</cell><cell>0.09</cell><cell></cell><cell>0.05</cell></row><row><cell></cell><cell>39.37/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Calibrated PEGASUS 2B comparing with prior SOTA results: BRIO a</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. ? is on validation set. * is on unknown split. See hyper-parameters in Appendix H.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Prior SOTA</cell><cell cols="2">Our fine-tuned (2B) Our calibrated (2B)</cell></row><row><cell></cell><cell>#params</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell></row><row><cell>CNN/DailyMail</cell><cell>340M a</cell><cell>47.78/23.55/44.57</cell><cell>44.31/21.91/41.41</cell><cell>47.97/24.18/44.88</cell></row><row><cell>XSUM</cell><cell>268B c</cell><cell>-/27.1/-</cell><cell>49.57/26.77/41.41</cell><cell>49.77/27.09/42.08</cell></row><row><cell>RedditTIFU-long</cell><cell>340M f</cell><cell>30.31/10.98/24.74 *</cell><cell>28.73/10.12/23.24</cell><cell>32.03/11.13/25.51</cell></row><row><cell>SAMSum</cell><cell>20B b</cell><cell>-/29.60/-</cell><cell>53.64/29.21/44.83</cell><cell>54.37/29.88/45.89</cell></row><row><cell>SQuAD QG</cell><cell>110M d</cell><cell>-/-/52.13</cell><cell>-/-/52.59</cell><cell>-/-/53.28</cell></row><row><cell>MSMARCO NLG  ?</cell><cell>UNK e</cell><cell>-/-/69.77</cell><cell>-/-/70.73</cell><cell>-/-/71.06</cell></row><row><cell>WebNLG-en</cell><cell>20B b</cell><cell>-/55.40/-</cell><cell>76.96/52.97/62.56</cell><cell>78.09/55.52/65.06</cell></row><row><cell>CommonGen  ?</cell><cell>20B b</cell><cell>-/37.40/-</cell><cell>66.49/36.17/58.82</cell><cell>68.95/38.49/60.13</cell></row><row><cell cols="5">TL;DR: PEGASUS 2B achieves SOTA results on a wide range of language generation tasks using a</cell></row><row><cell cols="4">simple SLiC recipe while eliminating decoding heuristics.</cell><cell></cell></row><row><cell cols="2">4 RELATED WORKS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1 RL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>APPROACHES Paulus et al. (2018) directly optimizes evaluation metric ROUGE in RL fine-tuning stage. One issue is that ROUGE metric does not enforce fluency. The authors found summaries to be not always readable and proposed that using a mixed training objective works better. Ziegler et al. (2019);</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>dataset</cell><cell># of examples</cell><cell cols="2">avg words</cell><cell cols="2">extractiveness</cell></row><row><cell></cell><cell>train/val/test</cell><cell>input</cell><cell cols="3">target coverage density</cell></row><row><cell>CNN/DailyMail</cell><cell cols="3">287K / 13K / 11K 698.60 49.53</cell><cell>87.8%</cell><cell>3.77</cell></row><row><cell>XSUM</cell><cell cols="3">203K / 11K / 11K 383.17 21.74</cell><cell>63.9%</cell><cell>1.06</cell></row><row><cell>RedditTIFU-long</cell><cell>34K / 4K / 4K</cell><cell cols="2">396.15 21.02</cell><cell>68.4%</cell><cell>1.27</cell></row><row><cell>SAMSum</cell><cell>14,732 / 818 / 819</cell><cell>97.23</cell><cell>21.00</cell><cell>68.0%</cell><cell>1.46</cell></row><row><cell>SQuAD QG</cell><cell>76K / 11K / 12K</cell><cell cols="2">128.72 10.24</cell><cell>64.7%</cell><cell>1.63</cell></row><row><cell cols="4">MSMARCO NLG 152K / 12K / 12K 588.50 14.07</cell><cell>97.5%</cell><cell>7.78</cell></row><row><cell>WebNLG-en</cell><cell>35K / 1667 / 1779</cell><cell>17.50</cell><cell>20.51</cell><cell>48.7%</cell><cell>1.3</cell></row><row><cell>CommonGen</cell><cell>67K / 993 / 1497</cell><cell>3.27</cell><cell>10.10</cell><cell>22.0%</cell><cell>0.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Model sizes.</figDesc><table><row><cell>name</cell><cell cols="4">num layers hidden size num heads MLP size</cell><cell cols="2"># num params</cell></row><row><cell></cell><cell>enc/dec</cell><cell></cell><cell></cell><cell></cell><cell>excluding embs</cell><cell># total</cell></row><row><cell>PEGASUSSMALL</cell><cell>8/8</cell><cell>512</cell><cell>8</cell><cell>1024</cell><cell>49M</cell><cell>108M</cell></row><row><cell>PEGASUSBASE</cell><cell>12/12</cell><cell>768</cell><cell>12</cell><cell>3072</cell><cell>198M</cell><cell>272M</cell></row><row><cell>PEGASUSLARGE</cell><cell>16/16</cell><cell>1024</cell><cell>16</cell><cell>4096</cell><cell>470M</cell><cell>568M</cell></row><row><cell>PEGASUS2B</cell><cell>24/24</cell><cell>1024</cell><cell>16</cell><cell>16384</cell><cell>1913M</cell><cell>2012M</cell></row><row><cell cols="2">C ABLATION STUDY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SLiC methods for ablation study are reported in Table 6.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Experimental settings for ablation studies.</figDesc><table><row><cell>Ablation</cell><cell></cell><cell></cell><cell cols="2">calibration</cell><cell></cell><cell></cell><cell>evaluation</cell></row><row><cell></cell><cell>decoding</cell><cell>sim fn</cell><cell>loss</cell><cell>regularization</cell><cell>ckpt</cell><cell>extra</cell><cell>decoding</cell></row><row><cell>fine-tuned</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>beam 5</cell></row><row><cell>similarity function</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROUGE</cell><cell>beam 15</cell><cell>ROUGE</cell><cell cols="3">reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell>decoder repr</cell><cell>beam 15</cell><cell cols="4">s ? (y,?, x) reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell>token emb</cell><cell>beam 15</cell><cell cols="4">s tok (y,?) reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell>calibration loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>rank</cell><cell>beam 15</cell><cell>s ? (y,?, x)</cell><cell>rank</cell><cell cols="4">cross entropy ROUGE best lr, ? beam 5</cell></row><row><cell>margin</cell><cell>beam 15</cell><cell cols="6">s ? (y,?, x) margin cross entropy ROUGE best lr, ? beam 5</cell></row><row><cell>list rank</cell><cell>beam 15</cell><cell cols="6">s ? (y,?, x) list rank cross entropy ROUGE best lr, ? beam 5</cell></row><row><cell>reward</cell><cell>beam 15</cell><cell cols="6">s ? (y,?, x) reward cross entropy ROUGE best lr, ? beam 5</cell></row><row><cell>regularization loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>none</cell><cell>beam 15</cell><cell>s ? (y,?, x)</cell><cell>rank</cell><cell>-</cell><cell cols="2">ROUGE fix lr, ?</cell><cell>beam 5</cell></row><row><cell>cross entropy</cell><cell>beam 15</cell><cell>s ? (y,?, x)</cell><cell>rank</cell><cell cols="3">cross entropy ROUGE fix lr, ?</cell><cell>beam 5</cell></row><row><cell>KL divergence</cell><cell>beam 15</cell><cell>s ? (y,?, x)</cell><cell>rank</cell><cell cols="3">KL divergence ROUGE fix lr, ?</cell><cell>beam 5</cell></row><row><cell cols="2">calibration decoding method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>beam</cell><cell>beam 15</cell><cell cols="4">s ? (y,?, x) reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell>diverse beam</cell><cell cols="5">diverse beam 15 s ? (y,?, x) reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell>nucleus</cell><cell>nucleus 15</cell><cell cols="4">s ? (y,?, x) reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell cols="2">calibration checkpoint selection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROUGE</cell><cell>beam 15</cell><cell cols="4">s ? (y,?, x) reward cross entropy ROUGE</cell><cell>fix lr</cell><cell>beam 5</cell></row><row><cell>perplexity</cell><cell>beam 15</cell><cell cols="4">s ? (y,?, x) reward cross entropy perplexity</cell><cell>fix lr</cell><cell>beam 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Experimental settings for calibrated models' decoding analysis.</figDesc><table><row><cell>name</cell><cell></cell><cell></cell><cell>calibration</cell><cell></cell><cell></cell><cell>evaluation</cell></row><row><cell></cell><cell>decoding</cell><cell>sim fn</cell><cell>loss regularization</cell><cell>ckpt</cell><cell>extra</cell><cell>decoding</cell><cell>?</cell></row><row><cell>/ ? beam</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beam 1-20</cell></row><row><cell>/ ? nucleus</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nucleus 1-20</cell></row><row><cell>beam ? beam</cell><cell cols="6">beam 15 s ? (y,?, x) reward cross entropy ROUGE fix lr beam 1-20</cell></row><row><cell>beam ? nucleus</cell><cell cols="6">beam 15 s ? (y,?, x) reward cross entropy ROUGE fix lr nucleus 1-20</cell></row><row><cell>nucleus ? beam</cell><cell cols="6">nucleus 15 s ? (y,?, x) reward cross entropy ROUGE fix lr beam 1-20</cell></row><row><cell cols="7">nucleus ? nucleus nucleus 15 s ? (y,?, x) reward cross entropy ROUGE fix lr nucleus 1-20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>ROUGE (R1 / R2 / RL) numbers of the decoding curves.</figDesc><table><row><cell cols="2">SLiC ? decoding num</cell><cell>CNN/DailyMail</cell><cell>XSUM</cell><cell>RedditTIFU-long</cell><cell>SAMSum</cell></row><row><cell></cell><cell>decodes</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell></row><row><cell>/ ? beam</cell><cell>1</cell><cell>45.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Experimental settings for length normalization study.</figDesc><table><row><cell>SLiC ?</cell><cell></cell><cell>calibration</cell><cell></cell><cell></cell><cell>evaluation</cell></row><row><cell>decoding</cell><cell>sim fn</cell><cell>loss regularization</cell><cell>ckpt</cell><cell>extra</cell><cell>decoding ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beam 5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beam 5</cell></row><row><cell cols="5">beam 15 s ? (y,?, x) best cross entropy ROUGE best lr, ?</cell><cell>beam 5</cell></row><row><cell cols="5">beam 15 s ? (y,?, x) best cross entropy ROUGE best lr, ?</cell><cell>beam 5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Experimental settings for scaling.</figDesc><table><row><cell>model</cell><cell></cell><cell></cell><cell cols="2">calibration</cell><cell></cell><cell></cell><cell>evaluation</cell></row><row><cell></cell><cell>decoding</cell><cell>sim fn</cell><cell>loss</cell><cell>regularization</cell><cell>ckpt</cell><cell>extra</cell><cell>decoding</cell></row><row><cell>fine-tuned</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beam 1-20</cell></row><row><cell>calibrated</cell><cell cols="7">beam 15 s ? (y,?, x) reward cross entropy ROUGE best lr beam 1-20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>ROUGE (R1 / R2 / RL) numbers of the scaling curve.20.96/41.93 44.51/21.34/36.47 27.32/8.06/21.56 51.77/26.44/42.38  2  44.06/21.44/41.33 45.24/22.22/37.15 27.36/8.49/21.89 52.35/27.40/43.27  5  44.08/21.54/41.27 45.65/22.83/37.71 26.61/8.78/21.67 52.48/27.72/43.70  10  43.84/21.30/40.96 45.61/22.93/37.70 25.80/8.37/20.85 52.40/27.64/43./21.47/42.60 46.27/23.02/38.12 27.79/8.42/22.18  53.05/27.96/43.66 2 44.93/21.83/42.15 46.99/23.90/38.89 27.76/8.99/22.36 53.73/29.07/44.75 5 44.78/21.98/41.92 47.26/24.37/39.23 26.85/9.09/21.94 53.94/29.01/44.53 10 44.59/21.86/41.71 47.27/24.59/39.40 25.97/8.74/20.99 53.67/29.31/44./21.70/42.73 47.89/24.54/39.67 28.82/9.29/23.13 53.40/28.01/43.82 2 45.37/21.95/42.54 48.66/25.61/40.55 28.60/9.60/23.12 53.89/29.47/44.88 5 45.40/22.09/42.56 48.94/26.18/40.91 27.86/9.87/22.84 53.98/29.08/44.62 10 45.29/21.82/42.44 48.91/26.08/40.84 27.52/9.01/21.86 53.95/29.61/44./20.82/41.65 41.41/17.95/33.15 27.15/7.57/21.48 49.85/24.62/40.33 2 44.91/21.76/42.13 42.27/18.99/34.11 27.34/8.00/21.70 50.89/25.71/41.82 5 45.12/22.10/42.25 42.88/19.84/34.89 27.49/8.43/22.02 51.53/26.58/42.34 10 45.15/22.20/42.22 43.01/20.13/35.11 27.32/8.42/21.92 52.08/26.67/42./21.16/42.57 44.54/21.18/36.27 27.81/8.10/21.80 52.12/26.48/42.40 2 45.97/22.25/43.21 45.47/22.12/37.18 28.38/8.68/22.38 53.29/28.24/43.92 5 46.18/22.78/43.41 46.04/22.90/37.86 28.51/9.03/22.79 53.79/28.75/44.15 10 46.26/22.88/43.47 46.21/22.99/38.01 28.35/9.07/22.78 54.06/28.86/44./20.85/42.76 46.42/22.93/38.12 29.29/9.10/23.26 53.31/28.43/44.18 2 46.30/21.92/43.43 47.29/23.95/39.02 29.80/9.59/23.75 54.14/29.29/44.47 5 46.55/22.48/43.68 47.88/24.62/39.62 29.83/9.84/23.91 54.61/29.95/45.10 10 46.63/22.58/43.78 47.93/24.74/39.76 29.87/9.95/24.03 54.89/30.05/45./21.92/43.47 48.11/24.59/39.68 30.20/9.86/24.15 54.71/29.45/45.03 2 46.84/22.93/43.95 49.04/25.55/40.46 30.59/10.38/24.50 55.17/30.68/46.09 5 47.08/23.45/44.19 49.56/26.31/41.08 30.65/10.70/24.76 55.46/30.71/46.11 10 47.08/23.57/44.19 49.79/26.56/41.32 30.75/10.79/24.91 55.47/30.60/46.</figDesc><table><row><cell>size</cell><cell cols="2">decodes CNN/DailyMail</cell><cell>XSUM</cell><cell>RedditTIFU-long</cell><cell>SAMSum</cell></row><row><cell></cell><cell></cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell><cell>R1 / R2 / RL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fine-tuned</cell><cell></cell></row><row><cell cols="2">50M 1</cell><cell cols="4">43.21/19.99/40.53 40.91/17.80/32.98 25.37/6.99/20.19 49.78/24.45/40.67</cell></row><row><cell></cell><cell>2</cell><cell cols="4">42.77/20.40/39.94 41.55/18.78/33.75 25.22/7.53/20.34 50.52/25.37/41.80</cell></row><row><cell></cell><cell>5</cell><cell cols="4">42.92/20.45/39.96 41.87/19.44/34.28 24.41/7.61/20.00 50.52/25.92/42.00</cell></row><row><cell></cell><cell>10</cell><cell cols="4">42.78/20.32/39.75 41.85/19.57/34.38 23.04/7.43/19.04 50.41/25.84/41.81</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>41.79/19.59/34.31</cell><cell>-</cell><cell>50.46/25.89/41.77</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>41.65/19.56/34.25</cell><cell>-</cell><cell>50.50/26.00/41.45</cell></row><row><cell cols="2">200M 1</cell><cell cols="4">44.59/67</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>45.55/22.94/37.71</cell><cell>-</cell><cell>52.35/27.69/43.67</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>45.54/22.99/37.71</cell><cell>-</cell><cell>52.38/27.68/43.68</cell></row><row><cell cols="2">500M 1</cell><cell cols="4">45.3462</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>47.20/24.63/39.41</cell><cell>-</cell><cell>53.71/29.22/44.63</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>47.15/24.62/39.37</cell><cell>-</cell><cell>53.68/29.16/44.61</cell></row><row><cell>2B</cell><cell>1</cell><cell cols="4">45.5261</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>48.96/26.12/40.78</cell><cell>-</cell><cell>53.92/29.61/44.63</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>48.75/26.20/40.83</cell><cell>-</cell><cell>53.86/29.57/44.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell>calibrated</cell><cell></cell></row><row><cell cols="2">50M 1</cell><cell cols="4">44.3117</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>43.13/20.15/35.16</cell><cell>-</cell><cell>52.04/26.66/42.10</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>43.14/20.19/35.21</cell><cell>-</cell><cell>51.90/26.71/42.16</cell></row><row><cell cols="2">200M 1</cell><cell cols="4">45.2649</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>46.29/23.07/38.05</cell><cell>-</cell><cell>54.03/28.85/44.41</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>46.28/23.09/38.03</cell><cell>-</cell><cell>53.99/28.90/44.39</cell></row><row><cell cols="2">500M 1</cell><cell cols="4">45.5518</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>48.05/24.80/39.83</cell><cell>-</cell><cell>54.88/30.27/45.34</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>48.06/24.85/39.86</cell><cell>-</cell><cell>54.87/30.31/45.39</cell></row><row><cell>2B</cell><cell>1</cell><cell cols="4">46.2900</cell></row><row><cell></cell><cell>15</cell><cell>-</cell><cell>49.79/26.55/41.35</cell><cell>-</cell><cell>55.41/30.63/46.15</cell></row><row><cell></cell><cell>20</cell><cell>-</cell><cell>49.76/26.54/41.30</cell><cell>-</cell><cell>55.38/30.65/46.14</cell></row></table><note>SLiC method for final results is reported in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Experimental settings for length normalization study.</figDesc><table><row><cell>model</cell><cell></cell><cell></cell><cell cols="2">calibration</cell><cell></cell><cell></cell><cell>evaluation</cell></row><row><cell></cell><cell>decoding</cell><cell>sim fn</cell><cell>loss</cell><cell>regularization</cell><cell>ckpt</cell><cell>extra</cell><cell>decoding</cell></row><row><cell>fine-tuned</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>beam 5</cell></row><row><cell>calibrated</cell><cell cols="7">beam 15 s ? (y,?, x) rank KL divergence perplexity best lr beam 10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Learning rate of final results.</figDesc><table><row><cell>CNN/DailyMail</cell><cell>XSUM</cell><cell>RedditTIFU-long</cell><cell>SAMSum</cell></row><row><cell>lr</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">At evaluation-decoding time, the candidate with the highest sequence probability is selected to compute quality for both beam search and nucleus sampling.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better fine-tuning by reducing representational collapse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural networks as weighted language recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Maletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1205</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2261" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to ask: Neural question generation for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1123</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classical structured prediction losses for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The WebNLG challenge: Generating text from RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
		<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The GEM benchmark: Natural language generation, its evaluation and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuoluwapo</forename><surname>Aremu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miruna-Adriana</forename><surname>Khyathi Raghavi Chandu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustubh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyu</forename><surname>Dhole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Chinenye</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Garbacea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shailza</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounica</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khyati</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mille</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.gem-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Generation</title>
		<editor>Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo?o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou</editor>
		<meeting>the 1st Workshop on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="96" to="120" />
		</imprint>
	</monogr>
	<note>and Metrics (GEM 2021. Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Gliwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iwona</forename><surname>Mochol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Biesek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Wawer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5409</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Frontiers in Summarization</title>
		<meeting>the 2nd Workshop on New Frontiers in Summarization<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<title level="m">The curious case of neural text degeneration</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">and Dario Amodei. 2020a. Scaling laws for neural language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>abs/2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abstractive summarization of Reddit posts with multi-level memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2519" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3204</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10959</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative reranking for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.563</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7250" to="7264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1127</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CommonGen: A constrained text generation challenge for generative commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.165</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1823" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SimCLS: A simple framework for contrastive learning of abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1065" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BRIO: Bringing order to abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2890" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic label smoothing for sequence to sequence problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno>abs/2010.07447</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Benjamins Publishing</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Automatic summarization. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="103" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-style generative reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazutoshi</forename><surname>Shinoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1220</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th</title>
		<meeting>the 57th</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2273" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified textto-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">SummaReranker: A multi-task mixture-ofexperts re-ranking framework for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Mathieu Ravaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4504" to="4524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Mathieu Ravaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">F</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06569</idno>
		<title level="m">Summareranker: A multi-task mixture-ofexperts re-ranking framework for abstractive summarization</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to summarize with human feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisan</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul F</forename><surname>Christiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Alleviating exposure bias via contrastive learning for abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2108.11846</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.308</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unifying language learning paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaixiu</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metzler</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2205.05131</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Word reordering and a dynamic programming beam search algorithm for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120103321337458</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="133" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno>abs/1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2253" to="2263" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush; Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Breaking the beam search curse: A study of (re-)scoring methods and stopping criteria for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1342</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3054" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1912.08777</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Bertscore: Evaluating text generation with BERT. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno>abs/1904.09675</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Neural question generation from text: A preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1704.01792</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08906</idno>
		<title level="m">Noam Shazeer, and William Fedus. 2022. Designing effective sparse expert models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
