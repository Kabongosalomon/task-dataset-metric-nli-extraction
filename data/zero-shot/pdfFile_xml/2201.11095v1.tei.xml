<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-attention fusion for audiovisual emotion recognition with incomplete data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Chumachenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Sciences</orgName>
								<orgName type="institution">Tampere University</orgName>
								<address>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Iosifidis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Aarhus University</orgName>
								<address>
									<settlement>Aarhus</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moncef</forename><surname>Gabbouj</surname></persName>
							<email>moncef.gabbouj@tuni.fi</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Sciences</orgName>
								<orgName type="institution">Tampere University</orgName>
								<address>
									<settlement>Tampere</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-attention fusion for audiovisual emotion recognition with incomplete data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider the problem of multimodal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recognition of human emotional states is an important task within the field of machine learning, enabling better understanding of social signals in the wide range of applications ranging from robotics to human-computer interaction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Multiple approaches and emotion models have been proposed to date, ranging from the task of recognizing discrete emotional states, such as 'happy', 'angry', or 'sad', to the estimation of emotional attributes, such as arousal and valence on a continuous scale <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The task has been approached from multiple angles with different data types used as input, including text <ref type="bibr" target="#b4">[5]</ref>, speech <ref type="bibr" target="#b5">[6]</ref>, and images <ref type="bibr" target="#b6">[7]</ref>.</p><p>With the abundance of available data, a wide range of methods aiming to fully take advantage of this data are emerging, giving momentum to the development of multimodal methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Multi-modal methods are a class of methods that operate jointly on multiple data types. These include, among others, video data that consists of audio and visual modalities <ref type="bibr" target="#b10">[11]</ref>, joint RGB and depth images <ref type="bibr" target="#b11">[12]</ref>, and RGB and skeleton data <ref type="bibr" target="#b12">[13]</ref>. Methods operating on such multi-modal representations range from simple decision-level fusion approaches to more advanced joint feature learning approaches. Although fusion of intermediate features can potentially yield better performance due to joint learning of representations of multiple modalities, late or early fusion remain a popular choice in modern architectures due to their simplicity and versatility <ref type="bibr" target="#b7">[8]</ref>. On the other hand, early fusion is not suitable for fusion of drastically different data types, This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 871449 (OpenDR). while late fusion primarily only considers features learnt in each modality independently.</p><p>Most multi-modal methods developed to date assume full presence of all the adopted modalities at all times during inference, and only evaluate the performance of the models in such a setting. Nevertheless, in real-world applications it is often probable for one of the modalities to be missing or having poor quality at certain times, hence robustness of the model to such scenarios is an essential factor in building multimodal systems operating on real-world data.</p><p>In the task of multi-modal emotion recognition, especially in the recent transformer-based architectures, the trend has been largely in utilization of pre-extracted features that are subsequently fused with a learnt model, rather than creating end-to-end trainable models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. This limits the applicability of such methods in real-world scenarios, as necessary feature extraction is often challenging in unconstrained settings and introduces another point of uncertainty to the overall processing pipeline. This is especially the case for the methods adopting language information <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, as text transcriptions of audio signals are rarely available in practical applications and require separate estimation. We therefore primarily target the task of audiovisual emotion recognition that does not require separate feature learning.</p><p>In our work, we aim to address these limitations of existing multi-modal emotion recognition methods by building an endto-end model that does not require prior feature learning, and performing fusion at intermediate level, while being robust to incomplete or noisy data samples. Our contributions can be summarized as follows:</p><p>? We propose a new architecture for audiovisual emotion recognition from facial videos and speech which does not rely on separately learnt features and learns end-to-end from raw videos; ? We employ several modality fusion approaches and propose an attention-based intermediate feature fusion approach that softly attends to modality-independent features. To the best of our knowledge, such approach has not been proposed before; ? We propose a new training scheme based on modality dropout mechanisms aimed to improve the robustness of the model under incomplete or noisy data of one modality. We additionally find that the proposed approach arXiv:2201.11095v1 [cs.CV] 26 Jan 2022 yields better performance also in the standard case under presence of both modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Emotion recognition has received a significant amount of attention by the machine learning community, and a number of methods aiming to solve this task have been proposed to date. These methods operate on various data types, such as images <ref type="bibr" target="#b6">[7]</ref>, speech <ref type="bibr" target="#b5">[6]</ref>, text <ref type="bibr" target="#b4">[5]</ref>, or biosignals <ref type="bibr" target="#b16">[17]</ref>. At the same time, methods combining these modalities have been proposed as well <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref> employing different multi-modal fusion techniques.</p><p>Within the field of multimodal machine learning, generally, three classes of multi-modal fusion approaches are identified: early fusion, where the input data of multiple modalities are simply combined via concatenation, addition, or any other operation and further processed together; late fusion, where modalities are treated independently and their features or softmax classification scores are only combined in the very last layers; and intermediate feature fusion, where feature sharing is performed at middle layers of the network and hence the feature representations of different modalities are learnt jointly.</p><p>A notable set of approaches in multimodal fusion rely on utilization of self-attention <ref type="bibr" target="#b17">[18]</ref>. Recall that self-attention is formulated via calculating the dot-product similarity in the latent space, where queries q, keys k, and values v are learnt from input feature representation via learnable projection matrices W q , W k , W v , and an attented representation is calculated based on them:</p><formula xml:id="formula_0">A n = sof tmax qk T ? d v,<label>(1)</label></formula><p>where d is the dimensionality of a latent space. Considering the task of fusion of two modalities a and b, self-attention can be utilized as a fusion approach by calculating queries from modality a and keys and values from modality b. This results in representation learnt from modality a attending to corresponding modality b and further applying the obtained attention matrix to the representation learnt from modality b. This idea has been extensively utilized for solving a plethora of tasks involving multimodal fusion. In the context of emotion/affect recognition, notable works include <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b13">[14]</ref> the authors propose a multimodal transformer-based architecture for unaligned multimodal language sequences and consider fusion of three modalities, namely, audio, vision, and text. Data from each modality is first projected via a 1D convolutional layer to the desired dimensionality, and further, a set of transformer blocks is applied. Specifically, two transformer modules are utilized in each modality branch, with each of the two modules being responsible for fusion of one of the other modalities with the modality of the given branch following the above-specified approach. These representations are subsequently concatenated and another transformer module is applied in each modality branch on joint representations. The processed representations from each of three branches are subsequently concatenated for final classification.</p><p>Another relevant work is <ref type="bibr" target="#b15">[16]</ref> where audio and visual modalities are considered for the task of emotion recognition. There, each modality is first preprocessed with a separate transformer block and representations learnt from each modality are fused with a transformer in a manner similar to Eq. (1). Compared to previous work, only one modality is fused into the other one, rather than performing fusion in a pair-wise manner in separate branches.</p><p>Another work described in <ref type="bibr" target="#b14">[15]</ref> considers emotion recognition from speech and text modalities. Similarly, they first perform modality-specific feature learning by means of convolutional blocks, and further employ two cross-modal attention blocks with one fusion audio into text, and the other one performing fusion into opposite direction. The statistics is pooled from each branch and concatenated for prediction.</p><p>As can be noticed, all the aforementioned methods are rather similar in their fusion strategies in that the transformer fusion is the building block of each of them, and the differences between the architectures are rather nominal and datasetspecific. At the same time, it can be noted that the models focus on building multimodal fusion methods rather than end-to-end emotion recognition systems, and often employ features that require separate estimation, especially for the vision modality. For example, <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b15">[16]</ref> rely on facial action units as features, and <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b14">[15]</ref> utilize language modality which requires separate annotation or estimation in practical application.</p><p>It should also be noticed that all three mentioned methods perform fusion in early or intermediate stages in the pipeline, forcing joint representations to be learnt. While benefiting from the joint feature learning, such fusion can become a curse if the learnt fused representations are too co-dependent and one of the modalities is noisy, incomplete, or simply nonexisting during inference. Indeed, common practice has been to only evaluate the performance of the models under the ideal scenario of both modalities being present and complete at all times, while real-world applications do not necessarily reflect such scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODS</head><p>Here we describe the overall architecture of the proposed audiovisual emotion recognition model as well as three selfattention based modality fusion methods. Further, we propose an approach for accounting for missing data in one modality during inference in a form of modality dropout. On a general level, the model consists of two branches responsible for learning audio and visual features, respectively, and fusion modules placed either in the end or in the middle of the two branches depending on the feature fusion type, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In both audio and visual branches, the 1D Convolutional blocks that are applied in a temporal dimension are primarily used.</p><p>A. Feature extraction 1) Vision branch: The vision branch consists of two parts, with the first part being the visual feature extraction from individual video frames, followed by learning of joint representation for the whole video sequence. To achieve an endto-end trainable model capable of learning from raw video, we employ feature extraction as part of our pipeline and optimize it jointly with the multimodal fusion module, unlike the vast majority of existing works that separate feature extraction from multimodal fusion and mostly utilize pre-extracted features, such as facial landmark locations, facial action units, or head pose information <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>. We choose one of the recently proposed facial expression recognition architectures, namely, EfficientFace <ref type="bibr" target="#b6">[7]</ref> and incorporate it for feature extraction from individual frames prior to introducing them to subsequent 1D convolutional blocks. Specifically, the 1D convolutional blocks are added after average-pooled output of the last convolutional block of EfficientFace.</p><p>Considering an input video sequence of k frames, each of the k frames is processed independently by a 2D feature extractor, resulting in a single vector descriptor for each frame. These representations are further concatenated and processed in a temporal dimension with the temporal convolution blocks described further. We choose to follow this approach, as opposed to directly employing 3D-convolutions as commonly done in video tasks, as it provides a number of advantages in the given task, with the first one being the lower computational overhead brought by 2D convolutional layers compared to 3D convolutions. It can also be argued that temporal relations are of less importance in emotion recognition task, hence 1D convolutional operations applied in temporal dimension are sufficient to capture this information. Another major benefit of following the proposed approach is the ability to employ 2D feature extractor pre-trained on larger image-based emotion recognition datasets, as such datasets are significantly less common for videos that are necessary for pre-training 3Dconvolutional models.</p><p>Although we are primarily interested in building an end-toend pipeline that can learn from raw data, the first part, i.e., visual feature extraction can be decoupled from the model and any other features can be used instead as input to the  has been extracted from input visual data, where N denotes the temporal dimension, and d denotes the feature dimension. Here X v can be represented by any feature types, either deep features extracted using a pre-trained model, or other features commonly used for emotion recognition, such as facial action units or landmarks. We further apply a sequence of four convolutional blocks for learning a temporal representation. Each convolutional block consists of an 1D Convoluitonal layer with a 3 ? 3 kernel, Batch Normalization, and a ReLU activation. Further details can be seen in <ref type="table">Table 1</ref> that provides full details of vision branch, where k denotes the kernel size, d denotes the number of filters in a convolutional layer, and s denotes the stride. The convolutional blocks are grouped into two stages for multimodal fusion described further.</p><p>2) Audio branch: Similarly to the vision branch, the audio branch operates on a feature representation, whether precomputed or optimized jointly, and applies four blocks of 1D convolutional layers. Each block consists of a Convolutional layer, Batch Normalization, ReLU activation, and MaxPooling, with the specifications defined in <ref type="table">Table 1</ref>. For audio, we primarily use mel-frequency cepstral coefficients as features.</p><p>We observed no benefit in using other feature representation types, such as chroma features or spectrograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Modality fusion approaches</head><p>In this section, we describe the considered fusion approaches. We will first describe the late transformer fusion approach that is similar to previous works described in the literature, and then describe the two proposed intermediate fusion approaches.</p><p>1) Late transformer fusion: In this setup, features learnt from two branches are fused with a transformer block. Specifically, we employ two transformers at the outputs of each branch, where fusion of one modality is performed into the other one. The outputs of these transformer blocks are further concatenated and passed to the final prediction layer. Formally, this can be defined as follows.</p><p>Let ? a and ? v be the feature representations of audio and vision modalities after the second feature extraction stage, i.e., after the fourth convolutional block. A transformer block is added in each branch, taking representations of two modalities as inputs. Considering the audio branch as an example, the transformer block takes the vision branch representation ? v as input and projects it to obtain keys and values, while queries are computed from the audio branch features ? a . That is, selfattention is calculated as</p><formula xml:id="formula_1">A = sof tmax ? a W q W T k ? T v ? d ? v W v ,<label>(2)</label></formula><p>followed by standard transformer block processing <ref type="bibr" target="#b17">[18]</ref>. The specific architecture of the transformer block is outlined in <ref type="figure" target="#fig_1">Figure 2</ref>. The outputs of the two transformer blocks are concatenated and passed to the final layer for prediciton.</p><p>2) Intermediate transformer fusion: We propose the utilization of similar to the above-described transformer blocks for fusion at intermediate feature layers. Specifically, fusion is performed with a transformer block in each branch after the first stage of feature extraction, i.e., after two convolutional layers. Similar architecture to the one described in <ref type="figure" target="#fig_0">Figure 1</ref> is used, and the fused feature representation is added to the corresponding branch.</p><p>Since data from complementary modality is introduced already at the earlier stage of the architecture, this allows to learn the features that are jointly meaningful for the task at hand between modalities during later convolutional layers.</p><p>3) Intermediate attention-based fusion: We further propose a fusion approach that is based merely on dot-product similarity that constitutes the attention in the transformer block. Formally, this is defined as follows. Given the two feature representations of different modalities ? a and ? b , we compute queries and keys with learnt weights, similarly to conventional attention. The scaled dot-product similarity is subsequently calculated as</p><formula xml:id="formula_2">A = sof tmax ? a W q W T k ? T v ? d .<label>(3)</label></formula><p>Softmax activation promotes competition in the attention matrix, hence highlighting more important attributes/timestamps of each modality, and as a result providing the importance score of each key with respect to each query, i.e., each representation of modality a with respect to modality b. This allows to calculate the relative importance of each attribute of modality a by aggregating the scores corresponding to all the attributes of modality b for each attribute of modality a. As a result, we obtain an attention vector that can be used to highlight more relevant attributes of the modality a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Considering the dot-product attention between features of audio and vision modalities shown in Equation 3, attention vector of vision modality is given by</head><formula xml:id="formula_3">v v = i=Nv A[:, i].</formula><p>Note that such a fusion approach does not directly fuse features of the two modalities. Instead, it identifies the attributes within each modality that are most relevant based to their similarity scores with data of the other modality. As a result, features that agree between the two modalities contribute the most to the final prediction, hence guiding the model towards learning modality-agnostic features or features with high level of agreement between the modalities. Such approach enables sharing of information between modalities, while not enforcing strong co-dependency of the learnt features in different branches as only attention scores are used for fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modality dropout</head><p>The vast majority of the multimodal learning methods described to date assume the presence of both modalities at all times during inference. Nevertheless, oftentimes in realworld applications data of one or more modalities might not be reliable or may be missing at times. In such scenarios, conventional multi-modal approaches tend to fail. Here, we aim to account for the potential cases of missing data and propose the modality dropout as a way of mitigating it. As will be shown further, utilization of this approach leads to improved performance also in situations where both modalities are present.</p><p>We propose the modality dropout, which randomly masks out or attenuates data of one of the modalities during training. Specifically, we consider three variants. In the first variant, during training, data of one modality in each sample is randomly selected and replaced with zeros, while the representation of the other modality for a given sample is kept intact. This approach imitates missing data and can also act as a regularizer similarly to Dropout layer utilized in neural networks. Note that in the case of the third fusion approach and absence of bias terms, this results in zero dot similarity matrix in the attention block, which after softmax and summation leads to constant attention vector, hence no information transfer from the zeroed modality.</p><p>In the second variant, for each pair of data samples we generate a random scaling factor ? in the range [0,1] <ref type="bibr" target="#b18">[19]</ref> and multiply one of the modalities by ?, while the other with 1 ? ?. The goal of this approach is to attenuate signals from different modalities at different training steps, and hence prevent the model from learning from strictly one modality. We further refer to this approach as 'soft' modality dropout. The third variant is aimed at the problem of noisy data, where the input signal of one modality is corrupted. Here, the masking is performed similarly to the first variant, except rather than zero-masking, the data is randomly generated from a normal distribution with zero mean and unit variance in one of the modalities for each sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section we describe the experimental protocol and the data used for assessing the performance of the proposed approaches. We report the results of the proposed model with three fusion variants, as well as recent multimodal emotion recognition methods, namely MULT <ref type="bibr" target="#b13">[14]</ref> and multimodal transformer <ref type="bibr" target="#b15">[16]</ref>. Note that both methods report results on datasets consisting of pre-extracted features. In addition, <ref type="bibr" target="#b13">[14]</ref> considers three modalities, and <ref type="bibr" target="#b15">[16]</ref> does not provide details on specific hyperparameters of the architecture, making direct comparison infeasible. We therefore adopt our feature extraction and compare with competing works only in terms of the fusion approaches described in these works. Specifically, to compare with <ref type="bibr" target="#b15">[16]</ref> we employ a transformer block on top of our two convolutional branches that performs fusion either from audio to video, or in the opposite direction, and replace linear layers in the transformer block with 1D-convolutional ones. Regarding MULT <ref type="bibr" target="#b13">[14]</ref>, we want to compare with purely audiovisual model, so we remove the transformer blocks responsible for fusion from/to the language modality. This yields the architecture that is similar to our late transformer fusion, with additional single modality transformer blocks added in each branch. Other hyperparameters, such as latent space dimensionality, are kept identical between the methods for fair comparison. Similarly to the comparison with <ref type="bibr" target="#b13">[14]</ref>, we add our feature extraction blocks to the model. Unless otherwise specified, we use a single transformer block with single head everywhere to achieve a lightweight model. Naturally, better performance can be expected from adding additional blocks and parameters to the models. We used <ref type="bibr" target="#b19">[20]</ref> for transformer block implementation.</p><p>Subsequently, we perform experiments with modality dropout in two settings. In the first one, we target the problem of missing data from one modality and apply both soft and hard modality dropout during training. That is, in this setting, in each batch the data consists of the pairs of full data, pairs without audio modality, pairs without video modality, and pairs multiplied with random coefficients as described above. We report the performance in the presence of both modalities (denoted by 'AV'), as well as in the full absence of one modality (denoted by 'A' or 'V' for presence of only audio and video modalities, respectively). We additionally report the average metric over the three modality settings (denoted by 'M') to simplify the comparison between methods. In the second setting, we consider robustness towards noise and apply the third variant of modality dropout during training, and replace one of the modalities with random noise during testing.</p><p>1) RAVDESS dataset: We choose RAVDESS dataset <ref type="bibr" target="#b20">[21]</ref> primarily due to availability of raw data in this dataset, as opposed to others. The dataset consists of video recordings of 24 people speaking with different emotions and poses a task of classification of emotional states into 7 classes: calm, happy, sad, angry, fearful, surprise, and disgust. 60 video sequences were recorded for each actor, and we crop or zero-pad them to 3.6 seconds, which is the average sequence length. For audio processing, we extract 10 Mel-frequency cepstral coefficients for further processing. For visual data, we select 15 uniformly distributed frames from 3.6 second video, and crop the faces of actors using a face detection algorithm <ref type="bibr" target="#b21">[22]</ref>. Images are resized into 224x224 pixels. We train the model on raw 15frame videos. We transfer the weights of EfficientFace pretrained on AffectNet dataset <ref type="bibr" target="#b3">[4]</ref>. We split the data into training, validation and test sets ensuring that the identities of actors are not repeated across sets. Specifically, we used four actors for testing, four for validation, and 16 for training, and report the result averaged over five folds. The videos are scaled into [0,1] scale, and random horizontal flip and random rotation are used for data augmentation. All the models are trained for 100 epochs with SGD, learning rate of 0.04, momentum of 0.9, weight decay of 1e-3, and reduction of learning rate on plateau of 10 epochs.</p><p>2) CMU-MOSEI dataset: We additionally conduct experiments on CMU-MOSEI dataset. The dataset consists of 23,454 movie review video clips taken from YouTube and labeled by human annotators with a sentiment score in the range [-3..3]. Note that we only consider audio and visual modalities in our experiments. Since the dataset provides pre-extracted features rather than raw data (specifically, 35 facial action units are provided for vision modality and audio data is represented by mfccs, pitch tracking, glottal source and peak slope parameters resulting in 74 features), we omit the EfficientFace feature extraction in the vision branch and training is performed starting from convolutional blocks directly. We rely on the implementation of <ref type="bibr" target="#b13">[14]</ref> for the experimental protocol on CMU-MOSEI dataset and adopt the training hyperparameters described in therein.   refers to the fusion approaches described in <ref type="bibr" target="#b15">[16]</ref>, and 'MULT' refers to <ref type="bibr" target="#b13">[14]</ref>. We report categorical accuracy on RAVDESS dataset, and binary accuracy (positive vs negative sentiment) on MOSEI dataset, as well as Mean Average Error between the true and predicted sentiment scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results and Discussion</head><p>As can be seen, in the setting without any type of dropout, late transformer fusion achieves the best result on RAVDESS dataset, while intermediate attention fusion achieves the best result on MOSEI dataset on both the accuracy and MAE metrics. Note that intermediate attention fusion is also the most lightweight fusion approach compared to any of the methods using full transformer blocks. At the same time, performance under the presence of only one modality is extremely poor on RAVDESS dataset. On MOSEI dataset the performance drop is not drastic in the majority of cases, likely due to the dataset consisting of already pre-extracted features, and hence guaranteeing presence of meaningful independent features in each modality even in the absence of the other one.</p><p>Further, it can be seen that utilization of modality dropout improves the performance drastically under incomplete data of one modality. This is the case for all fusion methods, while intermediate attention fusion benefits from it the most. Besides, the performance under the presence of both modalities is improved as well, with the best result on RAVDESS achieved by intermediate attention fusion. This is also the best result on this dataset among all methods and dropout settings. Similar conclusions can be made on MOSEI dataset; utilization of modality dropout improves the performance in both singlemodality and two-modality case. Under the noisy setting, we still observe the intermediate attention fusion performing best on the average metric on RAVDESS.</p><p>To provide better comparison with state-of-the-art, we additionally compare with full MULT model (omitting the language modality), following the implementations provided by <ref type="bibr" target="#b13">[14]</ref> and using their convolutional layers, transformer block implementations and other hyperparameters. Since in their implementation several dense layers are added after the fusion and prior to the output layer, we add similar dense layers to our model for fair comparison. The results are provided in <ref type="table" target="#tab_1">Table  III</ref>. As can be seen, while MULT outperforms the proposed intermediate fusion approaches in the vanilla setting with both modalities, intermediate fusion handles missing modalities better, and especially under the presence of modality dropouts. The best overall performance is achieved by intermediate attention fusion with the first modality dropout variant.</p><p>As can be seen, in the majority of the cases the best performance is achieved by the proposed intermediate attention fusion combined with one of the proposed dropout approaches. As in this approach no hard feature sharing is performed, the learnt feature representations are less likely to be co-dependent and therefore can be disentangled more easily, hence leading to better robustness of the model in incomplete data settings. This, in turn, leads to better generalization capabilities of the model overall, leading to improved performance also under the setting of both modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed a model for audiovisual emotion recognition that learns end-to-end and an attention-based fusion method. We evaluated the robustness of different modality fusion approaches under the absence of, or noise present in, one of the modalities and proposed an approach to improve the model's robustness. Importantly, the proposed approach also improves the performance under the (ideal) standard setting where both modalities are present.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Multimodal data fusion approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Structure of the Transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Architecture of the visual and audio modules second part of the vision branch. That is, in the second part of the architecture, we assume that certain feature representation X N ?d v</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table II shows the results of the proposed approaches on the RAVDESS and MOSEI datasets. Here, 'LT1' and 'LT4' denote late transformer fusion with one and four heads, respectively, LT1 79.33 19.83 36.41 45.19 63.89 48.70 62.85 58.48 0.806 0.840 1.063 0.903 LT4 76.42 27.92 30.00 44.78 66.56 62.63 53.16 60.78 0.806 0.839 0.831 0.825 IT1 76.41 21.16 18.33 38.63 67.72 37.14 62.87 55.91 0.792 0.843 0.809 0.815 IT4 78.50 20.33 17.33 38.72 64.91 62.60 62.85 63.45 0.817 0.840 0.832 0.830 IA1 76.00 18.58 22.83 39.13 64.94 62.08 62.86 63.29 0.802 0.837 0.806 0.815 IA4 77.41 20.66 29.83 42.63 67.72 63.07 65.77 65.52 0.794 0.837 0.803 0.811 TAV 77.75 24.25 13.33 38.44 64.94 62.08 62.86 62.18 0.814 0.841 1.093 0.916 TVA 76.00 15.16 42.67 44.61 66.48 37.15 56.96 53.53 0.809 0.852 0.838 0.833 MLT 74.16 22.33 35.42 43.97 62.90 62.85 64.44 63.40 0.804 0.838 0.804 0.815 MODALITY DROPOUT LT1 79.08 59.16 72.66 70.30 67.11 63.62 0.629 64.54 0.802 0.829 0.801 0.811 LT4 79.25 53.00 70.92 67.72 64.47 53.71 64.91 61.03 0.814 0.837 0.819 0.824 IT1 77.33 48.41 73.75 66.50 62.80 62.85 63.09 62.91 0.804 0.831 0.803 0.813 IT4 78.91 44.33 74.92 66.05 67.01 64.30 63.12 64.81 0.796 0.826 0.797 0.806 IA1 81.58 58.08 72.83 70.83 67.19 64.52 64.91 65.54 0.795 0.816 0.798 0.803 IA4 79.58 57.16 71.83 69.52 63.48 62.74 63.18 63.13 0.807 0.820 0.808 0.812 TAV 76.58 54.83 13.33 48.24 65.32 63.84 62.85 64.01 0.811 0.832 0.839 0.828 TVA 74.42 44.91 69.58 62.97 67.61 63.98 60.95 64.18 0.793 0.819 0.798 0.803 MLT 78.50 53.58 70.66 67.58 63.87 62.85 63.37 63.36 0.806 0.836 0.835 0.826 MODALITY DROPOUT with NOISE LT1 77.08 53.16 68.50 66.246 65.57 64.03 64.94 64.94 0.809 0.826 0.806 0.813 LT4 80.33 54.33 73.00 69.22 64.08 63.31 62.85 62.85 0.813 0.827 0.813 0.818 IT1 76.75 53.75 71.58 67.36 68.16 65.98 63.53 63.53 0.799 0.821 0.804 0.808 IT4 76.08 54.50 71.00 67.19 67.83 63.56 64.22 64.22 0.801 0.826 0.802 0.809 IA1 78.25 58.25 71.66 69.38 62.76 63.89 63.18 63.27 0.804 0.819 0.805 0.809 IA4 78.41 55.75 68.58 67.58 63.51 64.08 62.54 63.37 0.805 0.820 0.808 0.811 TAV 75.83 56.25 12.83 48.30 66.81 65.68 65.60 66.03 0.810 0.820 0.811 0.813 TVA 73.66 41.25 71.41 62.10 66.23 63.18 64.58 64.66 0.804 0.831 0.806 0.813 MLT 77.41 54.16 66.33 65.96 64.52 62.74 63.51 63.59 0.805 0.830 0.805 0.811</figDesc><table><row><cell cols="3">RAVDESS. ACC</cell><cell cols="2">MOSEI. ACC</cell><cell></cell><cell cols="2">MOSEI. MAE</cell><cell></cell></row><row><cell>AV A</cell><cell>V</cell><cell>M</cell><cell>AV A</cell><cell>V</cell><cell>M</cell><cell>AV A</cell><cell>V</cell><cell>M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Performance of different fusion methods on RAVDESS and MOSEI.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MOSEI. ACC</cell><cell></cell><cell></cell><cell cols="2">MOSEI. MAE</cell></row><row><cell></cell><cell>AV</cell><cell>A</cell><cell>V</cell><cell>M</cell><cell>AV</cell><cell>A</cell><cell>V</cell><cell>M</cell></row><row><cell>IT1</cell><cell cols="8">64.66 38.80 63.12 55.53 0.821 0.857 0.803 0.827</cell></row><row><cell>IT4</cell><cell cols="8">65.90 37.23 62.85 55.32 0.805 0.845 1.932 1.194</cell></row><row><cell>IA1</cell><cell cols="8">64.09 62.85 63.42 63.45 0.799 0.838 0.807 0.815</cell></row><row><cell>IA4</cell><cell cols="8">64.74 37.28 61.28 54.43 0.803 0.842 0.808 0.818</cell></row><row><cell>MLT</cell><cell cols="8">67.66 56.90 60.73 61.76 0.787 0.838 0.836 0.821</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MODALITY DROPOUT</cell><cell></cell><cell></cell></row><row><cell>IT1</cell><cell cols="8">65.41 62.85 64.06 64.11 0.805 0.838 0.805 0.816</cell></row><row><cell>IT4</cell><cell cols="8">66.57 64.78 65.02 65.45 0.792 0.812 0.795 0.800</cell></row><row><cell>IA1</cell><cell cols="8">68.76 65.96 63.92 66.21 0.791 0.815 0.799 0.802</cell></row><row><cell>IA4</cell><cell cols="8">66.18 64.67 64.22 65.02 0.794 0.815 0.801 0.803</cell></row><row><cell>MLT</cell><cell cols="8">66.12 65.41 63.62 65.05 0.801 0.831 0.808 0.813</cell></row><row><cell></cell><cell></cell><cell cols="5">MODALITY DROPOUT with NOISE</cell><cell></cell></row><row><cell>IT1</cell><cell cols="8">64.72 54.07 66.34 61.71 0.798 0.839 0.797 0.812</cell></row><row><cell>IT4</cell><cell cols="8">64.69 64.33 61.83 63.61 0.801 0.826 0.799 0.808</cell></row><row><cell>IA1</cell><cell cols="8">67.25 64.96 64.74 65.65 0.794 0.813 0.799 0.802</cell></row><row><cell>IA4</cell><cell cols="8">63.40 63.23 62.85 63.16 0.806 0.820 0.806 0.811</cell></row><row><cell>MLT</cell><cell cols="8">66.18 64.19 64.39 64.92 0.790 0.813 0.791 0.798</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Comparison with MULT<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>and similarly 'IT' denotes intermediate transformer fusion,</cell></row><row><cell>'IA' denotes intermediate attention fusion, 'TAV' and 'TVA'</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A facial expression emotion recognition based human-robot interaction system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="668" to="676" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic social signal analysis: Facial expression recognition using difference convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="97" to="102" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human emotion recognition: Review of sensors and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dzedzickis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaklauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bucinskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">592</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep learning techniques: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Babar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alhussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="117" to="327" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust lightweight facial expression recognition network with label distribution training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3510" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speedup and multi-view extensions to subclass discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chumachenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raitoharju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">107660</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Audiovisual slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08740</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Urfalioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to ignore: rethinking attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laakom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chumachenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raitoharju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05684</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. Meeting</title>
		<meeting>the conference. Association for Computational Linguistics. Meeting</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">6558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using crossmodal attention and 1d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech, 2020</title>
		<imprint>
			<biblScope unit="page" from="4243" to="4247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal transformer fusion for continuous emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3507" to="3511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review of emotion recognition using physiological signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2074</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3926" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">196391</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-task cascaded convolutional networks (mtcnn) for face detection and facial landmark alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gradilla</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Acessado em</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
