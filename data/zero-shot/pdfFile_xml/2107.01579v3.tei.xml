<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Similarity-Aware Fusion Network for 3D Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqing</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Similarity-Aware Fusion Network for 3D Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a similarity-aware fusion network (SAFNet) to adaptively fuse 2D images and 3D point clouds for 3D semantic segmentation. Existing fusionbased methods achieve superior performances by integrating information from multiple modalities. However, they heavily rely on the projection-based correspondence between 2D pixels and 3D points and can only perform the information fusion in a fixed manner, so that their performances cannot be easily migrated to a more realistic scenario where the collected data often lack strict pair-wise features for prediction. To address this, we employ a late fusion strategy where we first learn the geometric and contextual similarities between the input and back-projected (from 2D pixels) point clouds and utilize them to guide the fusion of two modalities to further exploit complementary information. Specifically, we employ a geometric similarity module (GSM) to directly compare the spatial coordinate distributions of pair-wise 3D neighborhoods, and a contextual similarity module (CSM) to aggregate and compare spatial contextual information of corresponding central points. The two proposed modules can effectively measure how much image features can help predictions, enabling the network to adaptively adjust the contributions of two modalities to the final prediction of each point. Experimental results on ScanNetV2 [1] benchmark demonstrate that SAFNet outperforms existing state-of-the-art fusion-based approaches across various data integrity.</p><p>Linqing Zhao is with the School of Mathematics, and the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3D semantic segmentation aims at predicting point-level annotations of different semantic categories for a given point cloud. It is a fundamental and crucial component of visual perception systems where autonomous robots work in a complex real-world environment composed of moving objects. It serves as the first procedure for a variety of downstream applications such as autonomous mapping <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, robot grasping <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and robot navigation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Since a single sensor cannot capture all necessary information, multiple types of sensors are often equipped on one robot in order to achieve more accurate segmentation results, where multi-modal fusion methods are needed to increase the capacity to compensate for disadvantages and rectify wrong predictions from each single sensor <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>To learn a representation that takes the advantages of both modalities, existing works (e.g., 3DMV <ref type="bibr" target="#b12">[13]</ref>, UPB <ref type="bibr" target="#b13">[14]</ref>, <ref type="figure">Fig. 1</ref>. The upper part shows the comparison of (a) input point cloud, (b) back-projected point cloud and (c) an overlaid version in the same spatial range. We highlight two types of unsolved challenges within the green (local mismatching) and gray (varying density) ellipses. We downsample the point clouds uniformly at the same sample rate for a clear view. The lower part is a high-level overview of our approach, where we use a comparison process to compute the similarity between the input and back-projected point clouds from different perspectives and further employ a fusion strategy to adaptively adjust the impact of the two modalities on classification results.</p><p>MVPNet <ref type="bibr" target="#b14">[15]</ref>, and FusionAwareConv <ref type="bibr" target="#b15">[16]</ref>) have explored different ways to mine connections between point clouds and multi-view images, yet all of them extract the features of point clouds and multi-view images separately and fuse features in a fixed manner in a 3D space instead of the image plane. However, two unsolved issues (i.e., local mismatching &amp; varying density) limit the effectiveness of these methods in real scenes, as shown in the upper part of <ref type="figure">Fig. 1</ref>. Local mismatching is caused by the incoordination of multiple sensors and possible occlusions, making it hard for unmatched points to combine with suitable 2D appearance. Meanwhile, the overlap of multi-view images causes the density of back-projected point cloud to vary greatly in different spatial locations.</p><p>To address these, in this paper, we propose a similarityaware late fusion framework to adaptively fuse image and point clouds, as shown in the lower part of <ref type="figure">Fig. 1</ref>. Firstly, we extract image features using a CNN for image segmentation and point features through a point-based network, which are high-dimensional informative representations of 2D appearance and 3D geometry, respectively. Secondly, instead of directly concatenating features from two modalities, we propose to perform a comparison procedure between the input point cloud and back-projected point cloud (backprojected from multi-view images) to compute a similarity metric as the guidance of the following fusion process. We obtain this similarity metric by employing two modules to measure the per-point similarity of two point clouds from both the geometric and contextual perspective. In addition, we employ a channel-wise attention layer for each modality to re-weight intra-modality channels prior to fusion to further boost performance. At last, we adaptively combine the two types of features with the guidance of the aforementioned similarity metric.</p><p>The contributions of this paper can be summarized as:</p><p>? We propose a joint, end-to-end late fusion network aiming to infer 3D semantic segmentation from 3D point clouds and 2D images. To the best of our knowledge, this is the first 2D-3D fusion network able to handle various data integrity by tackling local mismatching and varying density. ? We propose two efficient similarity modules to measure the geometric and contextual similarity of two point sets, which serve as a guidance to fuse image features and point cloud features. Note that this idea can be embedded in any 2D-3D fusion method. ? Our SAFNet outperforms previous published fusionbased methods by at least 1.3% mIoU on ScanNetV2 benchmark across various data integrity, using the same backbone. Meanwhile, we provide an in-depth analysis of several ablation studies to demonstrate the effectiveness of our careful design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Currently, convolutional neural networks (CNN) based segmentation models have been the mainstream in 2D semantic segmentation. While in the case of 3D, methods can be categorized into point-based, multi-view and fusionbased methods. We give a brief review of these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Point-based Methods</head><p>PointNet <ref type="bibr" target="#b16">[17]</ref> is a notable landmark in the progress of point clouds processing <ref type="bibr" target="#b17">[18]</ref> by deep learning approach, which leverages shared MLPs to learn per-point features and a max-pooling layer to obtain global features. However, PointNet fails to capture local structures, which makes it not suitable to handle large-scale point sets. To address this, PointNet++ <ref type="bibr" target="#b18">[19]</ref> presented a hierarchical architecture to learn local features with increasing contextual scales and improved the segmentation performance effectively. KPConv <ref type="bibr" target="#b19">[20]</ref> defined a deformable convolution, making convolution kernel adaptable to local geometry and robust to varying densities. Recently, several methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> have been proposed from the perspective of sampling points and aggregating information. However, existing point-based approaches only focus on the 3D geometry while ignoring the 2D clues that are complementary information to infer 3D labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-view Methods</head><p>Multi-view CNNs are widely used in 3D reconstruction <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, 3D shape retrieval <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> which motivates several multi-view based works to deal with the 3D semantic segmentation problem. They usually predict labels in the 2D domain and then transfer them to 3D domain. For example, Hermans et al. <ref type="bibr" target="#b26">[27]</ref> classified RGB-D images using a Randomized Decision Forest and refined the results using a dense CRF before using a 2D-3D transfer approach to assign a class label to each 3D point. SemanticFusion <ref type="bibr" target="#b27">[28]</ref> used multi-view images to produce a SLAM map and a set of probability prediction maps, and then use a Bayesian update scheme to combine maps to obtain a final dense semantic map. Recently, TangentConv <ref type="bibr" target="#b28">[29]</ref> built a U-type network using tangent convolutions as the main block for dense 3D segmentation. Virtual-MVFusion <ref type="bibr" target="#b29">[30]</ref> rendered synthetic images from meshes to train a 2D CNN and fused pixel predictions into 3D point predictions. However, 2D information does not contain 3D geometry structure, which plays an important role in determining which label each point belongs to. Differently, our method leverages both 2D appearance and 3D geometry to jointly produce final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fusion-based Methods</head><p>Recently, 2D-3D fusion has been introduced in 3D segmentation task to enhance the robustness of systems and error handling. For example, 3DMV <ref type="bibr" target="#b12">[13]</ref> presented a joint late fusion architecture, where the features extracted from RGB images are back-projected into voxel volumes and combined with the features obtained from 3D branch to predict voxel labels. However, volumetric representation brings memory inefficiency and quantization error. UPB <ref type="bibr" target="#b13">[14]</ref> proposed a point-based early fusion architecture to learn 2D appearance, 3D structures and global context features from 3D meshes, which ensures all points to match with corresponding features from rendered images, avoiding the need for alignment. MVPNet <ref type="bibr" target="#b14">[15]</ref> presented an early fusion framework that propagates image features to points by lifting dense pixel features to dense points features and then aggregating them into sparse points features. This strategy works effectively to fuse complementary information and achieves better results. However, it doesn't fully exploit the relationship between the input and the back-projected point cloud, which can serve as the mediator between 2D appearance and 3D geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>Compared with early fusion <ref type="bibr" target="#b30">[31]</ref>, more fusion networks adopt late fusion structure in many tasks <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> due to the independence of feature extraction and convenience of interaction. However, the current best performing fusion method, MVPNet <ref type="bibr" target="#b14">[15]</ref> is based on early fusion and we believe a late fusion structure with appropriate multi-modal interactions can achieve better performance and robustness. Let H 2D (? 2D ), H 3D (? 3D ) be the 2D and 3D network with corresponding parameters ? 2D and ? 3D . For input 2D image  The overall network architecture of the our proposed framework can be divided into two branches (i.e., 3D branch and 2D branch) and a fusion header. For 3D branch, two neighborhoods centered on the same point pass through the geometric and contextual similarity module successively to obtain the enriched neighbor representation, as well as two similarities. Then, the mapped neighborhood representation by CSM is taken as attributes of points and fed into a point-based network to extract high-level point features. For 2D branch, we abstract image features via a fixed 2D encoder-decoder and back-project them into 3D canonical space. For fusion header, a high-efficiency strategy is proposed to combine features from two modalities under the guidance of geometric and contextual similarity. Finally, fused features are fed into fully connected layers to predict the final semantic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GSM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point-based Network</head><p>samples X 2D and 3D point samples X 3D with ground truth Y, we denote the features extracted from two modalities as</p><formula xml:id="formula_0">F 2D = H 2D (? 2D ; X 2D ) and F 3D = H 3D (? 3D ; X 3D ).</formula><p>For the traditional late fusion methods (e.g. 3DMV <ref type="bibr" target="#b12">[13]</ref>), the objective function can be written as:</p><formula xml:id="formula_1">L f usion = L cls (H F C ([F 2D , F 3D ]); Y),<label>(1)</label></formula><p>where L cls is the widely used cross-entropy loss in segmentation tasks. H F C represents a fully-connected network which maps the concatenation of F 2D and F 3D to the final prediction logits. In this paper, we propose a similarity-aware fusion network and the architecture is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, where a per-point similarity is presented to realize multi-modal interaction by adjusting the contributions of two modalities to the final prediction. The proposed objective function can be written as:</p><formula xml:id="formula_2">L f usion = L cls (H F C ([?(F 2D , S 2D?3D ), F 3D ]); Y), (2)</formula><p>where ? is a transformation operation to reweight 2D features using the similarity S 2D?3D , which is learned from the geometric and contextual differences between input points and unprojected points (back-projected from image pixels). Note that we only reweight 2D features here for two reasons. On one hand, it is equivalent to adjusting the weights of both modalities. On the other hand, compared with 2D features, 3D features are more reliable and crucial for 3D segmentation so that 3D weights are fixed to accelerate convergence. In the following sections, components of the network will be explained in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Feature Extraction and Back-projection</head><p>In recent years, convolutional neural networks (CNN) have made great progress in understanding RGB images. To introduce informative 2D textures, a deep CNN is chosen as the 2D backbone. Since we cannot accurately process all points of the entire scene with a single forward, we follow the sliding window strategy used in PointNet++ <ref type="bibr" target="#b18">[19]</ref>. Hence, we need to select a number of RGB-D frames to cover as many points in each window as possible, where the depth channel is only used for back-projection. After selecting views, we feed the images into a pretrained 2D encoder-decoder network to obtain high-level feature maps F 2D . Subsequently, pixels with features are back-projected into 3D space, utilizing depth values and camera matrixes, see <ref type="figure">Fig. 1</ref>. Accordingly, the back-projected points share the same coordinate system with the input points. It ensures that the pixel features of back-projected points can be gathered and combined with 3D geometric features from the corresponding input points in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Similarity Comparison Module</head><p>In order to assign well suited pixel features to each point, it's necessary to distinguish some subsets of input points that need more or less appearance features. We argue that the point-wise similarity between input point set and backprojected point set can serve as the guidance information for reasonable fusion with image features. To this end, we present two effective similarity metrics to measure how similar two point sets are from the perspective of geometry and context, respectively. In this section, we elaborate on these two similarity module. 1) Geometric Similarity Module: For clarity and simplicity, we let P = {p 1 , p 2 , . . . , p n } denote original point clouds and Q = {q 1 , q 2 , . . . , q m } denote point clouds formed by back-projection, where n and m are the amount of points in two point sets, respectively. Note that m is usually much greater than n, yet the vast majority of points in Q actually are redundant due to their long distance to any point in P . So we only consider the neighborhood N Q,k (i), which consists of point p i ? P and its k-nearest neighbors within a radius r in Q.</p><p>Generally, no matter which set points belong to, the points on the same spatial region should describe the same geometric shape. Hence, the low-level geometric representations of neighbor N P,k (i) and N Q,k (i), such as the distributions of 3D coordinates , should be highly similar, indicating a highly similar shape. However, the density, matching relation of points from two point sets can vary greatly across neighborhoods, which limits the effectiveness of shape comparison.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the bidirectional nearestneighbor distance mapping (BNNDM) between pairwise neighbourhoods is proposed to directly measure the difference on geometry structures quantitatively. The concept of BNNDM consists of three parts: forward search, backward search and distance mapping. For forward search, we force each point in N P,k to find its nearest point in Q and denote d F as the distance between them. Note that the found nearest point is not necessarily in the neighborhood in case of local mismatching. For backward search, each point in N Q,k is forced to find its nearest point in N P,k and denote d B as the distance. In the same way, we treat the case of mismatching specially by selecting the nearest point in Q as the starting point. For distance mapping, we map d F and d B into a geometric similarity score S G . Taking into account the negative correlation between distance and similarity, we consider to fit the relationship with negative exponential functions:</p><formula xml:id="formula_3">S P 2Q i = exp(? 1 ? 1 n i ni j=1 d F i ) + ? 1 , p j ? N P,k (i),<label>(3)</label></formula><formula xml:id="formula_4">S Q2P i = exp(? 1 ? 2 m i mi j=1 d B i ) + ? 2 , q j ? N Q,k (i),<label>(4)</label></formula><formula xml:id="formula_5">S Geo i = ? 3 S P 2Q i + ? 4 S Q2P i + ? 3 ,<label>(5)</label></formula><p>where ? and ? are numerical variables, which can be adjusted to the optimal by network during training. Note that we also tried to use one-layer fully connected network with non-linear activation as an alternative mapping function but no improvement was brought on experimental results. The basic idea is twofold: on one hand, the average d F can represent distance of nearest point from Q, which is the key to determine whether there is a mismatch. On the other hand, the average d B is sensitive to the excess shape of Q, remaining unaffected by density changes of Q at the same time. With such mapping function, geometric discrepancies, such as local mismatching and varying density, can be easily handled without redundancy learning parameters.</p><p>2) Contextual Similarity Module: Having obtained the geometric similarity of two point clouds, an intuitive idea is to regard it as the guidance of fusion. However, only low-level geometric similarity is insufficient to achieve the optimal fusion, since contextual clues are not fully utilized yet. To solve this, we present a contextual similarity module to compare high-level features of two point sets. Note that contextual features are no longer determined by only geometry, but also semantics.</p><p>Inspired by <ref type="bibr" target="#b34">[35]</ref>, we first enrich the point representation by augmenting additional neighborhood characters of each point. Since encoding the distribution in eight directions <ref type="bibr" target="#b35">[36]</ref> helps to encode spatial information better, an orientationencoding convolution layer followed by a shared multilayer perceptron (MLP) can act as the mapping function M to capture the local context. Therefore, for point p i , the Cdimension neighborhood characters f c (x i , x j ) and contextual feature f i can be presented as:</p><formula xml:id="formula_6">f c (x i , x j ) = [x i ? x j , x i ? x j 2 , ...]<label>(6)</label></formula><formula xml:id="formula_7">f i = pj ?N k (i) ?(w i,j [x i , f c (x i , x j )] + b i,j ),<label>(7)</label></formula><p>where w i,j ? R C?C and b i,j ? R C are the trainable parameters in M. ? is the non-linear activation function. The illustration is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Different from <ref type="bibr" target="#b34">[35]</ref>, our contextual information consists of pair-wise coordinate difference, Euclidean distance, etc. and will be fed into point-based network as affiliated attributes. Given the contextual features f P i and f Q i , we can easily conduct a cosine similarity function to measure how similar two neighborhoods are, which can be represented as:</p><formula xml:id="formula_8">S Con i = (f P i ) T f Q i f P i f Q i ,<label>(8)</label></formula><p>where T means vector transformation. Cosine similarity is extensively used in metric learning <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> due to the special property that resulting similarity is always within [-1,1]. With contextual similarity, a more in-depth comparison between two neighborhoods can be conducted through a lightweight design. Finally, we feed all 3D coordinates x ? P and their corresponding contextual feature f into a point-based network to extract F 3D , which can be represented as:</p><formula xml:id="formula_9">{x, f } P ?N et ?? F 3D<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Fusion Network</head><p>With the appearance and shape features (i.e. F 2D and F 3D ) of each point obtained from 2D and 3D branch, now we generate the fusion guidance through combining the geometric and contextual similarity calculated above (i.e. S Geo and S Con ). Since labels should be predicted for input point cloud, we first attach dense image features to sparse points, by aggregating three nearest image features , following the practice of <ref type="bibr" target="#b14">[15]</ref>. Here we directly take the product of two similarities as the final weights, which can be represented by:</p><formula xml:id="formula_10">S 2D?3D = S Geo S Con ,<label>(10)</label></formula><p>where represents the Hadamard product. To further improve the feature representation of specific semantics, we introduce the channel-wise attention <ref type="bibr" target="#b41">[42]</ref> for both branches prior to fusion in order to model the interdependence between channels for each modality.</p><p>Last but not the least, auxiliary supervision signals are indispensable for both modalities to accelerate convergence and make training process more stable. In addition to 2D and 3D direct supervision, an 2D back-projected semantic supervision is introduced to address 2D-3D inconsistent labels sometimes appear on the edges of objects. We achieve this by computing a cross entropy loss L 2Dunp. between 3D labels and nearest 2D back-projected features within a specific distance r . Therefore, the total objective is composed of a fusion loss and three auxiliary loss: <ref type="bibr" target="#b10">(11)</ref> where ? 1 , ? 2 and ? 3 are the coefficients of various loss components, and are set to 0.2, 0.8 and 0.8 in our implementation empirically. Here ? 1 is smaller because 2D CNN has been pretrained and only needs fine-tuning.</p><formula xml:id="formula_11">L toal = L f usion + ? 1 L 2D + ? 2 L 3D + ? 3 L 2Dunp. ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To evaluate the effectiveness of the proposed approach, we conducted various experiments on the challenging ScanNetV2 dataset <ref type="bibr" target="#b0">[1]</ref>. We performed an ablation study to analyze the contribution of each similarity module, and the robustness to various data integrity. The evaluation metric is the mean intersection-over-union metric (mIoU). IoU = T P T P +F P +F N , where T P , F P , and F N are the numbers of true positive, false positive, and false negative points, respectively. Our experiments were conducted on the following specifications: i7-4790K CPU, 32GB RAM, and NVIDIA GTX1080Ti GPU, using the PyTorch and Open3D <ref type="bibr" target="#b42">[43]</ref> toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ScanNetV2 Dataset</head><p>ScanNetV2 is a large-scale richly annotated RGBD dataset of 1513 indoor scenes, which are provided with the RGB-D video sequences (depth-color aligned) and reconstructed meshes. There are 20 different scene types for the ScanNet dataset, such as apartments, bathrooms, conference rooms, including 40 types of indoor common objects. In our implementation, 1201 scenes were used for training and 312 scenes for testing. Since the ground truth of the test set is not available, our ablation study is based on the results of validation set.</p><p>B. Implementation Details 1) Pre-processing: To prevent spending much memory on image features, we first resized the resolution of RGBD images to 160x120. Since the network cannot handle too many points at once, we cut each scene into chunks of 1.5x1.5x3 m 3 to keep the average points in a chunk are about 25,000, following the practice of <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Then, we backprojected image pixels utilizing camera intrinsic parameters, poses and depth maps to compute matching rates with all points of the corresponding scene. Finally, we selected a fixed number of frames for each chunk from consecutive video sequence to maximally cover points in the chunk with a greedy algorithm. The image that covers the most yet uncovered points is selected in every iteration.</p><p>2) Training 2D branch: We utilized the combination of ResNet34 <ref type="bibr" target="#b43">[44]</ref> and UNet <ref type="bibr" target="#b44">[45]</ref> to extract 64-dimensional features as 2D appearance. We first initialized the CNN with weights pretrained on ImageNet and then we trained the CNN on ScanNetV2 2D Semantic dataset for 50000 iteration with the SGD optimizer from a learning rate of 0.005. We set the batch size to 84 and applied random cropping as well as horizontal random mirror for data augmentation. After training, we cut down the fully connected layers for classification and left the convolutional layers to provide image features in the overall network. Note that during point cloud network and fusion header training, parameters of CNN were fixed to speed up model convergence.</p><p>3) Training 3D branch: For both geometric and contextual similarity modules, we set the max number of points in a neighborhood, K, to 64. After passing through GSM, point representations were enriched and reshaped from  <ref type="bibr">(192,</ref><ref type="bibr">192,</ref><ref type="bibr">192,</ref><ref type="bibr" target="#b19">20)</ref>, where there is a BN layer and ReLU after the first three layers and a dropout of 0.5 after the third layer to prevent overfitting. For the whole 3D branch, we randomly sampled 8192 points in a chunk and trained the network on ScanNetV2 3D Semantic dataset using Adam optimizer with a learning rate of 10 ?3 (divided by 10 every 40 epochs) for 150 epochs. 5) Testing: For testing, we follow the practice of <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The network predicts all the chunks with a stride of 0.5m in a sliding-window fashion. A majority vote is performed for the points located in multiple chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussion</head><p>1) Quantitative Results: We compared our method with recently proposed state-of-the-art methods on ScanNetV2 3D Semantic label benchmark. We employed the proposed SAFNet using 5 views with the chunk size of 1.5x1.5x3 m 3 . For fair comparisons, all results listed here are provided by the published papers.</p><p>TABLE I shows the quantitative comparisons with exising fusion-based methods on the mean IoU and 20 different classes. Our proposed method achieves the best mIoU among state-of-the-art fusion-based approaches and top two performance of 70% single class IoUs. In addition, our method achieves the smallest performance variance for 20 classes, which is an important basis for evaluating system stability. It's worth noting that our 2D (i.e. ResNet34)and 3D backbones (i.e. PointNet++) are the same as MVPNet's, but our mean IoU exceeds MVPNet by 1.3 % without ensemble, which demonstrates the effectiveness of our similarity-aware fusion strategy. With the help of GSM and CSM, we learn a better fusion strategy to make use of the complementary information of two modailities.</p><p>We also conducted a comparison with state-of-the-art multi-view and point-based methods in TABLE III. As expected, our method outperforms most single-modality methods with a large margin. The insight is that fusionbased approaches can exploit both 2D appearance and 3D geometry to achieve joint prediction, compared with singlemodality methods. Although Virtual-MVFusion achieved superior results, it must take raw meshes as input in order to render much more images than ours from virtual perspectives, so that it can avoid the serious problems of mismatching and varying densities. In conclusion, the results well prove the effectiveness of our SAFNet in processing complete data.</p><p>Obtaining well-matched 2D images and 3D point clouds in practice is very expensive, so the robustness to various data integrity reflects the practicality of the system. Besides the complete data, SAFNet achieved impressive results on incomplete data, such as cases of fewer images. We compared with fusion-based 3DMV <ref type="bibr" target="#b12">[13]</ref> and MVPNet <ref type="bibr" target="#b14">[15]</ref> with the same number of views for training in <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>. We also conducted a detailed comparison with MVPNet with various numbers of views for training in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>. For a fair comparison, we used the same training settings for all models and the same backbones as MVPNet. Note that UPB <ref type="bibr" target="#b13">[14]</ref> uses images rendered from meshes to keep every point matching with image pixels so that we don't compare with UPB here. Clearly, our models with different training settings consistently outperform their counterparts. As decreasing available views for testing, the gap between our method and the others grows wider. In addition, our performance curves in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref> are much smoother, which demonstrates that our Example of learned geometric similarity (S Geo ), contextual similarity (S Con ) and the combination (S 2D?3D ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MVPNet</head><p>Ours Ground Truth PointNet++ method is more robust to data incompleteness. Interestingly, the training strategy with 3 views available achieves the best performance across different testing cases, compared with the other two training strategies. We conjecture that 3 views can provide enough well-matched pixel-voxel pairs for training 2D and 3D deep networks as well as some unmatched cases for training GSM and CSM. On the contrary, too few wellmatched pairs (1 view) would result in under-fitting while too few unmatched cases (5 views) would limit the generalization ability.</p><p>2) Qualitative Results: <ref type="figure" target="#fig_4">Fig. 6</ref> shows an example of learned similarities, which can effectively guide the fusion of two modalities. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the segmentation results of our method and other two state-of-the-art methods on the validation set of ScanNetV2.</p><p>3) Discussion: Experimental results show that our method achieves very competitive results on both complete and incomplete data. We conclude the insights as follows:</p><p>? Our GSM and CSM can efficiently capture geometric and contextual information, which indicates matching condition, density changing, situation of occlusion and other properties while previous methods didn't take them into consideration or only consider the distance from a point to its k-nearest neighbors in point set Q, N Q,k . Unfortunately, for the case of mismatches, the points in N Q,k (i) are usually too far away from the center point p i , so that the image features they provide are mostly wrong. ? We can adaptively adjust the impact ratio of features from images and point cloud in different areas while previous works simply combine them in a fixed manner, which results in suboptimal predictions even in wellmatched areas. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>In this section, we conduct ablation studies on the validation set of ScanNetV2 to investigate the contribution of individual components in the SAFNet. In order to control variables, the number of input views was fixed to 3 for both training and testing. For clarity, we consider the baseline model: ResNet34-UNet (2D) + PointNet++ (3D) + FC (header) and we separately analyzed each component as follows:</p><p>1) Effect of Geometric similarity module: We split GSM into Forward Search (GSM-FS) and Backward Search (GSM-BS) to compare with baseline, as shown in TABLE II. We can summarize three points from the comparison. In the first place, the model with GSM-FS achieves 2% performance improvement while GSM-BS can boost 1.3% over the baseline, which demonstrates both parts of GSM can effectively capture 3D geometry and map it to instructive fusion weights. Second, Forward Search can provide more direct guidance to fusion since it's more sensitive to local mismatch. At last, the combination of two parts can further boost performance, which means both parts extracted some unique information.</p><p>2) Effect of Contextual similarity module: In the experiments, we compared the models with or without CSM. For the model without CSM, we directly feed the 3D coordinates into PointNet++ to extract highdimension features. The significant performance gap between two models proves the effectiveness of CSM. We argue that extracting compact neighborhood representation before feeding into deep networks can be helpful in many tasks, especially segmentation and detection.</p><p>3) Loss functions: We conducted two experiments to investigate the effectiveness of auxiliary supervision and 2D unprojected loss. As shown in TABLE II, the model with 2D and 3D auxiliary supervision achieves 1.4% gain, compared with only 3D supervision at the end of network and this strategy can provide constraints for intermediate features, making network training more stable. In addition, the small gain obtained from 2D unprojected loss demonstrates that the proposed unprojected loss can address 2D-3D inconsistent labels. It can prevent the network from getting confused by inconsistent labels. 4) Channel-wise attention: Applying channel-wise attention to the two branches at the same time can indirectly realize the interaction of the two modalities. Experimental results show that channel-wise attention provides 0.2% gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have proposed a new framework to fuse 2D images and 3D point clouds by computing image features and point features individually and then fusing them with the help of two learned similarities from the perspectives of geometry and context. Comprehensive experiments have been conducted on ScanNetV2 Semantice segmentation benchmark, and the experimental results demonstrate our superior performance as well as the robustness to various data integrity. This method can be used in autonomous mapping, robot grasping, and robot navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall network architecture of the our proposed framework can be divided into two branches (i.e., 3D branch and 2D branch) and a fusion header. For 3D branch, two neighborhoods centered on the same point pass through the geometric and contextual similarity module successively to obtain the enriched neighbor representation, as well as two similarities. Then, the mapped neighborhood representation by CSM is taken as attributes of points and fed into a point-based network to extract high-level point features. For 2D branch, we abstract image features via a fixed 2D encoder-decoder and back-project them into 3D canonical space. For fusion header, a high-efficiency strategy is proposed to combine features from two modalities under the guidance of geometric and contextual similarity. Finally, fused features are fed into fully connected layers to predict the final semantic labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of our bidirectional nearest-neighbor distance search between input (P ) and unprojected (Q) point clouds. The black curve represents a slice of a 3D surface on a 2D plane near which points from P and Q locate. Points A,B and C are from P , at the center of the corresponding neighborhood with different density. (a) Forward search: For each point in N P,k , the search target is its nearest point in Q, which may not be in the neighborhood, like case A and C; (b) Backward search: For each point in N Q,k , the search target is its nearest point in N P,k . If N Q,k is empty like case A, the nearest point in Q will be selected as the starting point. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of point feature enrichment and contextual similarity comparison module. Neighborhood characters are be concatenated with the coordinates of central points to enrich point representations. Then, a following mapping operation convert enriched point representations to contextual features, from which the contextual similarity can be computed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Comparison of robustness to the number of available views for testing with the (a) same and (b) different number of views for training. The format of the legends is method name(number of views for training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Quantitive results of 3D segmentation on the validation set of ScanNetV2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH EXISING FUSION-BASED METHODS ON SCANNETV2 BENCHMARK Method mIoU bath bed bkshf cab chair cntr curt desk door floor other pic fridge shower sink sofa table toilet wall window 3DMV [13] 48.4 48.4 53.8 64.3 42.4 60.6 31.0 57.4 43.3 37.8 79.6 30.1 21.4 53.7 20.8 47.2 50.7 41.3 69.3 60.2 53.9 UPB [14] 63.4 61.4 77.8 66.7 63.3 82.5 42.0 80.4 46.7 56.1 95.1 49.4 29.1 56.6 45.8 57.9 76.4 55.9 83.8 81.4 59.8 MVPNet [15] 64.1 83.1 71.5 67.1 59.0 78.1 39.4 67.9 64.2 55.3 93.7 46.2 25.6 64.9 40.6 62.6 69.1 66.6 87.7 79.2 60.8 FAConv [16] 63.0 60.4 74.1 76.6 59.0 74.7 50.1 73.4 50.3 52.7 91.9 45.4 32.3 55.0 42.0 67.8 68.8 54.4 89.6 79.5 62.7 Ours 65.4 75.2 73.4 66.4 58.3 81.5 39.9 75.4 63.9 53.5 94.2 47.0 30.9 66.5 53.9 65.0 70.8 63.5 85.7 79.3 64.2 * The bold and red values indicate the first-best and second-best results, respectively.</figDesc><table><row><cell></cell><cell cols="2">TABLE II</cell><cell>All results are from published papers.</cell></row><row><cell cols="4">COMPARISON WITH MULTI-VIEW AND POINT-BASED METHODS ON</cell></row><row><cell cols="3">SCANNETV2 BENCHMARK</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">mIoU Input Modality</cell><cell>Type</cell></row><row><cell>TangentConv [29]</cell><cell>40.9</cell><cell>point</cell><cell>multi-view</cell></row><row><cell cols="2">Virtual-MVFusion [30] 74.6</cell><cell>mesh</cell><cell>multi-view</cell></row><row><cell>PN++ [19]</cell><cell>33.9</cell><cell>point</cell><cell>point-based</cell></row><row><cell>PointSIFT [36]</cell><cell>41.5</cell><cell>point</cell><cell>point-based</cell></row><row><cell>PointCNN [39]</cell><cell>45.8</cell><cell>point</cell><cell>point-based</cell></row><row><cell>DPC [40]</cell><cell>59.2</cell><cell>point</cell><cell>point-based</cell></row><row><cell>RandLA-Net [41]</cell><cell>64.5</cell><cell>point</cell><cell>point-based</cell></row><row><cell cols="2">Ours (Res34&amp;PN++) 65.4</cell><cell>image+point</cell><cell>fusion</cell></row><row><cell cols="4">(B,C,N,K) to (B,K(C+4),N) and fed into an orientation-</cell></row><row><cell cols="4">encoding convolution and a three-layer-MLP of output</cell></row><row><cell cols="4">dimensions (64,64,64), where there is a BN layer and ReLU</cell></row><row><cell cols="4">after every MLP convolution layer. Another linear layer</cell></row><row><cell cols="4">with 64 dimensional output are used to compute contextual</cell></row><row><cell cols="4">features. For point-based network, we used PointNet++ with</cell></row><row><cell cols="4">single-scale grouping to get 128-dimensional 3D features.</cell></row><row><cell cols="4">We set the input dimension of PointNet++ to 67 (3 for xyz</cell></row><row><cell cols="2">and 64 for output of CSM).</cell><cell></cell><cell></cell></row><row><cell cols="4">4) Training fusion header: We set the output dimension</cell></row><row><cell>of FC layers to</cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDIES ON THE VALIDATION SET OF SCANNETV2</figDesc><table><row><cell>Component</cell><cell>Design Choice</cell></row><row><cell>Baseline</cell><cell></cell></row><row><cell>GSM-FS</cell><cell></cell></row><row><cell>GSM-BS</cell><cell></cell></row><row><cell>CSM</cell><cell></cell></row><row><cell>Auxiliary Sup</cell><cell></cell></row><row><cell>2D Unp. loss</cell><cell></cell></row><row><cell>Channel Att</cell><cell></cell></row><row><cell>mIoU(%)</cell><cell>61.5 63.5 62.8 64.2 66.5 67.9 68.3 68.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We thank Prof. Zhanjie Song for supporting me in life and learning. We also thank Liangliang Ren and Wenzhao Zheng for their valuable advice. This work was supported in part by the National <ref type="figure">Key</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When 2.5 d is not enough: Simultaneous reconstruction, segmentation and recognition on dense slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2295" to="2302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time rgb-d semantic keyframe slam based on image segmentation learning from industrial cad models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marraud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Comport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On-policy dataset synthesis for learning robot grasping policies using fully convolutional deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1357" to="1364" />
		</imprint>
	</monogr>
	<note>RA-L</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Road segmentation for all-day outdoor robot navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Motion and depth augmented semantic segmentation for autonomous navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rashed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">El</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhelw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic segmentation to develop an indoor navigation system for an autonomous mobile robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teso-Fz-Beto?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zulueta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?nchez-Chica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Fernandez-Gamiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saenz-Aguirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="page">855</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-task multisensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning joint 2d-3d representations for depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="452" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A unified pointbased framework for 3d segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fusion-aware point convolution for online semantic 3d scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4534" to="4543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">TPAMI</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5099" to="5108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11236</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointasnl: Robust point clouds processing using nonlocal neural networks with adaptive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5589" to="5598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pix2vox: Contextaware 3d reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2690" to="2698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust attentional aggregation of deep feature sets for multi-view 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="53" to="73" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Triplet-center loss for multi-view 3d object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense 3d semantic mapping of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Floros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2631" to="2638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semanticfusion: Dense 3d semantic mapping with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4628" to="4635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Virtual multi-view fusion for 3d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brewington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="518" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for semantic image segmentation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidib?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>M?riaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="page">104042</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Samplespecific late fusion for visual category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7469" to="7478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Late fusion via subspace search with consistency preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xuanyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mingkui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="page" from="518" to="528" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting local and global structure for point cloud semantic segmentation with contextual point representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="4571" to="4581" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pointsift: A sift-like network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cosine similarity metric learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="709" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep cosine metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="820" to="830" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dilated point convolutions: On the receptive field size of point convolutions on 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9463" to="9469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient semantic segmentation of largescale point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Open3D: A modern library for 3D data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09847</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
