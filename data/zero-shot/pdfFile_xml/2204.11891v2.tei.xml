<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProCST: Boosting Semantic Segmentation Using Progressive Cyclic Style-Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahaf</forename><surname>Ettedgui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Abu-Hussein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ProCST: Boosting Semantic Segmentation Using Progressive Cyclic Style-Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Using synthetic data for training neural networks that achieve good performance on real-world data is an important task as it can reduce the need for costly data annotation. Yet, synthetic and real world data have a domain gap. Reducing this gap, also known as domain adaptation, has been widely studied in recent years. Closing the domain gap between the source (synthetic) and target (real) data by directly performing the adaptation between the two is challenging. In this work, we propose a novel two-stage framework for improving domain adaptation techniques on image data. In the first stage, we progressively train a multi-scale neural network to perform image translation from the source domain to the target domain. We denote the new transformed data as "Source in Target" (SiT). Then, we insert the generated SiT data as the input to any standard UDA approach. This new data has a reduced domain gap from the desired target domain, which facilitates the applied UDA approach to close the gap further. We emphasize the effectiveness of our method via a comparison to other leading UDA and image-to-image translation techniques when used as SiT generators. Moreover, we demonstrate the improvement of our framework with three state-of-the-art UDA methods for semantic segmentation, HRDA, DAFormer and ProDA, on two UDA tasks, GTA5 to Cityscapes and Synthia to Cityscapes. Code and translated datasets available at https://github.com/shahaf1313/ProCST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many deep learning approaches have been proposed in the past years for solving the semantic segmentation task, where the goal is to output a segmentation map: a per-pixel label for a given input image. However, as in many cases, a large amount of data is required to train the segmentation networks for achieving a good generalization. In contrast to classification, where a single label is required per each input image, in semantic segmentation each pixel should be labeled, which makes the annotation very time-consuming and prone to inaccuracies due to human error.</p><p>Unsupervised Domain Adaptation (UDA) aims at mitigating this lack of annotated training data. It uses the vast amount of annotated data in a given source domain to learn to perform segmentation in a given target domain that lacks annotations. A popular source dataset is synthetic simulation data that have the per-pixel annotations 'for free'. The target is real data with the same classes as in the simulation. <ref type="figure">Figure 1</ref>: Domain Gap Reduction using SiT images. Images from the source domain are translated onto a SiT domain, using ProCST. The translated image preserves the content of the source image, but changes the appearance and style to the ones of the target domain for reducing the domain gap. This helps existing UDA methods to learn improved semantic segmentation models for the target domain. The bottom chart shows the achieved improvement in semantic segmentation (in terms of mIoU) when we use SiT data for training state-of-the-art UDA methods.</p><p>The naive approach of training a segmentation network only using annotated source data fails to achieve good results because of the major gap between real-world and synthetic simulation images. Moreover, training a segmentation network on source images that were translated to the target domain using an image-to-image technique fails to achieve good results on the target domain <ref type="bibr" target="#b8">(Hoffman et al. 2018)</ref>.</p><p>Many UDA techniques have been proposed to achieve good performance in the segmentation of the target domain. In these approaches, the training of the segmentation neural network incorporates real images, which do not have <ref type="figure">Figure 2</ref>: ProCST model. Networks are colored by their input domain: red for source domain and blue for target domain. On the right, the bi-directional multiscale image pyramid structure that perform S ? ? T and T ? ? S image translation, respectively. On the left, a flowchart of our used losses for all the pyramid levels. The highest level n = N has an additional label loss.</p><p>ground-truth labels, together with the synthetic images and their annotations. For example, some suggested training a neural network using the annotated samples from the source domain, and then either adapt the network to the target domain  or, alternatively, transfer the samples of the source domain to the target while preserving the structure of the source images, such that the segmentation maps of the source can be used for the transferred images as labels <ref type="bibr" target="#b33">(Wang, Yang, and Betke 2021;</ref><ref type="bibr" target="#b19">Li, Yuan, and Vasconcelos 2019;</ref><ref type="bibr" target="#b39">Yang and Soatto 2020)</ref>.</p><p>These and most of the current methods try to close the entire domain gap at once. In this work, we offer a prior step to these methods, whose goal is to reduce the gap as a first step. This idea is depicted in <ref type="figure">Figure 1</ref>. We suggest a Progressive Cyclic Style-Transfer (ProCST) network, which has a multiscale architecture that uses progressive training for performing style transfer. This model uses the annotations of the source domain to preserve the semantic information in the transfer. <ref type="figure">Figure 2</ref> presents the ProCST architecture.</p><p>ProCST narrows the domain gap between the source and target domains, by translating the source images to mimic images from the target domain, which we refer to as sourcein-target (SiT). The SiT dataset is closer to the target domain and preserves the images content. It can be used as input to any UDA method and improve it as we show hereafter.</p><p>We also show that simply taking an off-the-shelf approach to generate such a SiT dataset fails to improve UDA, thus highlighting the importance of our proposed model and loss functions for generating SiT. The experiments we conduct indicate that existing UDA and image-to-image translation methods fail to preform well as SiT data generators. Those methods tend to either distort or delete objects from the source image, and thus the translated images do not contain content equal to the original image, as can be seen for example in <ref type="figure" target="#fig_1">Figure 4</ref>. Thus, the original source annotations are no longer accurate and are only partially valid. Nevertheless, ProCST preserves the content of the source image due to the unique combination of losses that we suggest for the training process, with our novel cyclic label loss among them. Hence, we can use the SiT data instead of the source domain data as input to state-of-the-art UDA methods for semantic segmentation. We show that the new SiT data improves their performance.</p><p>We evaluate ProCST on UDA for semantic segmentation, where the target domain in both cases is Cityscapes <ref type="bibr" target="#b1">(Cordts et al. 2016)</ref>, and the source domains are either GTA5 <ref type="bibr" target="#b27">(Richter et al. 2016)</ref> or Synthia <ref type="bibr" target="#b28">(Ros et al. 2016)</ref>. We demonstrate the effectiveness of our scheme using three stateof-the-art methods, namely, ProDA , DAFormer  and HRDA <ref type="bibr" target="#b10">(Hoyer, Dai, and Van Gool 2022)</ref>. We measure the Mean Intersection Over Union (mIoU) over the Cityscapes test data, and improve the performance of all these approaches. For example, we improve over HDRA, which is the current state-of-the-art, by 1.1% mIoU on GTA5 dataset. This demonstrates the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation has been examined widely using deep learning methods. <ref type="bibr" target="#b21">Long, Shelhamer, and Darrell (2015)</ref>; <ref type="bibr" target="#b41">Yu and Koltun (2015)</ref>; <ref type="bibr" target="#b0">Chen et al. (2017)</ref>; <ref type="bibr" target="#b45">Zhao et al. (2017)</ref> propose various architectures and train them on pixel-wise manually annotated datasets. However, the annotation procedure is time-consuming. Training using small datasets usually leads to poor segmentation quality. One way to overcome availability of annotations is to use synthetic data, where annotations are available generously. Yet, due to the domain gap between the synthetic and the real data, the resulting model does not generalize well on the latter.</p><p>Unsupervised Domain Adaptation methods <ref type="bibr" target="#b8">(Hoffman et al. 2018;</ref><ref type="bibr" target="#b34">Wang and Deng 2018;</ref><ref type="bibr" target="#b4">Gao et al. 2021;</ref><ref type="bibr" target="#b44">Zhang et al. 2019;</ref><ref type="bibr" target="#b37">Wilson and Cook 2020)</ref>, seek to align domains' distributions. <ref type="bibr" target="#b42">Zellinger et al. (2017)</ref> <ref type="bibr">;</ref><ref type="bibr" target="#b6">Geng, Tao, and Xu (2011);</ref><ref type="bibr" target="#b22">Mancini et al. (2018)</ref> adapt the source domain to the target domain by minimizing discrepancy measures and matching the statistical moments of the two domains. Yet, these methods struggle to align the domains appropriately, which leads to poor generalization over the target domain.</p><p>Another popular strategy in domain adaptation is Adversarial Learning. <ref type="bibr" target="#b3">Ganin and Lempitsky (2015)</ref>; <ref type="bibr" target="#b31">Tzeng et al. (2017)</ref>; <ref type="bibr" target="#b30">Shu et al. (2018)</ref>; <ref type="bibr" target="#b18">Kumar et al. (2018)</ref> use a discriminator to distinguish between target domain images and translated source domain images, where the translator (generator) tries to fool the discriminator. As a result, the generator learns to map the source images to the target domain. <ref type="bibr" target="#b26">Richter, AlHaija, and Koltun (2021)</ref> utilize buffers from the simulator used to create the synthetic images for guiding the translation network to generate better looking images. This technique outputs good image quality but it fails to preserve image content, and thus preforms poorly when incorporated in a semantic segmentation framework as shown in Sec. 5.</p><p>For domain adaptation, a successful training strategy is the addition of cyclic loss to the total objective. <ref type="bibr" target="#b8">Hoffman et al. (2018)</ref>; <ref type="bibr" target="#b46">Zhu et al. (2017)</ref>; <ref type="bibr" target="#b40">Yi et al. (2017)</ref> translated source images to the target domain and then translated back to the source domain and required consistency. Note that when used for UDA, the cyclic loss is applied with other loss functions as translating the data alone without using the segmentation map information results in poor performance.</p><p>One major step that enabled high-resolution image generation was progressive training <ref type="bibr" target="#b14">Karras et al. (2017)</ref>. The proposed training procedure first trains the network to generate low-resolution images, then adds layers and increases the generated images size sequentially. This concept enabled the generation of high resolution images with superior quality and was employed in many works <ref type="bibr" target="#b14">(Karras et al. 2017;</ref><ref type="bibr" target="#b17">Karras, Laine, and Aila 2019;</ref><ref type="bibr" target="#b15">Karras et al. 2020</ref><ref type="bibr" target="#b16">Karras et al. , 2021</ref><ref type="bibr" target="#b29">Shaham, Dekel, and Michaeli 2019)</ref>. <ref type="bibr" target="#b43">Zhang et al. (2021)</ref>; Hoyer, Dai, and Van Gool (2021) decrease the domain gap using self-supervised learning (SSL). For instance, <ref type="bibr" target="#b43">Zhang et al. (2021)</ref> use pseudo label denoising and target structure learning in a three stage training process. <ref type="bibr" target="#b9">Hoyer, Dai, and Van Gool (2021)</ref> base their network architecture on two vision transformers <ref type="bibr" target="#b20">(Liang et al. 2021;</ref><ref type="bibr" target="#b2">Dosovitskiy et al. 2021)</ref>, in a student-teacher training framework fed with domain-mixed augmented images. <ref type="bibr" target="#b10">Hoyer, Dai, and Van Gool (2022)</ref> extend this concept further, and combine multi-resolution training that fuses low-resolution context with high resolution crop using an attention mechanism.</p><p>In this work, we propose an algorithm for generating "Source in Target" (SiT) images, namely, a domain shifted version of the original source images, which narrows the source-target domain gap. We then show that the current leading UDA methods perform better when trained using our SiT images instead of the original source images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We present ProCST, an image-to-image translation framework that is used as a zero-step before applying existing UDA methods for improving their performance. ProCST is a multiscale network trained with a cyclic style transfer loss using a progressive training scheme. ProCST also uses two semantic segmentation networks that utilize source domain labels in the training procedure.</p><p>Our multiscale training is based on an image pyramid structure: a list of generators that creates a full scale translated image, where each generator is responsible for translating the image at a certain scale. The generator has two inputs: the first is a translated image from the generator of the previous scale, upscaled to the current scale size. The second is a resized version of the original source image that matches the current generator scale. Note that the generator of the lowest scale receives only the second input. The output of each generator is a translated image in the scale it is using. This process is depicted in <ref type="figure">Figure 2</ref>.</p><p>Each scale consists of two generators: G st for translating images from the source domain to the target domain, and G ts for the inverse translation. Each of the generators has a separate discriminator, D st and D ts , respectively. Both the generators and the discriminators are CNNs with depth that increases with the resolution. A similar approach has been used to train a GAN using a single image for the task of image generation that receives noise and outputs an image <ref type="bibr" target="#b29">(Shaham, Dekel, and Michaeli 2019)</ref>, which is very different from the translation task of our method that transfers an image from one domain to another.</p><p>The training process is progressive: we train scales sequentially beginning from the lowest. This training approach has proven to create realistic and clean images, as shown for the Progressive GAN <ref type="bibr" target="#b14">(Karras et al. 2017)</ref>, which is used for image generation (and not translation as we do here).</p><p>The losses are a unique combination of the adversarialcyclic-style criteria, which allow changing the visual style of the image but also preserving the original content. In addition, when training the full resolution image generators, we incorporate a label loss term and present the novel cyclic label loss. These terms are described in Section 4. We start with basic definitions that are used to describe our model.</p><p>Model definitions Let S be the source domain and S the source dataset with images s (l) and annotations Label(s (l) ),</p><formula xml:id="formula_0">S ? R H S ?W S ?3 ? R H S ?W S ?K S = {(s (l) , Label(s (l) )) ? S} N S l=0 ,<label>(1)</label></formula><p>where H S , W S are the height and width of source domain images, K is the number of class categories and N S is the number of data points in the source dataset. Also, let T be the target domain, and T the target dataset with images t (l) ,</p><formula xml:id="formula_1">T ? R H T ?W T ?3 T = {t (l) ? T} N T l=0 ,<label>(2)</label></formula><p>where H T , W T are the height and width of target domain images, and N T is the number of data points in the target dataset. Notice that the target domain annotations are not available, and we assume that source and target datasets share the same class categories, c = 1, .., K. Denote N the number of scales in the multiscale model of size N , and denote by 0 &lt; r &lt; 1 the scale factor between two adjacent scales. Then, at scale n = 1, .., N , the dimensions are (H n , W n ) = (r N ?n H N , r N ?n W N ), where H n and W n are the height and width of the images at scale n. Model architecture ProCST is a multiscale model. Scales are trained sequentially in a progressive training process. Each scale has two generators, G st n and G ts n and two discriminators, D st n and D ts n , where n denotes the scale level. We turn to describe the S ? T translation process, where the complementary translation T ? S is symmetric. At the nth scale, n = 1, .., N , ProCST generates SiT fake images using the generator chain G st k , k ? n. Each generator receives the upscaled output of the previous generator and the current source image resized to the current scale's size. This process is repeated till we reach the last scale. More formally, let s ? S be a source image, and t ? T be a target image. Let s n , t n be the resized source and target images to scale n, respectively. A fake SiT imaget n is generated using n generators G st k , k ? n by the following procedure:</p><formula xml:id="formula_2">t n = G st n [s n ,t n?1 (r ?)] n &gt; 1 t 1 = G st 1 [s 1 ] n = 1,<label>(3)</label></formula><p>where (r ?) means the image is upscaled by a factor of r ?1 . For instance, on the right-hand-side of <ref type="figure">Figure 2</ref> we can see that the second scale receives an upscaled output image from the first generator together with the resized source image. Both images share the same size: the size of the second scale. We use the above translation process to generate all the required images to obtain the unique combination of loss terms, which varies as a function of n. This loss combination, described deeply in Section 4, is the heart of ProCST.</p><p>Boosting generic UDA methods We turn to describe how ProCST may be used to improve existing UDA methods. These methods receive as input a source dataset and a target dataset, and output segmentation maps of the target dataset:</p><formula xml:id="formula_3">UDA[S, T ] ? ? Label(T ).<label>(4)</label></formula><p>These methods encounter a large gap between the source and target domains. Training a ProCST model results in a chain of domain adapters, {G st n , G ts n } N n=1 , that reduces this gap. Let ProCST S? ?T be a trained ProCST model that translates images from a source domain S to the target domain T. We can build a new SiT dataset using this model, with the same annotation maps as the original source dataset:</p><formula xml:id="formula_4">SiT = {(ProCST S? ?T {s}, Label(s)), ?s ? S}, = {(t (l) , Label(s (l) ))} N S l=1 .<label>(5)</label></formula><p>The SiT is an in-between dataset, i.e., its domain gap from the desired target dataset is smaller than the original source dataset -as depicted in <ref type="figure">Figure 1</ref>. One may now train any arbitrary UDA method with the domain adapted SiT data:</p><formula xml:id="formula_5">UDA[SiT, T ] ? ? Label(T ).<label>(6)</label></formula><p>In this way, the same UDA method encounters a smaller domain gap and thus achieves increased accuracy. This is a generic approach that boosts performance of UDA methods by narrowing the domain gap for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ProCST Loss Combination for UDA</head><p>For transforming the source data into SiT data in our framework, we propose the following novel combination of loss functions that adapt the style of the target while keeping the content of the source. This combination is specifically tailored for the task of UDA to make sure that the segmentation performance are improved when training UDA methods using the translated SiT data. We start by describing the cyclic style transfer loss. Without loss of generality, we will focus on the S ? ? T translation. The inverse direction is symmetric for this loss bundle. Recall that s n ,t n , and t n have already been defined earlier in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial loss</head><p>We train a full CNN discriminator for each scale, D st n . We use the Wasserstein GAN loss with gradient penalty <ref type="bibr" target="#b7">(Gulrajani et al. 2017)</ref>. D st n receives (t n , t n ) as fake and real images, respectively. The loss is given by</p><formula xml:id="formula_6">L st adv = L st adv (t n , t n ).<label>(7)</label></formula><p>Style loss Our method focuses on generating an image taken from a source domain to have the content of the original image but the appearance and style of the target domain. To fulfill this task we incorporate the style transfer loss, shown in <ref type="bibr" target="#b5">Gatys, Ecker, and Bethge (2016)</ref> and <ref type="bibr" target="#b13">Johnson, Alahi, and Fei-Fei (2016)</ref>. For this loss, they use features of a pretrained VGG-19 network. In <ref type="bibr" target="#b5">Gatys, Ecker, and Bethge (2016)</ref>, they have shown that some features control the texture and style and some control the visual content. The offered loss that calculates two matrices: Content features -2 norm between the content image's features and the fake image's features. In our work, s n is the content image andt n is the fake image.</p><p>Style features -2 norm between a Gram matrix (correlation matrix) of the style image's features and a Gram matrix of the fake image's features. In our work, t n is the style image andt n is the fake image. Following <ref type="bibr" target="#b13">Johnson, Alahi, and Fei-Fei (2016)</ref>, a total variation regularization is also added to the loss. Thus, a Style Loss term is given by:</p><formula xml:id="formula_7">L st style = ? content L st content (s n ,t n ) (8) + ? style,Gram L st style,Gram (t n ,t n ) + ? T V L st T V (t n )</formula><p>. Cyclic loss Cyclic training <ref type="bibr" target="#b46">(Zhu et al. 2017</ref>) is a well known strategy in domain adaptation <ref type="bibr" target="#b8">(Hoffman et al. 2018)</ref>. The main property of the cyclic training regime is the ability to trace back any translated image to the original domain. This is done by calculating the 1 loss between the original source image and the same source image that is translated twice (using G st n followed by G ts n ), namely s n ?s n 1 , wheres n denotes the source image that is translated twice.</p><p>The cyclic transfer procedure in ProCST is not trivial. First, we generatet n as described earlier. Then, we use the generators chain G ts k , k ? n with a new input imaget n , in order to output the result of the cycles n . Namely, we us? s n = G ts n [t n ,s n?1 (r ?)], L ss cyclic = s n ?s n 1 .</p><p>Cyclic style transfer (CST) loss The above loss terms are unified into one CST loss term, which is evaluated only at the current scale n in a progressive manner. The loss is</p><formula xml:id="formula_9">L CST = ? adv. L adv. + ? style L style + ? cyclic L cyclic = ? adv. (L st adv. + L ts adv. ) + ? style (L st style + L ts style ) + ? cyclic (L ss cyclic + L tt cyclic ).<label>(10)</label></formula><p>Label loss High resolution images contain a vast amount of knowledge and details. Thus, generating good translation requires fine tuning and as much information as possible. Adding a segmentation loss to image generation networks has proven to be effective <ref type="bibr" target="#b26">(Richter, AlHaija, and Koltun 2021)</ref>. Therefore, we use the segmentation maps of the source dataset in order to better adapt to the new domain when more details become present. Specifically, at the last scale of ProCST, we also use a label loss in addition to CST loss. This is the only loss in our model that is not symmetric, i.e., it is not used in the T ? S direction. The label loss has two different parts. The first consists of a segmentation network that is trained to segment SiT images, Segmentor SiT . The input to this network is a SiT image, and it is trained to output the segmentation map of the original source image. The second part is a novel cyclic label loss. We pretrained a segmentation network on the source dataset, as source annotations are available. Denote this pretrained network as Segmentor S . We take the source image after it has been transferred using the cyclic operation from source to target and then back to source, and enter it to the pretrained Segmentor S together with the original source segmentation map. This process results in a segmentation loss transfer between G st and G ts , which further encourages good segmentation features in both generators.</p><p>For both Segmentor SiT and Segmentor S , we use ResNet101 (He et al. 2016) as the segmentation network, which outputs a probability map p i,j,k per class k. Each Segmentor network is trained with the Cross Entropy (CE) loss that is calculated between the segmentor's output p i,j,k ? R H?W ?K and the matching Ground Truth (GT) segmentation map m i,j,k ? R H?W ?K . To train the segmentation networks we use the following loss</p><formula xml:id="formula_10">L CE [p, m] = ? H i=1 W j=1 K k=1 m i,j,k log p i,j,k .<label>(11)</label></formula><p>SiT label loss. We train Segmentor SiT together with ProCST. It receives as input the translated SiT images and is trained to output the GT segmentation maps of the corresponding source images. This network is used for training ProCST. Let Label(s N ) be the GT segmentation map of scale N , then the SiT label loss is</p><formula xml:id="formula_11">L CE,SiT = L CE [Segmentor SiT (t N ), Label(s N )]. (12)</formula><p>This loss updates both the weights of Segmentor SiT and ProCST. Specifically, it flows to the G st N generator and helps it distinguish both the class of the current pixel and the spatial distribution of each class in the source domain. These properties help the translation process and also tend to improve segmentation properties of the translated SiT image.</p><p>Cyclic label loss. In addition to the above network, we present our novel cyclic label loss. Feeding both generators with segmentation loss will transfer the segmentation loss between domains. This property is very important to the UDA task, as the goal is good segmentation in the target domain. We denote the output of the cycle of source?target?source bys, which is a source domain image. As annotations from the source domain are available, we pretrained another ResNet101 on the source dataset, and used this pretrained Segmentor S as part of a loss for the cyclic S ? ? T ? ? S loop. Specifically, our loss requires the segmentation map that is trained on the real source data to succeed also ons, i.e., the loss is given by</p><formula xml:id="formula_12">L CE,ss = L CE [Segmentor S (s N ), Label(s N )]. (13)</formula><p>This loss impacts both generators and acts as a segmentation information transporter between source and target domains. To conclude, the label loss is given by</p><formula xml:id="formula_13">L Labels = ? labels (L CE,sit + L CE,ss ),<label>(14)</label></formula><p>and thus the complete loss of scale 1 ? n ? N given by</p><formula xml:id="formula_14">L 1?n&lt;N = L CST , L n=N = L CST + L Labels .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We turn to present the results of our proposed framework. We first show the results of integrating three robust UDA methods within our framework. We also provide a qualitative comparison between the images in the SiT data and the original source data and a quantitative comparison between the results achieved using ProCST SiT data and to Cityscapes using EPE. Right: translated image from GTA5 to Cityscapes using ProCST. Note that the tree in the red box disappears after EPE's translation, but the translation of ProCST preservers the tree although it is surrounded entirely by sky.</p><p>those achieved using data generated by other state-of-theart image-to-image translation methods. Finally, we present an ablation study that demonstrates the importance of the different parts in the ProCST model. In the appendix, we present the implementation details of the ProCST model, qualitative comparison between ProCST's translation and other image-to-image translation methods, and additional outputs of ProCST.</p><p>Results We trained our model on two source datasets, GTA5 and Synthia, that were then translated to obtain the new SiT data (created per dataset). The data is used to train three state-of-the-art UDA methods, ProDA , DAFormer (Hoyer, Dai, and Van Gool 2021) and HRDA <ref type="bibr" target="#b10">(Hoyer, Dai, and Van Gool 2022)</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the results compared to other methods and also to HRDA, DAFormer and ProDA when trained with the regular source data. Notice that training ProDA using our SiT data resulted in an improvement of 1.1% mIoU over the original GTA5 dataset, and improvement of 0.6% mIoU over the original Synthia dataset. Training DAFormer using our SiT dataset achieved improvement of 1.1% mIoU over the original GTA5 dataset, and improvement of 0.9% mIoU over the original Synthia dataset. Training HRDA using our SiT dataset achieved improvement of 1.1% mIoU over the original GTA5 dataset, and improvement of 0.8% mIoU over the original Synthia dataset. Note that the results of DAFormer and HRDA refer to our runs, on 8 seeds. The SiT data generated by ProCST improves the accuracy of three state-of-theart UDA methods on both GTA5 and Synthia datasets. <ref type="figure" target="#fig_0">Figure 3</ref> shows the similarity between images from the SiT data, generated by ProCST, to images from the target data. The GTA5 data tries to simulate the view in the yellow and sunny Los Angeles, while the Cityscapes data was shot in the roads of the green and cloudy Germany. We can see how the ProCST model catches these differences and translates the yellow-red images of GTA5 onto images with blue-green atmosphere, while the translation in the opposite direction gives the image a red-yellow atmosphere.</p><p>In addition, we can see that ProCST changes appearance and textures. The colors and textures of the road turn from rough yellow-grey in GTA5 to smooth gray, as in Cityscapes. The sky turns from clear blue to foggy and grey. The color of the vegetation turns from green-yellow into deep green. These changes appear also in the opposite trans- lation. While the style and appearance of the images change, the visual content and details do not. This is an important property for UDA tasks, as annotations are available only on the original source dataset. Furthermore, we can see similar trends also in the Synthia to Cityscapes translation.</p><p>Comparison of ProCST to other image-to-image translation methods Other state of the art image-to-image translation methods lack content preservation property and thus do not achieve as good results on UDA tasks. For instance, we can see the difference between image translation of the state-of-the-art translation approach Enhancing Photorealism Enhancement, EPE <ref type="bibr" target="#b26">(Richter, AlHaija, and Koltun 2021)</ref>, and the image translation of ProCST in <ref type="figure" target="#fig_1">Figure 4</ref>. While EPE's method has very good visual quality, it sometimes deletes some of the content of the original image, such as trees surrounded by sky background. The combination of cyclic loss together with style loss helps ProCST to change the style but yet preserve the content, which is a crucial property to the success in a generic UDA task. More examples can be found in the appendix. Although EPE's translation performs well on image visual quality, it does not achieve good accuracy when used as a SiT data generator for UDA, as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Additionally, we have compared ProCST to other imageto-image translation methods when used as SiT data generators: BDL <ref type="bibr" target="#b19">(Li, Yuan, and Vasconcelos 2019)</ref>, CUT <ref type="bibr" target="#b23">(Park et al. 2020)</ref>, CycleGAN <ref type="bibr" target="#b46">(Zhu et al. 2017)</ref>, MUINT <ref type="bibr" target="#b11">(Huang et al. 2018</ref>) and FDA <ref type="bibr" target="#b39">(Yang and Soatto 2020)</ref>. We chose a large variety of translation methods for robust comparison. Qualitative comparison of the translations outputs appears in the appendix. As <ref type="table" target="#tab_0">Table 1</ref> clearly shows, a mere preprocessing of the data with other methods does not improve significantly UDA accuracy in the semantic segmentation task. The architecture and losses of ProCST (such as the cyclic label loss, which is novel in UDA context) that we proposed are key for allowing the improvement achieved.</p><p>Ablation study We trained a ProCST model using 5 configurations to find out what parts of the proposed architecture contribute most to the boost in performance. The examined parameters are the multiscale pyramid length (controlled by N ), label loss term (controlled by ? labels ), style loss term (controlled by ? style ) and cyclic loss term (controlled by ? cyclic ). Each ablation discards one parameter, namely, set either number of levels N to 1 or the corresponding ? to 0. We perform the ablation using the DAFormer on GTA5 ? Cityscapes with the SiT data as input and a minor hyper parameter tuning by changing DAFormer's EMA update parameter to ? = 0.997 from the original value of ? = 0.999. The reason for this fine tuning lays on the differences between the source and the SiT datasets. For a complete and fair comparison, we also show the results of DAFormer using the original GTA5 dataset as source with both values of ?. <ref type="table" target="#tab_2">Table 3</ref> shows that if we omit either the style or cyclic loss terms, ProCST fails to converge. Moreover, omitting either the multiscale structure or label loss results in performance degradation compared to the full ProCST model. Note that we use just one (same) seed in the ablation (thus, numbers slightly differ from <ref type="table" target="#tab_1">Table 2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have shown a new concept that achieves improved performance of generic UDA methods. Our imageto-image translation method narrows the domain gap as a prior step to UDA training. The unique loss combination is tailored to preserve image content yet change its style. We have shown that this improves the ProDA, DAFormer and HRDA state-of-the-art UDA methods. To our knowledge, our cyclic label loss was not used before. From the examination we performed in this work, one can observe that this term regularizes the training and allows the model to achieve superior translation quality. We believe that this concept can be extended to other types of problems, where annotating images in a target domain of interest is very costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In the following sections, we provide additional qualitative results and explanations that elaborate more deeply the unique contribution of ProCST as a booster to generic UDA tasks. First, we provide implementation details regarding ProCST architecture and training process. Then, we present a detailed comparison between ProCST SiT data and EPE <ref type="bibr" target="#b26">(Richter, AlHaija, and Koltun 2021</ref>) generated data. Additionally, we provide wide qualitative comparison between the translation of ProCST and this of other image-to-image translation methods. Those comparisons illustrate the content preservation property of ProCST, which is crucial for narrowing the domain gap. Moreover, we discuss the qualitative differences between images generated using all the models presented in the ablation study presented in Section 5. Finally, we present more examples for SiT and TiS translated images, generated using both GTA5 and Synthia as the source datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Datasets. Source datasets: The GTA5 dataset <ref type="bibr" target="#b27">(Richter et al. 2016</ref>) contains 24,966 images of size <ref type="bibr">(1052,</ref><ref type="bibr">1914)</ref> and their pixel-wise annotations. The Synthia dataset contains 9400 images of size <ref type="bibr">(760,</ref><ref type="bibr">1280)</ref> and their pixel-wise annotations. Target dataset: The Cityscapes dataset <ref type="bibr" target="#b1">(Cordts et al. 2016)</ref>, with the extra available train images: total of 22,973 images of size <ref type="bibr">(1024,</ref><ref type="bibr">2048)</ref>.</p><p>Prepossessing of the source datasets includes two steps: first, we resize the input images to a size that preserves aspect ratio of the original source image. This size varies between domains (different domains have different image size and aspect ratio), but all domain resized images are larger than <ref type="bibr">(512,</ref><ref type="bibr">1024)</ref>. Then, we take a random crop of size (512, 1024) from the resized images. The target dataset has an aspect ratio of 1:2. Therefore, its prepossessing includes only resizing to (512, 1024), without any cropping. Thus, (H N , W N ) = (512, 1024).</p><p>Network Architecture. We use N = 3 scales, with a scale factor r = 0.5. Generators and discriminators are fully convolutional, with width of 64 channels for all n and depth of 5 layers at scale n = 1, and 7 layers at scales n = 2, 3. For all generators and discriminators, we used a normalization according to the current scale's batch size. Following <ref type="bibr" target="#b38">Wu and He (2018)</ref>, we normalized using Batch Normalization only if the number of images per GPU in the current scale exceeds 16. Otherwise, we used Group Normalization with groups number G = 8. We used DeepLabV2 <ref type="bibr" target="#b0">(Chen et al. 2017)</ref> for both segmentation networks.</p><p>Training. We used the Adam optimizer for all generators and discriminators networks with parameters lr g , lr d = 0.0001. The optimizer for the segmentation networks is SGD with learning rate of lr semseg = 0.0001. The weights of the losses were chosen to prefer style transfer, but if we would have discarded one of the other losses it would have caused either a dramatic performance degradation or a lack of model convergence, as can be seen in Section 5. We adopt the values of ? content , ? style,Gram and ? T V from <ref type="bibr" target="#b13">Johnson, Alahi, and Fei-Fei (2016)</ref>. To control the relative size of this loss compared to the other losses in ProCST, we normalized the maximum weight to 1 and kept the ratio between all other weights. The original values are ? content = 1, ? style,Gram = 30, ? T V = 1. For L CST we set ? adv. = 1, ? style = 10, ? cyclic = 1, and for the label loss we set ? labels = 3.</p><p>SiT Data Creation. After training ProCST as described above, we generate the SiT data, where the input of the ProCST model are images from GTA5 or Synthia, and the output image are in Cityscapes style, as figures 13 and 14 show. Note that we use full resolution source images without any preprocessing, thus ProCST's output has the exact shape of the original source images and annotations. ProCST is fully convolutional and thus can generate SiT data with varying input shape. We train ProCST with maximum resolution of 1024x512, and use this pretrained ProCST model to generate images with varying resolution according to the full resolution of the source images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of ProCST and EPE</head><p>We turn to present more comparisons between ProCST and EPE. <ref type="figure" target="#fig_2">Figure 5</ref> presents more examples for objects that EPE's translation deletes or distorts, while ProCST's translation preserves. <ref type="figure" target="#fig_2">Figure 5</ref> shows that trees in the sky are sometimes distorted due to the EPE translation. Note that these examples imply a more general trend. The EPE image translation is focused on generating good visual image quality, but not on preserving the content of the image. Thus, it sometimes distorts objects that do not visually fit their close spatial space. For example, the long palm trees are not consistent with their spatial environment (i.e. sky) and thus distorted.</p><p>We can see such a trend also in other cases. For example, in <ref type="figure" target="#fig_3">Figure 6</ref>, we can see a crop from the original source image that contains a sidewalk surrounded by vegetation. While the EPE translation distorts the sidewalk because of the vegetation from both of its sides, the ProCST model preserves the content and results in a cleaner image that has matching content to that of the original source image.</p><p>Content preservation is a crucial property in image translation when used for domain adaptation. EPE fails to achieve this property, and thus achieves poorer performance when employed as a booster to the UDA semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of ProCST and Additional Image-To-Image Translation Methods</head><p>We provide qualitative comparison between the translation of ProCST and this of 6 other image-to-image translation methods: BDL <ref type="bibr" target="#b19">(Li, Yuan, and Vasconcelos 2019)</ref>, CUT <ref type="bibr" target="#b23">(Park et al. 2020)</ref>, CycleGAN <ref type="bibr" target="#b46">(Zhu et al. 2017)</ref>, MUINT <ref type="bibr" target="#b11">(Huang et al. 2018)</ref>, color transfer <ref type="bibr" target="#b25">(Reinhard et al. 2001</ref>) and FDA <ref type="bibr" target="#b39">(Yang and Soatto 2020)</ref>. We can see from <ref type="figure" target="#fig_4">Figure 7</ref> that the first four methods hallucinate objects such as trees in the sky and Mercedes stars either on the road or on the mountains. Additionally, we can see that the translation of those methods distorts objects and creates blurry image. The final two methods do not hallucinate objects, but they fail to create a meaningful translation in terms of textures and colors. We can clearly see that the translation of ProCST outputs a clean and sharp image, that both preserve the semantic content of the source image and mimics the textures and colors of the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study -Qualitative Results</head><p>We turn to discuss the differences and similarities between GTA5 ? Cityscapes translated images generated from three different ProCST ablation models: full ProCST model, ProCST model trained with no label loss (i.e. ? labels = 0), and ProCST model trained with only one scale (i.e. N = 1). The quantitative results of all three models appear in Section 5. <ref type="figure" target="#fig_5">Figures 8, 9</ref>, 10 and 11 present the original GTA5 image and all other SiT images generated using the compared models. Clearly, all three models create cloudy and green atmosphere as can be seen from <ref type="figure" target="#fig_5">Figure 8</ref>. This property matches Cityscapes domain.</p><p>Yet, in figures 9, 10 and 11 we can see visually that the full ProCST model adapts to the Cityscapes domain better than all other model. Visual quality, textures and general appearance are superior.</p><p>Images 10 and 11 show that the ablated models fail to generate good SiT images when dealing with complicated texture translations, like the smoothness of the road. Moreover, we can see that the quality of both ablated models is consistent with the quantitative results presented in the paper. While both variants are comparable in the results, we can see some minor differences between the ProCST model trained without the label loss term (i.e. ? labels = 0) and the one trained without multiscale structure (i.e. N = 1). Multiscale structure helps to translate complicated textures, as we can see for example from the road's texture. Road is smoother and with deeper colors in the model with ? labels = 0 compared to the one with N = 1. Despite this fact, the first model sometimes suffers from artifacts that the latter does not. We can see such a trend in <ref type="figure" target="#fig_9">Figure 12</ref>. In the first row, observe the smoothness of the road and the deep green color that has superior quality in ? labels = 0 than N = 1. Yet, in the second row, the sky of the N = 1 model is natural and clean while the sky of ? labels = 0 suffers from minor artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SiT and TiS Translation Examples</head><p>We provide additional examples of SiT and TiS images side by side with the original source images, for both source domains. <ref type="figure" target="#fig_0">Figure 13</ref> shows GTA5 ? ? Cityscapes translations and <ref type="figure" target="#fig_1">Figure 14</ref> shows Synthia ? ? Cityscapes translations.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Bi-directional translation of ProCST. Top row: GTA5 ? ? Cityscapes image translation; Bottom row: Synthia ? ? Cityscapes image translation. Both rows, left to right: source, SiT, target, and "Target in Source" translated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>ProCST preservers the source image content. Left: original source image. Middle: translated image from GTA5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Content Preservation. Left to right: Original source image; EPE translated image; ProCST translated image. ProCST's output results in a clean image while EPE (Richter, AlHaija, and Koltun 2021) distorts the palm trees due to the surrounding sky.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Content Preservation. Top row: original source image; Bottom row, left to right: Crop taken from the original image; Crop taken from EPE translated image; Crop taken from ProCST translated image. ProCST's output results in a clean image while EPE (Richter, AlHaija, and Koltun 2021) distorts the sidewalk due to the surrounding vegetation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Comparison to other image-to-image translation techniques. Left column, top down: source image, MUINT, CUT, color transfer. Right column, top down: ProCST, BDL, CycleGAN, FDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Ablation Study. Top Left: original image; Top Right: ProCST full model; Bottom Left: ProCST with ? labels = 0; Bottom Right: ProCST model N = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Ablation Study. Top Left: original image; Top Right: ProCST full model; Bottom Left: ProCST with ? labels = 0; Bottom Right: ProCST with N = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Ablation Study. Top Left: original image; Top Right: ProCST full model; Bottom Left: ProCST with ? labels = 0; Bottom Right: ProCST with N = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Ablation Study. Top Left: original image; Top Right: ProCST full model; Bottom Left: ProCST with ? labels = 0; Bottom Right: ProCST with N = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Ablation Study. Left: ProCST with ? labels = 0; Right: ProCST with N = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Bi-directional GTA5 ? ? Cityscapes image translation. Left to right: GTA5, SiT, Cityscapes, and TiS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Bi-directional Synthia ? ? Cityscapes image translation. Left to right: Synthia, SiT, Cityscapes, and TiS images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison to other SiT generation options. We used existing UDA and image-to-image translation methods to generate SiT data for the training of DAFormer. Double horizontal line indicates that results were tested on the exact subset of GTA5 dataset that was publicly available. Units of mIoU are %. We use the same seed in all experiments for a fair comparison. Notice how ProCST leads to a better improvement of DAFormer in all cases.</figDesc><table><row><cell>SiT mIoU</cell><cell>SiT mIoU</cell><cell>SiT mIoU</cell><cell>SiT mIoU</cell></row><row><cell>FDA 67.2</cell><cell cols="2">CUT 64.3 CyGAN 65.9</cell><cell>EPE 63.3</cell></row><row><cell cols="4">BDL 67.0 MUINT 64.4 Source 67.4 Source 66.5</cell></row><row><cell cols="4">Source 67.0 Source 66.9 ProCST 68.8 ProCST 69.1</cell></row><row><cell cols="2">ProCST 68.7 ProCST 68.5</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison to state-of-the-art UDA methods for semantic segmentation. Double vertical line separates the results of GTA5 ? Cityscapes (above the line), and the results of Synthia ? Cityscapes (below the line). *Average of 8 seeds, results using our runs.</figDesc><table><row><cell></cell><cell>Road</cell><cell>S.walk</cell><cell>Build.</cell><cell>Wall</cell><cell>Fence</cell><cell>Pole</cell><cell>Light</cell><cell>Sign</cell><cell>Vege.</cell><cell>Terrain</cell><cell>Sky</cell><cell>Person</cell><cell>Rider</cell><cell>Car</cell><cell>Truck</cell><cell>Bus</cell><cell>Train</cell><cell>M.bike</cell><cell>Bike</cell><cell>mIoU</cell></row><row><cell cols="21">CyCADA (2018) 86.7 35.6 80.1 19.8 17.5 38.0 39.9 41.5 82.7 27.9 73.6 64.9 19.0 65.0 12.0 28.6 4.5 31.1 42.0 42.7</cell></row><row><cell>CBST (2019)</cell><cell cols="20">91.8 53.5 80.5 32.7 21.0 34.0 28.9 20.4 83.9 34.2 80.9 53.1 24.0 82.7 30.3 35.9 16.0 25.9 42.8 45.9</cell></row><row><cell>FADA (2020)</cell><cell cols="20">91.0 50.6 86.0 43.4 29.8 36.8 43.4 25.0 86.8 38.3 87.4 64.0 38.0 85.2 31.6 46.1 6.5 25.4 37.1 50.1</cell></row><row><cell>DACS (2021)</cell><cell cols="20">89.9 39.7 87.9 30.7 39.5 38.5 46.4 52.8 88.0 44.0 88.8 67.2 35.8 84.5 45.7 50.2 0.0 27.3 34.0 52.1</cell></row><row><cell>CorDA (2021)</cell><cell cols="20">94.7 63.1 87.6 30.7 40.6 40.2 47.8 51.6 87.6 47.0 89.7 66.7 35.9 90.2 48.9 57.5 0.0 39.8 56.0 56.6</cell></row><row><cell>ProDA (2021)</cell><cell cols="20">87.8 56.0 79.7 46.3 44.8 45.6 53.5 53.5 88.6 45.2 82.1 70.7 39.2 88.8 45.5 59.4 1.0 48.9 56.4 57.5</cell></row><row><cell cols="21">ProDA+ProCST 90.0 61.2 81.2 40.8 45.9 49.0 57.8 59.8 88.9 46.9 80.1 72.8 34.6 89.5 46.3 58.7 0.0 51.0 59.2 58.6</cell></row><row><cell cols="21">DAFormer* (2021) 95.1 67.6 89.3 53.3 44.9 48.8 56.0 60.4 89.8 47.5 92.0 71.8 44.8 92.0 70.1 78.4 64.4 55.7 62.8 67.6</cell></row><row><cell cols="21">DAFormer*+ProCST 95.8 69.6 89.8 55.8 45.0 49.8 56.8 63.3 90.2 50.3 93.0 72.2 44.9 92.3 72.2 78.8 65.1 56.4 63.1 68.7</cell></row><row><cell cols="21">HRDA* (2022) 96.0 72.8 91.2 60.8 51.0 58.2 63.6 71.5 91.5 50.3 93.9 78.4 51.1 93.8 80.1 84.4 63.8 63.7 67.6 72.8</cell></row><row><cell cols="21">HRDA*+ProCST 96.7 75.8 91.3 60.3 52.8 58.8 66.0 72.1 91.5 50.4 93.8 78.6 52.1 94.0 81.4 85.7 71.8 63.8 67.6 73.9</cell></row><row><cell>CBST (2019)</cell><cell cols="20">68.0 29.9 76.3 10.8 1.4 33.9 22.8 29.5 77.6 -78.3 60.6 28.3 81.6 -23.5 -18.8 39.8 42.6</cell></row><row><cell>DACS (2021)</cell><cell cols="20">80.6 25.1 81.9 21.5 2.9 37.2 22.7 24.0 83.7 -90.8 67.6 38.3 82.9 -38.9 -28.5 47.6 48.3</cell></row><row><cell>CorDA (2021)</cell><cell cols="20">93.3 61.6 85.3 19.6 5.1 37.8 36.6 42.8 84.9 -90.4 69.7 41.8 85.6 -38.4 -32.6 53.9 55.0</cell></row><row><cell>ProDA (2021)</cell><cell cols="20">87.8 45.7 84.6 37.1 0.6 44 54.6 37.0 88.1 -84.4 74.2 24.3 88.2 -51.1 -40.5 45.6 55.5</cell></row><row><cell cols="21">ProDA+ProCST 87.8 48.2 85.8 22.9 0.7 47.1 56.4 47.1 88.0 -86.8 72.4 25.4 90.2 -58.0 -38.3 41.9 56.1</cell></row><row><cell cols="21">DAFormer* (2021) 84.4 41.8 87.9 40.4 6.9 48.8 54.0 53.7 86.0 -89.0 72.5 45.6 86.6 -58.3 -53.1 59.6 60.5</cell></row><row><cell cols="21">DAFormer*+ProCST 84.3 41.1 87.7 42.6 6.1 50.7 55.5 54.2 86.1 -87.9 74.7 47.2 87.6 -61.4 -53.3 62.5 61.4</cell></row><row><cell cols="21">HRDA* (2022) 84.5 43.9 88.7 50.0 7.0 57.3 64.8 60.5 86.1 -92.9 79.1 52.3 89.6 -65.3 -63.3 64.3 65.6</cell></row><row><cell cols="21">HRDA*+ProCST 86.7 50.7 88.7 50.0 6.4 58.0 66.6 62.2 86.4 -93.6 78.8 52.7 88.4 -64.6 -63.5 64.4 66.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>DAFormer trained on different ProCST generated SiT datasets. Training configuration of DAFormer is as its original configuration, excluding the EMA update parameter ?. We change it ? = 0.997 in rows marked with *.</figDesc><table><row><cell>Dataset</cell><cell>Label Loss</cell><cell>Multiscale Pyramid</cell><cell>Style Loss</cell><cell>Cyclic Loss</cell><cell cols="2">mIoU[%] Rel.</cell></row><row><cell>Source</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">66.4 +0.0</cell></row><row><cell>Source*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.3</cell><cell>-0.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">ProCST* ?</cell><cell></cell><cell></cell><cell></cell><cell cols="2">67.2 +0.8</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">67.2 +0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">68.2 +1.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This research was supported by ERC-StG SPADE grant no. 757497. We would like to thank Deborah Cohen for her helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dsp: Dual soft-paste for unsupervised domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2825" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DAML: Domain adaptation metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2980" to="2989" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advances in neural information processing systems, 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Deep residual learning for image recognition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cycada: Cycleconsistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14887</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13132</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal Unsupervised Image-to-image Translation</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-regularized alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wornell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6936" to="6945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Boosting domain adaptation by discovering latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3771" to="3780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04619</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Enhancing photorealism enhancement. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Playing for Data: Ground Truth from Computer Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>Leibe, B.</editor>
		<editor>Matas, J.</editor>
		<editor>Sebe, N.</editor>
		<editor>and Welling, M.</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Singan: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4570" to="4580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via cross-domain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1379" to="1389" />
		</imprint>
	</monogr>
	<note>6th International Conference on Learning Representations, ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classes matter: A fine-grained adversarial approach to cross-domain semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="642" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Consistency regularization with high-dimensional nonadversarial sourceguided perturbation for unsupervised domain adaptation in segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptive semantic segmentation with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8515" to="8525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8798" to="8807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Survey of Unsupervised Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2849" to="2857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Natschl?ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Category anchor-guided unsupervised domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
